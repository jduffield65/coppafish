{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"coppafish coppafish is a data analysis pipeline for decoding coppaFISH (combinatorial padlock-probe-amplified fluorescence in situ hybridization) datasets. coppaFISH is a method for in situ transcriptomics which produces a series of images arranged in terms of tiles, rounds and channels. coppafish then determines the distribution of genes via image processing , spot detection , registration and gene calling . The distribution of genes (like that shown above - each marker refers to a different gene) can then be used to determine the location and type of cells using pciSeq . Installation coppafish supports python 3.8 and above. It can be installed using pip: pip install coppafish To use the napari Viewer and matplotlib diagnostics , the plotting version must be installed: pip install coppafish [ plotting ] To use the optimised code, which is recommended for running the find spots and OMP sections of the pipeline (otherwise they are very slow ), the optimised version must be installed: pip install coppafish [ optimised ] Installing on Windows The optimised code requires jax which is not supported on Windows, thus the optimised version of coppafish cannot be used on Windows. The optimised and plotting features can both be installed by running: pip install coppafish [ optimised,plotting ]","title":"Home"},{"location":"#coppafish","text":"coppafish is a data analysis pipeline for decoding coppaFISH (combinatorial padlock-probe-amplified fluorescence in situ hybridization) datasets. coppaFISH is a method for in situ transcriptomics which produces a series of images arranged in terms of tiles, rounds and channels. coppafish then determines the distribution of genes via image processing , spot detection , registration and gene calling . The distribution of genes (like that shown above - each marker refers to a different gene) can then be used to determine the location and type of cells using pciSeq .","title":"coppafish"},{"location":"#installation","text":"coppafish supports python 3.8 and above. It can be installed using pip: pip install coppafish To use the napari Viewer and matplotlib diagnostics , the plotting version must be installed: pip install coppafish [ plotting ] To use the optimised code, which is recommended for running the find spots and OMP sections of the pipeline (otherwise they are very slow ), the optimised version must be installed: pip install coppafish [ optimised ] Installing on Windows The optimised code requires jax which is not supported on Windows, thus the optimised version of coppafish cannot be used on Windows. The optimised and plotting features can both be installed by running: pip install coppafish [ optimised,plotting ]","title":"Installation"},{"location":"config/","text":"Default Config Settings file_names The file_names section specifies the files that will be used throughout the pipeline. Variables in this section can be changed at any point in the pipeline, and the notebook created using it can still be loaded in. notebook_name : str . Name of notebook file in output directory will be notebook_name .npz Default: notebook input_dir : str . Directory where the raw .nd2 files or .npy stacks are Default: MUST BE SPECIFIED output_dir : str . Directory where notebook is saved Default: MUST BE SPECIFIED tile_dir : str . Directory where tile .npy files saved Default: MUST BE SPECIFIED round : maybe_list_str . Names of .nd2 files for the imaging rounds. Leave empty if only using anchor. Default: None anchor : maybe_str . Name of the file for the anchor round. Leave empty if not using anchor. Default: None raw_extension : str . .nd2 or .npy indicating the data type of the raw data. Default: .nd2 raw_metadata : maybe_str . If .npy raw_extension, this is the name of the .json file in input_dir which contains the metadata required extracted from the initial .nd2 files. I.e. it contains the output of coppafish/utils/nd2/save_metadata : xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. pixel_microns - float . xy pixel size in microns. pixel_microns_z - float . z pixel size in microns. sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Default: None dye_camera_laser : maybe_file . csv file giving the approximate raw intensity for each dye with each camera/laser combination. If not set, the file coppafish/setup/dye_camera_laser_raw_intensity.csv file will be used. Default: None code_book : str . Text file which contains the codes indicating which dye to expect on each round for each gene. Default: MUST BE SPECIFIED scale : str . Text file saved in tile_dir containing extract['scale'] and extract['scale_anchor'] values used to create the tile .npy files in the tile_dir . If the second value is 0, it means extract['scale_anchor'] has not been calculated yet. If the extract step of the pipeline is re-run with extract['scale'] or extract['scale_anchor'] different to values saved here, an error will be raised. Default: scale psf : str . npy file in output directory indicating average spot shape. If deconvolution required and file does not exist, will be computed automatically in extract step. (this is psf before tapering and scaled to fill uint16 range). Default: psf omp_spot_shape : str . npy file in output_dir indicating average shape in omp coefficient image. It only indicates the sign of the coefficient i.e. only contains -1, 0, 1. If file does not exist, it is computed from the coefficient images of all genes of the central tile. Default: omp_spot_shape omp_spot_info : str . npy file in output_dir containing information about spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. Default: omp_spot_info omp_spot_coef : str . npz file in output_dir containing gene coefficients for all spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. Default: omp_spot_coef big_dapi_image : maybe_str . npz file in output_dir where stitched DAPI image is saved. If it does not exist, it will be saved if basic_info['dapi_channel'] is not None . Leave blank to not save stitched anchor Default: dapi_image big_anchor_image : maybe_str . npz file in output_dir where stitched image of ref_round / ref_channel is saved. If it does not exist, it will be saved. Leave blank to not save stitched anchor Default: anchor_image pciseq : list_str . csv files in output_dir where plotting information for pciSeq will be saved. First file is name where omp method output will be saved. Second file is name where ref_spots method output will be saved. If files don't exist, they will be created when the function coppafish/export_to_pciseq is run. Default: pciseq_omp, pciseq_anchor basic_info The basic_info section indicates information required throughout the pipeline. is_3d : bool . Whether to use the 3d pipeline. Default: MUST BE SPECIFIED anchor_channel : maybe_int . Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. Leave blank if anchor not used. Default: None dapi_channel : maybe_int . Channel in anchor round that contains DAPI images. This does not have to be in use_channels as anchor round is dealt with separately. Leave blank if no DAPI . Default: None ref_round : maybe_int . Round to align all imaging rounds to. Will be set to anchor_round if anchor_channel and file_names['anchor'] specified. Default: None ref_channel : maybe_int . Channel in ref_round used as reference and to build coordinate system on. Usually channel with most spots. Will be set to anchor_channel if anchor_channel and file_names['anchor'] specified. Default: None use_channels : maybe_list_int . Channels in imaging rounds to use throughout pipeline. Leave blank to use all. Default: None use_rounds : maybe_list_int . Imaging rounds to use throughout pipeline. Leave blank to use all. Default: None use_z : maybe_list_int . z planes used to make tile .npy files. Leave blank to use all. If 2 values provided, all z-planes between and including the values given will be used. Default: None use_tiles : maybe_list_int . Tiles used throughout pipeline. Leave blank to use all. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | Default: None ignore_tiles : maybe_list_int . It is often easier to select tiles to remove than to use. All tiles listed here will be ignored. Leave blank to use all. Default: None use_dyes : maybe_list_int . Dyes to use when when assigning spots to genes. Leave blank to use all. Default: None dye_names : maybe_list_str . Name of dyes used in correct order. So for gene with code 360... , gene appears with dye_names[3] in round 0, dye_names[6] in round 1, dye_names[0] in round 2 etc. If left blank, then assumes each channel corresponds to a different dye i.e. code 0 in code_book = channel 0. For quad_cam data, this needs to be specified. Default: None channel_camera : maybe_list_int . channel_camera[i] is the wavelength in nm of the camera used for channel i . Only need to be provided if dye_names provided to help estimate dye intensity in each channel. Default: None channel_laser : maybe_list_int . channel_laser[i] is the wavelengths in nm of the camera/laser used for channel i . Only need to be provided if dye_names provided to help estimate dye intensity in each channel. Default: None tile_pixel_value_shift : int . This is added onto every tile (except DAPI) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to .npy as uint16. Default: 15000 ignore_first_z_plane : bool . Previously had cases where first z plane in .nd2 file was in wrong place and caused focus stacking to be weird or identify lots of spots on first plane. Hence it is safest to not load first plane and this is done if ignore_first_z_plane = True . Default: True extract The extract section contains parameters which specify how to filter the raw microscope images to produce the .npy files saved to file_names['tile_dir'] . wait_time : int . Time to wait in seconds for raw data to come in before crashing. Assumes first round is already in the file_names['input_dir'] Want this to be large so can run pipeline while collecting data. Default: 21600 r1 : maybe_int . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. Typical = 3. For r1 = 3 and r2 = 6 , a 2048 x 2048 x 50 image took 4.1s. For 2 <= r1 <= 5 and r2 double this, the time taken seemed to be constant. Leave blank to auto detect using r1_auto_microns micron . Default: None r2 : maybe_int . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. Typical = 6. Leave blank to set to twice r1 . Default: None r_dapi : maybe_int . Filtering for DAPI images is a tophat with r_dapi radius. Should be approx radius of object of interest. Typical = 48. Leave blank to auto detect using r_dapi_auto_microns . Default: None r1_auto_microns : number . If r1 not specified, will convert to units of pixels from this micron value. Default: 0.5 r_dapi_auto_microns : maybe_number . If r_dapi not specified. Will convert to units of pixels from this micron value. Typical = 8.0. If both this and r_dapi left blank, DAPI image will not be filtered and no .npy file saved. Instead DAPI will be loaded directly from raw data and then stitched. Default: None scale : maybe_number . Each filtered image is multiplied by scale. This is because the image is saved as uint16 so to gain information from the decimal points, should multiply image so max pixel number is in the 10,000s (less than 65,536). Leave empty to auto-detect using scale_norm . Default: None scale_norm : maybe_int . If scale not given, scale = scale_norm/max(scale_image) . Where scale_image is the n_channels x n_y x n_x x n_z image belonging to the central tile (saved as nb.extract_debug.scale_tile ) of round 0 after filtering and smoothing. Must be less than np.iinfo(np.uint16).max - config['basic_info']['tile_pixel_value_shift'] which is typically \\(65535 - 15000 = 50535\\) . Default: 35000 scale_anchor : maybe_number . Analogous to scale but have different normalisation for anchor round/anchor channel as not used in final spot_colors. Leave empty to auto-detect using scale_norm . Default: None auto_thresh_multiplier : number . nb.extract.auto_thresh[t,r,c] is default threshold to find spots on tile t, round r, channel c. Value is set to auto_thresh_multiplier * median(abs(image)) where image is the image produced for tile t, round r, channel c in the extract step of the pipeline and saved to file_names['tile_dir'] . Default: 10 deconvolve : bool . For 3D pipeline, whether to perform wiener deconvolution before hanning filtering. Default: False psf_detect_radius_xy : int . Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if deconvolve == True . To detect spot, pixel needs to be above dilation with this radius in xy plane. Default: 2 psf_detect_radius_z : int . Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if deconvolve == True . To detect spot, pixel needs to be above dilation with this radius in z direction. Default: 2 psf_intensity_thresh : maybe_number . Spots contribute to psf if they are above this intensity. If not given, will be computed the same as auto_thresh i.e. median(image) + auto_thresh_multiplier*median(abs(image-median(image))) . Note that for raw data, median(image) is not zero hence the difference. Default: None psf_isolation_dist : number . Spots contribute to psf if more than psf_isolation_dist from nearest spot. Default: 20 psf_min_spots : int . Need this many isolated spots to determine psf . Default: 300 psf_shape : list_int . Diameter of psf in y, x, z direction (in units of [xy_pixels, xy_pixels, z_pixels]). Default: 181, 181, 19 psf_annulus_width : number . psf is assumed to be radially symmetric within each z-plane so assume all values within annulus of this size (in xy_pixels) to be the same. Default: 1.4 wiener_constant : number . Constant used to compute wiener filter from psf . Default: 50000 wiener_pad_shape : list_int . When applying the wiener filter, we pad the raw image to median value linearly with this many pixels at end of each dimension. Default: 20, 20, 3 r_smooth : maybe_list_int . Radius of averaging filter to do smoothing of filtered image. Provide two numbers to do 2D smoothing and three numbers to do 3D smoothing. Typical 2D : 2, 2 . Typical 3D : 1, 1, 2 . Recommended use is in 3D only as it incorporates information between z-planes which filtering with difference of hanning kernels does not. Size of r_smooth has big influence on time taken for smoothing. For a 2048 x 2048 x 50 image: r_smooth = 1, 1, 2 : 2.8 seconds r_smooth = 2, 2, 2 : 8.5 seconds Leave empty to do no smoothing. Default: None n_clip_warn : int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_warn , a warning message will occur. Default: 1000 n_clip_error : maybe_int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_error for n_clip_error_images_thresh images, the extract and filter step will be halted. If left blank, n_clip_error will be set to 1% of pixels of a single z-plane. Default: None n_clip_error_images_thresh : int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_error for n_clip_error_images_thresh images, the extract and filter step will be halted. Default: 3 find_spots The find_spots section contains parameters which specify how to convert the images produced in the extract section to point clouds. radius_xy : int . To be detected as a spot, a pixel needs to be above dilation with structuring element which is a square ( np.ones ) of width 2*radius_xy-1 in the xy plane. Default: 2 radius_z : int . To be detected as a spot, a pixel needs to be above dilation with structuring element which is cuboid ( np.ones ) with width 2*radius_z-1 in z direction. Must be more than 1 to be 3D. Default: 2 max_spots_2d : int . If number of spots detected on particular z-plane of an imaging round is greater than this, then will only select the max_spots_2d most intense spots on that z-plane. I.e. PCR works better if trying to fit fewer more intense spots. This only applies to imaging rounds and not ref_round/ref_channel as need lots of spots then. In 2D, allow more spots as only 1 z-plane Default: 1500 max_spots_3d : int . Same as max_spots_2d for the 3D pipeline. In 3D, need to allow less spots on a z-plane as have many z-planes. Default: 500 isolation_radius_inner : number . To determine if spots are isolated, filter image with annulus between isolation_radius_inner and isolation_radius . isolation_radius_inner should be approx the radius where intensity of spot crosses from positive to negative. It is in units of xy-pixels. This filtering will only be applied to spots detected in the ref_round/ref_channel. Default: 4 isolation_radius_xy : number . Outer radius of annulus filtering kernel in xy direction in units of xy-pixels. Default: 14 isolation_radius_z : number . Outer radius of annulus filtering kernel in z direction in units of z-pixels. Default: 1 isolation_thresh : maybe_number . Spot is isolated if value of annular filtered image at spot location is below the isolation_thresh value. Leave blank to automatically determine value using auto_isolation_thresh_multiplier . multiplied by the threshold used to detect the spots i.e. the extract_auto_thresh value. Default: None auto_isolation_thresh_multiplier : number . If isolation_thresh left blank, it will be set to isolation_thresh = auto_isolation_thresh_multiplier * nb.extract.auto_thresh[:, r, c] . Default: -0.2 n_spots_warn_fraction : number . Used in coppafish/find_spots/base/check_n_spots A warning will be raised if for any tile, round, channel the number of spots detected is less than: n_spots_warn = n_spots_warn_fraction * max_spots * nb.basic_info.nz where max_spots is max_spots_2d if 2D and max_spots_3d if 3D . Default: 0.1 n_spots_error_fraction : number . Used in coppafish/find_spots/base/check_n_spots . An error is raised if any of the following are satisfied: For any given channel, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/rounds. For any given tile, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of rounds/channels. For any given round, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/channels. Default: 0.5 stitch The stitch section contains parameters which specify how the overlaps between neighbouring tiles are found. Note that references to south in this section should really be north and west should be east. expected_overlap : number . Expected fractional overlap between tiles. Used to get initial shift search if not provided. Default: 0.1 auto_n_shifts : list_int . If shift_south_min/max and/or shift_west_min/max not given, the initial shift search will have auto_n_shifts either side of the expected shift given the expected_overlap with step given by shift_step . First value gives \\(n_{shifts}\\) in direction of overlap (y for south, x for west). Second value gives \\(n_{shifts}\\) in other direction (x for south, y for west). Third value gives \\(n_{shifts}\\) in z. Default: 20, 20, 1 shift_south_min : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -1900, -100, -2 Default: None shift_south_max : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -1700, 100, 2 Default: None shift_west_min : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -100, -1900, -2 Default: None shift_west_max : maybe_list_int . Can manually specify initial shifts. Shift range will run between min to max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: 100, -1700, 2 Default: None shift_step : list_int . Step size to use in y, x, z when finding shift between tiles. Default: 5, 5, 3 shift_widen : list_int . If shift in initial search range has score which does not exceed shift_score_thresh , then range will be extrapolated with same step by shift_widen values in y, x, z direction. Default: 10, 10, 1 shift_max_range : list_int . The range of shifts searched over will continue to be increased according to shift_widen until the shift range in the y, x, z direction reaches shift_max_range . If a good shift is still not found, a warning will be printed. Default: 300, 300, 10 neighb_dist_thresh : number . Basically the distance in yx pixels below which neighbours are a good match. Default: 2 shift_score_thresh : maybe_number . A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the shift_score_thresh parameters below using the function coppafish/stitch/shift/get_score_thresh . Default: None shift_score_thresh_multiplier : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 2 shift_score_thresh_min_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 11 shift_score_thresh_max_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 20 nz_collapse : int . 3D data is converted into np.ceil(nz / nz_collapse) 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. Default: 30 n_shifts_error_fraction : number . Used in coppafish/stitch/check_shifts/check_shifts_stitch If more than this fraction of shifts found between neighbouring tiles have score < score_thresh , an error will be raised. Default: 0.5 save_image_zero_thresh : int . When saving stitched images, all pixels with absolute value less than or equal to save_image_zero_thresh will be set to 0. This helps reduce size of the .npz files and does not lose any important information. Default: 20 register_initial The register_initial section contains parameters which specify how the shifts from the ref_round/ref_channel to each imaging round/channel are found. These are then used as the starting point for determining the affine transforms in the register section. shift_channel : maybe_int . Channel to use to find shifts between rounds to use as starting point for PCR. Leave blank to set to basic_info['ref_channel'] . Default: None shift_min : list_int . Exhaustive search range will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: [-100, -100, -1] Default: -100, -100, -3 shift_max : list_int . Exhaustive search range will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: [100, 100, 1] Default: 100, 100, 3 shift_step : list_int . Step size to use in y, x, z when performing the exhaustive search to find the shift between tiles. Default: 5, 5, 3 shift_widen : list_int . If shift in initial search range has score which does not exceed shift_score_thresh , then the range will be extrapolated with same step by shift_widen values in y, x, z direction. Default: 10, 10, 1 shift_max_range : list_int . The range of shifts searched over will continue to be increased according to shift_widen until the shift range in the y, x, z direction reaches shift_max_range . If a good shift is still not found, a warning will be printed. Default: 500, 500, 10 neighb_dist_thresh : number . Basically the distance in yx pixels below which neighbours are a good match. Default: 2 shift_score_thresh : maybe_number . A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the shift_score_thresh parameters below using the function coppafish/stitch/shift/get_score_thresh . Default: None shift_score_thresh_multiplier : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 1.5 shift_score_thresh_min_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 11 shift_score_thresh_max_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 20 nz_collapse : int . 3D data is converted into np.ceil(nz / nz_collapse) 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. Default: 30 n_shifts_error_fraction : number . Used in coppafish/stitch/check_shifts/check_shifts_register If more than this fraction of shifts between the ref_round / ref_channel and each imaging round for each tile have score < score_thresh , an error will be raised. Default: 0.5 register The register section contains parameters which specify how the affine transforms from the ref_round/ref_channel to each imaging round/channel are found from the shifts found in the register_initial section. n_iter : int . Maximum number iterations to run point cloud registration, PCR Default: 100 neighb_dist_thresh_2d : number . Basically the distance in yx pixels below which neighbours are a good match. PCR updates transforms by minimising distances between neighbours which are closer than this. Default: 3 neighb_dist_thresh_3d : number . The same as neighb_dist_thresh_2d but in 3D, we use a larger distance because the size of a z-pixel is greater than a xy pixel. Default: 5 matches_thresh_fract : number . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 0.25 matches_thresh_min : int . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 25 matches_thresh_max : int . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 300 scale_dev_thresh : list_number . If a transform has a chromatic aberration scaling that has an absolute deviation of more than scale_dev_thresh[i] from the median for that colour channel in dimension i , it will be re-evaluated with regularization. There is a threshold for the y, x, z scaling. Default: 0.01, 0.01, 0.1 shift_dev_thresh : list_number . If a transform has a shift[i] that has an absolute deviation of more than shift_dev_thresh[i] from the median for that tile and round in any dimension i , it will be re-evaluated with regularization. There is a threshold for the y, x, z shift. shift_dev_thresh[2] is in z pixels. Default: 15, 15, 5 regularize_constant : int . Constant used when doing regularized least squares. If the number of neighbours are above this, regularization will have little effect. If the number of neighbours is less than this, regularization will have significant effect, and final transform will be similar to transform being regularized towards. Default: 500 regularize_factor : number . The loss function for finding the transform through regularized least squares is: \\(\\sum_s^{n_{neighb}}D_s^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_s^2\\) is the squared distance between the pair of neighbours indicated by \\(s\\) . Only neighbours with distance between them on previous iteration les than neighb_dist are considered. \\(\\lambda\\) is regularize_constant . \\(\\mu\\) is regularize_factor such that when \\(n_{neighb} = \\lambda\\) and \\(D_s^2 = D_{shift}^2\\) for all \\(s\\) , the two contributions to the loss function are approximately equal i.e. \\(\\mu = D_{shift}^2/D_{scale}^2\\) . \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and transform_regularize[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and transform_regularize[3] . I.e. the squared difference of the shift part of the transform from the target. So if a typical value of \\(D_{shift}\\) (or \\(D_s\\) ) is 2 and a typical value of \\(D_{scale}\\) is 0.009, \\(\\mu = 5\\times10^4\\) . Default: 5e4 n_transforms_error_fraction : number . Used in coppafish/register/check_transforms/check_transforms An error is raised if any of the following are satisfied where a failed transform is one with nb.register_debug.n_matches < nb.register_debug.n_matches_thresh . For any given channel, the fraction of failed transforms was greater than n_transforms_error_fraction of tiles/rounds. For any given tile, the fraction of failed transforms was greater than n_transforms_error_fraction of rounds/channels. For any given round, the fraction of failed transforms was greater than n_transforms_error_fraction of tiles/channels. Default: 0.5 call_spots The call_spots section contains parameters which determine how the bleed_matrix and gene_efficiency are computed, as well as how a gene is assigned to each spot found on the ref_round/ref_channel. bleed_matrix_method : str . bleed_matrix_method can only be single or separate . single : a single bleed matrix is produced for all rounds. separate : a different bleed matrix is made for each round. Default: single color_norm_intensities : list_number . Parameter used to get color normalisation factor. color_norm_intensities should be ascending and color_norm_probs should be descending and they should be the same size. The probability of normalised spot color being greater than color_norm_intensities[i] must be less than color_norm_probs[i] for all i . Default: 0.5, 1, 5 color_norm_probs : list_number . Parameter used to get color normalisation factor. color_norm_intensities should be ascending and color_norm_probs should be descending and they should be the same size. The probability of normalised spot color being greater than color_norm_intensities[i] must be less than color_norm_probs[i] for all i . Default: 0.01, 5e-4, 1e-5 bleed_matrix_score_thresh : number . In scaled_k_means part of bleed_matrix calculation, a mean vector for each dye is computed from all spots with a dot product to that mean greater than this. Default: 0 bleed_matrix_min_cluster_size : int . If less than this many vectors are assigned to a dye cluster in the scaled_k_means part of bleed_matrix calculation, the expected code for that dye will be set to 0 for all color channels i.e. bleed matrix computation will have failed. Default: 10 bleed_matrix_n_iter : int . Maximum number of iterations allowed in the scaled_k_means part of bleed_matrix calculation. Default: 100 bleed_matrix_anneal : bool . If True , the scaled_k_means calculation will be performed twice. The second time starting with the output of the first and with score_thresh for cluster i set to the median of the scores assigned to cluster i in the first run. This limits the influence of bad spots to the bleed matrix. Default: True background_weight_shift : maybe_number . Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round r for the fitting of the background vector for channel c is 1 / (spot_color[r, c] + background_weight_shift) so background_weight_shift ensures this does not go to infinity for small spot_color[r, c] . Typical spot_color[r, c] is 1 for intense spot so background_weight_shift is small fraction of this. Leave blank to set to median absolute intensity of all pixels on the mid z-plane of the central tile. Default: None dp_norm_shift : maybe_number . When calculating the dot_product_score , this is the small shift to apply when normalising spot_colors to ensure don't divide by zero. Value is for a single round and is multiplied by sqrt(n_rounds_used) when computing dot_product_score . Expected norm of a spot_color for a single round is 1 so dp_norm_shift is a small fraction of this. Leave blank to set to median L2 norm for a single round of all pixels on the mid z-plane of the central tile. Default: None norm_shift_min : number . Minimum possible value of dp_norm_shift and background_weight_shift . Default: 0.001 norm_shift_max : number . Maximum possible value of dp_norm_shift and background_weight_shift . Default: 0.5 norm_shift_precision : number . dp_norm_shift and background_weight_shift will be rounded to nearest norm_shift_precision . Default: 0.01 gene_efficiency_min_spots : int . If number of spots assigned to a gene less than or equal to this, gene_efficiency[g]=1 for all rounds. Default: 25 gene_efficiency_max : number . Maximum allowed value of gene_efficiency i.e. any one round can be at most this times more important than the median round for every gene. Default: 6 gene_efficiency_min : number . At most ceil(gene_efficiency_min_factor * n_rounds_use) rounds can have gene_efficiency below gene_efficiency_min for any given gene. Default: 0.05 gene_efficiency_min_factor : number . At most ceil(gene_efficiency_min_factor * n_rounds_use) rounds can have gene_efficiency below gene_efficiency_min for any given gene. Default: 0.2 gene_efficiency_n_iter : int . gene_efficiency is computed from spots which pass a quality thresholding based on the bled_codes computed with the gene_efficiency of the previous iteration. This process will continue until the gene_effiency converges or gene_efficiency_n_iter iterations are reached. 0 means gene_efficiency will not be used. Default: 10 gene_efficiency_score_thresh : number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Default: 0.6 gene_efficiency_score_diff_thresh : number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Default: 0.2 gene_efficiency_intensity_thresh : maybe_number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Leave blank to determine from gene_efficiency_intensity_thresh_percentile . Default: None gene_efficiency_intensity_thresh_percentile : int . gene_efficiency_intensity_thresh will be set to this percentile of the intensity computed for all pixels on the mid z-plane of the most central tile if not specified. Default: 37 gene_efficiency_intensity_thresh_precision : number . gene_efficiency_intensity_thresh will be rounded to nearest gene_efficiency_intensity_thresh_precision if not given. Default: 0.001 gene_efficiency_intensity_thresh_min : number . Min allowed value of gene_efficiency_intensity_thresh . Default: 0.001 gene_efficiency_intensity_thresh_max : number . Max allowed value of gene_efficiency_intensity_thresh . Default: 0.2 alpha : number . When computing the dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\) , rounds/channels with background already fit contribute less. The larger \\(\\alpha\\) , the lower the contribution. Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. Default: 120 beta : number . Constant used in weighting factor when computing dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\) . Default: 1 omp The omp section contains parameters which are use to carry out orthogonal matching pursuit (omp) on every pixel, as well as how to convert the results of this to spot locations. use_z : maybe_list_int . Can specify z-planes to find spots on If 2 values provided, all z-planes between and including the values given will be used. Default: None weight_coef_fit : bool . If False , gene coefficients are found through omp with normal least squares fitting. If True , gene coefficients are found through omp with weighted least squares fitting with rounds/channels which already containing genes contributing less. Default: False initial_intensity_thresh : maybe_number . To save time in call_spots_omp , coefficients only found for pixels with intensity of absolute spot_colors greater than initial_intensity_thresh . Leave blank to set to determine using initial_intensity_thresh_auto_param It is also clamped between the initial_intensity_thresh_min and initial_intensity_thresh_max . Default: None initial_intensity_thresh_percentile : int . If initial_intensity_thresh not given, it will be set to the initial_intensity_thresh_percentile percentile of the absolute intensity of all pixels on the mid z-plane of the central tile. It uses nb.call_spots.abs_intensity_percentile Default: 25 initial_intensity_thresh_min : number . Min allowed value of initial_intensity_thresh . Default: 0.001 initial_intensity_thresh_max : number . Max allowed value of initial_intensity_thresh . Default: 0.2 initial_intensity_precision : number . initial_intensity_thresh will be rounded to nearest initial_intensity_precision if not given. Default: 0.001 max_genes : int . The maximum number of genes that can be assigned to each pixel i.e. number of iterations of omp. Default: 30 dp_thresh : number . Pixels only have coefficient found for a gene if that gene has absolute dot_product_score greater than this i.e. this is the stopping criterion for the OMP. Default: 0.225 alpha : number . When computing the dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP , rounds/channels with genes already fit to them, contribute less. The larger \\(\\alpha\\) , the lower the contribution. Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. Default: 120 beta : number . Constant used in weighting factor when computing dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP . Default: 1 initial_pos_neighbour_thresh : maybe_int . Only save spots with number of positive coefficient neighbours greater than initial_pos_neighbour_thresh . Leave blank to determine using initial_pos_neighbour_thresh_param . It is also clipped between initial_pos_neighbour_thresh_min and initial_pos_neighbour_thresh_max . Default: None initial_pos_neighbour_thresh_param : number . If initial_pos_neighbour_thresh not given, it is set to initial_pos_neighbour_thresh_param multiplied by number of positive values in nb.omp.spot_shape i.e. with initial_pos_neighbour_thresh_param = 0.1 , it is set to 10% of the max value. Default: 0.1 initial_pos_neighbour_thresh_min : int . Min allowed value of initial_pos_neighbour_thresh . Default: 4 initial_pos_neighbour_thresh_max : int . Max allowed value of initial_pos_neighbour_thresh . Default: 40 radius_xy : int . To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is a square ( np.ones ) of width 2*radius_xy-1 in the xy plane. Default: 3 radius_z : int . To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is cuboid ( np.ones ) with width 2*radius_z-1 in z direction. Must be more than 1 to be 3D. Default: 2 shape_max_size : list_int . spot_shape specifies the neighbourhood about each spot in which we count coefficients which contribute to score. It is either given through file_names['omp_spot_shape'] or computed using the below parameters with shape prefix. Maximum Y, X, Z size of spot_shape. Will be cropped if there are zeros at the extremities. Default: 27, 27, 9 shape_pos_neighbour_thresh : int . For spot to be used to find spot_shape , it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Default: 9 shape_isolation_dist : number . Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find spot_shape . Default: 10 shape_sign_thresh : number . If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in spot_shape to be 0. Max mean absolute coefficient sign is 1 so must be less than this. Default: 0.15 thresholds The thresholds section contains the thresholds used to determine which spots pass a quality thresholding process such that we consider their gene assignments legitimate. intensity : maybe_number . Final accepted reference and OMP spots both require intensity > thresholds[intensity] . If not given, will be set to same value as nb.call_spots.gene_efficiency_intensity_thresh . intensity for a really intense spot is about 1 so intensity_thresh should be less than this. Default: None score_ref : number . Final accepted spots are those which pass quality_threshold which is nb.ref_spots.score > thresholds[score_ref] and nb.ref_spots.intensity > intensity_thresh . quality_threshold requires score computed with coppafish/call_spots/dot_prodduct/dot_product_score to exceed this. Max score is 1 so must be below this. Default: 0.25 score_omp : number . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) Max score is 1 so score_thresh should be less than this. 0.15 if more concerned for missed spots than false positives. Default: 0.263 score_omp_multiplier : number . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) 0.45 if more concerned for missed spots than false positives. Default: 0.95","title":"Config Default Settings"},{"location":"config/#default-config-settings","text":"","title":"Default Config Settings"},{"location":"config/#file_names","text":"The file_names section specifies the files that will be used throughout the pipeline. Variables in this section can be changed at any point in the pipeline, and the notebook created using it can still be loaded in. notebook_name : str . Name of notebook file in output directory will be notebook_name .npz Default: notebook input_dir : str . Directory where the raw .nd2 files or .npy stacks are Default: MUST BE SPECIFIED output_dir : str . Directory where notebook is saved Default: MUST BE SPECIFIED tile_dir : str . Directory where tile .npy files saved Default: MUST BE SPECIFIED round : maybe_list_str . Names of .nd2 files for the imaging rounds. Leave empty if only using anchor. Default: None anchor : maybe_str . Name of the file for the anchor round. Leave empty if not using anchor. Default: None raw_extension : str . .nd2 or .npy indicating the data type of the raw data. Default: .nd2 raw_metadata : maybe_str . If .npy raw_extension, this is the name of the .json file in input_dir which contains the metadata required extracted from the initial .nd2 files. I.e. it contains the output of coppafish/utils/nd2/save_metadata : xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. pixel_microns - float . xy pixel size in microns. pixel_microns_z - float . z pixel size in microns. sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Default: None dye_camera_laser : maybe_file . csv file giving the approximate raw intensity for each dye with each camera/laser combination. If not set, the file coppafish/setup/dye_camera_laser_raw_intensity.csv file will be used. Default: None code_book : str . Text file which contains the codes indicating which dye to expect on each round for each gene. Default: MUST BE SPECIFIED scale : str . Text file saved in tile_dir containing extract['scale'] and extract['scale_anchor'] values used to create the tile .npy files in the tile_dir . If the second value is 0, it means extract['scale_anchor'] has not been calculated yet. If the extract step of the pipeline is re-run with extract['scale'] or extract['scale_anchor'] different to values saved here, an error will be raised. Default: scale psf : str . npy file in output directory indicating average spot shape. If deconvolution required and file does not exist, will be computed automatically in extract step. (this is psf before tapering and scaled to fill uint16 range). Default: psf omp_spot_shape : str . npy file in output_dir indicating average shape in omp coefficient image. It only indicates the sign of the coefficient i.e. only contains -1, 0, 1. If file does not exist, it is computed from the coefficient images of all genes of the central tile. Default: omp_spot_shape omp_spot_info : str . npy file in output_dir containing information about spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. Default: omp_spot_info omp_spot_coef : str . npz file in output_dir containing gene coefficients for all spots found in omp step. After each tile is completed, information will be saved to this file. If file does not exist, it will be saved after first tile of OMP step. Default: omp_spot_coef big_dapi_image : maybe_str . npz file in output_dir where stitched DAPI image is saved. If it does not exist, it will be saved if basic_info['dapi_channel'] is not None . Leave blank to not save stitched anchor Default: dapi_image big_anchor_image : maybe_str . npz file in output_dir where stitched image of ref_round / ref_channel is saved. If it does not exist, it will be saved. Leave blank to not save stitched anchor Default: anchor_image pciseq : list_str . csv files in output_dir where plotting information for pciSeq will be saved. First file is name where omp method output will be saved. Second file is name where ref_spots method output will be saved. If files don't exist, they will be created when the function coppafish/export_to_pciseq is run. Default: pciseq_omp, pciseq_anchor","title":"file_names"},{"location":"config/#basic_info","text":"The basic_info section indicates information required throughout the pipeline. is_3d : bool . Whether to use the 3d pipeline. Default: MUST BE SPECIFIED anchor_channel : maybe_int . Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. Leave blank if anchor not used. Default: None dapi_channel : maybe_int . Channel in anchor round that contains DAPI images. This does not have to be in use_channels as anchor round is dealt with separately. Leave blank if no DAPI . Default: None ref_round : maybe_int . Round to align all imaging rounds to. Will be set to anchor_round if anchor_channel and file_names['anchor'] specified. Default: None ref_channel : maybe_int . Channel in ref_round used as reference and to build coordinate system on. Usually channel with most spots. Will be set to anchor_channel if anchor_channel and file_names['anchor'] specified. Default: None use_channels : maybe_list_int . Channels in imaging rounds to use throughout pipeline. Leave blank to use all. Default: None use_rounds : maybe_list_int . Imaging rounds to use throughout pipeline. Leave blank to use all. Default: None use_z : maybe_list_int . z planes used to make tile .npy files. Leave blank to use all. If 2 values provided, all z-planes between and including the values given will be used. Default: None use_tiles : maybe_list_int . Tiles used throughout pipeline. Leave blank to use all. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | Default: None ignore_tiles : maybe_list_int . It is often easier to select tiles to remove than to use. All tiles listed here will be ignored. Leave blank to use all. Default: None use_dyes : maybe_list_int . Dyes to use when when assigning spots to genes. Leave blank to use all. Default: None dye_names : maybe_list_str . Name of dyes used in correct order. So for gene with code 360... , gene appears with dye_names[3] in round 0, dye_names[6] in round 1, dye_names[0] in round 2 etc. If left blank, then assumes each channel corresponds to a different dye i.e. code 0 in code_book = channel 0. For quad_cam data, this needs to be specified. Default: None channel_camera : maybe_list_int . channel_camera[i] is the wavelength in nm of the camera used for channel i . Only need to be provided if dye_names provided to help estimate dye intensity in each channel. Default: None channel_laser : maybe_list_int . channel_laser[i] is the wavelengths in nm of the camera/laser used for channel i . Only need to be provided if dye_names provided to help estimate dye intensity in each channel. Default: None tile_pixel_value_shift : int . This is added onto every tile (except DAPI) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to .npy as uint16. Default: 15000 ignore_first_z_plane : bool . Previously had cases where first z plane in .nd2 file was in wrong place and caused focus stacking to be weird or identify lots of spots on first plane. Hence it is safest to not load first plane and this is done if ignore_first_z_plane = True . Default: True","title":"basic_info"},{"location":"config/#extract","text":"The extract section contains parameters which specify how to filter the raw microscope images to produce the .npy files saved to file_names['tile_dir'] . wait_time : int . Time to wait in seconds for raw data to come in before crashing. Assumes first round is already in the file_names['input_dir'] Want this to be large so can run pipeline while collecting data. Default: 21600 r1 : maybe_int . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. Typical = 3. For r1 = 3 and r2 = 6 , a 2048 x 2048 x 50 image took 4.1s. For 2 <= r1 <= 5 and r2 double this, the time taken seemed to be constant. Leave blank to auto detect using r1_auto_microns micron . Default: None r2 : maybe_int . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. Typical = 6. Leave blank to set to twice r1 . Default: None r_dapi : maybe_int . Filtering for DAPI images is a tophat with r_dapi radius. Should be approx radius of object of interest. Typical = 48. Leave blank to auto detect using r_dapi_auto_microns . Default: None r1_auto_microns : number . If r1 not specified, will convert to units of pixels from this micron value. Default: 0.5 r_dapi_auto_microns : maybe_number . If r_dapi not specified. Will convert to units of pixels from this micron value. Typical = 8.0. If both this and r_dapi left blank, DAPI image will not be filtered and no .npy file saved. Instead DAPI will be loaded directly from raw data and then stitched. Default: None scale : maybe_number . Each filtered image is multiplied by scale. This is because the image is saved as uint16 so to gain information from the decimal points, should multiply image so max pixel number is in the 10,000s (less than 65,536). Leave empty to auto-detect using scale_norm . Default: None scale_norm : maybe_int . If scale not given, scale = scale_norm/max(scale_image) . Where scale_image is the n_channels x n_y x n_x x n_z image belonging to the central tile (saved as nb.extract_debug.scale_tile ) of round 0 after filtering and smoothing. Must be less than np.iinfo(np.uint16).max - config['basic_info']['tile_pixel_value_shift'] which is typically \\(65535 - 15000 = 50535\\) . Default: 35000 scale_anchor : maybe_number . Analogous to scale but have different normalisation for anchor round/anchor channel as not used in final spot_colors. Leave empty to auto-detect using scale_norm . Default: None auto_thresh_multiplier : number . nb.extract.auto_thresh[t,r,c] is default threshold to find spots on tile t, round r, channel c. Value is set to auto_thresh_multiplier * median(abs(image)) where image is the image produced for tile t, round r, channel c in the extract step of the pipeline and saved to file_names['tile_dir'] . Default: 10 deconvolve : bool . For 3D pipeline, whether to perform wiener deconvolution before hanning filtering. Default: False psf_detect_radius_xy : int . Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if deconvolve == True . To detect spot, pixel needs to be above dilation with this radius in xy plane. Default: 2 psf_detect_radius_z : int . Need to detect spots to determine point spread function (psf) used in the wiener deconvolution. Only relevant if deconvolve == True . To detect spot, pixel needs to be above dilation with this radius in z direction. Default: 2 psf_intensity_thresh : maybe_number . Spots contribute to psf if they are above this intensity. If not given, will be computed the same as auto_thresh i.e. median(image) + auto_thresh_multiplier*median(abs(image-median(image))) . Note that for raw data, median(image) is not zero hence the difference. Default: None psf_isolation_dist : number . Spots contribute to psf if more than psf_isolation_dist from nearest spot. Default: 20 psf_min_spots : int . Need this many isolated spots to determine psf . Default: 300 psf_shape : list_int . Diameter of psf in y, x, z direction (in units of [xy_pixels, xy_pixels, z_pixels]). Default: 181, 181, 19 psf_annulus_width : number . psf is assumed to be radially symmetric within each z-plane so assume all values within annulus of this size (in xy_pixels) to be the same. Default: 1.4 wiener_constant : number . Constant used to compute wiener filter from psf . Default: 50000 wiener_pad_shape : list_int . When applying the wiener filter, we pad the raw image to median value linearly with this many pixels at end of each dimension. Default: 20, 20, 3 r_smooth : maybe_list_int . Radius of averaging filter to do smoothing of filtered image. Provide two numbers to do 2D smoothing and three numbers to do 3D smoothing. Typical 2D : 2, 2 . Typical 3D : 1, 1, 2 . Recommended use is in 3D only as it incorporates information between z-planes which filtering with difference of hanning kernels does not. Size of r_smooth has big influence on time taken for smoothing. For a 2048 x 2048 x 50 image: r_smooth = 1, 1, 2 : 2.8 seconds r_smooth = 2, 2, 2 : 8.5 seconds Leave empty to do no smoothing. Default: None n_clip_warn : int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_warn , a warning message will occur. Default: 1000 n_clip_error : maybe_int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_error for n_clip_error_images_thresh images, the extract and filter step will be halted. If left blank, n_clip_error will be set to 1% of pixels of a single z-plane. Default: None n_clip_error_images_thresh : int . If the number of pixels that are clipped when saving as uint16 is more than n_clip_error for n_clip_error_images_thresh images, the extract and filter step will be halted. Default: 3","title":"extract"},{"location":"config/#find_spots","text":"The find_spots section contains parameters which specify how to convert the images produced in the extract section to point clouds. radius_xy : int . To be detected as a spot, a pixel needs to be above dilation with structuring element which is a square ( np.ones ) of width 2*radius_xy-1 in the xy plane. Default: 2 radius_z : int . To be detected as a spot, a pixel needs to be above dilation with structuring element which is cuboid ( np.ones ) with width 2*radius_z-1 in z direction. Must be more than 1 to be 3D. Default: 2 max_spots_2d : int . If number of spots detected on particular z-plane of an imaging round is greater than this, then will only select the max_spots_2d most intense spots on that z-plane. I.e. PCR works better if trying to fit fewer more intense spots. This only applies to imaging rounds and not ref_round/ref_channel as need lots of spots then. In 2D, allow more spots as only 1 z-plane Default: 1500 max_spots_3d : int . Same as max_spots_2d for the 3D pipeline. In 3D, need to allow less spots on a z-plane as have many z-planes. Default: 500 isolation_radius_inner : number . To determine if spots are isolated, filter image with annulus between isolation_radius_inner and isolation_radius . isolation_radius_inner should be approx the radius where intensity of spot crosses from positive to negative. It is in units of xy-pixels. This filtering will only be applied to spots detected in the ref_round/ref_channel. Default: 4 isolation_radius_xy : number . Outer radius of annulus filtering kernel in xy direction in units of xy-pixels. Default: 14 isolation_radius_z : number . Outer radius of annulus filtering kernel in z direction in units of z-pixels. Default: 1 isolation_thresh : maybe_number . Spot is isolated if value of annular filtered image at spot location is below the isolation_thresh value. Leave blank to automatically determine value using auto_isolation_thresh_multiplier . multiplied by the threshold used to detect the spots i.e. the extract_auto_thresh value. Default: None auto_isolation_thresh_multiplier : number . If isolation_thresh left blank, it will be set to isolation_thresh = auto_isolation_thresh_multiplier * nb.extract.auto_thresh[:, r, c] . Default: -0.2 n_spots_warn_fraction : number . Used in coppafish/find_spots/base/check_n_spots A warning will be raised if for any tile, round, channel the number of spots detected is less than: n_spots_warn = n_spots_warn_fraction * max_spots * nb.basic_info.nz where max_spots is max_spots_2d if 2D and max_spots_3d if 3D . Default: 0.1 n_spots_error_fraction : number . Used in coppafish/find_spots/base/check_n_spots . An error is raised if any of the following are satisfied: For any given channel, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/rounds. For any given tile, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of rounds/channels. For any given round, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/channels. Default: 0.5","title":"find_spots"},{"location":"config/#stitch","text":"The stitch section contains parameters which specify how the overlaps between neighbouring tiles are found. Note that references to south in this section should really be north and west should be east. expected_overlap : number . Expected fractional overlap between tiles. Used to get initial shift search if not provided. Default: 0.1 auto_n_shifts : list_int . If shift_south_min/max and/or shift_west_min/max not given, the initial shift search will have auto_n_shifts either side of the expected shift given the expected_overlap with step given by shift_step . First value gives \\(n_{shifts}\\) in direction of overlap (y for south, x for west). Second value gives \\(n_{shifts}\\) in other direction (x for south, y for west). Third value gives \\(n_{shifts}\\) in z. Default: 20, 20, 1 shift_south_min : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -1900, -100, -2 Default: None shift_south_max : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -1700, 100, 2 Default: None shift_west_min : maybe_list_int . Can manually specify initial shifts. Exhaustive search will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: -100, -1900, -2 Default: None shift_west_max : maybe_list_int . Can manually specify initial shifts. Shift range will run between min to max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: 100, -1700, 2 Default: None shift_step : list_int . Step size to use in y, x, z when finding shift between tiles. Default: 5, 5, 3 shift_widen : list_int . If shift in initial search range has score which does not exceed shift_score_thresh , then range will be extrapolated with same step by shift_widen values in y, x, z direction. Default: 10, 10, 1 shift_max_range : list_int . The range of shifts searched over will continue to be increased according to shift_widen until the shift range in the y, x, z direction reaches shift_max_range . If a good shift is still not found, a warning will be printed. Default: 300, 300, 10 neighb_dist_thresh : number . Basically the distance in yx pixels below which neighbours are a good match. Default: 2 shift_score_thresh : maybe_number . A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the shift_score_thresh parameters below using the function coppafish/stitch/shift/get_score_thresh . Default: None shift_score_thresh_multiplier : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 2 shift_score_thresh_min_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 11 shift_score_thresh_max_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 20 nz_collapse : int . 3D data is converted into np.ceil(nz / nz_collapse) 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. Default: 30 n_shifts_error_fraction : number . Used in coppafish/stitch/check_shifts/check_shifts_stitch If more than this fraction of shifts found between neighbouring tiles have score < score_thresh , an error will be raised. Default: 0.5 save_image_zero_thresh : int . When saving stitched images, all pixels with absolute value less than or equal to save_image_zero_thresh will be set to 0. This helps reduce size of the .npz files and does not lose any important information. Default: 20","title":"stitch"},{"location":"config/#register_initial","text":"The register_initial section contains parameters which specify how the shifts from the ref_round/ref_channel to each imaging round/channel are found. These are then used as the starting point for determining the affine transforms in the register section. shift_channel : maybe_int . Channel to use to find shifts between rounds to use as starting point for PCR. Leave blank to set to basic_info['ref_channel'] . Default: None shift_min : list_int . Exhaustive search range will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: [-100, -100, -1] Default: -100, -100, -3 shift_max : list_int . Exhaustive search range will include all shifts between min and max with step given by shift_step . Each entry should be a list of 3 values: [y, x, z]. Typical: [100, 100, 1] Default: 100, 100, 3 shift_step : list_int . Step size to use in y, x, z when performing the exhaustive search to find the shift between tiles. Default: 5, 5, 3 shift_widen : list_int . If shift in initial search range has score which does not exceed shift_score_thresh , then the range will be extrapolated with same step by shift_widen values in y, x, z direction. Default: 10, 10, 1 shift_max_range : list_int . The range of shifts searched over will continue to be increased according to shift_widen until the shift range in the y, x, z direction reaches shift_max_range . If a good shift is still not found, a warning will be printed. Default: 500, 500, 10 neighb_dist_thresh : number . Basically the distance in yx pixels below which neighbours are a good match. Default: 2 shift_score_thresh : maybe_number . A shift between tiles must have a number of close neighbours exceeding this. If not given, it will be worked using the shift_score_thresh parameters below using the function coppafish/stitch/shift/get_score_thresh . Default: None shift_score_thresh_multiplier : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 1.5 shift_score_thresh_min_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 11 shift_score_thresh_max_dist : number . shift_score_thresh is set to shift_score_thresh_multiplier multiplied by the mean of scores of shifts a distance between shift_score_thresh_min_dist and shift_score_thresh_max_dist from the best shift. Default: 20 nz_collapse : int . 3D data is converted into np.ceil(nz / nz_collapse) 2D slices for exhaustive shift search to quicken it up. I.e. this is the maximum number of z-planes to be collapsed to a 2D slice when searching for the best shift. Default: 30 n_shifts_error_fraction : number . Used in coppafish/stitch/check_shifts/check_shifts_register If more than this fraction of shifts between the ref_round / ref_channel and each imaging round for each tile have score < score_thresh , an error will be raised. Default: 0.5","title":"register_initial"},{"location":"config/#register","text":"The register section contains parameters which specify how the affine transforms from the ref_round/ref_channel to each imaging round/channel are found from the shifts found in the register_initial section. n_iter : int . Maximum number iterations to run point cloud registration, PCR Default: 100 neighb_dist_thresh_2d : number . Basically the distance in yx pixels below which neighbours are a good match. PCR updates transforms by minimising distances between neighbours which are closer than this. Default: 3 neighb_dist_thresh_3d : number . The same as neighb_dist_thresh_2d but in 3D, we use a larger distance because the size of a z-pixel is greater than a xy pixel. Default: 5 matches_thresh_fract : number . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 0.25 matches_thresh_min : int . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 25 matches_thresh_max : int . If PCR produces transforms with fewer neighbours (pairs with distance between them less than neighb_dist_thresh ) than matches_thresh = np.clip(matches_thresh_fract * n_spots, matches_thresh_min, matches_thresh_max) , the transform will be re-evaluated with regularization so it is near the average transform. Default: 300 scale_dev_thresh : list_number . If a transform has a chromatic aberration scaling that has an absolute deviation of more than scale_dev_thresh[i] from the median for that colour channel in dimension i , it will be re-evaluated with regularization. There is a threshold for the y, x, z scaling. Default: 0.01, 0.01, 0.1 shift_dev_thresh : list_number . If a transform has a shift[i] that has an absolute deviation of more than shift_dev_thresh[i] from the median for that tile and round in any dimension i , it will be re-evaluated with regularization. There is a threshold for the y, x, z shift. shift_dev_thresh[2] is in z pixels. Default: 15, 15, 5 regularize_constant : int . Constant used when doing regularized least squares. If the number of neighbours are above this, regularization will have little effect. If the number of neighbours is less than this, regularization will have significant effect, and final transform will be similar to transform being regularized towards. Default: 500 regularize_factor : number . The loss function for finding the transform through regularized least squares is: \\(\\sum_s^{n_{neighb}}D_s^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_s^2\\) is the squared distance between the pair of neighbours indicated by \\(s\\) . Only neighbours with distance between them on previous iteration les than neighb_dist are considered. \\(\\lambda\\) is regularize_constant . \\(\\mu\\) is regularize_factor such that when \\(n_{neighb} = \\lambda\\) and \\(D_s^2 = D_{shift}^2\\) for all \\(s\\) , the two contributions to the loss function are approximately equal i.e. \\(\\mu = D_{shift}^2/D_{scale}^2\\) . \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and transform_regularize[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and transform_regularize[3] . I.e. the squared difference of the shift part of the transform from the target. So if a typical value of \\(D_{shift}\\) (or \\(D_s\\) ) is 2 and a typical value of \\(D_{scale}\\) is 0.009, \\(\\mu = 5\\times10^4\\) . Default: 5e4 n_transforms_error_fraction : number . Used in coppafish/register/check_transforms/check_transforms An error is raised if any of the following are satisfied where a failed transform is one with nb.register_debug.n_matches < nb.register_debug.n_matches_thresh . For any given channel, the fraction of failed transforms was greater than n_transforms_error_fraction of tiles/rounds. For any given tile, the fraction of failed transforms was greater than n_transforms_error_fraction of rounds/channels. For any given round, the fraction of failed transforms was greater than n_transforms_error_fraction of tiles/channels. Default: 0.5","title":"register"},{"location":"config/#call_spots","text":"The call_spots section contains parameters which determine how the bleed_matrix and gene_efficiency are computed, as well as how a gene is assigned to each spot found on the ref_round/ref_channel. bleed_matrix_method : str . bleed_matrix_method can only be single or separate . single : a single bleed matrix is produced for all rounds. separate : a different bleed matrix is made for each round. Default: single color_norm_intensities : list_number . Parameter used to get color normalisation factor. color_norm_intensities should be ascending and color_norm_probs should be descending and they should be the same size. The probability of normalised spot color being greater than color_norm_intensities[i] must be less than color_norm_probs[i] for all i . Default: 0.5, 1, 5 color_norm_probs : list_number . Parameter used to get color normalisation factor. color_norm_intensities should be ascending and color_norm_probs should be descending and they should be the same size. The probability of normalised spot color being greater than color_norm_intensities[i] must be less than color_norm_probs[i] for all i . Default: 0.01, 5e-4, 1e-5 bleed_matrix_score_thresh : number . In scaled_k_means part of bleed_matrix calculation, a mean vector for each dye is computed from all spots with a dot product to that mean greater than this. Default: 0 bleed_matrix_min_cluster_size : int . If less than this many vectors are assigned to a dye cluster in the scaled_k_means part of bleed_matrix calculation, the expected code for that dye will be set to 0 for all color channels i.e. bleed matrix computation will have failed. Default: 10 bleed_matrix_n_iter : int . Maximum number of iterations allowed in the scaled_k_means part of bleed_matrix calculation. Default: 100 bleed_matrix_anneal : bool . If True , the scaled_k_means calculation will be performed twice. The second time starting with the output of the first and with score_thresh for cluster i set to the median of the scores assigned to cluster i in the first run. This limits the influence of bad spots to the bleed matrix. Default: True background_weight_shift : maybe_number . Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round r for the fitting of the background vector for channel c is 1 / (spot_color[r, c] + background_weight_shift) so background_weight_shift ensures this does not go to infinity for small spot_color[r, c] . Typical spot_color[r, c] is 1 for intense spot so background_weight_shift is small fraction of this. Leave blank to set to median absolute intensity of all pixels on the mid z-plane of the central tile. Default: None dp_norm_shift : maybe_number . When calculating the dot_product_score , this is the small shift to apply when normalising spot_colors to ensure don't divide by zero. Value is for a single round and is multiplied by sqrt(n_rounds_used) when computing dot_product_score . Expected norm of a spot_color for a single round is 1 so dp_norm_shift is a small fraction of this. Leave blank to set to median L2 norm for a single round of all pixels on the mid z-plane of the central tile. Default: None norm_shift_min : number . Minimum possible value of dp_norm_shift and background_weight_shift . Default: 0.001 norm_shift_max : number . Maximum possible value of dp_norm_shift and background_weight_shift . Default: 0.5 norm_shift_precision : number . dp_norm_shift and background_weight_shift will be rounded to nearest norm_shift_precision . Default: 0.01 gene_efficiency_min_spots : int . If number of spots assigned to a gene less than or equal to this, gene_efficiency[g]=1 for all rounds. Default: 25 gene_efficiency_max : number . Maximum allowed value of gene_efficiency i.e. any one round can be at most this times more important than the median round for every gene. Default: 6 gene_efficiency_min : number . At most ceil(gene_efficiency_min_factor * n_rounds_use) rounds can have gene_efficiency below gene_efficiency_min for any given gene. Default: 0.05 gene_efficiency_min_factor : number . At most ceil(gene_efficiency_min_factor * n_rounds_use) rounds can have gene_efficiency below gene_efficiency_min for any given gene. Default: 0.2 gene_efficiency_n_iter : int . gene_efficiency is computed from spots which pass a quality thresholding based on the bled_codes computed with the gene_efficiency of the previous iteration. This process will continue until the gene_effiency converges or gene_efficiency_n_iter iterations are reached. 0 means gene_efficiency will not be used. Default: 10 gene_efficiency_score_thresh : number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Default: 0.6 gene_efficiency_score_diff_thresh : number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Default: 0.2 gene_efficiency_intensity_thresh : maybe_number . Spots used to compute gene_efficiency must have dot_product_score greater than gene_efficiency_score_thresh , difference to second best score greater than gene_efficiency_score_diff_thresh and intensity greater than gene_efficiency_intensity_thresh . Leave blank to determine from gene_efficiency_intensity_thresh_percentile . Default: None gene_efficiency_intensity_thresh_percentile : int . gene_efficiency_intensity_thresh will be set to this percentile of the intensity computed for all pixels on the mid z-plane of the most central tile if not specified. Default: 37 gene_efficiency_intensity_thresh_precision : number . gene_efficiency_intensity_thresh will be rounded to nearest gene_efficiency_intensity_thresh_precision if not given. Default: 0.001 gene_efficiency_intensity_thresh_min : number . Min allowed value of gene_efficiency_intensity_thresh . Default: 0.001 gene_efficiency_intensity_thresh_max : number . Max allowed value of gene_efficiency_intensity_thresh . Default: 0.2 alpha : number . When computing the dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\) , rounds/channels with background already fit contribute less. The larger \\(\\alpha\\) , the lower the contribution. Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. Default: 120 beta : number . Constant used in weighting factor when computing dot product score, \\(\\Delta_{s0g}\\) between spot \\(s\\) and gene \\(g\\) . Default: 1","title":"call_spots"},{"location":"config/#omp","text":"The omp section contains parameters which are use to carry out orthogonal matching pursuit (omp) on every pixel, as well as how to convert the results of this to spot locations. use_z : maybe_list_int . Can specify z-planes to find spots on If 2 values provided, all z-planes between and including the values given will be used. Default: None weight_coef_fit : bool . If False , gene coefficients are found through omp with normal least squares fitting. If True , gene coefficients are found through omp with weighted least squares fitting with rounds/channels which already containing genes contributing less. Default: False initial_intensity_thresh : maybe_number . To save time in call_spots_omp , coefficients only found for pixels with intensity of absolute spot_colors greater than initial_intensity_thresh . Leave blank to set to determine using initial_intensity_thresh_auto_param It is also clamped between the initial_intensity_thresh_min and initial_intensity_thresh_max . Default: None initial_intensity_thresh_percentile : int . If initial_intensity_thresh not given, it will be set to the initial_intensity_thresh_percentile percentile of the absolute intensity of all pixels on the mid z-plane of the central tile. It uses nb.call_spots.abs_intensity_percentile Default: 25 initial_intensity_thresh_min : number . Min allowed value of initial_intensity_thresh . Default: 0.001 initial_intensity_thresh_max : number . Max allowed value of initial_intensity_thresh . Default: 0.2 initial_intensity_precision : number . initial_intensity_thresh will be rounded to nearest initial_intensity_precision if not given. Default: 0.001 max_genes : int . The maximum number of genes that can be assigned to each pixel i.e. number of iterations of omp. Default: 30 dp_thresh : number . Pixels only have coefficient found for a gene if that gene has absolute dot_product_score greater than this i.e. this is the stopping criterion for the OMP. Default: 0.225 alpha : number . When computing the dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP , rounds/channels with genes already fit to them, contribute less. The larger \\(\\alpha\\) , the lower the contribution. Set \\(\\alpha = 0\\) to use the normal dot-product with no weighting. Default: 120 beta : number . Constant used in weighting factor when computing dot product score, \\(\\Delta_{sig}\\) between spot \\(s\\) and gene \\(g\\) on iteration \\(i\\) of OMP . Default: 1 initial_pos_neighbour_thresh : maybe_int . Only save spots with number of positive coefficient neighbours greater than initial_pos_neighbour_thresh . Leave blank to determine using initial_pos_neighbour_thresh_param . It is also clipped between initial_pos_neighbour_thresh_min and initial_pos_neighbour_thresh_max . Default: None initial_pos_neighbour_thresh_param : number . If initial_pos_neighbour_thresh not given, it is set to initial_pos_neighbour_thresh_param multiplied by number of positive values in nb.omp.spot_shape i.e. with initial_pos_neighbour_thresh_param = 0.1 , it is set to 10% of the max value. Default: 0.1 initial_pos_neighbour_thresh_min : int . Min allowed value of initial_pos_neighbour_thresh . Default: 4 initial_pos_neighbour_thresh_max : int . Max allowed value of initial_pos_neighbour_thresh . Default: 40 radius_xy : int . To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is a square ( np.ones ) of width 2*radius_xy-1 in the xy plane. Default: 3 radius_z : int . To detect spot in coefficient image of each gene, pixel needs to be above dilation with structuring element which is cuboid ( np.ones ) with width 2*radius_z-1 in z direction. Must be more than 1 to be 3D. Default: 2 shape_max_size : list_int . spot_shape specifies the neighbourhood about each spot in which we count coefficients which contribute to score. It is either given through file_names['omp_spot_shape'] or computed using the below parameters with shape prefix. Maximum Y, X, Z size of spot_shape. Will be cropped if there are zeros at the extremities. Default: 27, 27, 9 shape_pos_neighbour_thresh : int . For spot to be used to find spot_shape , it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Default: 9 shape_isolation_dist : number . Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find spot_shape . Default: 10 shape_sign_thresh : number . If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in spot_shape to be 0. Max mean absolute coefficient sign is 1 so must be less than this. Default: 0.15","title":"omp"},{"location":"config/#thresholds","text":"The thresholds section contains the thresholds used to determine which spots pass a quality thresholding process such that we consider their gene assignments legitimate. intensity : maybe_number . Final accepted reference and OMP spots both require intensity > thresholds[intensity] . If not given, will be set to same value as nb.call_spots.gene_efficiency_intensity_thresh . intensity for a really intense spot is about 1 so intensity_thresh should be less than this. Default: None score_ref : number . Final accepted spots are those which pass quality_threshold which is nb.ref_spots.score > thresholds[score_ref] and nb.ref_spots.intensity > intensity_thresh . quality_threshold requires score computed with coppafish/call_spots/dot_prodduct/dot_product_score to exceed this. Max score is 1 so must be below this. Default: 0.25 score_omp : number . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) Max score is 1 so score_thresh should be less than this. 0.15 if more concerned for missed spots than false positives. Default: 0.263 score_omp_multiplier : number . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) 0.45 if more concerned for missed spots than false positives. Default: 0.95","title":"thresholds"},{"location":"config_setup/","text":"Setting up the Config File A config (.ini) file needs to be created for each experiment to run the pipeline. All parameters not specified in this file will inherit the default values . The parameters with Default = MUST BE SPECIFIED are the bare minimum parameters which need to be set in the experiment config file. If any section, or parameter within a section, is added to the config file which is not included in the default file , an error will be raised when it is loaded in. Some example config files for typical experiments are listed below. Example Config Files 3D 2D .npy Raw Data No Anchor QuadCam Separate Round [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = False anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt raw_extension = .npy raw_metadata = metadata [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True ref_round = 2 ref_channel = 4 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6, Exp1_r7, Exp1_r8 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 dye_names = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425, AF647, AF750 channel_camera = 405, 555, 470, 470, 555, 640, 555, 640, 640 channel_laser = 405, 405, 445, 470, 520, 520, 555, 640, 730 [file_names] notebook_name = sep_round_notebook input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles anchor = Exp1_sep_round code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 Note on Separate Round Config File The Separate Round config file above is used for registering an additional round to an experiment already run to completion (using a config file like the 3D one indicated above). The pipeline for the Separate Round case cannot be run further that the stitching section. The run_sep_reg function (script is here ) runs the pipeline for the Separate Round case and then registers the anchor_round / anchor_channel to the anchor_round / anchor_channel of the full experiment. These parameters are explained below and here . file_names If the names of the files change during the pipeline or if they are being accessed from another computer, the file_names section of the configuration file can also be changed as explained here . input_dir The input directory is the path to the folder which contains the raw data. Examples for the two possible cases of raw_extension are given below (i.e. these are respectively what the input directory looks like for the config files 3D and .npy Raw Data listed above). .nd2 .npy Differences with raw_extension = .npy It is assumed that when raw_extension = .npy , there were initial .nd2 files which contained excess information (e.g. extra channels). These were then converted to .npy files to get rid of this. For the .npy case, input_dir must also contain a metadata .json file. This contains the metadata extracted from the initial .nd2 files using the function save_metadata . An example metadata file is given here for an experiment with 3 tiles and 7 channels. Also, each name listed in the round parameter indicates a folder not a file. It is assumed these folders were produced using dask.array.to_npy_stack so the contents of each folder should contain a file named info and a .npy file for each tile, with the name being the index of the tile in the initial .nd2 file. An example showing the folder for the first round of a three tile experiment is given below: output_dir The output directory is the path to the folder that you would like the notebook.npz file containing the experiment results to be saved. The image below shows what the output directory typically looks like at the end of the experiment. The names of the files produced can be changed by changing the parameters big_anchor_image , big_dapi_image , notebook_name , omp_spot_coef , omp_spot_info and omp_spot_shape in the config file . tile_dir The tile directory is the path to the folder that you would like the filtered images for each tile, round and colour channel to be saved to. If is_3d == True , a .npy file will be produced for each round, tile and channel with the name for round r, tile T , channel C being config['file_names']['round'][r] _t T c C .npy with axis in the order z-y-x (the name for the anchor round, tile T , channel C will be config['file_names']['anchor'] _t T c C .npy). If is_3d == False , a .npy file will be produced for each round and tile called config['file_names']['round'][r] _t T .npy with axis in the order c-y-x. An example of what the tile directory looks like at the end of the experiment is shown below for a 3D and 2D experiment with 3 tiles and 7 channels: 3D 2D code_book This is the path to the file containing the code for each gene. The file should be a text file containing two columns, the first being the gene name. The second is the code specifying which dye that gene should appear in each round. Thus it is of length n_rounds , containing numbers in the range from 0 to n_dyes-1 inclusive. Example An example is given here so that if config['basic_info'][dye_names] = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425 the gene Sst with the code 6200265 will be expected to appear with the following dyes in each round: Round 0: ATTO425 Round 1: AF488 Round 2: DY405 Round 3: DY405 Round 4: AF488 Round 5: ATTO425 Round 6: AF594 basic_info anchor_channel The anchor_channel is the channel in the anchor_round which contains spots corresponding to all genes. These spots are used for registration to the imaging rounds and to determine the expected bled_code for each gene. ref_round If there is no anchor_round , ref_round must be specified instead and it should be a round which contains a lot of spots in each channel. Spots in ref_round / ref_channel will then be used as reference spots for registration and to determine the expected bled_code for each gene. If the anchor_round is used and ref_round is specified, ref_round will be set to anchor_round (last round) in the notebook . Problem with not using anchor With no anchor, the registration is likely to be worse because an imaging round is used as a reference. Thus, not all genes will appear in ref_round / ref_channel , but only those which appear with a dye in the ref_round which have high intensity in the ref_channel . Also, we would expect the final spots saved in nb.ref_spots to only correspond to genes appearing in ref_round / ref_channel and thus lots of genes will be missing. With an anchor though, we expect all genes to show up in anchor_round / anchor_channel . ref_channel If there is no anchor_round , ref_channel must be specified instead and it should be the channel in ref_round which contains the most spots. If the anchor_round is used and both anchor_channel and ref_channel are specified, ref_channel will be set to anchor_channel in the notebook . dapi_channel This is the channel in the anchor_round that contains the DAPI images. The tiles of this channel will be stitched together and saved in the config['file_names']['output_dir'] with a name config['file_names']['big_dapi_image'] . dapi_channel does not have to be included in config['basic_info']['use_channels'] as the anchor round is dealt with separately. To tophat filter the raw DAPI images first, either config['extract']['r_dapi'] or config['extract']['r_dapi_auto_microns'] must be specified. Specifying Dyes It is expected that each gene will appear with a single dye in a given round as indicated by config['file_names']['code_book'] . If dye_names is not specified, it is assumed as a starting point for the bleed_matrix calculation that the number of dyes is equal to the number of channels and dye 0 will only appear in channel 0, dye 1 will only appear in channel 1 etc. If dye_names is specified, both channel_camera and channel_laser must also be specified. This is so that a starting point for the bleed_matrix calculation can be obtained by reading off the expected intensity of each dye in each channel using the file config['file_names']['dye_camera_laser'] . The default config['file_names']['dye_camera_laser'] is given here but if a dye, camera or laser not indicated in this file are used in an experiment, a new version must be made. Common Additional Parameters There are a few other parameters that may often need to be different to those given in the default config file . extract[r_smooth] The parameter r_smooth in the extract section specifies whether to smooth with an averaging kernel after the raw images have been convolved with a difference of hanning kernel . This will make the extract section of the pipeline slower but will reduce the influence of anomalously high or low intensity pixels. It may be particularly appropriate to 3D data because the difference of hanning convolution is done independently on each z-plane but the smoothing can incorporate information between z-planes. Time for smoothing The size of r_smooth has big influence on time taken for smoothing. For a 2048 x 2048 x 50 image: r_smooth = 1, 1, 2 : 2.8 seconds r_smooth = 2, 2, 2 : 8.5 seconds The convolution with the difference of hanning kernel takes 4.1 seconds on the same image so smoothing will make the extract section of the pipeline significantly longer. By default, this is not specified meaning no smoothing is done. If smoothing is needed, typical values are: 2D: r_smooth = 2, 2 3D: r_smooth = 1, 1, 2 The kernel which the image is correlated with is then np . ones ( 2 * r_smooth - 1 ) / np . sum ( np . ones ( 2 * r_smooth - 1 )) so for r_smooth = 2, 2 it will be: array ([[ 0.11111111 , 0.11111111 , 0.11111111 ], [ 0.11111111 , 0.11111111 , 0.11111111 ], [ 0.11111111 , 0.11111111 , 0.11111111 ]]) The effect of smoothing can be seen using view_filter . extract[r_dapi] By default, no filtering will be applied to the dapi_channel image of the anchor_round and thus no .npy file will be saved to the tile_dir . This can be changed by specifying r_dapi which should be approximately the radius of a feature in the DAPI image (typical r_dapi is 48). In this case, a 2D tophat filtering will be performed using a kernel of radius r_dapi . Alternatively, r_dapi_auto_microns can be specified to be the radius of the kernel in units of microns and r_dapi will be computed automatically by converting this into units of yx-pixels (typical r_dapi_auto_microns is 8). Time for DAPI filtering The size of r_dapi has big influence on time taken for tophat filtering. For a 2048 x 2048 x 50 image: r_dapi = 48 : 142.4 seconds r_smooth = 12 : 3.9 seconds The tophat filtering is only done on one channel for each tile but it is quite slow so it may be best to avoid it, especially for experiments with lots of tiles. The effect of DAPI filtering can be seen using view_filter . stitch[expected_overlap] This is the expected fractional overlap between neighbouring tiles. By default, it is 0.1 meaning a 10% overlap is expected. thresholds The parameters in the thresholds section of the config file contains the thresholds used to determine which spots pass a quality thresholding process such that we consider their gene assignments legitimate. The default values are based on an experiment run with ground truth data, but they will likely need adjusting after investigating the effect of the thresholds using the Viewer . Using a subset of the raw data To run the pipeline with a subset of tiles, imaging rounds, channels or z-planes the following parameters can be set in the basic_info section of the configuration file: use_tiles ignore_tiles use_rounds use_channels use_z If midway through the pipeline, it is decided that a particular tile, round or channel is not worth using, it can be removed without re-running all the steps of the pipeline completed so far.","title":"Setting up the Config File"},{"location":"config_setup/#setting-up-the-config-file","text":"A config (.ini) file needs to be created for each experiment to run the pipeline. All parameters not specified in this file will inherit the default values . The parameters with Default = MUST BE SPECIFIED are the bare minimum parameters which need to be set in the experiment config file. If any section, or parameter within a section, is added to the config file which is not included in the default file , an error will be raised when it is loaded in. Some example config files for typical experiments are listed below.","title":"Setting up the Config File"},{"location":"config_setup/#example-config-files","text":"3D 2D .npy Raw Data No Anchor QuadCam Separate Round [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = False anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt raw_extension = .npy raw_metadata = metadata [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True ref_round = 2 ref_channel = 4 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6, Exp1_r7, Exp1_r8 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 dye_names = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425, AF647, AF750 channel_camera = 405, 555, 470, 470, 555, 640, 555, 640, 640 channel_laser = 405, 405, 445, 470, 520, 520, 555, 640, 730 [file_names] notebook_name = sep_round_notebook input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles anchor = Exp1_sep_round code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 Note on Separate Round Config File The Separate Round config file above is used for registering an additional round to an experiment already run to completion (using a config file like the 3D one indicated above). The pipeline for the Separate Round case cannot be run further that the stitching section. The run_sep_reg function (script is here ) runs the pipeline for the Separate Round case and then registers the anchor_round / anchor_channel to the anchor_round / anchor_channel of the full experiment. These parameters are explained below and here .","title":"Example Config Files"},{"location":"config_setup/#file_names","text":"If the names of the files change during the pipeline or if they are being accessed from another computer, the file_names section of the configuration file can also be changed as explained here .","title":"file_names"},{"location":"config_setup/#input_dir","text":"The input directory is the path to the folder which contains the raw data. Examples for the two possible cases of raw_extension are given below (i.e. these are respectively what the input directory looks like for the config files 3D and .npy Raw Data listed above). .nd2 .npy Differences with raw_extension = .npy It is assumed that when raw_extension = .npy , there were initial .nd2 files which contained excess information (e.g. extra channels). These were then converted to .npy files to get rid of this. For the .npy case, input_dir must also contain a metadata .json file. This contains the metadata extracted from the initial .nd2 files using the function save_metadata . An example metadata file is given here for an experiment with 3 tiles and 7 channels. Also, each name listed in the round parameter indicates a folder not a file. It is assumed these folders were produced using dask.array.to_npy_stack so the contents of each folder should contain a file named info and a .npy file for each tile, with the name being the index of the tile in the initial .nd2 file. An example showing the folder for the first round of a three tile experiment is given below:","title":"input_dir"},{"location":"config_setup/#output_dir","text":"The output directory is the path to the folder that you would like the notebook.npz file containing the experiment results to be saved. The image below shows what the output directory typically looks like at the end of the experiment. The names of the files produced can be changed by changing the parameters big_anchor_image , big_dapi_image , notebook_name , omp_spot_coef , omp_spot_info and omp_spot_shape in the config file .","title":"output_dir"},{"location":"config_setup/#tile_dir","text":"The tile directory is the path to the folder that you would like the filtered images for each tile, round and colour channel to be saved to. If is_3d == True , a .npy file will be produced for each round, tile and channel with the name for round r, tile T , channel C being config['file_names']['round'][r] _t T c C .npy with axis in the order z-y-x (the name for the anchor round, tile T , channel C will be config['file_names']['anchor'] _t T c C .npy). If is_3d == False , a .npy file will be produced for each round and tile called config['file_names']['round'][r] _t T .npy with axis in the order c-y-x. An example of what the tile directory looks like at the end of the experiment is shown below for a 3D and 2D experiment with 3 tiles and 7 channels: 3D 2D","title":"tile_dir"},{"location":"config_setup/#code_book","text":"This is the path to the file containing the code for each gene. The file should be a text file containing two columns, the first being the gene name. The second is the code specifying which dye that gene should appear in each round. Thus it is of length n_rounds , containing numbers in the range from 0 to n_dyes-1 inclusive. Example An example is given here so that if config['basic_info'][dye_names] = DY405, CF405L, AF488, DY520XL, AF532, AF594, ATTO425 the gene Sst with the code 6200265 will be expected to appear with the following dyes in each round: Round 0: ATTO425 Round 1: AF488 Round 2: DY405 Round 3: DY405 Round 4: AF488 Round 5: ATTO425 Round 6: AF594","title":"code_book"},{"location":"config_setup/#basic_info","text":"","title":"basic_info"},{"location":"config_setup/#anchor_channel","text":"The anchor_channel is the channel in the anchor_round which contains spots corresponding to all genes. These spots are used for registration to the imaging rounds and to determine the expected bled_code for each gene.","title":"anchor_channel"},{"location":"config_setup/#ref_round","text":"If there is no anchor_round , ref_round must be specified instead and it should be a round which contains a lot of spots in each channel. Spots in ref_round / ref_channel will then be used as reference spots for registration and to determine the expected bled_code for each gene. If the anchor_round is used and ref_round is specified, ref_round will be set to anchor_round (last round) in the notebook . Problem with not using anchor With no anchor, the registration is likely to be worse because an imaging round is used as a reference. Thus, not all genes will appear in ref_round / ref_channel , but only those which appear with a dye in the ref_round which have high intensity in the ref_channel . Also, we would expect the final spots saved in nb.ref_spots to only correspond to genes appearing in ref_round / ref_channel and thus lots of genes will be missing. With an anchor though, we expect all genes to show up in anchor_round / anchor_channel .","title":"ref_round"},{"location":"config_setup/#ref_channel","text":"If there is no anchor_round , ref_channel must be specified instead and it should be the channel in ref_round which contains the most spots. If the anchor_round is used and both anchor_channel and ref_channel are specified, ref_channel will be set to anchor_channel in the notebook .","title":"ref_channel"},{"location":"config_setup/#dapi_channel","text":"This is the channel in the anchor_round that contains the DAPI images. The tiles of this channel will be stitched together and saved in the config['file_names']['output_dir'] with a name config['file_names']['big_dapi_image'] . dapi_channel does not have to be included in config['basic_info']['use_channels'] as the anchor round is dealt with separately. To tophat filter the raw DAPI images first, either config['extract']['r_dapi'] or config['extract']['r_dapi_auto_microns'] must be specified.","title":"dapi_channel"},{"location":"config_setup/#specifying-dyes","text":"It is expected that each gene will appear with a single dye in a given round as indicated by config['file_names']['code_book'] . If dye_names is not specified, it is assumed as a starting point for the bleed_matrix calculation that the number of dyes is equal to the number of channels and dye 0 will only appear in channel 0, dye 1 will only appear in channel 1 etc. If dye_names is specified, both channel_camera and channel_laser must also be specified. This is so that a starting point for the bleed_matrix calculation can be obtained by reading off the expected intensity of each dye in each channel using the file config['file_names']['dye_camera_laser'] . The default config['file_names']['dye_camera_laser'] is given here but if a dye, camera or laser not indicated in this file are used in an experiment, a new version must be made.","title":"Specifying Dyes"},{"location":"config_setup/#common-additional-parameters","text":"There are a few other parameters that may often need to be different to those given in the default config file .","title":"Common Additional Parameters"},{"location":"config_setup/#extractr_smooth","text":"The parameter r_smooth in the extract section specifies whether to smooth with an averaging kernel after the raw images have been convolved with a difference of hanning kernel . This will make the extract section of the pipeline slower but will reduce the influence of anomalously high or low intensity pixels. It may be particularly appropriate to 3D data because the difference of hanning convolution is done independently on each z-plane but the smoothing can incorporate information between z-planes. Time for smoothing The size of r_smooth has big influence on time taken for smoothing. For a 2048 x 2048 x 50 image: r_smooth = 1, 1, 2 : 2.8 seconds r_smooth = 2, 2, 2 : 8.5 seconds The convolution with the difference of hanning kernel takes 4.1 seconds on the same image so smoothing will make the extract section of the pipeline significantly longer. By default, this is not specified meaning no smoothing is done. If smoothing is needed, typical values are: 2D: r_smooth = 2, 2 3D: r_smooth = 1, 1, 2 The kernel which the image is correlated with is then np . ones ( 2 * r_smooth - 1 ) / np . sum ( np . ones ( 2 * r_smooth - 1 )) so for r_smooth = 2, 2 it will be: array ([[ 0.11111111 , 0.11111111 , 0.11111111 ], [ 0.11111111 , 0.11111111 , 0.11111111 ], [ 0.11111111 , 0.11111111 , 0.11111111 ]]) The effect of smoothing can be seen using view_filter .","title":"extract[r_smooth]"},{"location":"config_setup/#extractr_dapi","text":"By default, no filtering will be applied to the dapi_channel image of the anchor_round and thus no .npy file will be saved to the tile_dir . This can be changed by specifying r_dapi which should be approximately the radius of a feature in the DAPI image (typical r_dapi is 48). In this case, a 2D tophat filtering will be performed using a kernel of radius r_dapi . Alternatively, r_dapi_auto_microns can be specified to be the radius of the kernel in units of microns and r_dapi will be computed automatically by converting this into units of yx-pixels (typical r_dapi_auto_microns is 8). Time for DAPI filtering The size of r_dapi has big influence on time taken for tophat filtering. For a 2048 x 2048 x 50 image: r_dapi = 48 : 142.4 seconds r_smooth = 12 : 3.9 seconds The tophat filtering is only done on one channel for each tile but it is quite slow so it may be best to avoid it, especially for experiments with lots of tiles. The effect of DAPI filtering can be seen using view_filter .","title":"extract[r_dapi]"},{"location":"config_setup/#stitchexpected_overlap","text":"This is the expected fractional overlap between neighbouring tiles. By default, it is 0.1 meaning a 10% overlap is expected.","title":"stitch[expected_overlap]"},{"location":"config_setup/#thresholds","text":"The parameters in the thresholds section of the config file contains the thresholds used to determine which spots pass a quality thresholding process such that we consider their gene assignments legitimate. The default values are based on an experiment run with ground truth data, but they will likely need adjusting after investigating the effect of the thresholds using the Viewer .","title":"thresholds"},{"location":"config_setup/#using-a-subset-of-the-raw-data","text":"To run the pipeline with a subset of tiles, imaging rounds, channels or z-planes the following parameters can be set in the basic_info section of the configuration file: use_tiles ignore_tiles use_rounds use_channels use_z If midway through the pipeline, it is decided that a particular tile, round or channel is not worth using, it can be removed without re-running all the steps of the pipeline completed so far.","title":"Using a subset of the raw data"},{"location":"notebook/","text":"Notebook The Notebook is a write-once data structure which is saved as a npz file. It stores the output of each stage of the pipeline as a separate NotebookPage . Each NotebookPage of the Notebook is itself a write-once data structure. Each NotebookPage may contain many different variables. Times saved to Notebook and NotebookPage Whenever a variable is added to a NotebookPage , in addition to saving the value, it saves the time at which the variable was added ( nbp._times ). Likewise, the time at which a NotebookPage is created ( nbp._time_created ), and the time at which it is added to the Notebook ( nb._page_times ) are also recorded automatically. The time the Notebook was created is also recorded ( nb._created_time ). This both serves as a record of what was done, as well as a source for debugging and optimization. Conceptually, the idea is that a Notebook is like a lab notebook. In a lab notebook, you write things in a separate section (here, NotebookPage ) for each part of the experiment with the appropriate section name. You only add, you never erase or modify. Lab notebooks contain intermediate results, as well as the main data collected during the experiment. All times and labels of all results are written down. Create Notebook To create a Notebook , pass it the path to the file where the Notebook is to be saved ( /Users/user/coppafish/experiment/notebook.npz ) and the path to the configuration file ( /Users/user/coppafish/experiment/settings.ini ): from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( nb_file , ini_file ) Create just using config_file The Notebook can also be created with just the configuration file through: from coppafish import Notebook ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) The location where the Notebook is saved (nb._file) will then be set to: config['file_names']['output_dir'] + config['file_names']['notebook_name'] . If nb_file already exists, the Notebook located at this path will be loaded. If not, a new file will be created as soon as the first NotebookPage is added to the Notebook . When the Notebook is created, it will save the contents of the configuration file ( nb._config ) thus there is no need to pass the config_file argument when re-loading a Notebook . You can just run nb = Notebook('/Users/user/coppafish/experiment/notebook.npz') . Using Notebook outside the coppaFISH pipeline Passing the configuration file to the Notebook allows for several features , however a Notebook can be created without it: from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) You can then still add NotebookPages to the Notebook as normal. Adding a NotebookPage To add a NotebookPage called page_name with variable var_1 = 5 to the Notebook , you can do the following: from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nb += nbp # or nb.add_page(nbp) or nb.page_name = nbp Whenever a NotebookPage is added to the Notebook , it will trigger the Notebook to be saved (unless the NotebookPage has a name listed in Notebook._no_save_pages ). The variable var_1 of the NotebookPage called page_name can then be accessed from the Notebook via nb.page_name.var_1 . Adding variables to Notebook Varibles of forms other than NotebookPages can be added directly to the Notebook e.g. nb.var_1 = 5 . However, when the Notebook is saved and re-loaded, variables added in this way will no longer be present. Deleting a NotebookPage To delete a NotebookPage called page_name which is in the Notebook , run del nb.page_name . You may want to do this, for example, to re-run a section of the pipeline with different parameters in the corresponding section of the configuration file. Modifying a NotebookPage The NotebookPage is a write-once data structure so once a variable has been added to it, it cannot be changed (unless it is listed in NotebookPage._NON_RESULT_KEYS ). I.e. the following will raise an error: from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nbp . var_1 = 10 Once a NotebookPage has been added to a Notebook , nbp.finalized will change to True and no more variables will be allowed to be added to the NotebookPage : from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nbp . var_2 = 10 # fine as page not added to notebook yet nb += nbp nb . page_name . var_3 = 99 # will raise error as page is added to notebook now To delete the variable var_1 run del nb.var_1 . Again, you won't be able to do this once the NotebookPage has been added to a Notebook . coppafish Specific NotebookPages The names of all the NotebookPages added to the Notebook through the course of the pipeline are given as the headers in the notebook_comments.json file . Then the bullet points give all the variables that are added to each NotebookPage . When a NotebookPage has one of these names, an error will be raised if you try to assign a variable to it which is not listed in the relevant section of the notebook_comments.json file . When adding the NotebookPage to the Notebook , an error will be raised unless it contains all the variables listed in the relevant section of the notebook_comments.json file and no others. Examples of adding a NotebookPage named thresholds to a Notebook are given below: \u2705 \u274c Error adding variable to NotebookPage \u274c Error adding NotebookPage to Notebook from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . score_ref = 0.25 nbp . score_omp = 0.263 nbp . score_omp_multiplier = 0.95 nb += nbp from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . var_1 = 5 # Error here as 'var_1' is not listed in # 'thresholds' section of notebook_comments.json from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . score_ref = 0.25 = 5 nb += nbp # Error here as 'score_omp' and 'score_omp_multiplier' # are listed in 'thresholds' section of # notebook_comments.json but not added to page. Describe The comments given in the notebook_comments.json file can be accessed from the NotebookPage by calling the describe function. An example to print the comment for the variable gene_no in the omp page is given below: Code Output nb . omp . describe ( 'gene_no' ) Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. If describe is called from the Notebook instead, it will loop through all NotebookPages in the Notebook and print the comment for each variable with the correct name that it encounters: Code Output nb . describe ( 'gene_no' ) gene_no in ref_spots: Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. gene_no in omp: Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. If describe is called from the Notebook and finds the variable in the configuration file, it will print the section it was found in and its value. E.g. for dp_thresh in the omp section: Code Output nb . describe ( 'dp_thresh' ) No variable named dp_thresh in the omp page. But it is in the omp section of the config file and has value: 0.225 Configuration File The configuration file can be returned as a dictionary of dictionaries from the Notebook by using the function get_config : Code config config = nb . get_config () When the Notebook is re-loaded with a config_file ( nb = Notebook(nb_file, config_file) ), the configuration file supplied will be compared to the one saved in the Notebook ( nb._config ). If the comparison indicates that the two are different, an error will be raised. Otherwise, when the Notebook is loaded, the saved value of the configuration file ( nb._config ) will be changed to the one given by the provided config_file . What is compared? Each NotebookPage added during the coppafish pipeline has a name which is the same as a section in the configuration file or the same apart from a _debug suffix . Only sections with a corresponding NotebookPage in the Notebook are compared. The file_names section is also ignored in the comparison as it is included in Notebook._no_compare_config_sections . So if the pipeline has been run as far as the call_reference_spots stage, the Notebook will not have the omp page . In this case, the omp section of the config_file can be changed without causing an error as indicated below: nb._config (saved to Notebook ) \u2705 Allowed config_file \u274c config_file giving error [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [omp] dp_thresh = 0.3452 ; Allowed because variable is in the omp section ; and omp page not added to Notebook yet. [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [register] n_iter = 52 ; Not allowed because variable is in the register section ; but register page has been added to Notebook. Changing basic_info mid-pipeline It is quite common to want to change the basic_info section of the configuration file halfway through the pipeline without re-running the steps of the pipeline which have already been completed. For example, we may have specified the wrong dye_names , but this is not used until the call_reference_spots stage. Or after the find_spots or register sections, we may want to remove some problematic tiles, rounds or channels (through use_tiles , use_rounds and use_channels ). But if the basic_info section of the configuration file is changed, an error would be raised unless the basic_info NotebookPage is deleted. The code below illustrates how to save a new Notebook with a different basic_info page: Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini from coppafish import Notebook from coppafish.pipeline import set_basic_info nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) print ( 'Using config file saved to notebook:' ) print ( f \"use_channels: { nb . basic_info . use_channels } \" ) print ( f \"use_tiles: { nb . basic_info . use_tiles } \" ) # Change basic_info del nb . basic_info # delete old basic info nb . save ( nb_file_new ) # save Notebook with no basic_info page to new file # so does not overwrite old Notebook # Load in new notebook with new config file with different basic_info nb_new = Notebook ( nb_file_new , ini_file_new ) # add new basic_info page to Notebook config = nb_new . get_config () nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb_new += nbp_basic print ( f 'Using new config file { ini_file_new } :' ) print ( f \"use_channels: { nb_new . basic_info . use_channels } \" ) print ( f \"use_tiles: { nb_new . basic_info . use_tiles } \" ) Using config file saved to notebook: use_channels: [0, 1, 2, 3, 4, 5, 6] use_tiles: [0, 1, 2, 3] Using new config file /Users/user/coppafish/experiment/settings_new.ini: use_channels: [1, 2, 5, 6] use_tiles: [0, 2, 3] [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 use_channels = 1, 2, 5, 6 use_tiles = 0, 2, 3 file_names The file_names section of the Notebook is treated differently to deal with the case where the various file locations have changed e.g. when accessing them from a different computer. The file_names NotebookPage is never saved when the Notebook is saved, and adding a NotebookPage called file_names does not trigger a save. When the Notebook is loaded in, a file_names NotebookPage will automatically be created and added to the Notebook if the Notebook contains a basic_info NotebookPage . The file_names NotebookPage will then inherit information from the file_names section of the config_file which was passed to the Notebook when loading it, in as explained below: Code Output nb._config (saved to Notebook ) /Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' ini_file = '/Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini' # config_file not given so will use last one saved to Notebook nb_old = Notebook ( nb_file ) print ( 'Using config file saved to notebook:' ) print ( nb_old . file_names . output_dir ) print ( nb_old . file_names . big_anchor_image ) # tile file path for round 0, tile 0, channel 0 print ( nb_old . file_names . tile [ 0 ][ 0 ][ 0 ]) # config_file given so will update nb._config nb_new = Notebook ( nb_file , ini_file ) print ( f 'Using new config file { ini_file } :' ) print ( nb_new . file_names . output_dir ) print ( nb_new . file_names . big_anchor_image ) # tile file path for round 0, tile 0, channel 0 print ( nb_new . file_names . tile [ 0 ][ 0 ][ 0 ]) Using config file saved to notebook: /Users/user/coppafish/experiment1/output /Users/user/coppafish/experiment1/output/anchor_image.npz /Users/user/coppafish/experiment1/tiles/Exp1_r0_t0c0.npy Using new config file /Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini: /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output/anchor_image.npz /Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles/Exp1_r0_t0c0.npy [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/raw output_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output tile_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 Also, as soon as a NotebookPage named basic_info is added to the Notebook , the file_names NotebookPage will also be added due to information in the Notebook._no_save_pages dictionary.","title":"Notebook"},{"location":"notebook/#notebook","text":"The Notebook is a write-once data structure which is saved as a npz file. It stores the output of each stage of the pipeline as a separate NotebookPage . Each NotebookPage of the Notebook is itself a write-once data structure. Each NotebookPage may contain many different variables. Times saved to Notebook and NotebookPage Whenever a variable is added to a NotebookPage , in addition to saving the value, it saves the time at which the variable was added ( nbp._times ). Likewise, the time at which a NotebookPage is created ( nbp._time_created ), and the time at which it is added to the Notebook ( nb._page_times ) are also recorded automatically. The time the Notebook was created is also recorded ( nb._created_time ). This both serves as a record of what was done, as well as a source for debugging and optimization. Conceptually, the idea is that a Notebook is like a lab notebook. In a lab notebook, you write things in a separate section (here, NotebookPage ) for each part of the experiment with the appropriate section name. You only add, you never erase or modify. Lab notebooks contain intermediate results, as well as the main data collected during the experiment. All times and labels of all results are written down.","title":"Notebook"},{"location":"notebook/#create-notebook","text":"To create a Notebook , pass it the path to the file where the Notebook is to be saved ( /Users/user/coppafish/experiment/notebook.npz ) and the path to the configuration file ( /Users/user/coppafish/experiment/settings.ini ): from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( nb_file , ini_file ) Create just using config_file The Notebook can also be created with just the configuration file through: from coppafish import Notebook ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) The location where the Notebook is saved (nb._file) will then be set to: config['file_names']['output_dir'] + config['file_names']['notebook_name'] . If nb_file already exists, the Notebook located at this path will be loaded. If not, a new file will be created as soon as the first NotebookPage is added to the Notebook . When the Notebook is created, it will save the contents of the configuration file ( nb._config ) thus there is no need to pass the config_file argument when re-loading a Notebook . You can just run nb = Notebook('/Users/user/coppafish/experiment/notebook.npz') . Using Notebook outside the coppaFISH pipeline Passing the configuration file to the Notebook allows for several features , however a Notebook can be created without it: from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) You can then still add NotebookPages to the Notebook as normal.","title":"Create Notebook"},{"location":"notebook/#adding-a-notebookpage","text":"To add a NotebookPage called page_name with variable var_1 = 5 to the Notebook , you can do the following: from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nb += nbp # or nb.add_page(nbp) or nb.page_name = nbp Whenever a NotebookPage is added to the Notebook , it will trigger the Notebook to be saved (unless the NotebookPage has a name listed in Notebook._no_save_pages ). The variable var_1 of the NotebookPage called page_name can then be accessed from the Notebook via nb.page_name.var_1 . Adding variables to Notebook Varibles of forms other than NotebookPages can be added directly to the Notebook e.g. nb.var_1 = 5 . However, when the Notebook is saved and re-loaded, variables added in this way will no longer be present.","title":"Adding a NotebookPage"},{"location":"notebook/#deleting-a-notebookpage","text":"To delete a NotebookPage called page_name which is in the Notebook , run del nb.page_name . You may want to do this, for example, to re-run a section of the pipeline with different parameters in the corresponding section of the configuration file.","title":"Deleting a NotebookPage"},{"location":"notebook/#modifying-a-notebookpage","text":"The NotebookPage is a write-once data structure so once a variable has been added to it, it cannot be changed (unless it is listed in NotebookPage._NON_RESULT_KEYS ). I.e. the following will raise an error: from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nbp . var_1 = 10 Once a NotebookPage has been added to a Notebook , nbp.finalized will change to True and no more variables will be allowed to be added to the NotebookPage : from coppafish import NotebookPage nbp = NotebookPage ( 'page_name' ) nbp . var_1 = 5 nbp . var_2 = 10 # fine as page not added to notebook yet nb += nbp nb . page_name . var_3 = 99 # will raise error as page is added to notebook now To delete the variable var_1 run del nb.var_1 . Again, you won't be able to do this once the NotebookPage has been added to a Notebook .","title":"Modifying a NotebookPage"},{"location":"notebook/#coppafish-specific-notebookpages","text":"The names of all the NotebookPages added to the Notebook through the course of the pipeline are given as the headers in the notebook_comments.json file . Then the bullet points give all the variables that are added to each NotebookPage . When a NotebookPage has one of these names, an error will be raised if you try to assign a variable to it which is not listed in the relevant section of the notebook_comments.json file . When adding the NotebookPage to the Notebook , an error will be raised unless it contains all the variables listed in the relevant section of the notebook_comments.json file and no others. Examples of adding a NotebookPage named thresholds to a Notebook are given below: \u2705 \u274c Error adding variable to NotebookPage \u274c Error adding NotebookPage to Notebook from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . score_ref = 0.25 nbp . score_omp = 0.263 nbp . score_omp_multiplier = 0.95 nb += nbp from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . var_1 = 5 # Error here as 'var_1' is not listed in # 'thresholds' section of notebook_comments.json from coppafish import NotebookPage nbp = NotebookPage ( 'thresholds' ) nbp . intensity = 0.01 nbp . score_ref = 0.25 = 5 nb += nbp # Error here as 'score_omp' and 'score_omp_multiplier' # are listed in 'thresholds' section of # notebook_comments.json but not added to page.","title":"coppafish Specific NotebookPages"},{"location":"notebook/#describe","text":"The comments given in the notebook_comments.json file can be accessed from the NotebookPage by calling the describe function. An example to print the comment for the variable gene_no in the omp page is given below: Code Output nb . omp . describe ( 'gene_no' ) Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. If describe is called from the Notebook instead, it will loop through all NotebookPages in the Notebook and print the comment for each variable with the correct name that it encounters: Code Output nb . describe ( 'gene_no' ) gene_no in ref_spots: Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. gene_no in omp: Numpy int16 array [n_spots] gene_no[s] is the index of the gene assigned to spot s. If describe is called from the Notebook and finds the variable in the configuration file, it will print the section it was found in and its value. E.g. for dp_thresh in the omp section: Code Output nb . describe ( 'dp_thresh' ) No variable named dp_thresh in the omp page. But it is in the omp section of the config file and has value: 0.225","title":"Describe"},{"location":"notebook/#configuration-file","text":"The configuration file can be returned as a dictionary of dictionaries from the Notebook by using the function get_config : Code config config = nb . get_config () When the Notebook is re-loaded with a config_file ( nb = Notebook(nb_file, config_file) ), the configuration file supplied will be compared to the one saved in the Notebook ( nb._config ). If the comparison indicates that the two are different, an error will be raised. Otherwise, when the Notebook is loaded, the saved value of the configuration file ( nb._config ) will be changed to the one given by the provided config_file . What is compared? Each NotebookPage added during the coppafish pipeline has a name which is the same as a section in the configuration file or the same apart from a _debug suffix . Only sections with a corresponding NotebookPage in the Notebook are compared. The file_names section is also ignored in the comparison as it is included in Notebook._no_compare_config_sections . So if the pipeline has been run as far as the call_reference_spots stage, the Notebook will not have the omp page . In this case, the omp section of the config_file can be changed without causing an error as indicated below: nb._config (saved to Notebook ) \u2705 Allowed config_file \u274c config_file giving error [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [omp] dp_thresh = 0.3452 ; Allowed because variable is in the omp section ; and omp page not added to Notebook yet. [file_names] input_dir = /Users/.../experiment1/raw output_dir = /Users/.../experiment1/output tile_dir = /Users/.../experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/.../experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [register] n_iter = 52 ; Not allowed because variable is in the register section ; but register page has been added to Notebook.","title":"Configuration File"},{"location":"notebook/#changing-basic_info-mid-pipeline","text":"It is quite common to want to change the basic_info section of the configuration file halfway through the pipeline without re-running the steps of the pipeline which have already been completed. For example, we may have specified the wrong dye_names , but this is not used until the call_reference_spots stage. Or after the find_spots or register sections, we may want to remove some problematic tiles, rounds or channels (through use_tiles , use_rounds and use_channels ). But if the basic_info section of the configuration file is changed, an error would be raised unless the basic_info NotebookPage is deleted. The code below illustrates how to save a new Notebook with a different basic_info page: Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini from coppafish import Notebook from coppafish.pipeline import set_basic_info nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) print ( 'Using config file saved to notebook:' ) print ( f \"use_channels: { nb . basic_info . use_channels } \" ) print ( f \"use_tiles: { nb . basic_info . use_tiles } \" ) # Change basic_info del nb . basic_info # delete old basic info nb . save ( nb_file_new ) # save Notebook with no basic_info page to new file # so does not overwrite old Notebook # Load in new notebook with new config file with different basic_info nb_new = Notebook ( nb_file_new , ini_file_new ) # add new basic_info page to Notebook config = nb_new . get_config () nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb_new += nbp_basic print ( f 'Using new config file { ini_file_new } :' ) print ( f \"use_channels: { nb_new . basic_info . use_channels } \" ) print ( f \"use_tiles: { nb_new . basic_info . use_tiles } \" ) Using config file saved to notebook: use_channels: [0, 1, 2, 3, 4, 5, 6] use_tiles: [0, 1, 2, 3] Using new config file /Users/user/coppafish/experiment/settings_new.ini: use_channels: [1, 2, 5, 6] use_tiles: [0, 2, 3] [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 use_channels = 1, 2, 5, 6 use_tiles = 0, 2, 3","title":"Changing basic_info mid-pipeline"},{"location":"notebook/#file_names","text":"The file_names section of the Notebook is treated differently to deal with the case where the various file locations have changed e.g. when accessing them from a different computer. The file_names NotebookPage is never saved when the Notebook is saved, and adding a NotebookPage called file_names does not trigger a save. When the Notebook is loaded in, a file_names NotebookPage will automatically be created and added to the Notebook if the Notebook contains a basic_info NotebookPage . The file_names NotebookPage will then inherit information from the file_names section of the config_file which was passed to the Notebook when loading it, in as explained below: Code Output nb._config (saved to Notebook ) /Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini from coppafish import Notebook nb_file = '/Users/user/coppafish/experiment/notebook.npz' ini_file = '/Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini' # config_file not given so will use last one saved to Notebook nb_old = Notebook ( nb_file ) print ( 'Using config file saved to notebook:' ) print ( nb_old . file_names . output_dir ) print ( nb_old . file_names . big_anchor_image ) # tile file path for round 0, tile 0, channel 0 print ( nb_old . file_names . tile [ 0 ][ 0 ][ 0 ]) # config_file given so will update nb._config nb_new = Notebook ( nb_file , ini_file ) print ( f 'Using new config file { ini_file } :' ) print ( nb_new . file_names . output_dir ) print ( nb_new . file_names . big_anchor_image ) # tile file path for round 0, tile 0, channel 0 print ( nb_new . file_names . tile [ 0 ][ 0 ][ 0 ]) Using config file saved to notebook: /Users/user/coppafish/experiment1/output /Users/user/coppafish/experiment1/output/anchor_image.npz /Users/user/coppafish/experiment1/tiles/Exp1_r0_t0c0.npy Using new config file /Users/NEW_USER/coppafish/NEW_EXPERIMENT/settings.ini: /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output/anchor_image.npz /Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles/Exp1_r0_t0c0.npy [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/raw output_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/output tile_dir = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/NEW_USER/coppafish/NEW_EXPERIMENT/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 Also, as soon as a NotebookPage named basic_info is added to the Notebook , the file_names NotebookPage will also be added due to information in the Notebook._no_save_pages dictionary.","title":"file_names"},{"location":"notebook_comments/","text":"Notebook Comments file_names file_names page contains all files that are used throughout the pipeline. tile is the only variable not included in the config file and is worked out automatically. Page added to Notebook automatically as soon as basic_info page added. input_dir : Directory . Where raw nd2 files are output_dir : Directory . Where notebook is saved tile_dir : Directory . Where tile npy files saved round : List [n_rounds] . Names of nd2 files for the imaging rounds. If not using, will be an empty list. anchor : String or None . Name of nd2 file for the anchor round. None if anchor not used raw_extension : String . .nd2 or .npy indicating the data type of the raw data. raw_metadata : String or None . If raw_extension = .npy , this is the name of the json file in input_dir which contains the required metadata extracted from the initial nd2 files. I.e. it is the output of coppafish/utils/nd2/save_metadata dye_camera_laser : File . csv file giving the approximate raw intensity for each dye with each camera/laser combination code_book : File . Text file which contains the codes indicating which dye to expect on each round for each gene scale : File . Text file saved containing the extract['scale'] and extract['scale_anchor'] values used to create the tile npy files in the tile_dir . If the second value is 0, it means extract['scale_anchor'] has not been calculated yet. If the extract step of the pipeline is re-run with extract['scale'] or extract['scale_anchor'] different to values saved here, an error will be raised. psf : File or None . npy file indicating average spot shape (before padding and scaled to fill uint16 range). Will be None if 2D pipeline used. File won't exist/used if config['extract']['deconvolve'] = False . If 3D , 1st axis in npy file is z. omp_spot_shape : File . npy file indicating average spot shape in OMP coefficient sign images. Saved image is int8 npy with only values being -1, 0, 1. omp_spot_info : File . After each tile is finished in OMP , information about spots found is saved as array to npy file: numpy int16 array [n_spots x 7] containing \\(y\\) , \\(x\\) , \\(z\\) , gene_no , n_pos_neighb , n_neg_neighb , tile . If 3D , 1st axis in npy file is z. omp_spot_coef : File . After each tile is finished in OMP , coefficients for all spots found is saved as sparse csr_matrix to npz file: CSR_matrix float [n_spots x n_genes] giving coefficient found for each gene for each spot. big_dapi_image : File or None . npz file of stitched DAPI image. None if nb.basic_info.dapi_channel = None If 3D , 1st axis in npz file is z. big_anchor_image : File . npz file of stitched image of ref_round / ref_channel . Will be stitched anchor if anchor used. If 3D , 1st axis in npz file is z. pciseq : List of 2 files . csv files where plotting information for pciSeq is saved. pciseq[0] is the path where the OMP method output will be saved. pciseq[1] is the path where the ref_spots method output will be saved. If files don't exist, they will be created when the function coppafish/export_to_pciseq is run. tile : List of numpy string arrays [n_tiles][(n_rounds + n_extra_rounds) {x n_channels if 3d}] . 2D : tile[t][r] is the npy file containing all channels of tile \\(t\\) , round \\(r\\) . 3D : tile[t][r][c] is the npy file containing all z planes for tile \\(t\\) , round \\(r\\) , channel \\(c\\) basic_info basic_info page contains information that is used at all stages of the pipeline. Page added to Notebook in pipeline/basic_info.py is_3d : Boolean . True if 3D pipeline used, False if 2D anchor_round : Integer or None . Index of anchor round (typically the first round after imaging rounds so anchor_round = n_rounds ). None if anchor not used. anchor_channel : Integer or None . Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. None if anchor not used. dapi_channel : Integer or None . Channel in anchor round that contains DAPI images. None if no DAPI . ref_round : Integer . Round to align all imaging rounds to. Will be anchor if using. ref_channel : Integer . Channel in reference round used as reference and to build coordinate system on. Usually channel with most spots. Will be anchor_channel if using anchor round use_channels : Integer List [n_use_channels] . Channels in imaging rounds to use throughout pipeline. use_rounds : Integer List [n_use_rounds] . Imaging rounds to use throughout pipeline. use_z : Integer List [nz] . z planes used to make tile npy files use_tiles : Integer List [n_use_tiles] . Tiles to use throughout pipeline. For an experiment where the tiles are arranged in a \\(4 \\times 3\\) ( \\(n_y \\times n_x\\) ) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | use_dyes : Integer List [n_use_dyes] . Dyes to use when assigning spots to genes. dye_names : String List [n_dyes] or None . Names of all dyes so for gene with code \\(360...\\) , gene appears with dye_names[3] in round \\(0\\) , dye_names[6] in round \\(1\\) , dye_names[0] in round \\(2\\) etc. None if each channel corresponds to a different dye. channel_camera : Integer List [n_channels] or None . channel_camera[i] is the wavelength in nm of the camera on channel \\(i\\) . None if dye_names = None . channel_laser : Integer List [n_channels] or None . channel_laser[i] is the wavelength in nm of the laser on channel \\(i\\) . None if dye_names = None . tile_pixel_value_shift : Integer . This is added onto every tile (except DAPI ) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to npy as uint16 . Typical=15000 n_extra_rounds : Integer . Number of non-imaging rounds, typically 1 if using anchor and 0 if not. n_rounds : Integer . Number of imaging rounds in the raw data tile_sz : Integer . \\(yx\\) dimension of tiles in pixels n_tiles : Integer . Number of tiles in the raw data n_channels : Integer . Number of channels in the raw data nz : Integer . Number of z-planes used to make the npy tile images (can be different from number in raw data). n_dyes : Integer . Number of dyes used tile_centre : Numpy float array [3] . [y, x, z] location of tile centre in units of [yx_pixels, yx_pixels, z_pixels] . For 2D pipeline, tile_centre[2] = 0 tilepos_yx_nd2 : Numpy integer array [n_tiles x 2] . tilepos_yx_nd2[i, :] is the \\(yx\\) position of tile with fov index \\(i\\) in the nd2 file. Index 0 refers to YX = [0, 0] Index 1 refers to YX = [0, 1] if MaxX > 0 tilepos_yx : Numpy integer array [n_tiles x 2] . tilepos_yx[i, :] is the \\(yx\\) position of tile with tile directory ( npy files) index \\(i\\) . Equally, tilepos_yx[use_tiles[i], :] is \\(yx\\) position of tile use_tiles[i] . Index 0 refers to YX = [MaxY, MaxX] Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 pixel_size_xy : Float . \\(yx\\) pixel size in microns pixel_size_z : Float . \\(z\\) pixel size in microns use_anchor : Boolean . True if anchor round is used, False if not. extract The extract page contains variables from extract_and_filter step which are used later in the pipeline. auto_thresh is used in find_spots step. hist_values and hist_counts are used for normalisation between channels in the call_reference_spots step. Page added to Notebook in pipeline/extract_run.py auto_thresh : Numpy float array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . auto_thresh[t, r, c] is the threshold spot intensity for tile \\(t\\) , round \\(r\\) , channel \\(c\\) used for spot detection in the find_spots step of the pipeline. hist_values : Numpy integer array [n_pixel_values] . All possible pixel values in saved npy images i.e. length is approx np.iinfo(np.uint16).max hist_counts : Numpy integer array [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round \\(r\\) , channel \\(c\\) which had the value hist_values[i] . extract_debug extract_debug page stores variables from extract_and_filter step which are not needed later in the pipeline but may be useful for debugging purposes. Page added to Notebook in pipeline/extract_run.py n_clip_pixels : Numpy integer array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . n_clip_pixels[t, r, c] is the number of pixels in the saved npy tile for tile \\(t\\) , round \\(r\\) , channel \\(c\\) which had intensity exceeding max uint16 value and so had to be clipped. clip_extract_scale : Numpy float array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . clip_extract_scale[t, r, c] is the recommended value for extract_scale such that for tile \\(t\\) , round \\(r\\) , channel \\(c\\) n_clip_pixels[t, r, c] would be 0. Only computed for images where n_clip_pixels[t, r, c] > 0 r1 : Integer . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. By default this is 0.5 micron converted to yx-pixel units which is typically 3. r2 : Integer . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Units are yx-pixels and by default it will be twice r1. Typical: 6 r_dapi : Integer or None . Filtering for DAPI images is a tophat with r_dapi radius. Should be approx radius of object of interest. Typically this is 8 micron converted to yx-pixel units which is typically 48. By default, it is None meaning DAPI not filtered at all and npy file not saved. psf : Numpy float array [psf_shape[0] x psf_shape[1] x psf_shape[2]] or None (psf_shape is in config file) . Average shape of spot from individual raw spot images normalised so max is 1 and min is 0. None if config['deconvolve'] = False . psf_intensity_thresh : Float . Intensity threshold used to detect spots in raw images which were used to make the psf. None if config['deconvolve'] = False or psf provided without spot detection. psf_tiles_used : Integer list or None . Tiles where spots for psf calculation came from. None if config['deconvolve'] = False or psf provided without spot detection. scale : Float . Multiplier applied to filtered nd2 imaging round images before saving as npy so full uint16 occupied. scale_tile : Integer or None . Tile of image that scale was found from. None if config['extract']['scale'] provided. scale_channel : Integer or None . Channel of image that scale was found from. None if config['extract']['scale'] provided. scale_z : Integer or None . z plane of image that scale was found from. None if config['extract']['scale'] provided. scale_anchor : Float or None . Multiplier applied to filtered nd2 anchor round images before saving as npy so full uint16 occupied. None if use_anchor = False . scale_anchor_tile : Integer or None . Tile of image in anchor round/channel that scale was found from. None if config['extract']['scale'] provided or use_anchor = False . scale_anchor_z : Integer or None . z plane of image in anchor round/channel that scale_anchor was found from. None if config['extract']['scale_anchor'] provided or use_anchor = False . z_info : Integer . z plane in npy file from which auto_thresh and hist_counts were calculated. By default, this is the mid plane. find_spots find_spots page contains information about spots found on all tiles, rounds and channels. Page added to Notebook in pipeline/find_spots.py isolation_thresh : Numpy float array [n_tiles] . Spots found on tile \\(t\\) , ref_round , ref_channel are isolated if annular filtered image is below isolation_thresh[t] at spot location. Typical: 0 spot_no : Numpy int32 array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . spot_no[t, r, c] is the number of spots found on tile \\(t\\) , round \\(r\\) , channel \\(c\\) spot_details : Numpy int16 array [n_total_spots x 7] . spot_details[i,:] is [tile, round, channel, isolated, y, x, z] for spot \\(i\\) isolated is 0 for all non reference round/channel spots and is 1 for isolated reference spots. \\(y\\) , \\(x\\) gives the local tile coordinates in yx-pixels. \\(z\\) gives local tile coordinate in z-pixels (0 if 2D ) stitch stitch page contains information about how tiles were stitched together to give global coordinates. Only tile_origin is used in later stages of the pipeline. Note that references to south in this section should really be north and west should be east . Page added to Notebook in pipeline/stitch.py tile_origin : Numpy float array [n_tiles x 3] . tile_origin[t,:] is the bottom left \\(yxz\\) coordinate of tile \\(t\\) . \\(yx\\) coordinates in yx-pixels and z coordinate in z-pixels. south_start_shift_search : Numpy integer array [3 x 3] . Initial search range used to find overlap between south neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . west_start_shift_search : Numpy integer array [3 x 3] . Initial search range used to find overlap between west neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . south_final_shift_search : Numpy integer array [3 x 3] . Final search range used to find overlap between south neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . west_final_shift_search : Numpy integer array [3 x 3] . Final search range used to find overlap between west neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . south_pairs : Numpy integer array [n_south_overlap x 2] . south_pairs[i, 1] is the tile to the north of south_pairs[i, 0] west_pairs : Numpy integer array [n_west_overlap x 2] . west_pairs[i, 1] is the tile to the east of west_pairs[i, 0] south_shifts : Numpy integer array [n_south_overlap x 3] . south_shifts[i, :] is the \\(yxz\\) shift found that is applied to south_pairs[i, 0] to take it to south_pairs[i, 1] Units: [yx_pixels, yx_pixels, z_pixels] , [:, 2] = 0 if 2D . west_shifts : Numpy integer array [n_west_overlap x 3] . west_shifts[i, :] is the \\(yxz\\) shift found that is applied to west_pairs[i, 0] to take it to west_pairs[i, 1] Units: [yx_pixels, yx_pixels, z_pixels] , [:, 2] = 0 if 2D . south_score : Numpy float array [n_south_overlap] . south_score[i] is approximately the number of matches found for south_shifts[i, :] west_score : Numpy float array [n_west_overlap] . west_score[i] is approximately the number of matches found for west_shifts[i, :] south_score_thresh : Numpy float array [n_south_overlap] . If south_score[i] is below south_score_thresh[i] , it indicates south_shifts[i] may be incorrect. west_score_thresh : Numpy float array [n_west_overlap] . If west_score[i] is below west_score_thresh[i] , it indicates west_shifts[i] found may be incorrect. south_outlier_shifts : Numpy integer array [n_south_overlap x 3] . If south_score[i] was below south_score_thresh[i] , south_shifts[i] was found again and old shift recorded as south_outlier_shifts[i] . Will be zero if this did not happen. west_outlier_shifts : Numpy integer array [n_west_overlap x 3] . If west_score[i] was below west_score_thresh[i] , west_shifts[i] was found again and old shift recorded as west_outlier_shifts[i] . Will be zero if this did not happen. south_outlier_score : Numpy float array [n_south_overlap] . If south_score[i] was below south_score_thresh[i] , south_shifts[i] was found again and old score recorded as south_outlier_score[i] . Will be zero if this did not happen. west_outlier_score : Numpy float array [n_west_overlap] . If west_score[i] was below west_score_thresh[i] , west_shifts[i] was found again and old score recorded as west_outlier_score[i] . Will be zero if this did not happen. register_initial register_initial page contains information about how shift between ref round/channel to each imaging round for each tile was found. These are then used as the starting point for determining the affine transforms. Only shift is used in later stages of the pipeline. Page added to Notebook in pipeline/register_initial.py shift : Numpy integer array [n_tiles x n_rounds x 3] . shift[t, r, :] is the \\(yxz\\) shift found that is applied to tile \\(t\\) , ref_round to take it to tile \\(t\\) , round \\(r\\) . Units: [yx_pixels, yx_pixels, z_pixels] , [:, :, 2] = 0 if 2D . Same as initial_shift in register page. shift_channel : Integer . Channel used to find find shifts between rounds to use as starting point for point cloud registration. Typically this is ref_channel or a channel with lots of spots. start_shift_search : Numpy integer array [n_rounds x 3 x 3] . [r, :, :] is the initial search range used to find shift from reference round to round \\(r\\) for all tiles [r, i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [r, 2,:] is in units of z-pixels and is 0 if 2D . final_shift_search : Numpy integer array [n_rounds x 3 x 3] . [r, :, :] is the final search range used to find shift from reference round to round \\(r\\) for all tiles [r, i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [r, 2,:] is in units of z-pixels and is 0 if 2D . shift_score : Numpy float array [n_tiles x n_rounds] . shift_score[t, r] is is approximately the number of matches found for shift[t,r] shift_score_thresh : Numpy float array [n_tiles x n_rounds] . If shift_score[t, r] is below shift_score_thresh[t, r] , it indicates shift[t,r] may be incorrect. shift_outlier : Numpy integer array [n_tiles x n_rounds x 3] . If shift_score[t, r] was below shift_score_thresh[t, r] , shift[t, r] was found again and old shift recorded as shift_outlier[t, r] . Will be zero if this did not happen. shift_score_outlier : Numpy float array [n_tiles x n_rounds] . If shift_score[t, r] was below shift_score_thresh[t, r] , shift[t, r] was found again and old score recorded as shift_score_outlier[t, r] . Will be zero if this did not happen. register register page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. Page added to Notebook in pipeline/register.py initial_shift : Numpy integer array [n_tiles x n_rounds x 3] . shift[t, r, :] is the \\(yxz\\) shift found that is applied to tile \\(t\\) , ref_round to take it to tile \\(t\\) , round \\(r\\) . Units: [yx_pixels, yx_pixels, z_pixels] , [:, :, 2] = 0 if 2D . Same as shift in register_initial page. DON'T KNOW WHY COPIED THIS - PROBABLY SHOULD REMOVE transform : Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile \\(t\\) , ref_round , ref_channel to tile \\(t\\) , round \\(r\\) , channel \\(c\\) Before applying to coordinates, they must be centered and z coordinates put into units of yx-pixels. If 2D , z scaling set to 1 while shift and rotation set to 0. register_debug register_debug page contains information on how the affine transforms in register page were calculated. Page added to Notebook in pipeline/register.py n_matches : Numpy integer array [n_tiles x n_rounds x n_channels] . Number of matches found for each transform. A match is when distance between points is less than config['register']['neighb_dist_thresh'] . n_matches_thresh : Numpy integer array [n_tiles x n_rounds x n_channels] . n_matches[t, r, c] must exceed n_matches_thresh[t, r, c] otherwise failed[t, r, c] = True and transform found again using regularisation. error : Numpy float array [n_tiles x n_rounds x n_channels] . Average distance between neighbours closer than config['register']['neighb_dist_thresh'] for each transform. failed : Numpy boolean array [n_tiles x n_rounds x n_channels] . failed[t, r, c] = True if transform[t, r, c] had too few matches or was anomalous compared to average. n_matches_thresh in this page and scale_dev_thresh , shift_dev_thresh in config file quantify the required matches / deviation. converged : Numpy boolean array [n_tiles x n_rounds x n_channels] . This is False for transforms where the ICP algorithm reached config['register']['n_iter'] iterations before transform converged. av_scaling : Numpy float array [n_channels x 3] . av_scaling[c] is the \\(yxz\\) chromatic aberration scale factor to channel \\(c\\) from the ref_channel averaged over all rounds and tiles. Expect the \\(y\\) and \\(x\\) scaling to be the same and all scalings to be approx 1. av_shifts : Numpy float array [n_tiles x n_rounds x 3] . av_shifts[t, r] is the \\(yxz\\) shift from tile \\(t\\) , ref_round to tile \\(t\\) , round \\(r\\) averaged over all channels. All three directions are in yx-pixel units. transform_outlier : Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3] . [t, r, c] is the final transform found for tile \\(t\\) , round \\(r\\) , channel \\(c\\) without regularisation. Regularisation only used for \\(t\\) , \\(r\\) , \\(c\\) indicated by failed and so transform_outlier = 0 for others. ref_spots ref_spots page contains gene assignments and info for spots found on reference round. Page added to Notebook in pipeline/get_reference_spots.py . The variables gene_no , score , score_diff , intensity will be set to None after get_reference_spots . call_reference_spots should then be run to give their actual values. This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run. local_yxz : Numpy int16 array [n_spots x 3] . local_yxz[s] are the \\(yxz\\) coordinates of spot \\(s\\) found on tile[s] , ref_round , ref_channel . To get global_yxz , add nb.stitch.tile_origin[tile[s]] . isolated : Numpy boolean array [n_spots] . True for spots that are well isolated i.e. surroundings have low intensity so no nearby spots. tile : Numpy int16 array [n_spots] . Tile each spot was found on. colors : Numpy int32 array [n_spots x n_rounds x n_channels] . [s, r, c] is the intensity of spot \\(s\\) on round \\(r\\) , channel \\(c\\) . -tile_pixel_value_shift if that round/channel not used otherwise integer. gene_no : Numpy int16 array [n_spots] . gene_no[s] is the index of the gene assigned to spot \\(s\\) . score : Numpy float32 array [n_spots] . score[s] is the dot product score, \\(\\Delta_{s0g}\\) , between colors[s] and bled_codes[gene_no[s]] . Normalisation depends on config['call_spots']['dot_product_method'] . score_diff : Numpy float16 array [n_spots] . score_diff[s] is score[s] minus the score for the second best gene assignement for spot \\(s\\) . intensity : Numpy float32 array [n_spots] . \\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) colors[s, r]/color_norm_factor[r] . call_spots call_spots page contains bleed_matrix and expected code for each gene. Page added to Notebook in pipeline/call_reference_spots.py gene_names : Numpy string array [n_genes] . Names of all genes in the code book provided. gene_codes : Numpy integer array [n_genes x n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene \\(g\\) in round \\(r\\) . color_norm_factor : Numpy float array [n_rounds x n_channels] . Normalisation such that dividing colors by color_norm_factor equalizes intensity of channels. config['call_spots']['bleed_matrix_method'] indicates whether normalisation is for rounds and channels or just channels. initial_raw_bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . initial_raw_bleed_matrix[r, c, d] is the estimate of the raw intensity of dye \\(d\\) in round \\(r\\) , channel \\(c\\) . All will be nan if separate dye for each channel. initial_bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . Starting point for determination of bleed matrix. If separate dye for each channel, initial_bleed_matrix[r] will be the identity matrix for each \\(r\\) . Otherwise, it will be initial_raw_bleed_matrix divided by color_norm_factor . bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . For a spot, \\(s\\) , which should be dye \\(d\\) in round \\(r\\) , we expect color[s, r]/color_norm_factor[r] to be a constant multiple of bleed_matrix[r, :, d] background_codes : Numpy float array [n_channels x n_rounds x n_channels] . These are the background codes for which each spot has a background_coef . background_codes[C, r, c] = 1 if c==C and 0 otherwise for all rounds \\(r\\) . nan if \\(r\\) / \\(c\\) outside use_rounds / use_channels . bled_codes : Numpy float array [n_genes x n_rounds x n_channels] . color[s, r]/color_norm_factor[r] of spot, \\(s\\) , corresponding to gene \\(g\\) is expected to be a constant multiple of bled_codes[g, r] in round \\(r\\) . nan if \\(r\\) / \\(c\\) outside use_rounds / use_channels and 0 if gene_codes[g,r] outside use_dyes . All codes have L2 norm = 1 when summed across all use_rounds and use_channels . gene_efficiency : Numpy float array [n_genes x n_rounds] . gene_efficiency[g,r] gives the expected intensity of gene \\(g\\) in round \\(r\\) compared to that expected by the bleed_matrix . It is computed based on the average of isolated spot_colors assigned to that gene which exceed score , score_diff and intensity thresholds given in config file. For all \\(g\\) , there is an av_round[g] such that gene_efficiency[g, av_round[g]] = 1 . nan if \\(r\\) outside use_rounds and 1 if gene_codes[g,r] outside use_dyes . bled_codes_ge : Numpy float array [n_genes x n_rounds x n_channels] . bled_codes using gene_efficiency information i.e. bled_codes * gene_efficiency . All codes have L2 norm = 1 when summed across all use_rounds and use_channels . background_weight_shift : Float . Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round \\(r\\) for the fitting of the background vector for channel \\(c\\) is 1 / (spot_color[r, c] + background_weight_shift) so background_weight_shift ensures this does not go to infinity for small spot_color[r, c] . Typical spot_color[r, c] is 1 for intense spot so background_weight_shift is small fraction of this. dp_norm_shift : Float . When calculating the dot product score, this is the small shift to apply when normalising spot_colors to ensure don't divide by zero. Value is for a single round and is multiplied by sqrt(n_rounds_used) when computing dot product score. Expected norm of a spot_color for a single round is 1 so dp_norm_shift is a small fraction of this. abs_intensity_percentile : Numpy float array [100] or None . abs_intensity_percentile[i] is the i% percentile of absolute pixel_colors on norm_shift_tile , norm_shift_z . This is used to compute nb.omp.initial_intensity_thresh if not provided. norm_shift_tile : Integer . Tile that is used to compute abs_intensity_percentile from which dp_norm_shift , background_weight_shift and intensity_thresh are computed. norm_shift_z : Integer . z-plane that is used to compute abs_intensity_percentile from which dp_norm_shift , background_weight_shift and intensity_thresh are computed. gene_efficiency_intensity_thresh : Float . gene_efficiency is computed from spots with intensity greater than this. By default, it is set to the config['call_spots']['gene_efficiency_intensity_thresh_percentile'] percentile of the intensity computed for all pixels on the mid z-plane of the most central tile omp omp page contains gene assignments and info for spots located at the local maxima of the gene coefficients returned by OMP . Also contains info about spot_shape which indicates the expected sign of the OMP coefficient in a neighbourhood centered on a spot. Page added to Notebook in pipeline/call_spots_omp.py initial_intensity_thresh : Float . To save time in call_spots_omp , coefficients only found for pixels with intensity of absolute spot_colors greater than initial_intensity_thresh . This threshold is set to the config['omp']['initial_intensity_thresh_percentile'] percentile of the absolute intensity of all pixels on the mid z-plane of the central tile (uses nb.call_spots.abs_intensity_percentile ). It is also clamped between the min and max values given in config file. shape_tile : Integer or None . spot_shape was found from spots detected on this tile. None if spot_shape not computed in this experiment. shape_spot_local_yxz : Numpy integer array [n_shape_spots x 3] or None . \\(yxz\\) coordinates on shape_tile , ref_round / ref_channel of spots used to compute spot_shape None if spot_shape not computed in this experiment. shape_spot_gene_no : Numpy integer array [n_shape_spots] or None . shape_spot_gene_no[s] is the gene that the spot at shape_spot_local_yxz[s] was assigned to. None if spot_shape not computed in this experiment. spot_shape_float : Numpy float array [shape_max_size[0] x shape_max_size[1] x shape_max_size[2]] or None . Mean of OMP coefficient sign in neighbourhood centered on spot. None if spot_shape not computed in this experiment. initial_pos_neighbour_thresh : Integer . Only spots with number of positive coefficient neighbours greater than this are saved to notebook. Typical = 4 in 2D and 40 in 3D (set to 10% of max number by default). spot_shape : Numpy integer array [shape_size_y x shape_size_y x shape_size_x] . Expected sign of OMP coefficient in neighbourhood centered on spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign. local_yxz : Numpy int16 array [n_spots, 3] . local_yxz[s] are the \\(yxz\\) coordinates of spot \\(s\\) found on tile[s] , ref_round , ref_channel . To get global_yxz , add nb.stitch.tile_origin[tile[s]] . tile : Numpy int16 array [n_spots] . Tile each spot was found on. colors : Numpy int32 array [n_spots x n_rounds x n_channels] . [s, r, c] is the intensity of spot \\(s\\) on round \\(r\\) , channel \\(c\\) . It will be -tile_pixel_value_shift if that round/channel not used otherwise integer. gene_no : Numpy int16 array [n_spots] . gene_no[s] is the index of the gene assigned to spot \\(s\\) . n_neighbours_pos : Numpy int16 array [n_spots] . Number of positive pixels around each spot in neighbourhood given by spot_shape==1 . Max is sum(spot_shape==1) . n_neighbours_neg : Numpy int16 array [n_spots] . Number of negative pixels around each spot in neighbourhood given by spot_shape==-1 . Max is sum(spot_shape==-1) . intensity : Numpy float32 array [n_spots] . \\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) colors[s, r]/color_norm_factor[r] . thresholds thresholds page contains quality thresholds which affect which spots plotted and which are exported to pciSeq . Page added to Notebook when utils/pciseq/export_to_pciseq is run. intensity : Float . Final accepted reference and OMP spots require intensity > thresholds[intensity] . This is copied from config[thresholds] and if not given there, will be set to nb.call_spots.gene_efficiency_intensity_thresh . intensity for a really intense spot is about 1 so intensity_thresh should be less than this. score_ref : Float . Final accepted reference spots are those which pass quality_threshold which is: nb.ref_spots.score > thresholds[score_ref] and intensity > thresholds[intensity] . This is copied from config[thresholds] . Max score is 1 so score_ref should be less than this. score_omp : Float . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) . This is copied from config[thresholds] . Max score is 1 so score_thresh should be less than this. score_omp_multiplier : Float . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) . This is copied from config[thresholds] .","title":"Notebook Comments"},{"location":"notebook_comments/#notebook-comments","text":"","title":"Notebook Comments"},{"location":"notebook_comments/#file_names","text":"file_names page contains all files that are used throughout the pipeline. tile is the only variable not included in the config file and is worked out automatically. Page added to Notebook automatically as soon as basic_info page added. input_dir : Directory . Where raw nd2 files are output_dir : Directory . Where notebook is saved tile_dir : Directory . Where tile npy files saved round : List [n_rounds] . Names of nd2 files for the imaging rounds. If not using, will be an empty list. anchor : String or None . Name of nd2 file for the anchor round. None if anchor not used raw_extension : String . .nd2 or .npy indicating the data type of the raw data. raw_metadata : String or None . If raw_extension = .npy , this is the name of the json file in input_dir which contains the required metadata extracted from the initial nd2 files. I.e. it is the output of coppafish/utils/nd2/save_metadata dye_camera_laser : File . csv file giving the approximate raw intensity for each dye with each camera/laser combination code_book : File . Text file which contains the codes indicating which dye to expect on each round for each gene scale : File . Text file saved containing the extract['scale'] and extract['scale_anchor'] values used to create the tile npy files in the tile_dir . If the second value is 0, it means extract['scale_anchor'] has not been calculated yet. If the extract step of the pipeline is re-run with extract['scale'] or extract['scale_anchor'] different to values saved here, an error will be raised. psf : File or None . npy file indicating average spot shape (before padding and scaled to fill uint16 range). Will be None if 2D pipeline used. File won't exist/used if config['extract']['deconvolve'] = False . If 3D , 1st axis in npy file is z. omp_spot_shape : File . npy file indicating average spot shape in OMP coefficient sign images. Saved image is int8 npy with only values being -1, 0, 1. omp_spot_info : File . After each tile is finished in OMP , information about spots found is saved as array to npy file: numpy int16 array [n_spots x 7] containing \\(y\\) , \\(x\\) , \\(z\\) , gene_no , n_pos_neighb , n_neg_neighb , tile . If 3D , 1st axis in npy file is z. omp_spot_coef : File . After each tile is finished in OMP , coefficients for all spots found is saved as sparse csr_matrix to npz file: CSR_matrix float [n_spots x n_genes] giving coefficient found for each gene for each spot. big_dapi_image : File or None . npz file of stitched DAPI image. None if nb.basic_info.dapi_channel = None If 3D , 1st axis in npz file is z. big_anchor_image : File . npz file of stitched image of ref_round / ref_channel . Will be stitched anchor if anchor used. If 3D , 1st axis in npz file is z. pciseq : List of 2 files . csv files where plotting information for pciSeq is saved. pciseq[0] is the path where the OMP method output will be saved. pciseq[1] is the path where the ref_spots method output will be saved. If files don't exist, they will be created when the function coppafish/export_to_pciseq is run. tile : List of numpy string arrays [n_tiles][(n_rounds + n_extra_rounds) {x n_channels if 3d}] . 2D : tile[t][r] is the npy file containing all channels of tile \\(t\\) , round \\(r\\) . 3D : tile[t][r][c] is the npy file containing all z planes for tile \\(t\\) , round \\(r\\) , channel \\(c\\)","title":"file_names"},{"location":"notebook_comments/#basic_info","text":"basic_info page contains information that is used at all stages of the pipeline. Page added to Notebook in pipeline/basic_info.py is_3d : Boolean . True if 3D pipeline used, False if 2D anchor_round : Integer or None . Index of anchor round (typically the first round after imaging rounds so anchor_round = n_rounds ). None if anchor not used. anchor_channel : Integer or None . Channel in anchor round used as reference and to build coordinate system on. Usually channel with most spots. None if anchor not used. dapi_channel : Integer or None . Channel in anchor round that contains DAPI images. None if no DAPI . ref_round : Integer . Round to align all imaging rounds to. Will be anchor if using. ref_channel : Integer . Channel in reference round used as reference and to build coordinate system on. Usually channel with most spots. Will be anchor_channel if using anchor round use_channels : Integer List [n_use_channels] . Channels in imaging rounds to use throughout pipeline. use_rounds : Integer List [n_use_rounds] . Imaging rounds to use throughout pipeline. use_z : Integer List [nz] . z planes used to make tile npy files use_tiles : Integer List [n_use_tiles] . Tiles to use throughout pipeline. For an experiment where the tiles are arranged in a \\(4 \\times 3\\) ( \\(n_y \\times n_x\\) ) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | use_dyes : Integer List [n_use_dyes] . Dyes to use when assigning spots to genes. dye_names : String List [n_dyes] or None . Names of all dyes so for gene with code \\(360...\\) , gene appears with dye_names[3] in round \\(0\\) , dye_names[6] in round \\(1\\) , dye_names[0] in round \\(2\\) etc. None if each channel corresponds to a different dye. channel_camera : Integer List [n_channels] or None . channel_camera[i] is the wavelength in nm of the camera on channel \\(i\\) . None if dye_names = None . channel_laser : Integer List [n_channels] or None . channel_laser[i] is the wavelength in nm of the laser on channel \\(i\\) . None if dye_names = None . tile_pixel_value_shift : Integer . This is added onto every tile (except DAPI ) when it is saved and removed from every tile when loaded. Required so we can have negative pixel values when save to npy as uint16 . Typical=15000 n_extra_rounds : Integer . Number of non-imaging rounds, typically 1 if using anchor and 0 if not. n_rounds : Integer . Number of imaging rounds in the raw data tile_sz : Integer . \\(yx\\) dimension of tiles in pixels n_tiles : Integer . Number of tiles in the raw data n_channels : Integer . Number of channels in the raw data nz : Integer . Number of z-planes used to make the npy tile images (can be different from number in raw data). n_dyes : Integer . Number of dyes used tile_centre : Numpy float array [3] . [y, x, z] location of tile centre in units of [yx_pixels, yx_pixels, z_pixels] . For 2D pipeline, tile_centre[2] = 0 tilepos_yx_nd2 : Numpy integer array [n_tiles x 2] . tilepos_yx_nd2[i, :] is the \\(yx\\) position of tile with fov index \\(i\\) in the nd2 file. Index 0 refers to YX = [0, 0] Index 1 refers to YX = [0, 1] if MaxX > 0 tilepos_yx : Numpy integer array [n_tiles x 2] . tilepos_yx[i, :] is the \\(yx\\) position of tile with tile directory ( npy files) index \\(i\\) . Equally, tilepos_yx[use_tiles[i], :] is \\(yx\\) position of tile use_tiles[i] . Index 0 refers to YX = [MaxY, MaxX] Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 pixel_size_xy : Float . \\(yx\\) pixel size in microns pixel_size_z : Float . \\(z\\) pixel size in microns use_anchor : Boolean . True if anchor round is used, False if not.","title":"basic_info"},{"location":"notebook_comments/#extract","text":"The extract page contains variables from extract_and_filter step which are used later in the pipeline. auto_thresh is used in find_spots step. hist_values and hist_counts are used for normalisation between channels in the call_reference_spots step. Page added to Notebook in pipeline/extract_run.py auto_thresh : Numpy float array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . auto_thresh[t, r, c] is the threshold spot intensity for tile \\(t\\) , round \\(r\\) , channel \\(c\\) used for spot detection in the find_spots step of the pipeline. hist_values : Numpy integer array [n_pixel_values] . All possible pixel values in saved npy images i.e. length is approx np.iinfo(np.uint16).max hist_counts : Numpy integer array [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round \\(r\\) , channel \\(c\\) which had the value hist_values[i] .","title":"extract"},{"location":"notebook_comments/#extract_debug","text":"extract_debug page stores variables from extract_and_filter step which are not needed later in the pipeline but may be useful for debugging purposes. Page added to Notebook in pipeline/extract_run.py n_clip_pixels : Numpy integer array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . n_clip_pixels[t, r, c] is the number of pixels in the saved npy tile for tile \\(t\\) , round \\(r\\) , channel \\(c\\) which had intensity exceeding max uint16 value and so had to be clipped. clip_extract_scale : Numpy float array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . clip_extract_scale[t, r, c] is the recommended value for extract_scale such that for tile \\(t\\) , round \\(r\\) , channel \\(c\\) n_clip_pixels[t, r, c] would be 0. Only computed for images where n_clip_pixels[t, r, c] > 0 r1 : Integer . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Should be approx radius of spot. By default this is 0.5 micron converted to yx-pixel units which is typically 3. r2 : Integer . Filtering is done with a 2D difference of hanning filter with inner radius r1 within which it is positive and outer radius r2 so annulus between r1 and r2 is negative. Units are yx-pixels and by default it will be twice r1. Typical: 6 r_dapi : Integer or None . Filtering for DAPI images is a tophat with r_dapi radius. Should be approx radius of object of interest. Typically this is 8 micron converted to yx-pixel units which is typically 48. By default, it is None meaning DAPI not filtered at all and npy file not saved. psf : Numpy float array [psf_shape[0] x psf_shape[1] x psf_shape[2]] or None (psf_shape is in config file) . Average shape of spot from individual raw spot images normalised so max is 1 and min is 0. None if config['deconvolve'] = False . psf_intensity_thresh : Float . Intensity threshold used to detect spots in raw images which were used to make the psf. None if config['deconvolve'] = False or psf provided without spot detection. psf_tiles_used : Integer list or None . Tiles where spots for psf calculation came from. None if config['deconvolve'] = False or psf provided without spot detection. scale : Float . Multiplier applied to filtered nd2 imaging round images before saving as npy so full uint16 occupied. scale_tile : Integer or None . Tile of image that scale was found from. None if config['extract']['scale'] provided. scale_channel : Integer or None . Channel of image that scale was found from. None if config['extract']['scale'] provided. scale_z : Integer or None . z plane of image that scale was found from. None if config['extract']['scale'] provided. scale_anchor : Float or None . Multiplier applied to filtered nd2 anchor round images before saving as npy so full uint16 occupied. None if use_anchor = False . scale_anchor_tile : Integer or None . Tile of image in anchor round/channel that scale was found from. None if config['extract']['scale'] provided or use_anchor = False . scale_anchor_z : Integer or None . z plane of image in anchor round/channel that scale_anchor was found from. None if config['extract']['scale_anchor'] provided or use_anchor = False . z_info : Integer . z plane in npy file from which auto_thresh and hist_counts were calculated. By default, this is the mid plane.","title":"extract_debug"},{"location":"notebook_comments/#find_spots","text":"find_spots page contains information about spots found on all tiles, rounds and channels. Page added to Notebook in pipeline/find_spots.py isolation_thresh : Numpy float array [n_tiles] . Spots found on tile \\(t\\) , ref_round , ref_channel are isolated if annular filtered image is below isolation_thresh[t] at spot location. Typical: 0 spot_no : Numpy int32 array [n_tiles x (n_rounds + n_extra_rounds) x n_channels] . spot_no[t, r, c] is the number of spots found on tile \\(t\\) , round \\(r\\) , channel \\(c\\) spot_details : Numpy int16 array [n_total_spots x 7] . spot_details[i,:] is [tile, round, channel, isolated, y, x, z] for spot \\(i\\) isolated is 0 for all non reference round/channel spots and is 1 for isolated reference spots. \\(y\\) , \\(x\\) gives the local tile coordinates in yx-pixels. \\(z\\) gives local tile coordinate in z-pixels (0 if 2D )","title":"find_spots"},{"location":"notebook_comments/#stitch","text":"stitch page contains information about how tiles were stitched together to give global coordinates. Only tile_origin is used in later stages of the pipeline. Note that references to south in this section should really be north and west should be east . Page added to Notebook in pipeline/stitch.py tile_origin : Numpy float array [n_tiles x 3] . tile_origin[t,:] is the bottom left \\(yxz\\) coordinate of tile \\(t\\) . \\(yx\\) coordinates in yx-pixels and z coordinate in z-pixels. south_start_shift_search : Numpy integer array [3 x 3] . Initial search range used to find overlap between south neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . west_start_shift_search : Numpy integer array [3 x 3] . Initial search range used to find overlap between west neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . south_final_shift_search : Numpy integer array [3 x 3] . Final search range used to find overlap between south neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . west_final_shift_search : Numpy integer array [3 x 3] . Final search range used to find overlap between west neighbouring tiles [i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [2,:] is in units of z-pixels and is 0 if 2D . south_pairs : Numpy integer array [n_south_overlap x 2] . south_pairs[i, 1] is the tile to the north of south_pairs[i, 0] west_pairs : Numpy integer array [n_west_overlap x 2] . west_pairs[i, 1] is the tile to the east of west_pairs[i, 0] south_shifts : Numpy integer array [n_south_overlap x 3] . south_shifts[i, :] is the \\(yxz\\) shift found that is applied to south_pairs[i, 0] to take it to south_pairs[i, 1] Units: [yx_pixels, yx_pixels, z_pixels] , [:, 2] = 0 if 2D . west_shifts : Numpy integer array [n_west_overlap x 3] . west_shifts[i, :] is the \\(yxz\\) shift found that is applied to west_pairs[i, 0] to take it to west_pairs[i, 1] Units: [yx_pixels, yx_pixels, z_pixels] , [:, 2] = 0 if 2D . south_score : Numpy float array [n_south_overlap] . south_score[i] is approximately the number of matches found for south_shifts[i, :] west_score : Numpy float array [n_west_overlap] . west_score[i] is approximately the number of matches found for west_shifts[i, :] south_score_thresh : Numpy float array [n_south_overlap] . If south_score[i] is below south_score_thresh[i] , it indicates south_shifts[i] may be incorrect. west_score_thresh : Numpy float array [n_west_overlap] . If west_score[i] is below west_score_thresh[i] , it indicates west_shifts[i] found may be incorrect. south_outlier_shifts : Numpy integer array [n_south_overlap x 3] . If south_score[i] was below south_score_thresh[i] , south_shifts[i] was found again and old shift recorded as south_outlier_shifts[i] . Will be zero if this did not happen. west_outlier_shifts : Numpy integer array [n_west_overlap x 3] . If west_score[i] was below west_score_thresh[i] , west_shifts[i] was found again and old shift recorded as west_outlier_shifts[i] . Will be zero if this did not happen. south_outlier_score : Numpy float array [n_south_overlap] . If south_score[i] was below south_score_thresh[i] , south_shifts[i] was found again and old score recorded as south_outlier_score[i] . Will be zero if this did not happen. west_outlier_score : Numpy float array [n_west_overlap] . If west_score[i] was below west_score_thresh[i] , west_shifts[i] was found again and old score recorded as west_outlier_score[i] . Will be zero if this did not happen.","title":"stitch"},{"location":"notebook_comments/#register_initial","text":"register_initial page contains information about how shift between ref round/channel to each imaging round for each tile was found. These are then used as the starting point for determining the affine transforms. Only shift is used in later stages of the pipeline. Page added to Notebook in pipeline/register_initial.py shift : Numpy integer array [n_tiles x n_rounds x 3] . shift[t, r, :] is the \\(yxz\\) shift found that is applied to tile \\(t\\) , ref_round to take it to tile \\(t\\) , round \\(r\\) . Units: [yx_pixels, yx_pixels, z_pixels] , [:, :, 2] = 0 if 2D . Same as initial_shift in register page. shift_channel : Integer . Channel used to find find shifts between rounds to use as starting point for point cloud registration. Typically this is ref_channel or a channel with lots of spots. start_shift_search : Numpy integer array [n_rounds x 3 x 3] . [r, :, :] is the initial search range used to find shift from reference round to round \\(r\\) for all tiles [r, i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [r, 2,:] is in units of z-pixels and is 0 if 2D . final_shift_search : Numpy integer array [n_rounds x 3 x 3] . [r, :, :] is the final search range used to find shift from reference round to round \\(r\\) for all tiles [r, i, :] is the [min, max, step] of the search in direction \\(i\\) (0 is \\(y\\) , 1 is \\(x\\) , 2 is \\(z\\) ). [r, 2,:] is in units of z-pixels and is 0 if 2D . shift_score : Numpy float array [n_tiles x n_rounds] . shift_score[t, r] is is approximately the number of matches found for shift[t,r] shift_score_thresh : Numpy float array [n_tiles x n_rounds] . If shift_score[t, r] is below shift_score_thresh[t, r] , it indicates shift[t,r] may be incorrect. shift_outlier : Numpy integer array [n_tiles x n_rounds x 3] . If shift_score[t, r] was below shift_score_thresh[t, r] , shift[t, r] was found again and old shift recorded as shift_outlier[t, r] . Will be zero if this did not happen. shift_score_outlier : Numpy float array [n_tiles x n_rounds] . If shift_score[t, r] was below shift_score_thresh[t, r] , shift[t, r] was found again and old score recorded as shift_score_outlier[t, r] . Will be zero if this did not happen.","title":"register_initial"},{"location":"notebook_comments/#register","text":"register page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. Page added to Notebook in pipeline/register.py initial_shift : Numpy integer array [n_tiles x n_rounds x 3] . shift[t, r, :] is the \\(yxz\\) shift found that is applied to tile \\(t\\) , ref_round to take it to tile \\(t\\) , round \\(r\\) . Units: [yx_pixels, yx_pixels, z_pixels] , [:, :, 2] = 0 if 2D . Same as shift in register_initial page. DON'T KNOW WHY COPIED THIS - PROBABLY SHOULD REMOVE transform : Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile \\(t\\) , ref_round , ref_channel to tile \\(t\\) , round \\(r\\) , channel \\(c\\) Before applying to coordinates, they must be centered and z coordinates put into units of yx-pixels. If 2D , z scaling set to 1 while shift and rotation set to 0.","title":"register"},{"location":"notebook_comments/#register_debug","text":"register_debug page contains information on how the affine transforms in register page were calculated. Page added to Notebook in pipeline/register.py n_matches : Numpy integer array [n_tiles x n_rounds x n_channels] . Number of matches found for each transform. A match is when distance between points is less than config['register']['neighb_dist_thresh'] . n_matches_thresh : Numpy integer array [n_tiles x n_rounds x n_channels] . n_matches[t, r, c] must exceed n_matches_thresh[t, r, c] otherwise failed[t, r, c] = True and transform found again using regularisation. error : Numpy float array [n_tiles x n_rounds x n_channels] . Average distance between neighbours closer than config['register']['neighb_dist_thresh'] for each transform. failed : Numpy boolean array [n_tiles x n_rounds x n_channels] . failed[t, r, c] = True if transform[t, r, c] had too few matches or was anomalous compared to average. n_matches_thresh in this page and scale_dev_thresh , shift_dev_thresh in config file quantify the required matches / deviation. converged : Numpy boolean array [n_tiles x n_rounds x n_channels] . This is False for transforms where the ICP algorithm reached config['register']['n_iter'] iterations before transform converged. av_scaling : Numpy float array [n_channels x 3] . av_scaling[c] is the \\(yxz\\) chromatic aberration scale factor to channel \\(c\\) from the ref_channel averaged over all rounds and tiles. Expect the \\(y\\) and \\(x\\) scaling to be the same and all scalings to be approx 1. av_shifts : Numpy float array [n_tiles x n_rounds x 3] . av_shifts[t, r] is the \\(yxz\\) shift from tile \\(t\\) , ref_round to tile \\(t\\) , round \\(r\\) averaged over all channels. All three directions are in yx-pixel units. transform_outlier : Numpy float array [n_tiles x n_rounds x n_channels x 4 x 3] . [t, r, c] is the final transform found for tile \\(t\\) , round \\(r\\) , channel \\(c\\) without regularisation. Regularisation only used for \\(t\\) , \\(r\\) , \\(c\\) indicated by failed and so transform_outlier = 0 for others.","title":"register_debug"},{"location":"notebook_comments/#ref_spots","text":"ref_spots page contains gene assignments and info for spots found on reference round. Page added to Notebook in pipeline/get_reference_spots.py . The variables gene_no , score , score_diff , intensity will be set to None after get_reference_spots . call_reference_spots should then be run to give their actual values. This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run. local_yxz : Numpy int16 array [n_spots x 3] . local_yxz[s] are the \\(yxz\\) coordinates of spot \\(s\\) found on tile[s] , ref_round , ref_channel . To get global_yxz , add nb.stitch.tile_origin[tile[s]] . isolated : Numpy boolean array [n_spots] . True for spots that are well isolated i.e. surroundings have low intensity so no nearby spots. tile : Numpy int16 array [n_spots] . Tile each spot was found on. colors : Numpy int32 array [n_spots x n_rounds x n_channels] . [s, r, c] is the intensity of spot \\(s\\) on round \\(r\\) , channel \\(c\\) . -tile_pixel_value_shift if that round/channel not used otherwise integer. gene_no : Numpy int16 array [n_spots] . gene_no[s] is the index of the gene assigned to spot \\(s\\) . score : Numpy float32 array [n_spots] . score[s] is the dot product score, \\(\\Delta_{s0g}\\) , between colors[s] and bled_codes[gene_no[s]] . Normalisation depends on config['call_spots']['dot_product_method'] . score_diff : Numpy float16 array [n_spots] . score_diff[s] is score[s] minus the score for the second best gene assignement for spot \\(s\\) . intensity : Numpy float32 array [n_spots] . \\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) colors[s, r]/color_norm_factor[r] .","title":"ref_spots"},{"location":"notebook_comments/#call_spots","text":"call_spots page contains bleed_matrix and expected code for each gene. Page added to Notebook in pipeline/call_reference_spots.py gene_names : Numpy string array [n_genes] . Names of all genes in the code book provided. gene_codes : Numpy integer array [n_genes x n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene \\(g\\) in round \\(r\\) . color_norm_factor : Numpy float array [n_rounds x n_channels] . Normalisation such that dividing colors by color_norm_factor equalizes intensity of channels. config['call_spots']['bleed_matrix_method'] indicates whether normalisation is for rounds and channels or just channels. initial_raw_bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . initial_raw_bleed_matrix[r, c, d] is the estimate of the raw intensity of dye \\(d\\) in round \\(r\\) , channel \\(c\\) . All will be nan if separate dye for each channel. initial_bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . Starting point for determination of bleed matrix. If separate dye for each channel, initial_bleed_matrix[r] will be the identity matrix for each \\(r\\) . Otherwise, it will be initial_raw_bleed_matrix divided by color_norm_factor . bleed_matrix : Numpy float array [n_rounds x n_channels x n_dyes] . For a spot, \\(s\\) , which should be dye \\(d\\) in round \\(r\\) , we expect color[s, r]/color_norm_factor[r] to be a constant multiple of bleed_matrix[r, :, d] background_codes : Numpy float array [n_channels x n_rounds x n_channels] . These are the background codes for which each spot has a background_coef . background_codes[C, r, c] = 1 if c==C and 0 otherwise for all rounds \\(r\\) . nan if \\(r\\) / \\(c\\) outside use_rounds / use_channels . bled_codes : Numpy float array [n_genes x n_rounds x n_channels] . color[s, r]/color_norm_factor[r] of spot, \\(s\\) , corresponding to gene \\(g\\) is expected to be a constant multiple of bled_codes[g, r] in round \\(r\\) . nan if \\(r\\) / \\(c\\) outside use_rounds / use_channels and 0 if gene_codes[g,r] outside use_dyes . All codes have L2 norm = 1 when summed across all use_rounds and use_channels . gene_efficiency : Numpy float array [n_genes x n_rounds] . gene_efficiency[g,r] gives the expected intensity of gene \\(g\\) in round \\(r\\) compared to that expected by the bleed_matrix . It is computed based on the average of isolated spot_colors assigned to that gene which exceed score , score_diff and intensity thresholds given in config file. For all \\(g\\) , there is an av_round[g] such that gene_efficiency[g, av_round[g]] = 1 . nan if \\(r\\) outside use_rounds and 1 if gene_codes[g,r] outside use_dyes . bled_codes_ge : Numpy float array [n_genes x n_rounds x n_channels] . bled_codes using gene_efficiency information i.e. bled_codes * gene_efficiency . All codes have L2 norm = 1 when summed across all use_rounds and use_channels . background_weight_shift : Float . Shift to apply to weighting of each background vector to limit boost of weak spots. The weighting of round \\(r\\) for the fitting of the background vector for channel \\(c\\) is 1 / (spot_color[r, c] + background_weight_shift) so background_weight_shift ensures this does not go to infinity for small spot_color[r, c] . Typical spot_color[r, c] is 1 for intense spot so background_weight_shift is small fraction of this. dp_norm_shift : Float . When calculating the dot product score, this is the small shift to apply when normalising spot_colors to ensure don't divide by zero. Value is for a single round and is multiplied by sqrt(n_rounds_used) when computing dot product score. Expected norm of a spot_color for a single round is 1 so dp_norm_shift is a small fraction of this. abs_intensity_percentile : Numpy float array [100] or None . abs_intensity_percentile[i] is the i% percentile of absolute pixel_colors on norm_shift_tile , norm_shift_z . This is used to compute nb.omp.initial_intensity_thresh if not provided. norm_shift_tile : Integer . Tile that is used to compute abs_intensity_percentile from which dp_norm_shift , background_weight_shift and intensity_thresh are computed. norm_shift_z : Integer . z-plane that is used to compute abs_intensity_percentile from which dp_norm_shift , background_weight_shift and intensity_thresh are computed. gene_efficiency_intensity_thresh : Float . gene_efficiency is computed from spots with intensity greater than this. By default, it is set to the config['call_spots']['gene_efficiency_intensity_thresh_percentile'] percentile of the intensity computed for all pixels on the mid z-plane of the most central tile","title":"call_spots"},{"location":"notebook_comments/#omp","text":"omp page contains gene assignments and info for spots located at the local maxima of the gene coefficients returned by OMP . Also contains info about spot_shape which indicates the expected sign of the OMP coefficient in a neighbourhood centered on a spot. Page added to Notebook in pipeline/call_spots_omp.py initial_intensity_thresh : Float . To save time in call_spots_omp , coefficients only found for pixels with intensity of absolute spot_colors greater than initial_intensity_thresh . This threshold is set to the config['omp']['initial_intensity_thresh_percentile'] percentile of the absolute intensity of all pixels on the mid z-plane of the central tile (uses nb.call_spots.abs_intensity_percentile ). It is also clamped between the min and max values given in config file. shape_tile : Integer or None . spot_shape was found from spots detected on this tile. None if spot_shape not computed in this experiment. shape_spot_local_yxz : Numpy integer array [n_shape_spots x 3] or None . \\(yxz\\) coordinates on shape_tile , ref_round / ref_channel of spots used to compute spot_shape None if spot_shape not computed in this experiment. shape_spot_gene_no : Numpy integer array [n_shape_spots] or None . shape_spot_gene_no[s] is the gene that the spot at shape_spot_local_yxz[s] was assigned to. None if spot_shape not computed in this experiment. spot_shape_float : Numpy float array [shape_max_size[0] x shape_max_size[1] x shape_max_size[2]] or None . Mean of OMP coefficient sign in neighbourhood centered on spot. None if spot_shape not computed in this experiment. initial_pos_neighbour_thresh : Integer . Only spots with number of positive coefficient neighbours greater than this are saved to notebook. Typical = 4 in 2D and 40 in 3D (set to 10% of max number by default). spot_shape : Numpy integer array [shape_size_y x shape_size_y x shape_size_x] . Expected sign of OMP coefficient in neighbourhood centered on spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign. local_yxz : Numpy int16 array [n_spots, 3] . local_yxz[s] are the \\(yxz\\) coordinates of spot \\(s\\) found on tile[s] , ref_round , ref_channel . To get global_yxz , add nb.stitch.tile_origin[tile[s]] . tile : Numpy int16 array [n_spots] . Tile each spot was found on. colors : Numpy int32 array [n_spots x n_rounds x n_channels] . [s, r, c] is the intensity of spot \\(s\\) on round \\(r\\) , channel \\(c\\) . It will be -tile_pixel_value_shift if that round/channel not used otherwise integer. gene_no : Numpy int16 array [n_spots] . gene_no[s] is the index of the gene assigned to spot \\(s\\) . n_neighbours_pos : Numpy int16 array [n_spots] . Number of positive pixels around each spot in neighbourhood given by spot_shape==1 . Max is sum(spot_shape==1) . n_neighbours_neg : Numpy int16 array [n_spots] . Number of negative pixels around each spot in neighbourhood given by spot_shape==-1 . Max is sum(spot_shape==-1) . intensity : Numpy float32 array [n_spots] . \\(\\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}})\\) where \\(\\pmb{\\zeta}_s=\\) colors[s, r]/color_norm_factor[r] .","title":"omp"},{"location":"notebook_comments/#thresholds","text":"thresholds page contains quality thresholds which affect which spots plotted and which are exported to pciSeq . Page added to Notebook when utils/pciseq/export_to_pciseq is run. intensity : Float . Final accepted reference and OMP spots require intensity > thresholds[intensity] . This is copied from config[thresholds] and if not given there, will be set to nb.call_spots.gene_efficiency_intensity_thresh . intensity for a really intense spot is about 1 so intensity_thresh should be less than this. score_ref : Float . Final accepted reference spots are those which pass quality_threshold which is: nb.ref_spots.score > thresholds[score_ref] and intensity > thresholds[intensity] . This is copied from config[thresholds] . Max score is 1 so score_ref should be less than this. score_omp : Float . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) . This is copied from config[thresholds] . Max score is 1 so score_thresh should be less than this. score_omp_multiplier : Float . Final accepted OMP spots are those which pass quality_threshold which is: score > thresholds[score_omp] and intensity > thresholds[intensity] . score is given by: score = (score_omp_multiplier * n_neighbours_pos + n_neighbours_neg) / (score_omp_multiplier * n_neighbours_pos_max + n_neighbours_neg_max) . This is copied from config[thresholds] .","title":"thresholds"},{"location":"run_code/","text":"Running the code Once the configuration file has been set up with the path /Users/user/coppafish/experiment/settings.ini , the code can be run via the command line or using a python script: Command Line Python Script python -m coppafish /Users/user/coppafish/experiment/settings.ini from coppafish import run_pipeline ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = run_pipeline ( ini_file ) If the pipeline has already been partially run and the notebook.npz file exists in the output directory, the above will pick up the pipeline from the last stage it finished. So for a notebook that contains the pages file_names , basic_info , extract , extract_debug and find_spots , running the above code will start the pipeline at the stitch stage . Check data before running The functions view_raw , view_filter and view_find_spots can be run before the Notebook is created if a valid configuration file is provided. So if there is a dataset of questionable quality, it may be worth running some of these first to see if it looks ok. In particular, view_raw may be useful for checking the correct channels are used, or to see if specific tiles/z-planes should be removed . Re-run section If at any stage, if a section of the pipeline needs re-running, then the relevant NotebookPage must first be removed from the Notebook, before the configuration file parameters for that section can be altered . Re-run register_initial The code below illustrates how you can re-run the register_initial step of the pipeline with different configuration file parameters. If the last line is uncommented, the full pipeline will be run, starting with register_initial , and the Notebook will be saved as notebook_new.npz in the output directory. Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini from coppafish import Notebook , run_pipeline nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) config = nb . get_config ()[ 'register_initial' ] print ( 'Using config file saved to notebook:' ) print ( f \"shift_max_range: { config [ 'shift_max_range' ] } \" ) print ( f \"shift_score_thresh_multiplier: { config [ 'shift_score_thresh_multiplier' ] } \" ) # Change register_initial del nb . register_initial # delete old register_initial nb . save ( nb_file_new ) # save Notebook with no register_initial page to new file # so does not overwrite old Notebook # Load in new notebook with new config file nb_new = Notebook ( nb_file_new , ini_file_new ) config_new = nb_new . get_config ()[ 'register_initial' ] print ( f 'Using new config file { ini_file_new } :' ) print ( f \"shift_max_range: { config_new [ 'shift_max_range' ] } \" ) print ( f \"shift_score_thresh_multiplier: { config_new [ 'shift_score_thresh_multiplier' ] } \" ) # nb = run_pipeline(ini_file_new) # Uncomment this line to run pipeline starting from # register_initial Using config file saved to notebook: shift_max_range: [500, 500, 10] shift_score_thresh_multiplier: 1.5 Using new config file /Users/user/coppafish/experiment/settings_new.ini: shift_max_range: [600, 600, 20] shift_score_thresh_multiplier: 1.2 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [register_initial] shift_max_range = 600, 600, 20 shift_score_thresh_multiplier = 1.2 If the section that needs running is call_reference_spots , then the procedure is slightly different because this step adds variables to the ref_spots page as well as creating the call_spots page. The OMP section is slightly different too because it saves files to the output directory. Exporting to pciSeq To save the results of the pipeline as a .csv file which can then be plotted with pciSeq, one of the following can be run (assuming path to the config file is /Users/user/coppafish/experiment/settings.ini and the path to the notebook file is /Users/user/coppafish/experiment/notebook.npz ): Command Line Python Script Using Config Path Python Script Using Notebook Path python -m coppafish /Users/user/coppafish/experiment/settings.ini -export from coppafish import Notebook , export_to_pciseq ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) export_to_pciseq ( nb ) from coppafish import Notebook , export_to_pciseq nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) export_to_pciseq ( nb ) This will save a csv file in the output_dir for each method ( omp and ref_spots ) of finding spots, and assigning genes to them. The names of the files are specified through config['file_names']['pciseq'] . Each file will contain: y - y coordinate of each spot in stitched coordinate system. x - x coordinate of each spot in stitched coordinate system. z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). Gene - Name of gene each spot was assigned to. An example file is given here . Thresholding Only spots which pass quality_threshold are saved. This depends on parameters given in config['thresholds'] . For a reference spot, \\(s\\) , to pass the thresholding, it must satisfy the following: \\[ \\displaylines{\\Delta_s > \\Delta_{thresh}\\\\ \\chi_s > \\chi_{thresh}} \\] Where: \\(\\Delta_s\\) is the maximum dot product score on iteration 0 for spot \\(s\\) across all genes (i.e. \\(\\Delta_s = \\max_g(\\Delta_{s0g})\\) ). \\(\\Delta_{thresh}\\) is config['thresholds']['score_ref'] . \\(\\chi_s\\) is the intensity of spot \\(s\\) . \\(\\chi_{thresh}\\) is config['thresholds']['intensity'] . If this is not provided, it is set to nb.call_spots.gene_efficiency_intensity_thresh . For an OMP spot, \\(s\\) , to pass the thresholding, it must satisfy the following: \\[ \\displaylines{\\gamma_s > \\gamma_{thresh}\\\\ \\chi_s > \\chi_{thresh}} \\] Where: \\(\\gamma_s\\) is the OMP score for spot \\(s\\) . \\(\\gamma_{thresh}\\) is config['thresholds']['score_omp'] . \\(\\chi_s\\) and \\(\\chi_{thresh}\\) are the same as for the reference spots . It is important that these thresholds are greater than 0, because when running the pipeline, we try to save a lot of spots. The idea being this is that it is better to do the thresholding after the pipeline has been run, rather than during the pipeline. This is because, if there were too few spots in the latter case, much of the pipeline would have to be re-run to obtain new spots, but in the former case, you can just change the threshold values. Once export_to_pciseq is run, the thresholds page will be added to the notebook. This inherits all the values from the thresholds section of the config file, the purpose of which is to remove the possibility of the thresholds section in the configuration file being changed once the results have been exported.","title":"Running the code"},{"location":"run_code/#running-the-code","text":"Once the configuration file has been set up with the path /Users/user/coppafish/experiment/settings.ini , the code can be run via the command line or using a python script: Command Line Python Script python -m coppafish /Users/user/coppafish/experiment/settings.ini from coppafish import run_pipeline ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = run_pipeline ( ini_file ) If the pipeline has already been partially run and the notebook.npz file exists in the output directory, the above will pick up the pipeline from the last stage it finished. So for a notebook that contains the pages file_names , basic_info , extract , extract_debug and find_spots , running the above code will start the pipeline at the stitch stage .","title":"Running the code"},{"location":"run_code/#check-data-before-running","text":"The functions view_raw , view_filter and view_find_spots can be run before the Notebook is created if a valid configuration file is provided. So if there is a dataset of questionable quality, it may be worth running some of these first to see if it looks ok. In particular, view_raw may be useful for checking the correct channels are used, or to see if specific tiles/z-planes should be removed .","title":"Check data before running"},{"location":"run_code/#re-run-section","text":"If at any stage, if a section of the pipeline needs re-running, then the relevant NotebookPage must first be removed from the Notebook, before the configuration file parameters for that section can be altered . Re-run register_initial The code below illustrates how you can re-run the register_initial step of the pipeline with different configuration file parameters. If the last line is uncommented, the full pipeline will be run, starting with register_initial , and the Notebook will be saved as notebook_new.npz in the output directory. Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini from coppafish import Notebook , run_pipeline nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) config = nb . get_config ()[ 'register_initial' ] print ( 'Using config file saved to notebook:' ) print ( f \"shift_max_range: { config [ 'shift_max_range' ] } \" ) print ( f \"shift_score_thresh_multiplier: { config [ 'shift_score_thresh_multiplier' ] } \" ) # Change register_initial del nb . register_initial # delete old register_initial nb . save ( nb_file_new ) # save Notebook with no register_initial page to new file # so does not overwrite old Notebook # Load in new notebook with new config file nb_new = Notebook ( nb_file_new , ini_file_new ) config_new = nb_new . get_config ()[ 'register_initial' ] print ( f 'Using new config file { ini_file_new } :' ) print ( f \"shift_max_range: { config_new [ 'shift_max_range' ] } \" ) print ( f \"shift_score_thresh_multiplier: { config_new [ 'shift_score_thresh_multiplier' ] } \" ) # nb = run_pipeline(ini_file_new) # Uncomment this line to run pipeline starting from # register_initial Using config file saved to notebook: shift_max_range: [500, 500, 10] shift_score_thresh_multiplier: 1.5 Using new config file /Users/user/coppafish/experiment/settings_new.ini: shift_max_range: [600, 600, 20] shift_score_thresh_multiplier: 1.2 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [register_initial] shift_max_range = 600, 600, 20 shift_score_thresh_multiplier = 1.2 If the section that needs running is call_reference_spots , then the procedure is slightly different because this step adds variables to the ref_spots page as well as creating the call_spots page. The OMP section is slightly different too because it saves files to the output directory.","title":"Re-run section"},{"location":"run_code/#exporting-to-pciseq","text":"To save the results of the pipeline as a .csv file which can then be plotted with pciSeq, one of the following can be run (assuming path to the config file is /Users/user/coppafish/experiment/settings.ini and the path to the notebook file is /Users/user/coppafish/experiment/notebook.npz ): Command Line Python Script Using Config Path Python Script Using Notebook Path python -m coppafish /Users/user/coppafish/experiment/settings.ini -export from coppafish import Notebook , export_to_pciseq ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) export_to_pciseq ( nb ) from coppafish import Notebook , export_to_pciseq nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) export_to_pciseq ( nb ) This will save a csv file in the output_dir for each method ( omp and ref_spots ) of finding spots, and assigning genes to them. The names of the files are specified through config['file_names']['pciseq'] . Each file will contain: y - y coordinate of each spot in stitched coordinate system. x - x coordinate of each spot in stitched coordinate system. z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). Gene - Name of gene each spot was assigned to. An example file is given here .","title":"Exporting to pciSeq"},{"location":"run_code/#thresholding","text":"Only spots which pass quality_threshold are saved. This depends on parameters given in config['thresholds'] . For a reference spot, \\(s\\) , to pass the thresholding, it must satisfy the following: \\[ \\displaylines{\\Delta_s > \\Delta_{thresh}\\\\ \\chi_s > \\chi_{thresh}} \\] Where: \\(\\Delta_s\\) is the maximum dot product score on iteration 0 for spot \\(s\\) across all genes (i.e. \\(\\Delta_s = \\max_g(\\Delta_{s0g})\\) ). \\(\\Delta_{thresh}\\) is config['thresholds']['score_ref'] . \\(\\chi_s\\) is the intensity of spot \\(s\\) . \\(\\chi_{thresh}\\) is config['thresholds']['intensity'] . If this is not provided, it is set to nb.call_spots.gene_efficiency_intensity_thresh . For an OMP spot, \\(s\\) , to pass the thresholding, it must satisfy the following: \\[ \\displaylines{\\gamma_s > \\gamma_{thresh}\\\\ \\chi_s > \\chi_{thresh}} \\] Where: \\(\\gamma_s\\) is the OMP score for spot \\(s\\) . \\(\\gamma_{thresh}\\) is config['thresholds']['score_omp'] . \\(\\chi_s\\) and \\(\\chi_{thresh}\\) are the same as for the reference spots . It is important that these thresholds are greater than 0, because when running the pipeline, we try to save a lot of spots. The idea being this is that it is better to do the thresholding after the pipeline has been run, rather than during the pipeline. This is because, if there were too few spots in the latter case, much of the pipeline would have to be re-run to obtain new spots, but in the former case, you can just change the threshold values. Once export_to_pciseq is run, the thresholds page will be added to the notebook. This inherits all the values from the thresholds section of the config file, the purpose of which is to remove the possibility of the thresholds section in the configuration file being changed once the results have been exported.","title":"Thresholding"},{"location":"view_results/","text":"Viewing the results Once the pipeline has completed the reference_spots step such that the Notebook contains the call_spots and ref_spots pages, the gene assignments of the spots found can be visualised using coppafish.Viewer . This can be opened via the command line or using a python script. It requires either the path to the config file ( /Users/user/coppafish/experiment/settings.ini ) or the path to the notebook file ( /Users/user/coppafish/experiment/notebook.npz ): Command Line Python Script Using Config Path Python Script Using Notebook Path python -m coppafish /Users/user/coppafish/experiment/settings.ini -view from coppafish import Notebook , Viewer ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) Viewer ( nb ) from coppafish import Notebook , Viewer nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) Viewer ( nb ) This will then open the napari viewer which will show the spots with a marker indicating which gene they were assigned to. If the Notebook contains the omp page, the spots plotted will be those found with the OMP algorithm , otherwise it will show the reference spots (those found on nb.basic_info.ref_round / nb.basic_info.ref_channel ) and their gene assignments found using call_reference_spots . An example is shown below: Markers not visible When the napari viewer first opens, the markers are often not visible because it is so far zoomed out. After zooming in by scrolling with the mouse, they should show up. pydevd Warning When opening the napari viewer, a series of warnings, starting with UserWarning: incompatible copy of pydevd already imported may occur e.g. These can be removed by uninstalling debugpy : pip uninstall debugpy Background Image By default, the spots will be plotted on top of the stitched DAPI image ( config['file_names']['big_dapi_image'] ) if it exists, otherwise there will not be a background image. To use a particular background image, when calling Viewer a second argument needs to be given ( Viewer(nb, background_image) . There are several options: 'dapi' : Will use config['file_names']['big_dapi_image'] if it exists, otherwise will be no background (default). 'anchor' : Will use config['file_names']['big_anchor_image'] if it exists, otherwise will be no background. Path to .npy or .npz file: An example would be '/Users/user/coppafish/experiment/background_image.npz' . This file must contain an image with axis in the order z-y-x (y-x if 2D). Numpy array: Can explicitly give the [n_z x n_y x n_x] ( [n_y x n_x] in 2D ) desired image. If a 2D image is provided for a 3D dataset, then this image will be used as the background image for each z-plane. Gene Markers The color and marker used for each gene can be provided through a csv file e.g. Viewer(nb, gene_marker_file='/Users/user/coppafish/experiment/gene_markers.csv') This csv file must contain 6 columns with the following headers: GeneNames - str , name of gene with first letter capital. ColorR - float , R gb color for plotting. ColorG - float , r G b color for plotting. ColorB - float , rg B color for plotting. napari_symbol - str , symbol used to plot in napari. mpl_symbol - str , equivalent of napari symbol in matplotlib. Only genes with names indicated in the GeneNames column will be shown in the viewer. If this file is not specified, then the default file will be used. Sidebar The sidebar on the left of the viewer includes various widgets which can change which spots are plotted. Select Genes To remove a gene from the plot, click on it in the gene legend. To add a gene that has been removed, click on it again. To view only one gene, right-click on it. To go back to viewing all genes, right-click on a gene which is the only gene selected. Image contrast The image contrast slider controls the brightness of the background image. Method If the notebook contains the omp page, a pair of buttons labelled OMP and Anchor will appear at the bottom of the sidebar. Initially the OMP button is selected meaning the spots shown are those saved in the omp page. Pressing the Anchor button will change the spots shown to be those saved in the ref_spots page. If the Notebook does not have the omp page, these buttons will not be present and the spots shown will be those saved in the ref_spots page. Score Range Only spots which pass a quality thresholding are shown in the viewer. Spots are assigned a score between 0 and 1 (can be larger for ref_spots ) which indicates the likelihood that the gene assignment is legitimate. When the viewer is first opened, only spots with score > config['thresholds']['score_omp'] ( score > config['thresholds']['score_ref'] if no omp page in notebook) are shown and the lower value of the score slider is set to this. The slider can then be used to view only spots which satisfy: slider_low_value < score < slider_high_value Effect of changing Method on Score Range slider The score computed for spots using the omp method , \\(\\gamma\\) , differs from that used with the ref_spots method , \\(\\Delta\\) . Thus, we keep a unique score range slider for each method so that when the method is changed using the buttons, the score range slider values will also change to the last values used with that method. Intensity Threshold As well as a score, each spot has an intensity value. The quality thresholding also means that only spots with intensity > intensity_thresh are shown in the viewer. Initially, intensity_thresh will be set to config['thresholds']['intensity'] and the slider can be used to change it. Effect of changing Method on Intensity Threshold slider The intensity is computed in the same way for OMP spots and reference spots . Thus the value of intensity_thresh will not change when the method is changed using the buttons. OMP Score Multiplier This is the \\(\\rho\\) parameter used in the calculation of OMP score . It will not affect anything if the method is Anchor . Diagnostics There are a few diagnostic plots which can be called with keyboard shortcuts while the viewer is open. i : Remove background image The background image can be removed from the viewer by pressing the i key. Once it has been removed, it can be put back by pressing i again. b : view_bleed_matrix The bleed matrix computed for the experiment can be shown by pressing b . This will then show the expected intensity of each dye in each channel. An example is shown below. Normalised Bleed Matrix Un-normalised Bleed Matrix Norm Button This plot as well as the view_bled_codes , view_codes and view_spot plots below have a Norm button. When these plots open the colorbar gives the intensity after normalisation has been applied to equalise the intensity between color channels i.e. weaker channels are boosted. To remove this normalisation, press the Norm button ( Norm will go red). The range of the colorbar will then change from approximately -1 to 1 to approximately -1000 to 1000 . This is then then the intensity that is read off from the filtered images saved as .npy files in config['file_names']['tile_dir'] . You can see above that in the un-normalised bleed matrix, channel 2 appears much weaker than it does in the normalised version. In both the call_reference_spots and OMP sections of the pipeline, spots are assigned to genes by comparing the spot_color to the bled_code of each gene. This is done using the normalised spot_color (with background removed ) and normalised bled_codes . This plot is useful to check that the dyes can be distinguished. I.e. that each column above (in the normalised version) is relatively unique. This is required so that genes can be distinguished based on their barcodes which indicate which dye each gene should appear with in each round. g : view_bled_codes The bled_code for each gene can be shown by pressing g . This shows two plots for each gene as shown below for Plp1 : The bottom plot shows the predicted code for the gene based on its barcode and the bleed_matrix . For this experiment, Plp1 has the barcode 2364463 meaning the column for round 0 in the bled_code is the column for dye 2 in the bleed_matrix , the column for round 1 above is the column for dye 3 in the bleed_matrix etc. These bled_codes are saved as nb.call_spots.bled_codes in the call_reference_spots section of the pipeline. The top plot shows the bled_code incorporating the calculated gene_efficiency . The gene_efficiency is the expected strength of a gene in each round and is given in brackets in the x-tick labels. These bled_codes are saved as nb.call_spots.bled_codes_ge in the call_reference_spots section of the pipeline. It is these bled_codes which are compared to spot_colors when assigning each spot to a particular gene. You can view other gene codes by scrolling up and down with the mouse when this plot is open. A green rectangle is added to each round/channel where the bled_code value is greater than 0.2. This indicates rounds/channels where the gene is particularly strong. It is also done in view_codes and view_omp_fit . Shift-g : gene_counts This plot indicates the number of reference spots assigned to each gene which also have nb.call_spots.score > score_thresh and nb.call_spots.intensity > intensity_thresh . The initial values of score_thresh and intensity_thresh used will be the current slider values. If the Notebook has the OMP page , then it will also show the number of OMP spots assigned to each gene which also have \\(\\gamma_s >\\) omp_score_thresh and nb.omp.intensity > intensity_thresh . The initial values of omp_score_thresh , omp_score_multiplier and intensity_thresh used will be the current slider values. h : histogram_score This plot shows the histogram of the score assigned to each spot for the current method . If the current method is OMP , the initial value of omp_score_multiplier will be the current slider value. Shift-h : histogram_2d_score This plot shows the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\) for spots detected with the OMP algorithm. The initial value of omp_score_multiplier will be the current slider value. k : view_scaled_k_means This plot shows how the bleed_matrix was computed. space : Change to select mode To run the diagnostics listed below, you need to change to select mode. This is done by pressing space-bar . In select mode, you won't be able to pan or zoom. To change back to pan/zoom mode, press space-bar again. On pressing space-bar , it should tell you in the bottom right corner of the viewer which mode you are in. When clicking on a spot in select mode, it should tell you in the bottom left corner, the spot_no of that spot and to which gene it was assigned. c : view_codes The spot_color for a particular spot can be compared to the bled_code (including gene_efficiency ) of the gene it was assigned to by pressing c after clicking on the spot in select mode : Background Not Removed Background Removed Pressing the Background button ( Background will turn red), shows what the spot_color looks like after the background genes have been removed . The spot_color and bled_code that are used when computing the dot product score are shown when the Background button is red but the Norm button is white. The subsequent plots all show the same spot as used here. s : view_spot The intensity in the neighbourhood of a particular spot in each round/channel can be viewed by pressing s after clicking on the spot in select mode : For a 3D experiment, this will only show the neighbourhood on the z-plane where the spot was found. This is also the case for view_omp . The cross-hair is in green in each round/channel where the bled_code (with gene_efficiency ) of the predicted gene ( Snca here) is greater than 0.2. d : view_score This plot indicates how the dot product score, \\(\\Delta_s\\) , was computed for a particular spot. Shift-i : view_intensity This plot shows how the intensity, \\(\\chi_s\\) was computed for a particular spot. o : view_omp The omp coefficients of all genes in neighbourhood of a particular spot can be viewed by pressing o after clicking on the spot in select mode : If a gene is not plotted, it means hardly any pixels had a non-zero coefficient for that gene. The gene indicated by BG2 is the background code for channel 2, which is equal to 1 in all rounds of channel 2 and 0 otherwise. It is then normalised to have an L2 norm of 1. E.g. for an experiment with 7 rounds and 7 channels, the \\(n_{rounds}\\times n_{channels}\\) code would be: array ([[ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ]]) The background codes are saved as nb.call_spots.background_codes in the call_reference_spots section of the pipeline. Each background vector is always fitted to each pixel by the omp algorithm, so they will always be shown. OMP Diagnostics on Reference Spots The functions view_omp , view_omp_fit and view_omp_score can also be run for reference spots by pressing the relavent key once a spot has been selected in the Anchor method . The view_omp_score function requires the Notebook to have the OMP page but the other two do not. shift-o : view_omp_fit The genes fitted to a particular spot at each stage of the OMP algorithm can be viewed by pressing shift-o after clicking on the spot in select mode : The image shown at column \\(i\\) of the first row is the residual spot_color before the gene shown at the column \\(i\\) of the second row has been fitted. The image shown at column \\(i+1\\) of the first row is the residual spot_color after the gene at column \\(i\\) of the second row has been removed. The image shown at column \\(i\\) of the second row is the gene that best explains the residual spot_color shown at column \\(i\\) of the first row. The second row shows the bled_code (with gene_efficiency ) for the gene multiplied by the coefficient found by the OMP algorithm for that gene at that iteration. The first plot of the second row shows the sum of the contribution of all background vectors. They are combined because there is no overlap between the different background_codes . The OMP algorithm stops when the absolute value of the dot_product_score ( DP in the title of images in the second row) to the best gene drops below the DP Threshold indicated in the title (0.225 here, this value is config['omp']['dp_thresh'] ). The gene shown in red is the first gene with a dot_product_score less than this and won't be fitted. Res in the title of images in the first row gives the L2 norm of the residual spot_color at that iteration of the OMP algorithm. I.e. we expect this to decrease as the omp algorithm proceeds. If you right-click on a column , it will run the view_score function to indicate how the dot product was calculated for that gene on that iteration. Shift-s : view_omp_score This shows how the OMP score, \\(\\gamma_s\\) , was computed for a particular spot. The initial value of omp_score_multiplier will be the current slider value.","title":"Viewing the results"},{"location":"view_results/#viewing-the-results","text":"Once the pipeline has completed the reference_spots step such that the Notebook contains the call_spots and ref_spots pages, the gene assignments of the spots found can be visualised using coppafish.Viewer . This can be opened via the command line or using a python script. It requires either the path to the config file ( /Users/user/coppafish/experiment/settings.ini ) or the path to the notebook file ( /Users/user/coppafish/experiment/notebook.npz ): Command Line Python Script Using Config Path Python Script Using Notebook Path python -m coppafish /Users/user/coppafish/experiment/settings.ini -view from coppafish import Notebook , Viewer ini_file = '/Users/user/coppafish/experiment/settings.ini' nb = Notebook ( config_file = ini_file ) Viewer ( nb ) from coppafish import Notebook , Viewer nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) Viewer ( nb ) This will then open the napari viewer which will show the spots with a marker indicating which gene they were assigned to. If the Notebook contains the omp page, the spots plotted will be those found with the OMP algorithm , otherwise it will show the reference spots (those found on nb.basic_info.ref_round / nb.basic_info.ref_channel ) and their gene assignments found using call_reference_spots . An example is shown below: Markers not visible When the napari viewer first opens, the markers are often not visible because it is so far zoomed out. After zooming in by scrolling with the mouse, they should show up. pydevd Warning When opening the napari viewer, a series of warnings, starting with UserWarning: incompatible copy of pydevd already imported may occur e.g. These can be removed by uninstalling debugpy : pip uninstall debugpy","title":"Viewing the results"},{"location":"view_results/#background-image","text":"By default, the spots will be plotted on top of the stitched DAPI image ( config['file_names']['big_dapi_image'] ) if it exists, otherwise there will not be a background image. To use a particular background image, when calling Viewer a second argument needs to be given ( Viewer(nb, background_image) . There are several options: 'dapi' : Will use config['file_names']['big_dapi_image'] if it exists, otherwise will be no background (default). 'anchor' : Will use config['file_names']['big_anchor_image'] if it exists, otherwise will be no background. Path to .npy or .npz file: An example would be '/Users/user/coppafish/experiment/background_image.npz' . This file must contain an image with axis in the order z-y-x (y-x if 2D). Numpy array: Can explicitly give the [n_z x n_y x n_x] ( [n_y x n_x] in 2D ) desired image. If a 2D image is provided for a 3D dataset, then this image will be used as the background image for each z-plane.","title":"Background Image"},{"location":"view_results/#gene-markers","text":"The color and marker used for each gene can be provided through a csv file e.g. Viewer(nb, gene_marker_file='/Users/user/coppafish/experiment/gene_markers.csv') This csv file must contain 6 columns with the following headers: GeneNames - str , name of gene with first letter capital. ColorR - float , R gb color for plotting. ColorG - float , r G b color for plotting. ColorB - float , rg B color for plotting. napari_symbol - str , symbol used to plot in napari. mpl_symbol - str , equivalent of napari symbol in matplotlib. Only genes with names indicated in the GeneNames column will be shown in the viewer. If this file is not specified, then the default file will be used.","title":"Gene Markers"},{"location":"view_results/#sidebar","text":"The sidebar on the left of the viewer includes various widgets which can change which spots are plotted.","title":"Sidebar"},{"location":"view_results/#select-genes","text":"To remove a gene from the plot, click on it in the gene legend. To add a gene that has been removed, click on it again. To view only one gene, right-click on it. To go back to viewing all genes, right-click on a gene which is the only gene selected.","title":"Select Genes"},{"location":"view_results/#image-contrast","text":"The image contrast slider controls the brightness of the background image.","title":"Image contrast"},{"location":"view_results/#method","text":"If the notebook contains the omp page, a pair of buttons labelled OMP and Anchor will appear at the bottom of the sidebar. Initially the OMP button is selected meaning the spots shown are those saved in the omp page. Pressing the Anchor button will change the spots shown to be those saved in the ref_spots page. If the Notebook does not have the omp page, these buttons will not be present and the spots shown will be those saved in the ref_spots page.","title":"Method"},{"location":"view_results/#score-range","text":"Only spots which pass a quality thresholding are shown in the viewer. Spots are assigned a score between 0 and 1 (can be larger for ref_spots ) which indicates the likelihood that the gene assignment is legitimate. When the viewer is first opened, only spots with score > config['thresholds']['score_omp'] ( score > config['thresholds']['score_ref'] if no omp page in notebook) are shown and the lower value of the score slider is set to this. The slider can then be used to view only spots which satisfy: slider_low_value < score < slider_high_value Effect of changing Method on Score Range slider The score computed for spots using the omp method , \\(\\gamma\\) , differs from that used with the ref_spots method , \\(\\Delta\\) . Thus, we keep a unique score range slider for each method so that when the method is changed using the buttons, the score range slider values will also change to the last values used with that method.","title":"Score Range"},{"location":"view_results/#intensity-threshold","text":"As well as a score, each spot has an intensity value. The quality thresholding also means that only spots with intensity > intensity_thresh are shown in the viewer. Initially, intensity_thresh will be set to config['thresholds']['intensity'] and the slider can be used to change it. Effect of changing Method on Intensity Threshold slider The intensity is computed in the same way for OMP spots and reference spots . Thus the value of intensity_thresh will not change when the method is changed using the buttons.","title":"Intensity Threshold"},{"location":"view_results/#omp-score-multiplier","text":"This is the \\(\\rho\\) parameter used in the calculation of OMP score . It will not affect anything if the method is Anchor .","title":"OMP Score Multiplier"},{"location":"view_results/#diagnostics","text":"There are a few diagnostic plots which can be called with keyboard shortcuts while the viewer is open.","title":"Diagnostics"},{"location":"view_results/#i-remove-background-image","text":"The background image can be removed from the viewer by pressing the i key. Once it has been removed, it can be put back by pressing i again.","title":"i: Remove background image"},{"location":"view_results/#b-view_bleed_matrix","text":"The bleed matrix computed for the experiment can be shown by pressing b . This will then show the expected intensity of each dye in each channel. An example is shown below. Normalised Bleed Matrix Un-normalised Bleed Matrix Norm Button This plot as well as the view_bled_codes , view_codes and view_spot plots below have a Norm button. When these plots open the colorbar gives the intensity after normalisation has been applied to equalise the intensity between color channels i.e. weaker channels are boosted. To remove this normalisation, press the Norm button ( Norm will go red). The range of the colorbar will then change from approximately -1 to 1 to approximately -1000 to 1000 . This is then then the intensity that is read off from the filtered images saved as .npy files in config['file_names']['tile_dir'] . You can see above that in the un-normalised bleed matrix, channel 2 appears much weaker than it does in the normalised version. In both the call_reference_spots and OMP sections of the pipeline, spots are assigned to genes by comparing the spot_color to the bled_code of each gene. This is done using the normalised spot_color (with background removed ) and normalised bled_codes . This plot is useful to check that the dyes can be distinguished. I.e. that each column above (in the normalised version) is relatively unique. This is required so that genes can be distinguished based on their barcodes which indicate which dye each gene should appear with in each round.","title":"b: view_bleed_matrix"},{"location":"view_results/#g-view_bled_codes","text":"The bled_code for each gene can be shown by pressing g . This shows two plots for each gene as shown below for Plp1 : The bottom plot shows the predicted code for the gene based on its barcode and the bleed_matrix . For this experiment, Plp1 has the barcode 2364463 meaning the column for round 0 in the bled_code is the column for dye 2 in the bleed_matrix , the column for round 1 above is the column for dye 3 in the bleed_matrix etc. These bled_codes are saved as nb.call_spots.bled_codes in the call_reference_spots section of the pipeline. The top plot shows the bled_code incorporating the calculated gene_efficiency . The gene_efficiency is the expected strength of a gene in each round and is given in brackets in the x-tick labels. These bled_codes are saved as nb.call_spots.bled_codes_ge in the call_reference_spots section of the pipeline. It is these bled_codes which are compared to spot_colors when assigning each spot to a particular gene. You can view other gene codes by scrolling up and down with the mouse when this plot is open. A green rectangle is added to each round/channel where the bled_code value is greater than 0.2. This indicates rounds/channels where the gene is particularly strong. It is also done in view_codes and view_omp_fit .","title":"g: view_bled_codes"},{"location":"view_results/#shift-g-gene_counts","text":"This plot indicates the number of reference spots assigned to each gene which also have nb.call_spots.score > score_thresh and nb.call_spots.intensity > intensity_thresh . The initial values of score_thresh and intensity_thresh used will be the current slider values. If the Notebook has the OMP page , then it will also show the number of OMP spots assigned to each gene which also have \\(\\gamma_s >\\) omp_score_thresh and nb.omp.intensity > intensity_thresh . The initial values of omp_score_thresh , omp_score_multiplier and intensity_thresh used will be the current slider values.","title":"Shift-g: gene_counts"},{"location":"view_results/#h-histogram_score","text":"This plot shows the histogram of the score assigned to each spot for the current method . If the current method is OMP , the initial value of omp_score_multiplier will be the current slider value.","title":"h: histogram_score"},{"location":"view_results/#shift-h-histogram_2d_score","text":"This plot shows the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\) for spots detected with the OMP algorithm. The initial value of omp_score_multiplier will be the current slider value.","title":"Shift-h: histogram_2d_score"},{"location":"view_results/#k-view_scaled_k_means","text":"This plot shows how the bleed_matrix was computed.","title":"k: view_scaled_k_means"},{"location":"view_results/#space-change-to-select-mode","text":"To run the diagnostics listed below, you need to change to select mode. This is done by pressing space-bar . In select mode, you won't be able to pan or zoom. To change back to pan/zoom mode, press space-bar again. On pressing space-bar , it should tell you in the bottom right corner of the viewer which mode you are in. When clicking on a spot in select mode, it should tell you in the bottom left corner, the spot_no of that spot and to which gene it was assigned.","title":"space: Change to select mode"},{"location":"view_results/#c-view_codes","text":"The spot_color for a particular spot can be compared to the bled_code (including gene_efficiency ) of the gene it was assigned to by pressing c after clicking on the spot in select mode : Background Not Removed Background Removed Pressing the Background button ( Background will turn red), shows what the spot_color looks like after the background genes have been removed . The spot_color and bled_code that are used when computing the dot product score are shown when the Background button is red but the Norm button is white. The subsequent plots all show the same spot as used here.","title":"c: view_codes"},{"location":"view_results/#s-view_spot","text":"The intensity in the neighbourhood of a particular spot in each round/channel can be viewed by pressing s after clicking on the spot in select mode : For a 3D experiment, this will only show the neighbourhood on the z-plane where the spot was found. This is also the case for view_omp . The cross-hair is in green in each round/channel where the bled_code (with gene_efficiency ) of the predicted gene ( Snca here) is greater than 0.2.","title":"s: view_spot"},{"location":"view_results/#d-view_score","text":"This plot indicates how the dot product score, \\(\\Delta_s\\) , was computed for a particular spot.","title":"d: view_score"},{"location":"view_results/#shift-i-view_intensity","text":"This plot shows how the intensity, \\(\\chi_s\\) was computed for a particular spot.","title":"Shift-i: view_intensity"},{"location":"view_results/#o-view_omp","text":"The omp coefficients of all genes in neighbourhood of a particular spot can be viewed by pressing o after clicking on the spot in select mode : If a gene is not plotted, it means hardly any pixels had a non-zero coefficient for that gene. The gene indicated by BG2 is the background code for channel 2, which is equal to 1 in all rounds of channel 2 and 0 otherwise. It is then normalised to have an L2 norm of 1. E.g. for an experiment with 7 rounds and 7 channels, the \\(n_{rounds}\\times n_{channels}\\) code would be: array ([[ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ], [ 0. , 0. , 0.378 , 0. , 0. , 0. , 0. ]]) The background codes are saved as nb.call_spots.background_codes in the call_reference_spots section of the pipeline. Each background vector is always fitted to each pixel by the omp algorithm, so they will always be shown. OMP Diagnostics on Reference Spots The functions view_omp , view_omp_fit and view_omp_score can also be run for reference spots by pressing the relavent key once a spot has been selected in the Anchor method . The view_omp_score function requires the Notebook to have the OMP page but the other two do not.","title":"o: view_omp"},{"location":"view_results/#shift-o-view_omp_fit","text":"The genes fitted to a particular spot at each stage of the OMP algorithm can be viewed by pressing shift-o after clicking on the spot in select mode : The image shown at column \\(i\\) of the first row is the residual spot_color before the gene shown at the column \\(i\\) of the second row has been fitted. The image shown at column \\(i+1\\) of the first row is the residual spot_color after the gene at column \\(i\\) of the second row has been removed. The image shown at column \\(i\\) of the second row is the gene that best explains the residual spot_color shown at column \\(i\\) of the first row. The second row shows the bled_code (with gene_efficiency ) for the gene multiplied by the coefficient found by the OMP algorithm for that gene at that iteration. The first plot of the second row shows the sum of the contribution of all background vectors. They are combined because there is no overlap between the different background_codes . The OMP algorithm stops when the absolute value of the dot_product_score ( DP in the title of images in the second row) to the best gene drops below the DP Threshold indicated in the title (0.225 here, this value is config['omp']['dp_thresh'] ). The gene shown in red is the first gene with a dot_product_score less than this and won't be fitted. Res in the title of images in the first row gives the L2 norm of the residual spot_color at that iteration of the OMP algorithm. I.e. we expect this to decrease as the omp algorithm proceeds. If you right-click on a column , it will run the view_score function to indicate how the dot product was calculated for that gene on that iteration.","title":"shift-o: view_omp_fit"},{"location":"view_results/#shift-s-view_omp_score","text":"This shows how the OMP score, \\(\\gamma_s\\) , was computed for a particular spot. The initial value of omp_score_multiplier will be the current slider value.","title":"Shift-s: view_omp_score"},{"location":"code/sep_round_reg/","text":"get_shift ( config , spot_yxz_base , spot_yxz_transform , z_scale_base , z_scale_transform , is_3d ) Find shift from base to transform. Parameters: Name Type Description Default config dict register_initial section of config file corresponding to spot_yxz_base. required spot_yxz_base np . ndarray Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. required spot_yxz_transform np . ndarray Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. required z_scale_base float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required z_scale_transform float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required is_3d bool Whether pipeline is 3D or not. required Returns: Type Description np . ndarray shift - float [shift_y, shift_x, shift_z] . Best shift found. np . ndarray shift_score - float . Score of best shift found. np . ndarray min_score - float . Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this. Source code in docs/scripts/sep_round_reg.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def get_shift ( config : dict , spot_yxz_base : np . ndarray , spot_yxz_transform : np . ndarray , z_scale_base : float , z_scale_transform : float , is_3d : bool ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , dict ]: \"\"\" Find shift from base to transform. Args: config: register_initial section of config file corresponding to spot_yxz_base. spot_yxz_base: Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. spot_yxz_transform: Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. is_3d: Whether pipeline is 3D or not. Returns: `shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. `shift_score` - `float`. Score of best shift found. `min_score` - `float`. Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this. \"\"\" coords = [ 'y' , 'x' , 'z' ] shifts = {} for i in range ( len ( coords )): shifts [ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if not is_3d : config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 'z' ] = np . array ([ 0 ], dtype = int ) shift , shift_score , shift_score_thresh , debug_info = \\ compute_shift ( spot_yxz_base , spot_yxz_transform , config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ 'y' ], shifts [ 'x' ], shifts [ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], [ z_scale_base , z_scale_transform ], config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) return shift , np . asarray ( shift_score ), np . asarray ( shift_score_thresh ), debug_info run_sep_round_reg ( config_file , config_file_full , channels_to_save , transform = None ) This runs the pipeline for a separate round up till the end of the stitching stage and then finds the affine transform that takes it to the anchor image of the full pipeline run. It then saves the corresponding transformed images for the channels of the separate round indicated by channels_to_save . Parameters: Name Type Description Default config_file str Path to config file for separate round. This should have only 1 round, that round being an anchor round and only one channel being used so filtering is only done on the anchor channel. required config_file_full str Path to config file for full pipeline run, for which full notebook exists. required channels_to_save List Channels of the separate round, that will be saved to the output directory in the same coordinate system as the anchor round of the full run. required transform Optional [ np . ndarray ] float [4 x 3] . Can provide the affine transform which transforms the separate round onto the anchor image of the full pipeline run. If not provided, it will be computed. None Source code in docs/scripts/sep_round_reg.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def run_sep_round_reg ( config_file : str , config_file_full : str , channels_to_save : List , transform : Optional [ np . ndarray ] = None ): \"\"\" This runs the pipeline for a separate round up till the end of the stitching stage and then finds the affine transform that takes it to the anchor image of the full pipeline run. It then saves the corresponding transformed images for the channels of the separate round indicated by `channels_to_save`. Args: config_file: Path to config file for separate round. This should have only 1 round, that round being an anchor round and only one channel being used so filtering is only done on the anchor channel. config_file_full: Path to config file for full pipeline run, for which full notebook exists. channels_to_save: Channels of the separate round, that will be saved to the output directory in the same coordinate system as the anchor round of the full run. transform: `float [4 x 3]`. Can provide the affine transform which transforms the separate round onto the anchor image of the full pipeline run. If not provided, it will be computed. \"\"\" # Get all information from full pipeline results - global spot positions and z scaling nb_full = initialize_nb ( config_file_full ) global_yxz_full = nb_full . ref_spots . local_yxz + nb_full . stitch . tile_origin [ nb_full . ref_spots . tile ] # run pipeline to get as far as a set of global coordinates for the separate round anchor. nb = initialize_nb ( config_file ) config = nb . get_config () run_extract ( nb ) run_find_spots ( nb ) if not nb . has_page ( \"stitch\" ): nbp_stitch = stitch ( config [ 'stitch' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_stitch else : warnings . warn ( 'stitch' , utils . warnings . NotebookPageWarning ) # scale z coordinate so in units of xy pixels as other 2 coordinates are. z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy # z_scale_full = nb_full.basic_info.pixel_size_z / nb_full.basic_info.pixel_size_xy # need both z_scales to be the same for final transform_image to work. Does not always seem to be the case though z_scale_full = z_scale # Compute centre of stitched image, as when running PCR, coordinates are centred first. yx_origin = np . round ( nb . stitch . tile_origin [:, : 2 ]) . astype ( int ) z_origin = np . round ( nb . stitch . tile_origin [:, 2 ]) . astype ( int ) . flatten () yx_size = np . max ( yx_origin , axis = 0 ) + nb . basic_info . tile_sz if nb . basic_info . is_3d : z_size = z_origin . max () + nb . basic_info . nz image_centre = np . floor ( np . append ( yx_size , z_size ) / 2 ) . astype ( int ) else : image_centre = np . append ( np . floor ( yx_size / 2 ) . astype ( int ), 0 ) if not nb . has_page ( 'reg_to_anchor_info' ): nbp = setup . NotebookPage ( 'reg_to_anchor_info' ) if transform is not None : nbp . transform = transform else : # remove duplicate spots spot_local_yxz = nb . find_spots . spot_details [:, - 3 :] spot_tile = nb . find_spots . spot_details [:, 0 ] not_duplicate = get_non_duplicate ( nb . stitch . tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , spot_local_yxz , spot_tile ) global_yxz = spot_local_yxz [ not_duplicate ] + nb . stitch . tile_origin [ spot_tile [ not_duplicate ]] # Only keep isolated points far from neighbour if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'register' ][ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'register' ][ 'neighb_dist_thresh_2d' ] n_iter = config [ 'register' ][ 'n_iter' ] isolated = get_isolated_points ( global_yxz * [ 1 , 1 , z_scale ], 2 * neighb_dist_thresh ) isolated_full = get_isolated_points ( global_yxz_full * [ 1 , 1 , z_scale_full ], 2 * neighb_dist_thresh ) global_yxz = global_yxz [ isolated , :] global_yxz_full = global_yxz_full [ isolated_full , :] # get initial shift from separate round to the full anchor image nbp . shift , nbp . shift_score , nbp . shift_score_thresh , debug_info = \\ get_shift ( config [ 'register_initial' ], global_yxz , global_yxz_full , z_scale , z_scale_full , nb . basic_info . is_3d ) # # Uncomment to produce plot showing best shift found # view_shifts(debug_info['shifts_2d'], debug_info['scores_2d'], debug_info['shifts_3d'], # debug_info['scores_3d'], nbp.shift, debug_info['min_score_2d'], # debug_info['shift_2d_initial'], # nbp.shift_score_thresh, debug_info['shift_thresh'], # config['register_initial']['shift_score_thresh_min_dist'], # config['register_initial']['shift_score_thresh_max_dist']) # Get affine transform from separate round to full anchor image start_transform = np . eye ( 4 , 3 ) # no scaling just shift to start off icp start_transform [ 3 ] = nbp . shift * [ 1 , 1 , z_scale ] nbp . transform , n_matches , error , is_converged = \\ get_single_affine_transform ( global_yxz , global_yxz_full , z_scale , z_scale_full , start_transform , neighb_dist_thresh , image_centre , n_iter ) nbp . n_matches = int ( n_matches ) nbp . error = float ( error ) nbp . is_converged = bool ( is_converged ) nb += nbp # save results of transform found else : nbp = nb . reg_to_anchor_info if transform is not None : if ( transform != nb . reg_to_anchor_info . transform ) . any (): raise ValueError ( f \"transform given is: \\n { transform } . \\n This differs \" f \"from nb.reg_to_anchor_info.transform: \\n { nb . reg_to_anchor_info . transform } \" ) # save all the images for c in channels_to_save : im_file = os . path . join ( nb . file_names . output_dir , f 'sep_round_channel { c } _transformed.npz' ) if c == nb . basic_info . ref_channel : from_nd2 = False else : from_nd2 = True image_stitch = utils . npy . save_stitched ( None , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . ref_round , c , from_nd2 , config [ 'stitch' ][ 'save_image_zero_thresh' ]) image_transform = transform_image ( image_stitch , nbp . transform , image_centre [: image_stitch . ndim ], z_scale ) if nb . basic_info . is_3d : # Put z axis first for saving image_transform = np . moveaxis ( image_transform , - 1 , 0 ) np . savez_compressed ( im_file , image_transform ) transform_image ( image , transform , image_centre , z_scale ) This transforms image to a new coordinate system by applying transform to every pixel in image . Parameters: Name Type Description Default image np . ndarray int [n_y x n_x (x n_z)] . image which is to be transformed. required transform np . ndarray float [4 x 3] . Affine transform which transforms image which is applied to every pixel in image to form a new transformed image. required image_centre np . ndarray int [image.ndim] . Pixel coordinates were centred by subtracting this first when computing affine transform. So when applying affine transform, pixels will also be shifted by this amount. z centre i.e. image_centre[2] is in units of z-pixels. required z_scale int Scaling to put z coordinates in same units as yx coordinates. required Returns: Type Description np . ndarray int [n_y x n_x (x n_z)] . image transformed according to transform . Source code in docs/scripts/sep_round_reg.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def transform_image ( image : np . ndarray , transform : np . ndarray , image_centre : np . ndarray , z_scale : int ) -> np . ndarray : \"\"\" This transforms `image` to a new coordinate system by applying `transform` to every pixel in `image`. Args: image: `int [n_y x n_x (x n_z)]`. image which is to be transformed. transform: `float [4 x 3]`. Affine transform which transforms image which is applied to every pixel in image to form a new transformed image. image_centre: `int [image.ndim]`. Pixel coordinates were centred by subtracting this first when computing affine transform. So when applying affine transform, pixels will also be shifted by this amount. z centre i.e. `image_centre[2]` is in units of z-pixels. z_scale: Scaling to put z coordinates in same units as yx coordinates. Returns: `int [n_y x n_x (x n_z)]`. `image` transformed according to `transform`. \"\"\" im_transformed = np . zeros_like ( image ) yxz = jnp . asarray ( np . where ( image != 0 )) . T . reshape ( - 1 , image . ndim ) image_values = image [ tuple ([ yxz [:, i ] for i in range ( image . ndim )])] tile_size = jnp . asarray ( im_transformed . shape ) if image . ndim == 2 : tile_size = jnp . append ( tile_size , 1 ) image_centre = np . append ( image_centre , 0 ) yxz = np . hstack (( yxz , np . zeros (( yxz . shape [ 0 ], 1 )))) yxz_transform , in_range = apply_transform ( yxz , jnp . asarray ( transform ), jnp . asarray ( image_centre ), z_scale , tile_size ) yxz_transform = np . asarray ( yxz_transform [ in_range ]) image_values = image_values [ np . asarray ( in_range )] im_transformed [ tuple ([ yxz_transform [:, i ] for i in range ( image . ndim )])] = image_values return im_transformed","title":"Sep round reg"},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.get_shift","text":"Find shift from base to transform. Parameters: Name Type Description Default config dict register_initial section of config file corresponding to spot_yxz_base. required spot_yxz_base np . ndarray Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. required spot_yxz_transform np . ndarray Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. required z_scale_base float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required z_scale_transform float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required is_3d bool Whether pipeline is 3D or not. required Returns: Type Description np . ndarray shift - float [shift_y, shift_x, shift_z] . Best shift found. np . ndarray shift_score - float . Score of best shift found. np . ndarray min_score - float . Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this. Source code in docs/scripts/sep_round_reg.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def get_shift ( config : dict , spot_yxz_base : np . ndarray , spot_yxz_transform : np . ndarray , z_scale_base : float , z_scale_transform : float , is_3d : bool ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , dict ]: \"\"\" Find shift from base to transform. Args: config: register_initial section of config file corresponding to spot_yxz_base. spot_yxz_base: Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. spot_yxz_transform: Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. is_3d: Whether pipeline is 3D or not. Returns: `shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. `shift_score` - `float`. Score of best shift found. `min_score` - `float`. Threshold score that was calculated, i.e. range of shifts searched changed until score exceeded this. \"\"\" coords = [ 'y' , 'x' , 'z' ] shifts = {} for i in range ( len ( coords )): shifts [ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if not is_3d : config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 'z' ] = np . array ([ 0 ], dtype = int ) shift , shift_score , shift_score_thresh , debug_info = \\ compute_shift ( spot_yxz_base , spot_yxz_transform , config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ 'y' ], shifts [ 'x' ], shifts [ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], [ z_scale_base , z_scale_transform ], config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) return shift , np . asarray ( shift_score ), np . asarray ( shift_score_thresh ), debug_info","title":"get_shift()"},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.run_sep_round_reg","text":"This runs the pipeline for a separate round up till the end of the stitching stage and then finds the affine transform that takes it to the anchor image of the full pipeline run. It then saves the corresponding transformed images for the channels of the separate round indicated by channels_to_save . Parameters: Name Type Description Default config_file str Path to config file for separate round. This should have only 1 round, that round being an anchor round and only one channel being used so filtering is only done on the anchor channel. required config_file_full str Path to config file for full pipeline run, for which full notebook exists. required channels_to_save List Channels of the separate round, that will be saved to the output directory in the same coordinate system as the anchor round of the full run. required transform Optional [ np . ndarray ] float [4 x 3] . Can provide the affine transform which transforms the separate round onto the anchor image of the full pipeline run. If not provided, it will be computed. None Source code in docs/scripts/sep_round_reg.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def run_sep_round_reg ( config_file : str , config_file_full : str , channels_to_save : List , transform : Optional [ np . ndarray ] = None ): \"\"\" This runs the pipeline for a separate round up till the end of the stitching stage and then finds the affine transform that takes it to the anchor image of the full pipeline run. It then saves the corresponding transformed images for the channels of the separate round indicated by `channels_to_save`. Args: config_file: Path to config file for separate round. This should have only 1 round, that round being an anchor round and only one channel being used so filtering is only done on the anchor channel. config_file_full: Path to config file for full pipeline run, for which full notebook exists. channels_to_save: Channels of the separate round, that will be saved to the output directory in the same coordinate system as the anchor round of the full run. transform: `float [4 x 3]`. Can provide the affine transform which transforms the separate round onto the anchor image of the full pipeline run. If not provided, it will be computed. \"\"\" # Get all information from full pipeline results - global spot positions and z scaling nb_full = initialize_nb ( config_file_full ) global_yxz_full = nb_full . ref_spots . local_yxz + nb_full . stitch . tile_origin [ nb_full . ref_spots . tile ] # run pipeline to get as far as a set of global coordinates for the separate round anchor. nb = initialize_nb ( config_file ) config = nb . get_config () run_extract ( nb ) run_find_spots ( nb ) if not nb . has_page ( \"stitch\" ): nbp_stitch = stitch ( config [ 'stitch' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_stitch else : warnings . warn ( 'stitch' , utils . warnings . NotebookPageWarning ) # scale z coordinate so in units of xy pixels as other 2 coordinates are. z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy # z_scale_full = nb_full.basic_info.pixel_size_z / nb_full.basic_info.pixel_size_xy # need both z_scales to be the same for final transform_image to work. Does not always seem to be the case though z_scale_full = z_scale # Compute centre of stitched image, as when running PCR, coordinates are centred first. yx_origin = np . round ( nb . stitch . tile_origin [:, : 2 ]) . astype ( int ) z_origin = np . round ( nb . stitch . tile_origin [:, 2 ]) . astype ( int ) . flatten () yx_size = np . max ( yx_origin , axis = 0 ) + nb . basic_info . tile_sz if nb . basic_info . is_3d : z_size = z_origin . max () + nb . basic_info . nz image_centre = np . floor ( np . append ( yx_size , z_size ) / 2 ) . astype ( int ) else : image_centre = np . append ( np . floor ( yx_size / 2 ) . astype ( int ), 0 ) if not nb . has_page ( 'reg_to_anchor_info' ): nbp = setup . NotebookPage ( 'reg_to_anchor_info' ) if transform is not None : nbp . transform = transform else : # remove duplicate spots spot_local_yxz = nb . find_spots . spot_details [:, - 3 :] spot_tile = nb . find_spots . spot_details [:, 0 ] not_duplicate = get_non_duplicate ( nb . stitch . tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , spot_local_yxz , spot_tile ) global_yxz = spot_local_yxz [ not_duplicate ] + nb . stitch . tile_origin [ spot_tile [ not_duplicate ]] # Only keep isolated points far from neighbour if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'register' ][ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'register' ][ 'neighb_dist_thresh_2d' ] n_iter = config [ 'register' ][ 'n_iter' ] isolated = get_isolated_points ( global_yxz * [ 1 , 1 , z_scale ], 2 * neighb_dist_thresh ) isolated_full = get_isolated_points ( global_yxz_full * [ 1 , 1 , z_scale_full ], 2 * neighb_dist_thresh ) global_yxz = global_yxz [ isolated , :] global_yxz_full = global_yxz_full [ isolated_full , :] # get initial shift from separate round to the full anchor image nbp . shift , nbp . shift_score , nbp . shift_score_thresh , debug_info = \\ get_shift ( config [ 'register_initial' ], global_yxz , global_yxz_full , z_scale , z_scale_full , nb . basic_info . is_3d ) # # Uncomment to produce plot showing best shift found # view_shifts(debug_info['shifts_2d'], debug_info['scores_2d'], debug_info['shifts_3d'], # debug_info['scores_3d'], nbp.shift, debug_info['min_score_2d'], # debug_info['shift_2d_initial'], # nbp.shift_score_thresh, debug_info['shift_thresh'], # config['register_initial']['shift_score_thresh_min_dist'], # config['register_initial']['shift_score_thresh_max_dist']) # Get affine transform from separate round to full anchor image start_transform = np . eye ( 4 , 3 ) # no scaling just shift to start off icp start_transform [ 3 ] = nbp . shift * [ 1 , 1 , z_scale ] nbp . transform , n_matches , error , is_converged = \\ get_single_affine_transform ( global_yxz , global_yxz_full , z_scale , z_scale_full , start_transform , neighb_dist_thresh , image_centre , n_iter ) nbp . n_matches = int ( n_matches ) nbp . error = float ( error ) nbp . is_converged = bool ( is_converged ) nb += nbp # save results of transform found else : nbp = nb . reg_to_anchor_info if transform is not None : if ( transform != nb . reg_to_anchor_info . transform ) . any (): raise ValueError ( f \"transform given is: \\n { transform } . \\n This differs \" f \"from nb.reg_to_anchor_info.transform: \\n { nb . reg_to_anchor_info . transform } \" ) # save all the images for c in channels_to_save : im_file = os . path . join ( nb . file_names . output_dir , f 'sep_round_channel { c } _transformed.npz' ) if c == nb . basic_info . ref_channel : from_nd2 = False else : from_nd2 = True image_stitch = utils . npy . save_stitched ( None , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . ref_round , c , from_nd2 , config [ 'stitch' ][ 'save_image_zero_thresh' ]) image_transform = transform_image ( image_stitch , nbp . transform , image_centre [: image_stitch . ndim ], z_scale ) if nb . basic_info . is_3d : # Put z axis first for saving image_transform = np . moveaxis ( image_transform , - 1 , 0 ) np . savez_compressed ( im_file , image_transform )","title":"run_sep_round_reg()"},{"location":"code/sep_round_reg/#docs.scripts.sep_round_reg.transform_image","text":"This transforms image to a new coordinate system by applying transform to every pixel in image . Parameters: Name Type Description Default image np . ndarray int [n_y x n_x (x n_z)] . image which is to be transformed. required transform np . ndarray float [4 x 3] . Affine transform which transforms image which is applied to every pixel in image to form a new transformed image. required image_centre np . ndarray int [image.ndim] . Pixel coordinates were centred by subtracting this first when computing affine transform. So when applying affine transform, pixels will also be shifted by this amount. z centre i.e. image_centre[2] is in units of z-pixels. required z_scale int Scaling to put z coordinates in same units as yx coordinates. required Returns: Type Description np . ndarray int [n_y x n_x (x n_z)] . image transformed according to transform . Source code in docs/scripts/sep_round_reg.py 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def transform_image ( image : np . ndarray , transform : np . ndarray , image_centre : np . ndarray , z_scale : int ) -> np . ndarray : \"\"\" This transforms `image` to a new coordinate system by applying `transform` to every pixel in `image`. Args: image: `int [n_y x n_x (x n_z)]`. image which is to be transformed. transform: `float [4 x 3]`. Affine transform which transforms image which is applied to every pixel in image to form a new transformed image. image_centre: `int [image.ndim]`. Pixel coordinates were centred by subtracting this first when computing affine transform. So when applying affine transform, pixels will also be shifted by this amount. z centre i.e. `image_centre[2]` is in units of z-pixels. z_scale: Scaling to put z coordinates in same units as yx coordinates. Returns: `int [n_y x n_x (x n_z)]`. `image` transformed according to `transform`. \"\"\" im_transformed = np . zeros_like ( image ) yxz = jnp . asarray ( np . where ( image != 0 )) . T . reshape ( - 1 , image . ndim ) image_values = image [ tuple ([ yxz [:, i ] for i in range ( image . ndim )])] tile_size = jnp . asarray ( im_transformed . shape ) if image . ndim == 2 : tile_size = jnp . append ( tile_size , 1 ) image_centre = np . append ( image_centre , 0 ) yxz = np . hstack (( yxz , np . zeros (( yxz . shape [ 0 ], 1 )))) yxz_transform , in_range = apply_transform ( yxz , jnp . asarray ( transform ), jnp . asarray ( image_centre ), z_scale , tile_size ) yxz_transform = np . asarray ( yxz_transform [ in_range ]) image_values = image_values [ np . asarray ( in_range )] im_transformed [ tuple ([ yxz_transform [:, i ] for i in range ( image . ndim )])] = image_values return im_transformed","title":"transform_image()"},{"location":"code/call_spots/background/","text":"fit_background ( spot_colors , weight_shift = 0 ) This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. 0 Returns: Type Description np . ndarray residual - float [n_spots x n_rounds x n_channels] . spot_colors after background removed. np . ndarray coef - float [n_spots, n_channels] . coefficient value for each background vector found for each spot. np . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def fit_background ( spot_colors : np . ndarray , weight_shift : float = 0 ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_spots x n_rounds x n_channels]`. `spot_colors` after background removed. - coef - `float [n_spots, n_channels]`. coefficient value for each background vector found for each spot. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" if weight_shift < 1e-20 : warnings . warn ( f 'weight_shift value given, { weight_shift } is below 1e-20.' f 'Using weight_shift=1e-20 to stop blow up to infinity.' ) weight_shift = np . clip ( weight_shift , 1e-20 , np . inf ) # ensure weight_shift > 1e-20 to avoid blow up to infinity. n_rounds , n_channels = spot_colors [ 0 ] . shape background_vectors = np . repeat ( np . expand_dims ( np . eye ( n_channels ), axis = 1 ), n_rounds , axis = 1 ) # give background_vectors an L2 norm of 1 so can compare coefficients with other genes. background_vectors = background_vectors / np . linalg . norm ( background_vectors , axis = ( 1 , 2 ), keepdims = True ) weight_factor = 1 / ( np . abs ( spot_colors ) + weight_shift ) spot_weight = spot_colors * weight_factor background_weight = np . ones (( 1 , n_rounds , n_channels )) * background_vectors [ 0 , 0 , 0 ] * weight_factor coef = np . sum ( spot_weight * background_weight , axis = 1 ) / np . sum ( background_weight ** 2 , axis = 1 ) residual = spot_colors - np . expand_dims ( coef , 1 ) * np . ones (( 1 , n_rounds , n_channels )) * background_vectors [ 0 , 0 , 0 ] # # Old method, about 10x slower # n_spots = spot_colors.shape[0] # coef = np.zeros([n_spots, n_channels]) # background_contribution = np.zeros_like(spot_colors) # background_vectors = np.zeros([n_channels, n_rounds, n_channels]) # for c in range(n_channels): # weight_factor = np.zeros([n_spots, n_rounds]) # for r in range(n_rounds): # weight_factor[:, r] = 1 / (abs(spot_colors[:, r, c]) + weight_shift) # weight_factor = np.expand_dims(weight_factor, 2) # # background_vector = np.zeros([1, n_rounds, n_channels]) # background_vector[:, :, c] = 1 # # give background_vector an L2 norm of 1 so can compare coefficients with other genes. # background_vector = background_vector / np.expand_dims(np.linalg.norm(background_vector, axis=(1, 2)), (1, 2)) # background_vectors[c] = background_vector # # background_weight = background_vector * weight_factor # spot_weight = spot_colors * weight_factor # # coef[:, c] = np.sum(spot_weight * background_weight, axis=(1, 2) # ) / np.sum(background_weight ** 2, axis=(1, 2)) # background_contribution[:, :, c] = np.expand_dims(coef[:, c], 1) * background_vector[0, 0, c] # # residual = spot_colors - background_contribution return residual , coef , background_vectors Optimised fit_background ( spot_colors , weight_shift ) This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. required Returns: Type Description jnp . ndarray residual - float [n_spots x n_rounds x n_channels] . spot_colors after background removed. jnp . ndarray coef - float [n_spots, n_channels] . coefficient value for each background vector found for each spot. jnp . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background_optimised.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @partial ( jax . jit , static_argnums = 1 ) def fit_background ( spot_colors : jnp . ndarray , weight_shift : float ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_spots x n_rounds x n_channels]`. `spot_colors` after background removed. - coef - `float [n_spots, n_channels]`. coefficient value for each background vector found for each spot. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" return jax . vmap ( fit_background_single , in_axes = ( 0 , None ), out_axes = ( 0 , 0 , None ))( spot_colors , weight_shift ) fit_background_single ( spot_color , weight_shift ) This determines the coefficient of the background vectors. Coefficients determined using a weighted dot product as to avoid over-fitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_color jnp . ndarray float [n_rounds x n_channels] . Spot color normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. required Returns: Type Description jnp . ndarray residual - float [n_rounds x n_channels] . spot_color after background removed. jnp . ndarray coefs - float [n_channels] . coefficient value for each background vector. jnp . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background_optimised.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def fit_background_single ( spot_color : jnp . ndarray , weight_shift : float ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" This determines the coefficient of the background vectors. Coefficients determined using a weighted dot product as to avoid over-fitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_color: `float [n_rounds x n_channels]`. Spot color normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_rounds x n_channels]`. `spot_color` after background removed. - coefs - `float [n_channels]`. coefficient value for each background vector. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" n_rounds , n_channels = spot_color . shape background_vectors = jnp . repeat ( jnp . expand_dims ( jnp . eye ( n_channels ), axis = 1 ), n_rounds , axis = 1 ) # give background_vectors an L2 norm of 1 so can compare coefficients with other genes. background_vectors = background_vectors / jnp . linalg . norm ( background_vectors , axis = ( 1 , 2 ), keepdims = True ) # array of correct shape containing the non-zero value of background_vectors everywhere. background_nz_value = jnp . full (( n_rounds , n_channels ), background_vectors [ 0 , 0 , 0 ]) weight_factor = 1 / ( jnp . abs ( spot_color ) + weight_shift ) spot_weight = spot_color * weight_factor background_weight = background_nz_value * weight_factor coefs = jnp . sum ( spot_weight * background_weight , axis = 0 ) / jnp . sum ( background_weight ** 2 , axis = 0 ) residual = spot_color - coefs * background_nz_value return residual , coefs , background_vectors","title":"Background"},{"location":"code/call_spots/background/#coppafish.call_spots.background.fit_background","text":"This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. 0 Returns: Type Description np . ndarray residual - float [n_spots x n_rounds x n_channels] . spot_colors after background removed. np . ndarray coef - float [n_spots, n_channels] . coefficient value for each background vector found for each spot. np . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def fit_background ( spot_colors : np . ndarray , weight_shift : float = 0 ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_spots x n_rounds x n_channels]`. `spot_colors` after background removed. - coef - `float [n_spots, n_channels]`. coefficient value for each background vector found for each spot. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" if weight_shift < 1e-20 : warnings . warn ( f 'weight_shift value given, { weight_shift } is below 1e-20.' f 'Using weight_shift=1e-20 to stop blow up to infinity.' ) weight_shift = np . clip ( weight_shift , 1e-20 , np . inf ) # ensure weight_shift > 1e-20 to avoid blow up to infinity. n_rounds , n_channels = spot_colors [ 0 ] . shape background_vectors = np . repeat ( np . expand_dims ( np . eye ( n_channels ), axis = 1 ), n_rounds , axis = 1 ) # give background_vectors an L2 norm of 1 so can compare coefficients with other genes. background_vectors = background_vectors / np . linalg . norm ( background_vectors , axis = ( 1 , 2 ), keepdims = True ) weight_factor = 1 / ( np . abs ( spot_colors ) + weight_shift ) spot_weight = spot_colors * weight_factor background_weight = np . ones (( 1 , n_rounds , n_channels )) * background_vectors [ 0 , 0 , 0 ] * weight_factor coef = np . sum ( spot_weight * background_weight , axis = 1 ) / np . sum ( background_weight ** 2 , axis = 1 ) residual = spot_colors - np . expand_dims ( coef , 1 ) * np . ones (( 1 , n_rounds , n_channels )) * background_vectors [ 0 , 0 , 0 ] # # Old method, about 10x slower # n_spots = spot_colors.shape[0] # coef = np.zeros([n_spots, n_channels]) # background_contribution = np.zeros_like(spot_colors) # background_vectors = np.zeros([n_channels, n_rounds, n_channels]) # for c in range(n_channels): # weight_factor = np.zeros([n_spots, n_rounds]) # for r in range(n_rounds): # weight_factor[:, r] = 1 / (abs(spot_colors[:, r, c]) + weight_shift) # weight_factor = np.expand_dims(weight_factor, 2) # # background_vector = np.zeros([1, n_rounds, n_channels]) # background_vector[:, :, c] = 1 # # give background_vector an L2 norm of 1 so can compare coefficients with other genes. # background_vector = background_vector / np.expand_dims(np.linalg.norm(background_vector, axis=(1, 2)), (1, 2)) # background_vectors[c] = background_vector # # background_weight = background_vector * weight_factor # spot_weight = spot_colors * weight_factor # # coef[:, c] = np.sum(spot_weight * background_weight, axis=(1, 2) # ) / np.sum(background_weight ** 2, axis=(1, 2)) # background_contribution[:, :, c] = np.expand_dims(coef[:, c], 1) * background_vector[0, 0, c] # # residual = spot_colors - background_contribution return residual , coef , background_vectors","title":"fit_background()"},{"location":"code/call_spots/background/#optimised","text":"","title":"Optimised"},{"location":"code/call_spots/background/#coppafish.call_spots.background_optimised.fit_background","text":"This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. required Returns: Type Description jnp . ndarray residual - float [n_spots x n_rounds x n_channels] . spot_colors after background removed. jnp . ndarray coef - float [n_spots, n_channels] . coefficient value for each background vector found for each spot. jnp . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background_optimised.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 @partial ( jax . jit , static_argnums = 1 ) def fit_background ( spot_colors : jnp . ndarray , weight_shift : float ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" This determines the coefficient of the background vectors for each spot. Coefficients determined using a weighted dot product as to avoid overfitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_spots x n_rounds x n_channels]`. `spot_colors` after background removed. - coef - `float [n_spots, n_channels]`. coefficient value for each background vector found for each spot. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" return jax . vmap ( fit_background_single , in_axes = ( 0 , None ), out_axes = ( 0 , 0 , None ))( spot_colors , weight_shift )","title":"fit_background()"},{"location":"code/call_spots/background/#coppafish.call_spots.background_optimised.fit_background_single","text":"This determines the coefficient of the background vectors. Coefficients determined using a weighted dot product as to avoid over-fitting and accounting for the fact that background coefficients are not updated after this. Note background_vectors[i] is 1 in channel i for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Parameters: Name Type Description Default spot_color jnp . ndarray float [n_rounds x n_channels] . Spot color normalised to equalise intensities between channels (and rounds). required weight_shift float shift to apply to weighting of each background vector to limit boost of weak spots. required Returns: Type Description jnp . ndarray residual - float [n_rounds x n_channels] . spot_color after background removed. jnp . ndarray coefs - float [n_channels] . coefficient value for each background vector. jnp . ndarray background_vectors float [n_channels x n_rounds x n_channels] . background_vectors[c] is the background vector for channel c. Source code in coppafish/call_spots/background_optimised.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def fit_background_single ( spot_color : jnp . ndarray , weight_shift : float ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" This determines the coefficient of the background vectors. Coefficients determined using a weighted dot product as to avoid over-fitting and accounting for the fact that background coefficients are not updated after this. !!! note `background_vectors[i]` is 1 in channel `i` for all rounds and 0 otherwise. It is then normalised to have L2 norm of 1 when summed over all rounds and channels. Args: spot_color: `float [n_rounds x n_channels]`. Spot color normalised to equalise intensities between channels (and rounds). weight_shift: shift to apply to weighting of each background vector to limit boost of weak spots. Returns: - residual - `float [n_rounds x n_channels]`. `spot_color` after background removed. - coefs - `float [n_channels]`. coefficient value for each background vector. - background_vectors `float [n_channels x n_rounds x n_channels]`. background_vectors[c] is the background vector for channel c. \"\"\" n_rounds , n_channels = spot_color . shape background_vectors = jnp . repeat ( jnp . expand_dims ( jnp . eye ( n_channels ), axis = 1 ), n_rounds , axis = 1 ) # give background_vectors an L2 norm of 1 so can compare coefficients with other genes. background_vectors = background_vectors / jnp . linalg . norm ( background_vectors , axis = ( 1 , 2 ), keepdims = True ) # array of correct shape containing the non-zero value of background_vectors everywhere. background_nz_value = jnp . full (( n_rounds , n_channels ), background_vectors [ 0 , 0 , 0 ]) weight_factor = 1 / ( jnp . abs ( spot_color ) + weight_shift ) spot_weight = spot_color * weight_factor background_weight = background_nz_value * weight_factor coefs = jnp . sum ( spot_weight * background_weight , axis = 0 ) / jnp . sum ( background_weight ** 2 , axis = 0 ) residual = spot_color - coefs * background_nz_value return residual , coefs , background_vectors","title":"fit_background_single()"},{"location":"code/call_spots/base/","text":"color_normalisation ( hist_values , hist_counts , thresh_intensities , thresh_probs , method ) This finds the normalisations for each round, r , and channel, c , such that if norm_spot_color[r,c] = spot_color[r,c] / norm_factor[r,c] , the probability of norm_spot_color being larger than thresh_intensities[i] is less than thresh_probs[i] for every i . Where the probability is based on all pixels from all tiles in that round and channel. Parameters: Name Type Description Default hist_values np . ndarray int [n_pixel_values] . All possible pixel values in saved tiff images i.e. n_pixel_values is approximately np.iinfo(np.uint16).max because tiffs saved as uint16 images. required hist_counts np . ndarray int [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round r , channel c which had the value hist_values[i] . required thresh_intensities Union [ float , List [ float ], np . ndarray ] float [n_thresholds] . Thresholds such that the probability of having a normalised spot_color greater than this are quite low. Need to be ascending. Typical: [0.5, 1, 5] i.e. we want most of normalised spot_colors to be less than 0.5 so high normalised spot color is on the order of 1 . required thresh_probs Union [ float , List [ float ], np . ndarray ] float [n_thresholds] . Probability of normalised spot color being greater than thresh_intensities[i] must be less than thresh_probs[i] . Needs to be same shape as thresh_intensities and descending. Typical: [0.01, 5e-4, 1e-5] i.e. want almost all non spot pixels to have normalised intensity less than 0.5 . required method str Must be one of the following: 'single' - A single normalisation factor is produced for all rounds of each channel i.e. norm_factor[r, b] for a given b value, will be the same for all r values. 'separate' - A different normalisation factor is made for each round and channel. required Returns: Type Description np . ndarray float [n_rounds x n_channels] . norm_factor such that norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c] . Source code in coppafish/call_spots/base.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def color_normalisation ( hist_values : np . ndarray , hist_counts : np . ndarray , thresh_intensities : Union [ float , List [ float ], np . ndarray ], thresh_probs : Union [ float , List [ float ], np . ndarray ], method : str ) -> np . ndarray : \"\"\" This finds the normalisations for each round, ```r```, and channel, ```c```, such that if ```norm_spot_color[r,c] = spot_color[r,c] / norm_factor[r,c]```, the probability of ```norm_spot_color``` being larger than ``` thresh_intensities[i]``` is less than ```thresh_probs[i]``` for every ```i```. Where the probability is based on all pixels from all tiles in that round and channel. Args: hist_values: ```int [n_pixel_values]```. All possible pixel values in saved tiff images i.e. n_pixel_values is approximately ```np.iinfo(np.uint16).max``` because tiffs saved as ```uint16``` images. hist_counts: ```int [n_pixel_values x n_rounds x n_channels]```. ```hist_counts[i, r, c]``` is the number of pixels across all tiles in round ```r```, channel ```c``` which had the value ```hist_values[i]```. thresh_intensities: ```float [n_thresholds]```. Thresholds such that the probability of having a normalised spot_color greater than this are quite low. Need to be ascending. Typical: ```[0.5, 1, 5]``` i.e. we want most of ```normalised spot_colors``` to be less than ```0.5``` so high normalised spot color is on the order of ```1```. thresh_probs: ```float [n_thresholds]```. Probability of normalised spot color being greater than ```thresh_intensities[i]``` must be less than ```thresh_probs[i]```. Needs to be same shape as thresh_intensities and descending. Typical: ```[0.01, 5e-4, 1e-5]``` i.e. want almost all non spot pixels to have normalised intensity less than ```0.5```. method: Must be one of the following: - ```'single'``` - A single normalisation factor is produced for all rounds of each channel i.e. ```norm_factor[r, b]``` for a given ```b``` value, will be the same for all ```r``` values. - ```'separate'``` - A different normalisation factor is made for each round and channel. Returns: ```float [n_rounds x n_channels]```. ```norm_factor``` such that ```norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c]```. \"\"\" if not utils . errors . check_shape ( hist_values , hist_counts . shape [: 1 ]): raise utils . errors . ShapeError ( 'hist_values' , hist_values . shape , hist_counts . shape [: 1 ]) # if only one value provided, turn to a list if isinstance ( thresh_intensities , ( float , int )): thresh_intensities = [ thresh_intensities ] if isinstance ( thresh_probs , ( float , int )): thresh_probs = [ thresh_probs ] if not utils . errors . check_shape ( np . array ( thresh_intensities ), np . array ( thresh_probs ) . shape ): raise utils . errors . ShapeError ( 'thresh_intensities' , np . array ( thresh_intensities ) . shape , np . array ( thresh_probs ) . shape ) # sort thresholds and check that thresh_probs descend as thresh_intensities increase ind = np . argsort ( thresh_intensities ) thresh_intensities = np . array ( thresh_intensities )[ ind ] thresh_probs = np . array ( thresh_probs )[ ind ] if not np . all ( np . diff ( thresh_probs ) <= 0 ): raise ValueError ( f \"thresh_probs given, { thresh_probs } , do not all descend as thresh_intensities,\" f \" { thresh_intensities } , increase.\" ) n_rounds , n_channels = hist_counts . shape [ 1 :] norm_factor = np . zeros (( n_rounds , n_channels )) for r_ind in range ( n_rounds ): if method . lower () == 'single' : r = np . arange ( n_rounds ) elif method . lower () == 'separate' : r = r_ind else : raise ValueError ( f \"method given was { method } but should be either 'single' or 'separate'\" ) for b in range ( n_channels ): hist_counts_rb = np . sum ( hist_counts [:, r , b ] . reshape ( hist_values . shape [ 0 ], - 1 ), axis = 1 ) # if not np.int32, get error in windows when cumsum goes negative. cum_sum_rb = np . cumsum ( hist_counts_rb . astype ( np . int64 )) n_pixels = cum_sum_rb [ - 1 ] norm_factor_rb = - np . inf for thresh_intensity , thresh_prob in zip ( thresh_intensities , thresh_probs ): prob = np . sum ( hist_counts_rb [ hist_values >= thresh_intensity * norm_factor_rb ]) / n_pixels if prob > thresh_prob : norm_factor_rb = hist_values [ np . where ( cum_sum_rb > ( 1 - thresh_prob ) * n_pixels )[ 0 ][ 1 ] ] / thresh_intensity norm_factor [ r , b ] = norm_factor_rb if r_ind == 0 and method . lower () == 'single' : break return norm_factor get_bled_codes ( gene_codes , bleed_matrix ) This gets bled_codes such that the spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan. Otherwise, will get confusion with dye indices in gene_codes being outside size of bleed_matrix . Note All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels with any nan values assumed to be 0. Parameters: Name Type Description Default gene_codes np . ndarray int [n_genes x n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene g in round r . required bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . Expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, :, d] . required Returns: Type Description np . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Source code in coppafish/call_spots/base.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def get_bled_codes ( gene_codes : np . ndarray , bleed_matrix : np . ndarray ) -> np . ndarray : \"\"\" This gets ```bled_codes``` such that the spot_color of a gene ```g``` in round ```r``` is expected to be a constant multiple of ```bled_codes[g, r]```. This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan. Otherwise, will get confusion with dye indices in `gene_codes` being outside size of `bleed_matrix`. !!! note All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels with any nan values assumed to be 0. Args: gene_codes: ```int [n_genes x n_rounds]```. ```gene_codes[g, r]``` indicates the dye that should be present for gene ```g``` in round ```r```. bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```. Expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, :, d]```. Returns: ```float [n_genes x n_rounds x n_channels]```. ```bled_codes``` such that ```spot_color``` of a gene ```g``` in round ```r``` is expected to be a constant multiple of ```bled_codes[g, r]```. \"\"\" n_genes = gene_codes . shape [ 0 ] n_rounds , n_channels , n_dyes = bleed_matrix . shape if not utils . errors . check_shape ( gene_codes , [ n_genes , n_rounds ]): raise utils . errors . ShapeError ( 'gene_codes' , gene_codes . shape , ( n_genes , n_rounds )) if gene_codes . max () >= n_dyes : ind_1 , ind_2 = np . where ( gene_codes == gene_codes . max ()) raise ValueError ( f \"gene_code for gene { ind_1 [ 0 ] } , round { ind_2 [ 0 ] } has a dye with index { gene_codes . max () } \" f \" but there are only { n_dyes } dyes.\" ) if gene_codes . min () < 0 : ind_1 , ind_2 = np . where ( gene_codes == gene_codes . min ()) raise ValueError ( f \"gene_code for gene { ind_1 [ 0 ] } , round { ind_2 [ 0 ] } has a dye with a negative index:\" f \" { gene_codes . min () } \" ) bled_codes = np . zeros (( n_genes , n_rounds , n_channels )) for g in range ( n_genes ): for r in range ( n_rounds ): for c in range ( n_channels ): bled_codes [ g , r , c ] = bleed_matrix [ r , c , gene_codes [ g , r ]] # Give all bled codes an L2 norm of 1 assuming any nan values are 0 norm_factor = np . expand_dims ( np . linalg . norm ( np . nan_to_num ( bled_codes ), axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in any rounds, this avoids blow up on next line bled_codes = bled_codes / norm_factor return bled_codes get_gene_efficiency ( spot_colors , spot_gene_no , gene_codes , bleed_matrix , min_spots , max_gene_efficiency = np . inf , min_gene_efficiency = 0 , min_gene_efficiency_factor = 1 ) gene_efficiency[g,r] gives the expected intensity of gene g in round r compared to that expected by the bleed_matrix . It is computed based on the average of all spot_colors assigned to that gene. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required spot_gene_no np . ndarray int [n_spots] . Gene each spot was assigned to. required gene_codes np . ndarray int [n_genes, n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene g in round r . required bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . For a spot, s matched to gene with dye d in round r , we expect spot_colors[s, r] \", to be a constant multiple of bleed_matrix[r, :, d] \" required min_spots int If number of spots assigned to a gene less than or equal to this, gene_efficiency[g]=1 for all rounds. Typical = 30. required max_gene_efficiency float Maximum allowed gene efficiency, i.e. any one round can be at most this times more important than the median round for every gene. Typical = 6. np.inf min_gene_efficiency float At most ceil(min_gene_efficiency_factor * n_rounds) rounds can have gene_efficiency below min_gene_efficiency for any given gene. Typical = 0.05 0 min_gene_efficiency_factor float At most ceil(min_gene_efficiency_factor * n_rounds) rounds can have gene_efficiency below min_gene_efficiency for any given gene. Typical = 0.2 1 Returns: Type Description np . ndarray float [n_genes x n_rounds] . gene_efficiency[g,r] gives the expected intensity of gene g in round r compared to that expected by the bleed_matrix . Source code in coppafish/call_spots/base.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_gene_efficiency ( spot_colors : np . ndarray , spot_gene_no : np . ndarray , gene_codes : np . ndarray , bleed_matrix : np . ndarray , min_spots : int , max_gene_efficiency : float = np . inf , min_gene_efficiency : float = 0 , min_gene_efficiency_factor : float = 1 ) -> np . ndarray : \"\"\" `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected by the `bleed_matrix`. It is computed based on the average of all `spot_colors` assigned to that gene. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). spot_gene_no: `int [n_spots]`. Gene each spot was assigned to. gene_codes: `int [n_genes, n_rounds]`. `gene_codes[g, r]` indicates the dye that should be present for gene `g` in round `r`. bleed_matrix: `float [n_rounds x n_channels x n_dyes]`. For a spot, `s` matched to gene with dye `d` in round `r`, we expect `spot_colors[s, r]`\", to be a constant multiple of `bleed_matrix[r, :, d]`\" min_spots: If number of spots assigned to a gene less than or equal to this, `gene_efficiency[g]=1` for all rounds. Typical = 30. max_gene_efficiency: Maximum allowed gene efficiency, i.e. any one round can be at most this times more important than the median round for every gene. Typical = 6. min_gene_efficiency: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.05 min_gene_efficiency_factor: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.2 Returns: `float [n_genes x n_rounds]`. `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected by the `bleed_matrix`. \"\"\" # Check n_spots, n_rounds, n_channels, n_genes consistent across all variables. if not utils . errors . check_shape ( spot_colors [ 0 ], bleed_matrix [:, :, 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( spot_colors . shape [ 0 ],) + bleed_matrix [:, :, 0 ] . shape ) if not utils . errors . check_shape ( spot_colors [:, 0 , 0 ], spot_gene_no . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , spot_gene_no . shape + bleed_matrix [:, :, 0 ] . shape ) n_genes , n_rounds = gene_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 , :, 0 ] . squeeze (), gene_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , spot_gene_no . shape + ( n_rounds ,) + ( bleed_matrix . shape [ 1 ],)) gene_no_oob = [ val for val in spot_gene_no if val < 0 or val >= n_genes ] if len ( gene_no_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"spot_gene_no\" , gene_no_oob [ 0 ], 0 , n_genes - 1 ) gene_efficiency = np . ones ([ n_genes , n_rounds ]) n_min_thresh = int ( np . ceil ( min_gene_efficiency_factor * n_rounds )) for g in range ( n_genes ): use = spot_gene_no == g if np . sum ( use ) > min_spots : round_strength = np . zeros ([ np . sum ( use ), n_rounds ]) for r in range ( n_rounds ): dye_ind = gene_codes [ g , r ] # below is equivalent to MATLAB spot_colors / bleed_matrix. round_strength [:, r ] = np . linalg . lstsq ( bleed_matrix [ r , :, dye_ind : dye_ind + 1 ], spot_colors [ use , r ] . transpose (), rcond = None )[ 0 ] # find a reference round for each gene as that with median strength. av_round_strength = np . median ( round_strength , 0 ) av_round = np . abs ( av_round_strength - np . median ( av_round_strength )) . argmin () # for each spot, find strength of each round relative to strength in # av_round. Need relative strength not absolute strength # because expect spot color to be constant multiple of bled code. # So for all genes, gene_efficiency[g, av_round] = 1 but av_round is different between genes. # Only use spots whose strength in RefRound is positive. use = round_strength [:, av_round ] > 0 if np . sum ( use ) > min_spots : relative_round_strength = round_strength [ use ] / np . expand_dims ( round_strength [ use , av_round ], 1 ) # Only use spots with gene efficiency below the maximum allowed. below_max = np . max ( relative_round_strength , axis = 1 ) < max_gene_efficiency # Only use spots with at most n_min_thresh rounds below the minimum. above_min = np . sum ( relative_round_strength < min_gene_efficiency , axis = 1 ) <= n_min_thresh use = np . array ([ below_max , above_min ]) . all ( axis = 0 ) if np . sum ( use ) > min_spots : gene_efficiency [ g ] = np . median ( relative_round_strength [ use ], 0 ) # set negative values to 0 gene_efficiency = np . clip ( gene_efficiency , 0 , np . inf ) return gene_efficiency get_non_duplicate ( tile_origin , use_tiles , tile_centre , spot_local_yxz , spot_tile ) Find duplicate spots as those detected on a tile which is not tile centre they are closest to. Parameters: Name Type Description Default tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required use_tiles List int [n_use_tiles] . Tiles used in the experiment. required tile_centre np . ndarray float [3] tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required spot_local_yxz np . ndarray int [n_spots x 3] . Coordinates of a spot s on tile spot_tile[s]. yxz[s, :2] are the yx coordinates in yx_pixels for spot s . yxz[s, 2] is the z coordinate in z_pixels for spot s . required spot_tile np . ndarray int [n_spots] . Tile each spot was found on. required Returns: Type Description np . ndarray bool [n_spots] . Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to. Source code in coppafish/call_spots/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_non_duplicate ( tile_origin : np . ndarray , use_tiles : List , tile_centre : np . ndarray , spot_local_yxz : np . ndarray , spot_tile : np . ndarray ) -> np . ndarray : \"\"\" Find duplicate spots as those detected on a tile which is not tile centre they are closest to. Args: tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. use_tiles: ```int [n_use_tiles]```. Tiles used in the experiment. tile_centre: ```float [3]``` ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. spot_local_yxz: ```int [n_spots x 3]```. Coordinates of a spot s on tile spot_tile[s]. ```yxz[s, :2]``` are the yx coordinates in ```yx_pixels``` for spot ```s```. ```yxz[s, 2]``` is the z coordinate in ```z_pixels``` for spot ```s```. spot_tile: ```int [n_spots]```. Tile each spot was found on. Returns: ```bool [n_spots]```. Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to. \"\"\" tile_centres = tile_origin [ use_tiles ] + tile_centre # Do not_duplicate search in 2D as overlap is only 2D tree_tiles = KDTree ( tile_centres [:, : 2 ]) if np . isnan ( tile_origin [ np . unique ( spot_tile )]) . any (): nan_tiles = np . unique ( spot_tile )[ np . unique ( np . where ( np . isnan ( tile_origin [ np . unique ( spot_tile )]))[ 0 ])] raise ValueError ( f \"tile_origin for tiles \\n { nan_tiles } \\n contains nan values but some spot_tile \" f \"also contains these tiles. Maybe remove these from use_tiles to continue. \\n \" f \"Also, consider coppafish.plot.n_spots_grid to check if these tiles have few spots.\" ) spot_global_yxz = spot_local_yxz + tile_origin [ spot_tile ] all_nearest_tile_ind = tree_tiles . query ( spot_global_yxz [:, : 2 ])[ 1 ] not_duplicate = np . asarray ( use_tiles )[ all_nearest_tile_ind . flatten ()] == spot_tile return not_duplicate","title":"Base"},{"location":"code/call_spots/base/#coppafish.call_spots.base.color_normalisation","text":"This finds the normalisations for each round, r , and channel, c , such that if norm_spot_color[r,c] = spot_color[r,c] / norm_factor[r,c] , the probability of norm_spot_color being larger than thresh_intensities[i] is less than thresh_probs[i] for every i . Where the probability is based on all pixels from all tiles in that round and channel. Parameters: Name Type Description Default hist_values np . ndarray int [n_pixel_values] . All possible pixel values in saved tiff images i.e. n_pixel_values is approximately np.iinfo(np.uint16).max because tiffs saved as uint16 images. required hist_counts np . ndarray int [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round r , channel c which had the value hist_values[i] . required thresh_intensities Union [ float , List [ float ], np . ndarray ] float [n_thresholds] . Thresholds such that the probability of having a normalised spot_color greater than this are quite low. Need to be ascending. Typical: [0.5, 1, 5] i.e. we want most of normalised spot_colors to be less than 0.5 so high normalised spot color is on the order of 1 . required thresh_probs Union [ float , List [ float ], np . ndarray ] float [n_thresholds] . Probability of normalised spot color being greater than thresh_intensities[i] must be less than thresh_probs[i] . Needs to be same shape as thresh_intensities and descending. Typical: [0.01, 5e-4, 1e-5] i.e. want almost all non spot pixels to have normalised intensity less than 0.5 . required method str Must be one of the following: 'single' - A single normalisation factor is produced for all rounds of each channel i.e. norm_factor[r, b] for a given b value, will be the same for all r values. 'separate' - A different normalisation factor is made for each round and channel. required Returns: Type Description np . ndarray float [n_rounds x n_channels] . norm_factor such that norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c] . Source code in coppafish/call_spots/base.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def color_normalisation ( hist_values : np . ndarray , hist_counts : np . ndarray , thresh_intensities : Union [ float , List [ float ], np . ndarray ], thresh_probs : Union [ float , List [ float ], np . ndarray ], method : str ) -> np . ndarray : \"\"\" This finds the normalisations for each round, ```r```, and channel, ```c```, such that if ```norm_spot_color[r,c] = spot_color[r,c] / norm_factor[r,c]```, the probability of ```norm_spot_color``` being larger than ``` thresh_intensities[i]``` is less than ```thresh_probs[i]``` for every ```i```. Where the probability is based on all pixels from all tiles in that round and channel. Args: hist_values: ```int [n_pixel_values]```. All possible pixel values in saved tiff images i.e. n_pixel_values is approximately ```np.iinfo(np.uint16).max``` because tiffs saved as ```uint16``` images. hist_counts: ```int [n_pixel_values x n_rounds x n_channels]```. ```hist_counts[i, r, c]``` is the number of pixels across all tiles in round ```r```, channel ```c``` which had the value ```hist_values[i]```. thresh_intensities: ```float [n_thresholds]```. Thresholds such that the probability of having a normalised spot_color greater than this are quite low. Need to be ascending. Typical: ```[0.5, 1, 5]``` i.e. we want most of ```normalised spot_colors``` to be less than ```0.5``` so high normalised spot color is on the order of ```1```. thresh_probs: ```float [n_thresholds]```. Probability of normalised spot color being greater than ```thresh_intensities[i]``` must be less than ```thresh_probs[i]```. Needs to be same shape as thresh_intensities and descending. Typical: ```[0.01, 5e-4, 1e-5]``` i.e. want almost all non spot pixels to have normalised intensity less than ```0.5```. method: Must be one of the following: - ```'single'``` - A single normalisation factor is produced for all rounds of each channel i.e. ```norm_factor[r, b]``` for a given ```b``` value, will be the same for all ```r``` values. - ```'separate'``` - A different normalisation factor is made for each round and channel. Returns: ```float [n_rounds x n_channels]```. ```norm_factor``` such that ```norm_spot_color[s,r,c] = spot_color[s,r,c] / norm_factor[r,c]```. \"\"\" if not utils . errors . check_shape ( hist_values , hist_counts . shape [: 1 ]): raise utils . errors . ShapeError ( 'hist_values' , hist_values . shape , hist_counts . shape [: 1 ]) # if only one value provided, turn to a list if isinstance ( thresh_intensities , ( float , int )): thresh_intensities = [ thresh_intensities ] if isinstance ( thresh_probs , ( float , int )): thresh_probs = [ thresh_probs ] if not utils . errors . check_shape ( np . array ( thresh_intensities ), np . array ( thresh_probs ) . shape ): raise utils . errors . ShapeError ( 'thresh_intensities' , np . array ( thresh_intensities ) . shape , np . array ( thresh_probs ) . shape ) # sort thresholds and check that thresh_probs descend as thresh_intensities increase ind = np . argsort ( thresh_intensities ) thresh_intensities = np . array ( thresh_intensities )[ ind ] thresh_probs = np . array ( thresh_probs )[ ind ] if not np . all ( np . diff ( thresh_probs ) <= 0 ): raise ValueError ( f \"thresh_probs given, { thresh_probs } , do not all descend as thresh_intensities,\" f \" { thresh_intensities } , increase.\" ) n_rounds , n_channels = hist_counts . shape [ 1 :] norm_factor = np . zeros (( n_rounds , n_channels )) for r_ind in range ( n_rounds ): if method . lower () == 'single' : r = np . arange ( n_rounds ) elif method . lower () == 'separate' : r = r_ind else : raise ValueError ( f \"method given was { method } but should be either 'single' or 'separate'\" ) for b in range ( n_channels ): hist_counts_rb = np . sum ( hist_counts [:, r , b ] . reshape ( hist_values . shape [ 0 ], - 1 ), axis = 1 ) # if not np.int32, get error in windows when cumsum goes negative. cum_sum_rb = np . cumsum ( hist_counts_rb . astype ( np . int64 )) n_pixels = cum_sum_rb [ - 1 ] norm_factor_rb = - np . inf for thresh_intensity , thresh_prob in zip ( thresh_intensities , thresh_probs ): prob = np . sum ( hist_counts_rb [ hist_values >= thresh_intensity * norm_factor_rb ]) / n_pixels if prob > thresh_prob : norm_factor_rb = hist_values [ np . where ( cum_sum_rb > ( 1 - thresh_prob ) * n_pixels )[ 0 ][ 1 ] ] / thresh_intensity norm_factor [ r , b ] = norm_factor_rb if r_ind == 0 and method . lower () == 'single' : break return norm_factor","title":"color_normalisation()"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_bled_codes","text":"This gets bled_codes such that the spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan. Otherwise, will get confusion with dye indices in gene_codes being outside size of bleed_matrix . Note All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels with any nan values assumed to be 0. Parameters: Name Type Description Default gene_codes np . ndarray int [n_genes x n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene g in round r . required bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . Expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, :, d] . required Returns: Type Description np . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Source code in coppafish/call_spots/base.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def get_bled_codes ( gene_codes : np . ndarray , bleed_matrix : np . ndarray ) -> np . ndarray : \"\"\" This gets ```bled_codes``` such that the spot_color of a gene ```g``` in round ```r``` is expected to be a constant multiple of ```bled_codes[g, r]```. This function should be run with full bleed_matrix with any rounds/channels/dyes outside those using set to nan. Otherwise, will get confusion with dye indices in `gene_codes` being outside size of `bleed_matrix`. !!! note All bled_codes returned with an L2 norm of 1 when summed over all rounds and channels with any nan values assumed to be 0. Args: gene_codes: ```int [n_genes x n_rounds]```. ```gene_codes[g, r]``` indicates the dye that should be present for gene ```g``` in round ```r```. bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```. Expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, :, d]```. Returns: ```float [n_genes x n_rounds x n_channels]```. ```bled_codes``` such that ```spot_color``` of a gene ```g``` in round ```r``` is expected to be a constant multiple of ```bled_codes[g, r]```. \"\"\" n_genes = gene_codes . shape [ 0 ] n_rounds , n_channels , n_dyes = bleed_matrix . shape if not utils . errors . check_shape ( gene_codes , [ n_genes , n_rounds ]): raise utils . errors . ShapeError ( 'gene_codes' , gene_codes . shape , ( n_genes , n_rounds )) if gene_codes . max () >= n_dyes : ind_1 , ind_2 = np . where ( gene_codes == gene_codes . max ()) raise ValueError ( f \"gene_code for gene { ind_1 [ 0 ] } , round { ind_2 [ 0 ] } has a dye with index { gene_codes . max () } \" f \" but there are only { n_dyes } dyes.\" ) if gene_codes . min () < 0 : ind_1 , ind_2 = np . where ( gene_codes == gene_codes . min ()) raise ValueError ( f \"gene_code for gene { ind_1 [ 0 ] } , round { ind_2 [ 0 ] } has a dye with a negative index:\" f \" { gene_codes . min () } \" ) bled_codes = np . zeros (( n_genes , n_rounds , n_channels )) for g in range ( n_genes ): for r in range ( n_rounds ): for c in range ( n_channels ): bled_codes [ g , r , c ] = bleed_matrix [ r , c , gene_codes [ g , r ]] # Give all bled codes an L2 norm of 1 assuming any nan values are 0 norm_factor = np . expand_dims ( np . linalg . norm ( np . nan_to_num ( bled_codes ), axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in any rounds, this avoids blow up on next line bled_codes = bled_codes / norm_factor return bled_codes","title":"get_bled_codes()"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_gene_efficiency","text":"gene_efficiency[g,r] gives the expected intensity of gene g in round r compared to that expected by the bleed_matrix . It is computed based on the average of all spot_colors assigned to that gene. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required spot_gene_no np . ndarray int [n_spots] . Gene each spot was assigned to. required gene_codes np . ndarray int [n_genes, n_rounds] . gene_codes[g, r] indicates the dye that should be present for gene g in round r . required bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . For a spot, s matched to gene with dye d in round r , we expect spot_colors[s, r] \", to be a constant multiple of bleed_matrix[r, :, d] \" required min_spots int If number of spots assigned to a gene less than or equal to this, gene_efficiency[g]=1 for all rounds. Typical = 30. required max_gene_efficiency float Maximum allowed gene efficiency, i.e. any one round can be at most this times more important than the median round for every gene. Typical = 6. np.inf min_gene_efficiency float At most ceil(min_gene_efficiency_factor * n_rounds) rounds can have gene_efficiency below min_gene_efficiency for any given gene. Typical = 0.05 0 min_gene_efficiency_factor float At most ceil(min_gene_efficiency_factor * n_rounds) rounds can have gene_efficiency below min_gene_efficiency for any given gene. Typical = 0.2 1 Returns: Type Description np . ndarray float [n_genes x n_rounds] . gene_efficiency[g,r] gives the expected intensity of gene g in round r compared to that expected by the bleed_matrix . Source code in coppafish/call_spots/base.py 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_gene_efficiency ( spot_colors : np . ndarray , spot_gene_no : np . ndarray , gene_codes : np . ndarray , bleed_matrix : np . ndarray , min_spots : int , max_gene_efficiency : float = np . inf , min_gene_efficiency : float = 0 , min_gene_efficiency_factor : float = 1 ) -> np . ndarray : \"\"\" `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected by the `bleed_matrix`. It is computed based on the average of all `spot_colors` assigned to that gene. Args: spot_colors: `float [n_spots x n_rounds x n_channels]`. Spot colors normalised to equalise intensities between channels (and rounds). spot_gene_no: `int [n_spots]`. Gene each spot was assigned to. gene_codes: `int [n_genes, n_rounds]`. `gene_codes[g, r]` indicates the dye that should be present for gene `g` in round `r`. bleed_matrix: `float [n_rounds x n_channels x n_dyes]`. For a spot, `s` matched to gene with dye `d` in round `r`, we expect `spot_colors[s, r]`\", to be a constant multiple of `bleed_matrix[r, :, d]`\" min_spots: If number of spots assigned to a gene less than or equal to this, `gene_efficiency[g]=1` for all rounds. Typical = 30. max_gene_efficiency: Maximum allowed gene efficiency, i.e. any one round can be at most this times more important than the median round for every gene. Typical = 6. min_gene_efficiency: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.05 min_gene_efficiency_factor: At most `ceil(min_gene_efficiency_factor * n_rounds)` rounds can have `gene_efficiency` below `min_gene_efficiency` for any given gene. Typical = 0.2 Returns: `float [n_genes x n_rounds]`. `gene_efficiency[g,r]` gives the expected intensity of gene `g` in round `r` compared to that expected by the `bleed_matrix`. \"\"\" # Check n_spots, n_rounds, n_channels, n_genes consistent across all variables. if not utils . errors . check_shape ( spot_colors [ 0 ], bleed_matrix [:, :, 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( spot_colors . shape [ 0 ],) + bleed_matrix [:, :, 0 ] . shape ) if not utils . errors . check_shape ( spot_colors [:, 0 , 0 ], spot_gene_no . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , spot_gene_no . shape + bleed_matrix [:, :, 0 ] . shape ) n_genes , n_rounds = gene_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 , :, 0 ] . squeeze (), gene_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , spot_gene_no . shape + ( n_rounds ,) + ( bleed_matrix . shape [ 1 ],)) gene_no_oob = [ val for val in spot_gene_no if val < 0 or val >= n_genes ] if len ( gene_no_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"spot_gene_no\" , gene_no_oob [ 0 ], 0 , n_genes - 1 ) gene_efficiency = np . ones ([ n_genes , n_rounds ]) n_min_thresh = int ( np . ceil ( min_gene_efficiency_factor * n_rounds )) for g in range ( n_genes ): use = spot_gene_no == g if np . sum ( use ) > min_spots : round_strength = np . zeros ([ np . sum ( use ), n_rounds ]) for r in range ( n_rounds ): dye_ind = gene_codes [ g , r ] # below is equivalent to MATLAB spot_colors / bleed_matrix. round_strength [:, r ] = np . linalg . lstsq ( bleed_matrix [ r , :, dye_ind : dye_ind + 1 ], spot_colors [ use , r ] . transpose (), rcond = None )[ 0 ] # find a reference round for each gene as that with median strength. av_round_strength = np . median ( round_strength , 0 ) av_round = np . abs ( av_round_strength - np . median ( av_round_strength )) . argmin () # for each spot, find strength of each round relative to strength in # av_round. Need relative strength not absolute strength # because expect spot color to be constant multiple of bled code. # So for all genes, gene_efficiency[g, av_round] = 1 but av_round is different between genes. # Only use spots whose strength in RefRound is positive. use = round_strength [:, av_round ] > 0 if np . sum ( use ) > min_spots : relative_round_strength = round_strength [ use ] / np . expand_dims ( round_strength [ use , av_round ], 1 ) # Only use spots with gene efficiency below the maximum allowed. below_max = np . max ( relative_round_strength , axis = 1 ) < max_gene_efficiency # Only use spots with at most n_min_thresh rounds below the minimum. above_min = np . sum ( relative_round_strength < min_gene_efficiency , axis = 1 ) <= n_min_thresh use = np . array ([ below_max , above_min ]) . all ( axis = 0 ) if np . sum ( use ) > min_spots : gene_efficiency [ g ] = np . median ( relative_round_strength [ use ], 0 ) # set negative values to 0 gene_efficiency = np . clip ( gene_efficiency , 0 , np . inf ) return gene_efficiency","title":"get_gene_efficiency()"},{"location":"code/call_spots/base/#coppafish.call_spots.base.get_non_duplicate","text":"Find duplicate spots as those detected on a tile which is not tile centre they are closest to. Parameters: Name Type Description Default tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required use_tiles List int [n_use_tiles] . Tiles used in the experiment. required tile_centre np . ndarray float [3] tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required spot_local_yxz np . ndarray int [n_spots x 3] . Coordinates of a spot s on tile spot_tile[s]. yxz[s, :2] are the yx coordinates in yx_pixels for spot s . yxz[s, 2] is the z coordinate in z_pixels for spot s . required spot_tile np . ndarray int [n_spots] . Tile each spot was found on. required Returns: Type Description np . ndarray bool [n_spots] . Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to. Source code in coppafish/call_spots/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_non_duplicate ( tile_origin : np . ndarray , use_tiles : List , tile_centre : np . ndarray , spot_local_yxz : np . ndarray , spot_tile : np . ndarray ) -> np . ndarray : \"\"\" Find duplicate spots as those detected on a tile which is not tile centre they are closest to. Args: tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. use_tiles: ```int [n_use_tiles]```. Tiles used in the experiment. tile_centre: ```float [3]``` ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. spot_local_yxz: ```int [n_spots x 3]```. Coordinates of a spot s on tile spot_tile[s]. ```yxz[s, :2]``` are the yx coordinates in ```yx_pixels``` for spot ```s```. ```yxz[s, 2]``` is the z coordinate in ```z_pixels``` for spot ```s```. spot_tile: ```int [n_spots]```. Tile each spot was found on. Returns: ```bool [n_spots]```. Whether spot_tile[s] is the tile that spot_global_yxz[s] is closest to. \"\"\" tile_centres = tile_origin [ use_tiles ] + tile_centre # Do not_duplicate search in 2D as overlap is only 2D tree_tiles = KDTree ( tile_centres [:, : 2 ]) if np . isnan ( tile_origin [ np . unique ( spot_tile )]) . any (): nan_tiles = np . unique ( spot_tile )[ np . unique ( np . where ( np . isnan ( tile_origin [ np . unique ( spot_tile )]))[ 0 ])] raise ValueError ( f \"tile_origin for tiles \\n { nan_tiles } \\n contains nan values but some spot_tile \" f \"also contains these tiles. Maybe remove these from use_tiles to continue. \\n \" f \"Also, consider coppafish.plot.n_spots_grid to check if these tiles have few spots.\" ) spot_global_yxz = spot_local_yxz + tile_origin [ spot_tile ] all_nearest_tile_ind = tree_tiles . query ( spot_global_yxz [:, : 2 ])[ 1 ] not_duplicate = np . asarray ( use_tiles )[ all_nearest_tile_ind . flatten ()] == spot_tile return not_duplicate","title":"get_non_duplicate()"},{"location":"code/call_spots/bleed_matrix/","text":"get_bleed_matrix ( spot_colors , initial_bleed_matrix , method , score_thresh = 0 , min_cluster_size = 10 , n_iter = 100 , score_thresh_anneal = True , debug =- 1 ) This returns a bleed matrix such that the expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, :, d] . Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Intensity found for each spot in each round and channel, normalized in some way to equalize channel intensities typically, the normalisation will be such that spot_colors vary between around -5 to 10 with most near 0 . required initial_bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . Initial guess for intensity we expect each dye to produce in each channel and round. Should be normalized in same way as spot_colors. required method str Must be one of the following: 'single' - A single bleed matrix is produced for all rounds. 'separate' - A different bleed matrix is made for each round. required score_thresh float Scalar between 0 and 1 . Threshold used for scaled_k_means affecting which spots contribute to bleed matrix estimate. 0 min_cluster_size int If less than this many points assigned to a dye, that dye mean vector will be set to 0 . 10 n_iter int Maximum number of iterations performed in scaled_k_means . 100 score_thresh_anneal bool If True , scaled_k_means will be performed twice. The second time starting with the output of the first and with score_thresh for cluster i set to the median of the scores assigned to cluster i in the first run. This limits the influence of bad spots to the bleed matrix. True debug int If this is >=0, then the debug_info dictionary will also be returned. If method == 'separate' , this specifies the round of the bleed_matrix calculation to return debugging info for. -1 Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , dict ]] bleed_matrix - float [n_rounds x n_channels x n_dyes] . bleed_matrix such that the expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, _, d] . Union [ np . ndarray , Tuple [ np . ndarray , dict ]] debug_info - dictionary containing useful information for debugging bleed matrix calculation. Each variable has size=3 in first dimension. var[0] refers to value with initial_bleed_matrix . var[1] refers to value after first scaled_k_means . var[2] refers to value after second k means (only if score_thresh_anneal == True ). cluster_ind : int8 [3 x n_vectors] . Index of dye each vector was assigned to in scaled_k_means . -1 means fell below score_thresh and not assigned. cluster_score : float16 [3 x n_vectors] . Value of dot product between each vector and dye assigned to. bleed_matrix : float16 [3 x n_channels x n_dyes] . bleed_matrix computed at each stage of calculation. Source code in coppafish/call_spots/bleed_matrix.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def get_bleed_matrix ( spot_colors : np . ndarray , initial_bleed_matrix : np . ndarray , method : str , score_thresh : float = 0 , min_cluster_size : int = 10 , n_iter : int = 100 , score_thresh_anneal : bool = True , debug : int = - 1 ) -> Union [ np . ndarray , Tuple [ np . ndarray , dict ]]: \"\"\" This returns a bleed matrix such that the expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, :, d]```. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Intensity found for each spot in each round and channel, normalized in some way to equalize channel intensities typically, the normalisation will be such that spot_colors vary between around ```-5``` to ```10``` with most near ```0```. initial_bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```. Initial guess for intensity we expect each dye to produce in each channel and round. Should be normalized in same way as spot_colors. method: Must be one of the following: - ```'single'``` - A single bleed matrix is produced for all rounds. - ```'separate'``` - A different bleed matrix is made for each round. score_thresh: Scalar between ```0``` and ```1```. Threshold used for ```scaled_k_means``` affecting which spots contribute to bleed matrix estimate. min_cluster_size: If less than this many points assigned to a dye, that dye mean vector will be set to ```0```. n_iter: Maximum number of iterations performed in ```scaled_k_means```. score_thresh_anneal: If `True`, `scaled_k_means` will be performed twice. The second time starting with the output of the first and with `score_thresh` for cluster `i` set to the median of the scores assigned to cluster `i` in the first run. This limits the influence of bad spots to the bleed matrix. debug: If this is >=0, then the `debug_info` dictionary will also be returned. If `method == 'separate'`, this specifies the round of the `bleed_matrix` calculation to return debugging info for. Returns: `bleed_matrix` - ```float [n_rounds x n_channels x n_dyes]```. ```bleed_matrix``` such that the expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, _, d]```. `debug_info` - dictionary containing useful information for debugging bleed matrix calculation. Each variable has size=3 in first dimension. `var[0]` refers to value with `initial_bleed_matrix`. `var[1]` refers to value after first `scaled_k_means`. `var[2]` refers to value after second k means (only if `score_thresh_anneal == True`). - `cluster_ind`: `int8 [3 x n_vectors]`. Index of dye each vector was assigned to in `scaled_k_means`. ```-1``` means fell below score_thresh and not assigned. - `cluster_score`: `float16 [3 x n_vectors]`. Value of dot product between each vector and dye assigned to. - `bleed_matrix`: `float16 [3 x n_channels x n_dyes]`. `bleed_matrix` computed at each stage of calculation. \"\"\" n_rounds , n_channels = spot_colors . shape [ 1 :] n_dyes = initial_bleed_matrix . shape [ 2 ] if not utils . errors . check_shape ( initial_bleed_matrix , [ n_rounds , n_channels , n_dyes ]): raise utils . errors . ShapeError ( 'initial_bleed_matrix' , initial_bleed_matrix . shape , ( n_rounds , n_channels , n_dyes )) bleed_matrix = np . zeros (( n_rounds , n_channels , n_dyes )) # Round, Measured, Real debug_info = None if method . lower () == 'separate' : for r in range ( n_rounds ): spot_channel_intensity = spot_colors [:, r , :] # get rid of any nan codes spot_channel_intensity = spot_channel_intensity [ ~ np . isnan ( spot_channel_intensity ) . any ( axis = 1 )] if r == debug : debug_info = { 'cluster_ind' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . int8 ), 'cluster_score' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . float16 ), 'bleed_matrix' : np . zeros (( 2 + score_thresh_anneal , n_channels , n_dyes )), 'round' : r } dye_codes , dye_eig_vals , cluster_ind , cluster_score , cluster_ind0 , cluster_score0 = \\ scaled_k_means ( spot_channel_intensity , initial_bleed_matrix [ r ] . transpose (), score_thresh , min_cluster_size , n_iter ) if r == debug : debug_info [ 'bleed_matrix' ][ 0 ] = initial_bleed_matrix [ r ] debug_info [ 'cluster_ind' ][ 0 ] = cluster_ind0 debug_info [ 'cluster_ind' ][ 1 ] = cluster_ind debug_info [ 'cluster_score' ][ 0 ] = cluster_score0 debug_info [ 'cluster_score' ][ 1 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 1 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) if score_thresh_anneal : # repeat with higher score_thresh so bad spots contribute less. score_thresh2 = np . zeros ( n_dyes ) for d in range ( n_dyes ): score_thresh2 [ d ] = np . median ( cluster_score [ cluster_ind == d ]) dye_codes , dye_eig_vals , cluster_ind , cluster_score = \\ scaled_k_means ( spot_channel_intensity , dye_codes , score_thresh2 , min_cluster_size , n_iter )[: 4 ] if r == debug : debug_info [ 'cluster_ind' ][ 2 ] = cluster_ind debug_info [ 'cluster_score' ][ 2 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 2 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) for d in range ( n_dyes ): bleed_matrix [ r , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) elif method . lower () == 'single' : initial_bleed_matrix_round_diff = initial_bleed_matrix . max ( axis = 0 ) - initial_bleed_matrix . min ( axis = 0 ) if np . max ( np . abs ( initial_bleed_matrix_round_diff )) > 1e-10 : raise ValueError ( f \"method is { method } , but initial_bleed_matrix is different for different rounds.\" ) spot_channel_intensity = spot_colors . reshape ( - 1 , n_channels ) # get rid of any nan codes spot_channel_intensity = spot_channel_intensity [ ~ np . isnan ( spot_channel_intensity ) . any ( axis = 1 )] if debug >= 0 : debug_info = { 'cluster_ind' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . int8 ), 'cluster_score' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . float16 ), 'bleed_matrix' : np . zeros (( 2 + score_thresh_anneal , n_channels , n_dyes ))} dye_codes , dye_eig_vals , cluster_ind , cluster_score , cluster_ind0 , cluster_score0 = \\ scaled_k_means ( spot_channel_intensity , initial_bleed_matrix [ 0 ] . transpose (), score_thresh , min_cluster_size , n_iter ) if debug >= 0 : debug_info [ 'bleed_matrix' ][ 0 ] = initial_bleed_matrix [ 0 ] debug_info [ 'cluster_ind' ][ 0 ] = cluster_ind0 debug_info [ 'cluster_ind' ][ 1 ] = cluster_ind debug_info [ 'cluster_score' ][ 0 ] = cluster_score0 debug_info [ 'cluster_score' ][ 1 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 1 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) if score_thresh_anneal : # repeat with higher score_thresh so bad spots contribute less. score_thresh2 = np . zeros ( n_dyes ) for d in range ( n_dyes ): score_thresh2 [ d ] = np . median ( cluster_score [ cluster_ind == d ]) dye_codes , dye_eig_vals , cluster_ind , cluster_score = \\ scaled_k_means ( spot_channel_intensity , dye_codes , score_thresh2 , min_cluster_size , n_iter )[: 4 ] if debug >= 0 : debug_info [ 'cluster_ind' ][ 2 ] = cluster_ind debug_info [ 'cluster_score' ][ 2 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 2 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) for r in range ( n_rounds ): for d in range ( n_dyes ): bleed_matrix [ r , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) else : raise ValueError ( f \"method given was { method } but should be either 'single' or 'separate'\" ) if debug_info is not None : return bleed_matrix , debug_info else : return bleed_matrix get_dye_channel_intensity_guess ( csv_file_name , dyes , cameras , lasers ) This gets an estimate for the intensity of each dye in each channel (before any channel normalisation) which is then used as the starting point for the bleed matrix computation. Parameters: Name Type Description Default csv_file_name str Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity: Dye is a column of names of different dyes Camera is a column of integers indicating the wavelength in nm of the camera. Laser is a column of integers indicating the wavelength in nm of the laser. Intensity [i] is the approximate intensity of Dye [i] in a channel with Camera [i] and Laser [i] . required dyes Union [ List [ str ], np . ndarray ] str [n_dyes] . Names of dyes used in particular experiment. required cameras Union [ List [ int ], np . ndarray ] int [n_channels] . Wavelength of camera in nm used in each channel. required lasers Union [ List [ int ], np . ndarray ] int [n_channels] . Wavelength of laser in nm used in each channel. required Returns: Type Description np . ndarray float [n_dyes x n_channels] . [d, c] is estimate of intensity of dye d in channel c . Source code in coppafish/call_spots/bleed_matrix.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def get_dye_channel_intensity_guess ( csv_file_name : str , dyes : Union [ List [ str ], np . ndarray ], cameras : Union [ List [ int ], np . ndarray ], lasers : Union [ List [ int ], np . ndarray ]) -> np . ndarray : \"\"\" This gets an estimate for the intensity of each dye in each channel (before any channel normalisation) which is then used as the starting point for the bleed matrix computation. Args: csv_file_name: Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity: - Dye is a column of names of different dyes - Camera is a column of integers indicating the wavelength in nm of the camera. - Laser is a column of integers indicating the wavelength in nm of the laser. - Intensity```[i]``` is the approximate intensity of Dye```[i]``` in a channel with Camera```[i]``` and Laser```[i]```. dyes: ```str [n_dyes]```. Names of dyes used in particular experiment. cameras: ```int [n_channels]```. Wavelength of camera in nm used in each channel. lasers: ```int [n_channels]```. Wavelength of laser in nm used in each channel. Returns: ```float [n_dyes x n_channels]```. ```[d, c]``` is estimate of intensity of dye ```d``` in channel ```c```. \"\"\" n_dyes = len ( dyes ) cameras = np . array ( cameras ) lasers = np . array ( lasers ) n_channels = cameras . shape [ 0 ] if not utils . errors . check_shape ( cameras , lasers . shape ): raise utils . errors . ShapeError ( 'cameras' , cameras . shape , lasers . shape ) # load in csv info csv_dyes = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 0 , dtype = str , skip_header = 1 ) csv_cameras = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 1 , dtype = int , skip_header = 1 ) csv_lasers = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 2 , dtype = int , skip_header = 1 ) csv_intensities = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 3 , dtype = float , skip_header = 1 ) # read in intensity from csv info for desired dyes in each channel dye_channel_intensity = np . zeros (( n_dyes , n_channels )) for d in range ( n_dyes ): correct_dye = csv_dyes == dyes [ d ] . upper () for c in range ( n_channels ): correct_camera = csv_cameras == cameras [ c ] correct_laser = csv_lasers == lasers [ c ] correct_all = np . all (( correct_dye , correct_camera , correct_laser ), axis = 0 ) if sum ( correct_all ) != 1 : raise ValueError ( f \"Expected intensity for dye { dyes [ d ] } , camera { cameras [ c ] } and laser { lasers [ c ] } \" f \"to be found once in csv_file. Instead, it was found { sum ( correct_all ) } times.\" ) dye_channel_intensity [ d , c ] = csv_intensities [ np . where ( correct_all )[ 0 ][ 0 ]] return dye_channel_intensity scaled_k_means ( x , initial_cluster_mean , score_thresh = 0 , min_cluster_size = 10 , n_iter = 100 ) Does a clustering that minimizes the norm of x[i] - g[i] * cluster_mean[cluster_ind[i]] for each data point i in x , where g is the gain which is not explicitly computed. Parameters: Name Type Description Default x np . ndarray float [n_points x n_dims] . Data set of vectors to build cluster means from. required initial_cluster_mean np . ndarray float [n_clusters x n_dims] . Starting point of mean cluster vectors. required score_thresh Union [ float , np . ndarray ] float or give different score for each cluster as float [n_clusters] Scalar between 0 and 1 . Points in x with dot product to a cluster mean vector greater than this contribute to new estimate of mean vector. 0 min_cluster_size int If less than this many points assigned to a cluster, that cluster mean vector will be set to 0 . 10 n_iter int Maximum number of iterations performed. 100 Returns: Type Description np . ndarray norm_cluster_mean - float [n_clusters x n_dims] . Final normalised mean cluster vectors. np . ndarray cluster_eig_value - float [n_clusters] . First eigenvalue of outer product matrix for each cluster. np . ndarray cluster_ind - int [n_points] . Index of cluster each point was assigned to. -1 means fell below score_thresh and not assigned. np . ndarray top_score - float [n_points] . top_score[i] is the dot product score between x[i] and norm_cluster_mean[cluster_ind[i]] . np . ndarray cluster_ind0 - int [n_points] . Index of cluster each point was assigned to on first iteration. -1 means fell below score_thresh and not assigned. np . ndarray top_score0 - float [n_points] . top_score0[i] is the dot product score between x[i] and initial_cluster_mean[cluster_ind0[i]] . Source code in coppafish/call_spots/bleed_matrix.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def scaled_k_means ( x : np . ndarray , initial_cluster_mean : np . ndarray , score_thresh : Union [ float , np . ndarray ] = 0 , min_cluster_size : int = 10 , n_iter : int = 100 ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Does a clustering that minimizes the norm of ```x[i] - g[i] * cluster_mean[cluster_ind[i]]``` for each data point ```i``` in ```x```, where ```g``` is the gain which is not explicitly computed. Args: x: ```float [n_points x n_dims]```. Data set of vectors to build cluster means from. initial_cluster_mean: ```float [n_clusters x n_dims]```. Starting point of mean cluster vectors. score_thresh: `float` or give different score for each cluster as `float [n_clusters]` Scalar between ```0``` and ```1```. Points in ```x``` with dot product to a cluster mean vector greater than this contribute to new estimate of mean vector. min_cluster_size: If less than this many points assigned to a cluster, that cluster mean vector will be set to ```0```. n_iter: Maximum number of iterations performed. Returns: - norm_cluster_mean - ```float [n_clusters x n_dims]```. Final normalised mean cluster vectors. - cluster_eig_value - ```float [n_clusters]```. First eigenvalue of outer product matrix for each cluster. - cluster_ind - ```int [n_points]```. Index of cluster each point was assigned to. ```-1``` means fell below score_thresh and not assigned. - top_score - ```float [n_points]```. `top_score[i]` is the dot product score between `x[i]` and `norm_cluster_mean[cluster_ind[i]]`. - cluster_ind0 - ```int [n_points]```. Index of cluster each point was assigned to on first iteration. ```-1``` means fell below score_thresh and not assigned. - top_score0 - ```float [n_points]```. `top_score0[i]` is the dot product score between `x[i]` and `initial_cluster_mean[cluster_ind0[i]]`. \"\"\" # normalise starting points and original data norm_cluster_mean = initial_cluster_mean / np . linalg . norm ( initial_cluster_mean , axis = 1 ) . reshape ( - 1 , 1 ) x_norm = x / np . linalg . norm ( x , axis = 1 ) . reshape ( - 1 , 1 ) n_clusters = initial_cluster_mean . shape [ 0 ] n_points , n_dims = x . shape cluster_ind = np . ones ( x . shape [ 0 ], dtype = int ) * - 2 # set all to -2 so won't end on first iteration cluster_eig_val = np . zeros ( n_clusters ) if not utils . errors . check_shape ( initial_cluster_mean , [ n_clusters , n_dims ]): raise utils . errors . ShapeError ( 'initial_cluster_mean' , initial_cluster_mean . shape , ( n_clusters , n_dims )) if len ( np . array ([ score_thresh ]) . flatten ()) == 1 : # if single threshold, set the same for each cluster score_thresh = np . ones ( n_clusters ) * score_thresh if not utils . errors . check_shape ( score_thresh , [ n_clusters ]): raise utils . errors . ShapeError ( 'score_thresh' , score_thresh . shape , ( n_clusters ,)) for i in range ( n_iter ): cluster_ind_old = cluster_ind . copy () # project each point onto each cluster. Use normalized so we can interpret score score = x_norm @ norm_cluster_mean . transpose () cluster_ind = np . argmax ( score , axis = 1 ) # find best cluster for each point top_score = score [ np . arange ( n_points ), cluster_ind ] top_score [ np . where ( np . isnan ( top_score ))[ 0 ]] = score_thresh . min () - 1 # don't include nan values cluster_ind [ top_score < score_thresh [ cluster_ind ]] = - 1 # unclusterable points if i == 0 : top_score0 = top_score . copy () cluster_ind0 = cluster_ind . copy () if ( cluster_ind == cluster_ind_old ) . all (): break for c in range ( n_clusters ): my_points = x [ cluster_ind == c ] # don't use normalized, to avoid overweighting weak points n_my_points = my_points . shape [ 0 ] if n_my_points < min_cluster_size : norm_cluster_mean [ c ] = 0 warnings . warn ( f \"Cluster c only had { n_my_points } vectors assigned to it. \\n \" f \"This is less than min_cluster_size = { min_cluster_size } so setting this cluster to 0.\" ) continue eig_vals , eigs = np . linalg . eig ( my_points . transpose () @ my_points / n_my_points ) best_eig_ind = np . argmax ( eig_vals ) norm_cluster_mean [ c ] = eigs [:, best_eig_ind ] * np . sign ( eigs [:, best_eig_ind ] . mean ()) # make them positive cluster_eig_val [ c ] = eig_vals [ best_eig_ind ] return norm_cluster_mean , cluster_eig_val , cluster_ind , top_score , cluster_ind0 , top_score0","title":"Bleed Matrix"},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.get_bleed_matrix","text":"This returns a bleed matrix such that the expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, :, d] . Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Intensity found for each spot in each round and channel, normalized in some way to equalize channel intensities typically, the normalisation will be such that spot_colors vary between around -5 to 10 with most near 0 . required initial_bleed_matrix np . ndarray float [n_rounds x n_channels x n_dyes] . Initial guess for intensity we expect each dye to produce in each channel and round. Should be normalized in same way as spot_colors. required method str Must be one of the following: 'single' - A single bleed matrix is produced for all rounds. 'separate' - A different bleed matrix is made for each round. required score_thresh float Scalar between 0 and 1 . Threshold used for scaled_k_means affecting which spots contribute to bleed matrix estimate. 0 min_cluster_size int If less than this many points assigned to a dye, that dye mean vector will be set to 0 . 10 n_iter int Maximum number of iterations performed in scaled_k_means . 100 score_thresh_anneal bool If True , scaled_k_means will be performed twice. The second time starting with the output of the first and with score_thresh for cluster i set to the median of the scores assigned to cluster i in the first run. This limits the influence of bad spots to the bleed matrix. True debug int If this is >=0, then the debug_info dictionary will also be returned. If method == 'separate' , this specifies the round of the bleed_matrix calculation to return debugging info for. -1 Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , dict ]] bleed_matrix - float [n_rounds x n_channels x n_dyes] . bleed_matrix such that the expected intensity of dye d in round r is a constant multiple of bleed_matrix[r, _, d] . Union [ np . ndarray , Tuple [ np . ndarray , dict ]] debug_info - dictionary containing useful information for debugging bleed matrix calculation. Each variable has size=3 in first dimension. var[0] refers to value with initial_bleed_matrix . var[1] refers to value after first scaled_k_means . var[2] refers to value after second k means (only if score_thresh_anneal == True ). cluster_ind : int8 [3 x n_vectors] . Index of dye each vector was assigned to in scaled_k_means . -1 means fell below score_thresh and not assigned. cluster_score : float16 [3 x n_vectors] . Value of dot product between each vector and dye assigned to. bleed_matrix : float16 [3 x n_channels x n_dyes] . bleed_matrix computed at each stage of calculation. Source code in coppafish/call_spots/bleed_matrix.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def get_bleed_matrix ( spot_colors : np . ndarray , initial_bleed_matrix : np . ndarray , method : str , score_thresh : float = 0 , min_cluster_size : int = 10 , n_iter : int = 100 , score_thresh_anneal : bool = True , debug : int = - 1 ) -> Union [ np . ndarray , Tuple [ np . ndarray , dict ]]: \"\"\" This returns a bleed matrix such that the expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, :, d]```. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Intensity found for each spot in each round and channel, normalized in some way to equalize channel intensities typically, the normalisation will be such that spot_colors vary between around ```-5``` to ```10``` with most near ```0```. initial_bleed_matrix: ```float [n_rounds x n_channels x n_dyes]```. Initial guess for intensity we expect each dye to produce in each channel and round. Should be normalized in same way as spot_colors. method: Must be one of the following: - ```'single'``` - A single bleed matrix is produced for all rounds. - ```'separate'``` - A different bleed matrix is made for each round. score_thresh: Scalar between ```0``` and ```1```. Threshold used for ```scaled_k_means``` affecting which spots contribute to bleed matrix estimate. min_cluster_size: If less than this many points assigned to a dye, that dye mean vector will be set to ```0```. n_iter: Maximum number of iterations performed in ```scaled_k_means```. score_thresh_anneal: If `True`, `scaled_k_means` will be performed twice. The second time starting with the output of the first and with `score_thresh` for cluster `i` set to the median of the scores assigned to cluster `i` in the first run. This limits the influence of bad spots to the bleed matrix. debug: If this is >=0, then the `debug_info` dictionary will also be returned. If `method == 'separate'`, this specifies the round of the `bleed_matrix` calculation to return debugging info for. Returns: `bleed_matrix` - ```float [n_rounds x n_channels x n_dyes]```. ```bleed_matrix``` such that the expected intensity of dye ```d``` in round ```r``` is a constant multiple of ```bleed_matrix[r, _, d]```. `debug_info` - dictionary containing useful information for debugging bleed matrix calculation. Each variable has size=3 in first dimension. `var[0]` refers to value with `initial_bleed_matrix`. `var[1]` refers to value after first `scaled_k_means`. `var[2]` refers to value after second k means (only if `score_thresh_anneal == True`). - `cluster_ind`: `int8 [3 x n_vectors]`. Index of dye each vector was assigned to in `scaled_k_means`. ```-1``` means fell below score_thresh and not assigned. - `cluster_score`: `float16 [3 x n_vectors]`. Value of dot product between each vector and dye assigned to. - `bleed_matrix`: `float16 [3 x n_channels x n_dyes]`. `bleed_matrix` computed at each stage of calculation. \"\"\" n_rounds , n_channels = spot_colors . shape [ 1 :] n_dyes = initial_bleed_matrix . shape [ 2 ] if not utils . errors . check_shape ( initial_bleed_matrix , [ n_rounds , n_channels , n_dyes ]): raise utils . errors . ShapeError ( 'initial_bleed_matrix' , initial_bleed_matrix . shape , ( n_rounds , n_channels , n_dyes )) bleed_matrix = np . zeros (( n_rounds , n_channels , n_dyes )) # Round, Measured, Real debug_info = None if method . lower () == 'separate' : for r in range ( n_rounds ): spot_channel_intensity = spot_colors [:, r , :] # get rid of any nan codes spot_channel_intensity = spot_channel_intensity [ ~ np . isnan ( spot_channel_intensity ) . any ( axis = 1 )] if r == debug : debug_info = { 'cluster_ind' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . int8 ), 'cluster_score' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . float16 ), 'bleed_matrix' : np . zeros (( 2 + score_thresh_anneal , n_channels , n_dyes )), 'round' : r } dye_codes , dye_eig_vals , cluster_ind , cluster_score , cluster_ind0 , cluster_score0 = \\ scaled_k_means ( spot_channel_intensity , initial_bleed_matrix [ r ] . transpose (), score_thresh , min_cluster_size , n_iter ) if r == debug : debug_info [ 'bleed_matrix' ][ 0 ] = initial_bleed_matrix [ r ] debug_info [ 'cluster_ind' ][ 0 ] = cluster_ind0 debug_info [ 'cluster_ind' ][ 1 ] = cluster_ind debug_info [ 'cluster_score' ][ 0 ] = cluster_score0 debug_info [ 'cluster_score' ][ 1 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 1 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) if score_thresh_anneal : # repeat with higher score_thresh so bad spots contribute less. score_thresh2 = np . zeros ( n_dyes ) for d in range ( n_dyes ): score_thresh2 [ d ] = np . median ( cluster_score [ cluster_ind == d ]) dye_codes , dye_eig_vals , cluster_ind , cluster_score = \\ scaled_k_means ( spot_channel_intensity , dye_codes , score_thresh2 , min_cluster_size , n_iter )[: 4 ] if r == debug : debug_info [ 'cluster_ind' ][ 2 ] = cluster_ind debug_info [ 'cluster_score' ][ 2 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 2 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) for d in range ( n_dyes ): bleed_matrix [ r , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) elif method . lower () == 'single' : initial_bleed_matrix_round_diff = initial_bleed_matrix . max ( axis = 0 ) - initial_bleed_matrix . min ( axis = 0 ) if np . max ( np . abs ( initial_bleed_matrix_round_diff )) > 1e-10 : raise ValueError ( f \"method is { method } , but initial_bleed_matrix is different for different rounds.\" ) spot_channel_intensity = spot_colors . reshape ( - 1 , n_channels ) # get rid of any nan codes spot_channel_intensity = spot_channel_intensity [ ~ np . isnan ( spot_channel_intensity ) . any ( axis = 1 )] if debug >= 0 : debug_info = { 'cluster_ind' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . int8 ), 'cluster_score' : np . zeros (( 2 + score_thresh_anneal , spot_channel_intensity . shape [ 0 ]), dtype = np . float16 ), 'bleed_matrix' : np . zeros (( 2 + score_thresh_anneal , n_channels , n_dyes ))} dye_codes , dye_eig_vals , cluster_ind , cluster_score , cluster_ind0 , cluster_score0 = \\ scaled_k_means ( spot_channel_intensity , initial_bleed_matrix [ 0 ] . transpose (), score_thresh , min_cluster_size , n_iter ) if debug >= 0 : debug_info [ 'bleed_matrix' ][ 0 ] = initial_bleed_matrix [ 0 ] debug_info [ 'cluster_ind' ][ 0 ] = cluster_ind0 debug_info [ 'cluster_ind' ][ 1 ] = cluster_ind debug_info [ 'cluster_score' ][ 0 ] = cluster_score0 debug_info [ 'cluster_score' ][ 1 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 1 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) if score_thresh_anneal : # repeat with higher score_thresh so bad spots contribute less. score_thresh2 = np . zeros ( n_dyes ) for d in range ( n_dyes ): score_thresh2 [ d ] = np . median ( cluster_score [ cluster_ind == d ]) dye_codes , dye_eig_vals , cluster_ind , cluster_score = \\ scaled_k_means ( spot_channel_intensity , dye_codes , score_thresh2 , min_cluster_size , n_iter )[: 4 ] if debug >= 0 : debug_info [ 'cluster_ind' ][ 2 ] = cluster_ind debug_info [ 'cluster_score' ][ 2 ] = cluster_score for d in range ( n_dyes ): debug_info [ 'bleed_matrix' ][ 2 , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) for r in range ( n_rounds ): for d in range ( n_dyes ): bleed_matrix [ r , :, d ] = dye_codes [ d ] * np . sqrt ( dye_eig_vals [ d ]) else : raise ValueError ( f \"method given was { method } but should be either 'single' or 'separate'\" ) if debug_info is not None : return bleed_matrix , debug_info else : return bleed_matrix","title":"get_bleed_matrix()"},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.get_dye_channel_intensity_guess","text":"This gets an estimate for the intensity of each dye in each channel (before any channel normalisation) which is then used as the starting point for the bleed matrix computation. Parameters: Name Type Description Default csv_file_name str Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity: Dye is a column of names of different dyes Camera is a column of integers indicating the wavelength in nm of the camera. Laser is a column of integers indicating the wavelength in nm of the laser. Intensity [i] is the approximate intensity of Dye [i] in a channel with Camera [i] and Laser [i] . required dyes Union [ List [ str ], np . ndarray ] str [n_dyes] . Names of dyes used in particular experiment. required cameras Union [ List [ int ], np . ndarray ] int [n_channels] . Wavelength of camera in nm used in each channel. required lasers Union [ List [ int ], np . ndarray ] int [n_channels] . Wavelength of laser in nm used in each channel. required Returns: Type Description np . ndarray float [n_dyes x n_channels] . [d, c] is estimate of intensity of dye d in channel c . Source code in coppafish/call_spots/bleed_matrix.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def get_dye_channel_intensity_guess ( csv_file_name : str , dyes : Union [ List [ str ], np . ndarray ], cameras : Union [ List [ int ], np . ndarray ], lasers : Union [ List [ int ], np . ndarray ]) -> np . ndarray : \"\"\" This gets an estimate for the intensity of each dye in each channel (before any channel normalisation) which is then used as the starting point for the bleed matrix computation. Args: csv_file_name: Path to csv file which has 4 columns with headers Dye, Camera, Laser, Intensity: - Dye is a column of names of different dyes - Camera is a column of integers indicating the wavelength in nm of the camera. - Laser is a column of integers indicating the wavelength in nm of the laser. - Intensity```[i]``` is the approximate intensity of Dye```[i]``` in a channel with Camera```[i]``` and Laser```[i]```. dyes: ```str [n_dyes]```. Names of dyes used in particular experiment. cameras: ```int [n_channels]```. Wavelength of camera in nm used in each channel. lasers: ```int [n_channels]```. Wavelength of laser in nm used in each channel. Returns: ```float [n_dyes x n_channels]```. ```[d, c]``` is estimate of intensity of dye ```d``` in channel ```c```. \"\"\" n_dyes = len ( dyes ) cameras = np . array ( cameras ) lasers = np . array ( lasers ) n_channels = cameras . shape [ 0 ] if not utils . errors . check_shape ( cameras , lasers . shape ): raise utils . errors . ShapeError ( 'cameras' , cameras . shape , lasers . shape ) # load in csv info csv_dyes = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 0 , dtype = str , skip_header = 1 ) csv_cameras = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 1 , dtype = int , skip_header = 1 ) csv_lasers = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 2 , dtype = int , skip_header = 1 ) csv_intensities = np . genfromtxt ( csv_file_name , delimiter = ',' , usecols = 3 , dtype = float , skip_header = 1 ) # read in intensity from csv info for desired dyes in each channel dye_channel_intensity = np . zeros (( n_dyes , n_channels )) for d in range ( n_dyes ): correct_dye = csv_dyes == dyes [ d ] . upper () for c in range ( n_channels ): correct_camera = csv_cameras == cameras [ c ] correct_laser = csv_lasers == lasers [ c ] correct_all = np . all (( correct_dye , correct_camera , correct_laser ), axis = 0 ) if sum ( correct_all ) != 1 : raise ValueError ( f \"Expected intensity for dye { dyes [ d ] } , camera { cameras [ c ] } and laser { lasers [ c ] } \" f \"to be found once in csv_file. Instead, it was found { sum ( correct_all ) } times.\" ) dye_channel_intensity [ d , c ] = csv_intensities [ np . where ( correct_all )[ 0 ][ 0 ]] return dye_channel_intensity","title":"get_dye_channel_intensity_guess()"},{"location":"code/call_spots/bleed_matrix/#coppafish.call_spots.bleed_matrix.scaled_k_means","text":"Does a clustering that minimizes the norm of x[i] - g[i] * cluster_mean[cluster_ind[i]] for each data point i in x , where g is the gain which is not explicitly computed. Parameters: Name Type Description Default x np . ndarray float [n_points x n_dims] . Data set of vectors to build cluster means from. required initial_cluster_mean np . ndarray float [n_clusters x n_dims] . Starting point of mean cluster vectors. required score_thresh Union [ float , np . ndarray ] float or give different score for each cluster as float [n_clusters] Scalar between 0 and 1 . Points in x with dot product to a cluster mean vector greater than this contribute to new estimate of mean vector. 0 min_cluster_size int If less than this many points assigned to a cluster, that cluster mean vector will be set to 0 . 10 n_iter int Maximum number of iterations performed. 100 Returns: Type Description np . ndarray norm_cluster_mean - float [n_clusters x n_dims] . Final normalised mean cluster vectors. np . ndarray cluster_eig_value - float [n_clusters] . First eigenvalue of outer product matrix for each cluster. np . ndarray cluster_ind - int [n_points] . Index of cluster each point was assigned to. -1 means fell below score_thresh and not assigned. np . ndarray top_score - float [n_points] . top_score[i] is the dot product score between x[i] and norm_cluster_mean[cluster_ind[i]] . np . ndarray cluster_ind0 - int [n_points] . Index of cluster each point was assigned to on first iteration. -1 means fell below score_thresh and not assigned. np . ndarray top_score0 - float [n_points] . top_score0[i] is the dot product score between x[i] and initial_cluster_mean[cluster_ind0[i]] . Source code in coppafish/call_spots/bleed_matrix.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def scaled_k_means ( x : np . ndarray , initial_cluster_mean : np . ndarray , score_thresh : Union [ float , np . ndarray ] = 0 , min_cluster_size : int = 10 , n_iter : int = 100 ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Does a clustering that minimizes the norm of ```x[i] - g[i] * cluster_mean[cluster_ind[i]]``` for each data point ```i``` in ```x```, where ```g``` is the gain which is not explicitly computed. Args: x: ```float [n_points x n_dims]```. Data set of vectors to build cluster means from. initial_cluster_mean: ```float [n_clusters x n_dims]```. Starting point of mean cluster vectors. score_thresh: `float` or give different score for each cluster as `float [n_clusters]` Scalar between ```0``` and ```1```. Points in ```x``` with dot product to a cluster mean vector greater than this contribute to new estimate of mean vector. min_cluster_size: If less than this many points assigned to a cluster, that cluster mean vector will be set to ```0```. n_iter: Maximum number of iterations performed. Returns: - norm_cluster_mean - ```float [n_clusters x n_dims]```. Final normalised mean cluster vectors. - cluster_eig_value - ```float [n_clusters]```. First eigenvalue of outer product matrix for each cluster. - cluster_ind - ```int [n_points]```. Index of cluster each point was assigned to. ```-1``` means fell below score_thresh and not assigned. - top_score - ```float [n_points]```. `top_score[i]` is the dot product score between `x[i]` and `norm_cluster_mean[cluster_ind[i]]`. - cluster_ind0 - ```int [n_points]```. Index of cluster each point was assigned to on first iteration. ```-1``` means fell below score_thresh and not assigned. - top_score0 - ```float [n_points]```. `top_score0[i]` is the dot product score between `x[i]` and `initial_cluster_mean[cluster_ind0[i]]`. \"\"\" # normalise starting points and original data norm_cluster_mean = initial_cluster_mean / np . linalg . norm ( initial_cluster_mean , axis = 1 ) . reshape ( - 1 , 1 ) x_norm = x / np . linalg . norm ( x , axis = 1 ) . reshape ( - 1 , 1 ) n_clusters = initial_cluster_mean . shape [ 0 ] n_points , n_dims = x . shape cluster_ind = np . ones ( x . shape [ 0 ], dtype = int ) * - 2 # set all to -2 so won't end on first iteration cluster_eig_val = np . zeros ( n_clusters ) if not utils . errors . check_shape ( initial_cluster_mean , [ n_clusters , n_dims ]): raise utils . errors . ShapeError ( 'initial_cluster_mean' , initial_cluster_mean . shape , ( n_clusters , n_dims )) if len ( np . array ([ score_thresh ]) . flatten ()) == 1 : # if single threshold, set the same for each cluster score_thresh = np . ones ( n_clusters ) * score_thresh if not utils . errors . check_shape ( score_thresh , [ n_clusters ]): raise utils . errors . ShapeError ( 'score_thresh' , score_thresh . shape , ( n_clusters ,)) for i in range ( n_iter ): cluster_ind_old = cluster_ind . copy () # project each point onto each cluster. Use normalized so we can interpret score score = x_norm @ norm_cluster_mean . transpose () cluster_ind = np . argmax ( score , axis = 1 ) # find best cluster for each point top_score = score [ np . arange ( n_points ), cluster_ind ] top_score [ np . where ( np . isnan ( top_score ))[ 0 ]] = score_thresh . min () - 1 # don't include nan values cluster_ind [ top_score < score_thresh [ cluster_ind ]] = - 1 # unclusterable points if i == 0 : top_score0 = top_score . copy () cluster_ind0 = cluster_ind . copy () if ( cluster_ind == cluster_ind_old ) . all (): break for c in range ( n_clusters ): my_points = x [ cluster_ind == c ] # don't use normalized, to avoid overweighting weak points n_my_points = my_points . shape [ 0 ] if n_my_points < min_cluster_size : norm_cluster_mean [ c ] = 0 warnings . warn ( f \"Cluster c only had { n_my_points } vectors assigned to it. \\n \" f \"This is less than min_cluster_size = { min_cluster_size } so setting this cluster to 0.\" ) continue eig_vals , eigs = np . linalg . eig ( my_points . transpose () @ my_points / n_my_points ) best_eig_ind = np . argmax ( eig_vals ) norm_cluster_mean [ c ] = eigs [:, best_eig_ind ] * np . sign ( eigs [:, best_eig_ind ] . mean ()) # make them positive cluster_eig_val [ c ] = eig_vals [ best_eig_ind ] return norm_cluster_mean , cluster_eig_val , cluster_ind , top_score , cluster_ind0 , top_score0","title":"scaled_k_means()"},{"location":"code/call_spots/dot_product/","text":"dot_product_score ( spot_colors , bled_codes , norm_shift = 0 , weight_squared = None ) Computes sum(W**2(s * b) / W**2) where s is a spot_color , b is a bled_code and W**2 is weight_squared for a particular spot_color . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. 0 weight_squared Optional [ np . ndarray ] float [n_spots x (n_rounds x n_channels)] . squared weight to apply to each round/channel for each spot when computing dot product. If None , all rounds, channels treated equally. None Returns: Type Description np . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def dot_product_score ( spot_colors : np . ndarray , bled_codes : np . ndarray , norm_shift : float = 0 , weight_squared : Optional [ np . ndarray ] = None ) -> np . ndarray : \"\"\" Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared for a particular `spot_color`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. weight_squared: `float [n_spots x (n_rounds x n_channels)]`. squared weight to apply to each round/channel for each spot when computing dot product. If `None`, all rounds, channels treated equally. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" n_spots = spot_colors . shape [ 0 ] n_genes , n_round_channels = bled_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 ], bled_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( n_spots , n_round_channels )) spot_norm_factor = np . linalg . norm ( spot_colors , axis = 1 , keepdims = True ) spot_norm_factor = spot_norm_factor + norm_shift spot_colors = spot_colors / spot_norm_factor gene_norm_factor = np . linalg . norm ( bled_codes , axis = 1 , keepdims = True ) gene_norm_factor [ gene_norm_factor == 0 ] = 1 # so don't blow up if bled_code is all 0 for a gene. bled_codes = bled_codes / gene_norm_factor if weight_squared is not None : if not utils . errors . check_shape ( weight_squared , spot_colors . shape ): raise utils . errors . ShapeError ( 'weight' , weight_squared . shape , spot_colors . shape ) spot_colors = spot_colors * weight_squared score = spot_colors @ bled_codes . transpose () if weight_squared is not None : score = score / np . expand_dims ( np . sum ( weight_squared , axis = 1 ), 1 ) score = score * n_round_channels # make maximum score 1 if all weight the same and dot product perfect. return score dot_product_score_no_weight ( spot_colors , bled_codes , norm_shift = 0 ) Computes sum((s * b)) where s is a spot_color , b is a bled_code . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. 0 Returns: Type Description np . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def dot_product_score_no_weight ( spot_colors : np . ndarray , bled_codes : np . ndarray , norm_shift : float = 0 ) -> np . ndarray : \"\"\" Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" n_spots = spot_colors . shape [ 0 ] n_genes , n_round_channels = bled_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 ], bled_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( n_spots , n_round_channels )) spot_norm_factor = np . linalg . norm ( spot_colors , axis = 1 , keepdims = True ) spot_norm_factor = spot_norm_factor + norm_shift spot_colors = spot_colors / spot_norm_factor gene_norm_factor = np . linalg . norm ( bled_codes , axis = 1 , keepdims = True ) gene_norm_factor [ gene_norm_factor == 0 ] = 1 # so don't blow up if bled_code is all 0 for a gene. bled_codes = bled_codes / gene_norm_factor score = spot_colors @ bled_codes . transpose () return score Optimised dot_product_score ( spot_colors , bled_codes , norm_shift , weight_squared ) Computes sum(W**2(s * b) / W**2) where s is a spot_color , b is a bled_code and W**2 is weight_squared for a particular spot_color . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required weight_squared jnp . ndarray float [n_spots x (n_rounds x n_channels)] . squared weight to apply to each round/channel for each spot when computing dot product. required Returns: Type Description jnp . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product_optimised.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @partial ( jax . jit , static_argnums = 2 ) def dot_product_score ( spot_colors : jnp . ndarray , bled_codes : jnp . ndarray , norm_shift : float , weight_squared : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared for a particular `spot_color`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. weight_squared: `float [n_spots x (n_rounds x n_channels)]`. squared weight to apply to each round/channel for each spot when computing dot product. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" score = jax . vmap ( dot_product_score_single , in_axes = ( 0 , None , None , 0 ), out_axes = 0 )( spot_colors , bled_codes , norm_shift , weight_squared ) return score dot_product_score_no_weight ( spot_colors , bled_codes , norm_shift ) Computes sum((s * b)) where s is a spot_color , b is a bled_code . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required Returns: Type Description jnp . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product_optimised.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @partial ( jax . jit , static_argnums = 2 ) def dot_product_score_no_weight ( spot_colors : jnp . ndarray , bled_codes : jnp . ndarray , norm_shift : float ) -> jnp . ndarray : \"\"\" Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" score = jax . vmap ( dot_product_score_no_weight_single , in_axes = ( 0 , None , None ), out_axes = 0 )( spot_colors , bled_codes , norm_shift ) return score","title":"Dot Product"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product.dot_product_score","text":"Computes sum(W**2(s * b) / W**2) where s is a spot_color , b is a bled_code and W**2 is weight_squared for a particular spot_color . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. 0 weight_squared Optional [ np . ndarray ] float [n_spots x (n_rounds x n_channels)] . squared weight to apply to each round/channel for each spot when computing dot product. If None , all rounds, channels treated equally. None Returns: Type Description np . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def dot_product_score ( spot_colors : np . ndarray , bled_codes : np . ndarray , norm_shift : float = 0 , weight_squared : Optional [ np . ndarray ] = None ) -> np . ndarray : \"\"\" Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared for a particular `spot_color`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. weight_squared: `float [n_spots x (n_rounds x n_channels)]`. squared weight to apply to each round/channel for each spot when computing dot product. If `None`, all rounds, channels treated equally. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" n_spots = spot_colors . shape [ 0 ] n_genes , n_round_channels = bled_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 ], bled_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( n_spots , n_round_channels )) spot_norm_factor = np . linalg . norm ( spot_colors , axis = 1 , keepdims = True ) spot_norm_factor = spot_norm_factor + norm_shift spot_colors = spot_colors / spot_norm_factor gene_norm_factor = np . linalg . norm ( bled_codes , axis = 1 , keepdims = True ) gene_norm_factor [ gene_norm_factor == 0 ] = 1 # so don't blow up if bled_code is all 0 for a gene. bled_codes = bled_codes / gene_norm_factor if weight_squared is not None : if not utils . errors . check_shape ( weight_squared , spot_colors . shape ): raise utils . errors . ShapeError ( 'weight' , weight_squared . shape , spot_colors . shape ) spot_colors = spot_colors * weight_squared score = spot_colors @ bled_codes . transpose () if weight_squared is not None : score = score / np . expand_dims ( np . sum ( weight_squared , axis = 1 ), 1 ) score = score * n_round_channels # make maximum score 1 if all weight the same and dot product perfect. return score","title":"dot_product_score()"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product.dot_product_score_no_weight","text":"Computes sum((s * b)) where s is a spot_color , b is a bled_code . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. 0 Returns: Type Description np . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def dot_product_score_no_weight ( spot_colors : np . ndarray , bled_codes : np . ndarray , norm_shift : float = 0 ) -> np . ndarray : \"\"\" Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" n_spots = spot_colors . shape [ 0 ] n_genes , n_round_channels = bled_codes . shape if not utils . errors . check_shape ( spot_colors [ 0 ], bled_codes [ 0 ] . shape ): raise utils . errors . ShapeError ( 'spot_colors' , spot_colors . shape , ( n_spots , n_round_channels )) spot_norm_factor = np . linalg . norm ( spot_colors , axis = 1 , keepdims = True ) spot_norm_factor = spot_norm_factor + norm_shift spot_colors = spot_colors / spot_norm_factor gene_norm_factor = np . linalg . norm ( bled_codes , axis = 1 , keepdims = True ) gene_norm_factor [ gene_norm_factor == 0 ] = 1 # so don't blow up if bled_code is all 0 for a gene. bled_codes = bled_codes / gene_norm_factor score = spot_colors @ bled_codes . transpose () return score","title":"dot_product_score_no_weight()"},{"location":"code/call_spots/dot_product/#optimised","text":"","title":"Optimised"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product_optimised.dot_product_score","text":"Computes sum(W**2(s * b) / W**2) where s is a spot_color , b is a bled_code and W**2 is weight_squared for a particular spot_color . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required weight_squared jnp . ndarray float [n_spots x (n_rounds x n_channels)] . squared weight to apply to each round/channel for each spot when computing dot product. required Returns: Type Description jnp . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product_optimised.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 @partial ( jax . jit , static_argnums = 2 ) def dot_product_score ( spot_colors : jnp . ndarray , bled_codes : jnp . ndarray , norm_shift : float , weight_squared : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes `sum(W**2(s * b) / W**2)` where `s` is a `spot_color`, `b` is a `bled_code` and `W**2` is weight_squared for a particular `spot_color`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. weight_squared: `float [n_spots x (n_rounds x n_channels)]`. squared weight to apply to each round/channel for each spot when computing dot product. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" score = jax . vmap ( dot_product_score_single , in_axes = ( 0 , None , None , 0 ), out_axes = 0 )( spot_colors , bled_codes , norm_shift , weight_squared ) return score","title":"dot_product_score()"},{"location":"code/call_spots/dot_product/#coppafish.call_spots.dot_product_optimised.dot_product_score_no_weight","text":"Computes sum((s * b)) where s is a spot_color , b is a bled_code . Sum is over all rounds and channels. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x (n_rounds x n_channels)] . Spot colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required Returns: Type Description jnp . ndarray float [n_spots x n_genes] . score such that score[d, c] gives dot product between spot_colors vector d with bled_codes vector c . Source code in coppafish/call_spots/dot_product_optimised.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 @partial ( jax . jit , static_argnums = 2 ) def dot_product_score_no_weight ( spot_colors : jnp . ndarray , bled_codes : jnp . ndarray , norm_shift : float ) -> jnp . ndarray : \"\"\" Computes `sum((s * b))` where `s` is a `spot_color`, `b` is a `bled_code`. Sum is over all rounds and channels. Args: spot_colors: `float [n_spots x (n_rounds x n_channels)]`. Spot colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. Returns: `float [n_spots x n_genes]`. `score` such that `score[d, c]` gives dot product between `spot_colors` vector `d` with `bled_codes` vector `c`. \"\"\" score = jax . vmap ( dot_product_score_no_weight_single , in_axes = ( 0 , None , None ), out_axes = 0 )( spot_colors , bled_codes , norm_shift ) return score","title":"dot_product_score_no_weight()"},{"location":"code/call_spots/qual_check/","text":"get_intensity_thresh ( nb ) Gets threshold for intensity from parameters in config file or Notebook. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots page. required Returns: Type Description float intensity threshold Source code in coppafish/call_spots/qual_check.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get_intensity_thresh ( nb : Notebook ) -> float : \"\"\" Gets threshold for intensity from parameters in `config file` or Notebook. Args: nb: Notebook containing at least the `call_spots` page. Returns: intensity threshold \"\"\" if nb . has_page ( 'thresholds' ): intensity_thresh = nb . thresholds . intensity else : config = nb . get_config ()[ 'thresholds' ] intensity_thresh = config [ 'intensity' ] if intensity_thresh is None : intensity_thresh = nb . call_spots . gene_efficiency_intensity_thresh return intensity_thresh get_spot_intensity ( spot_colors ) Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required Returns: Type Description np . ndarray float [n_spots] . [s] is the intensity of spot s . Source code in coppafish/call_spots/qual_check.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_spot_intensity ( spot_colors : np . ndarray ) -> np . ndarray : \"\"\" Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Spot colors normalised to equalise intensities between channels (and rounds). Returns: ```float [n_spots]```. ```[s]``` is the intensity of spot ```s```. \"\"\" check_spot = np . random . randint ( spot_colors . shape [ 0 ]) diff_to_int = np . round ( spot_colors [ check_spot ]) . astype ( int ) - spot_colors [ check_spot ] if np . abs ( diff_to_int ) . max () == 0 : raise ValueError ( f \"spot_intensities should be found using normalised spot_colors.\" f \" \\n But for spot { check_spot } , spot_colors given are integers indicating they are \" f \"the raw intensities.\" ) round_max_color = np . max ( spot_colors , axis = 2 ) return np . median ( round_max_color , axis = 1 ) omp_spot_score ( nbp , score_multiplier , spot_no = None , n_neighbours_pos = None , n_neighbours_neg = None ) Score for omp gene assignment Parameters: Name Type Description Default nbp NotebookPage OMP Notebook page required score_multiplier float score = score_multiplier * n_pos_neighb + n_neg_neighb . So this influences the importance of positive coefficient neighbours vs negative. required spot_no Optional [ Union [ int , List , np . ndarray ]] Which spots to get score for. If None , all scores will be found. None Returns: Type Description Union [ float , np . ndarray ] Score for each spot in spot_no if given, otherwise all spot scores. Source code in coppafish/call_spots/qual_check.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def omp_spot_score ( nbp : NotebookPage , score_multiplier : float , spot_no : Optional [ Union [ int , List , np . ndarray ]] = None , n_neighbours_pos : Optional [ Union [ np . ndarray , int ]] = None , n_neighbours_neg : Optional [ Union [ np . ndarray , int ]] = None ) -> Union [ float , np . ndarray ]: \"\"\" Score for omp gene assignment Args: nbp: OMP Notebook page score_multiplier: `score = score_multiplier * n_pos_neighb + n_neg_neighb`. So this influences the importance of positive coefficient neighbours vs negative. spot_no: Which spots to get score for. If `None`, all scores will be found. Returns: Score for each spot in spot_no if given, otherwise all spot scores. \"\"\" max_score = score_multiplier * np . sum ( nbp . spot_shape == 1 ) + np . sum ( nbp . spot_shape == - 1 ) if n_neighbours_pos is None : n_neighbours_pos = nbp . n_neighbours_pos if n_neighbours_neg is None : n_neighbours_neg = nbp . n_neighbours_neg if spot_no is None : score = ( score_multiplier * n_neighbours_pos + n_neighbours_neg ) / max_score else : score = ( score_multiplier * n_neighbours_pos [ spot_no ] + n_neighbours_neg [ spot_no ]) / max_score return score quality_threshold ( nb , method = 'omp' ) Indicates which spots pass both the score and intensity quality thresholding. Parameters: Name Type Description Default nb Notebook Notebook containing at least the ref_spots page. required method str 'ref' or 'omp' indicating which spots to consider. 'omp' Returns: Type Description np . ndarray bool [n_spots] indicating which spots pass quality thresholding. Source code in coppafish/call_spots/qual_check.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def quality_threshold ( nb : Notebook , method : str = 'omp' ) -> np . ndarray : \"\"\" Indicates which spots pass both the score and intensity quality thresholding. Args: nb: Notebook containing at least the `ref_spots` page. method: `'ref'` or `'omp'` indicating which spots to consider. Returns: `bool [n_spots]` indicating which spots pass quality thresholding. \"\"\" if method . lower () != 'omp' and method . lower () != 'ref' and method . lower () != 'anchor' : raise ValueError ( f \"method must be 'omp' or 'anchor' but { method } given.\" ) intensity_thresh = get_intensity_thresh ( nb ) if nb . has_page ( 'thresholds' ): if method . lower () == 'omp' : score_thresh = nb . thresholds . score_omp score_multiplier = nb . thresholds . score_omp_multiplier else : score_thresh = nb . thresholds . score_ref else : config = nb . get_config ()[ 'thresholds' ] if method . lower () == 'omp' : score_thresh = config [ 'score_omp' ] score_multiplier = config [ 'score_omp_multiplier' ] else : score_thresh = config [ 'score_ref' ] if method . lower () == 'omp' : intensity = nb . omp . intensity score = omp_spot_score ( nb . omp , score_multiplier ) else : intensity = nb . ref_spots . intensity score = nb . ref_spots . score qual_ok = np . array ([ score > score_thresh , intensity > intensity_thresh ]) . all ( axis = 0 ) return qual_ok Optimised get_spot_intensity ( spot_colors ) Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required Returns: Type Description jnp . ndarray float [n_spots] . [s] is the intensity of spot s . Source code in coppafish/call_spots/qual_check_optimised.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @jax . jit def get_spot_intensity ( spot_colors : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Spot colors normalised to equalise intensities between channels (and rounds). Returns: ```float [n_spots]```. ```[s]``` is the intensity of spot ```s```. \"\"\" return jax . vmap ( lambda x : jnp . median ( jnp . max ( x , axis = 1 )), in_axes = 0 , out_axes = 0 )( spot_colors )","title":"Quality Check"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.get_intensity_thresh","text":"Gets threshold for intensity from parameters in config file or Notebook. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots page. required Returns: Type Description float intensity threshold Source code in coppafish/call_spots/qual_check.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def get_intensity_thresh ( nb : Notebook ) -> float : \"\"\" Gets threshold for intensity from parameters in `config file` or Notebook. Args: nb: Notebook containing at least the `call_spots` page. Returns: intensity threshold \"\"\" if nb . has_page ( 'thresholds' ): intensity_thresh = nb . thresholds . intensity else : config = nb . get_config ()[ 'thresholds' ] intensity_thresh = config [ 'intensity' ] if intensity_thresh is None : intensity_thresh = nb . call_spots . gene_efficiency_intensity_thresh return intensity_thresh","title":"get_intensity_thresh()"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.get_spot_intensity","text":"Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required Returns: Type Description np . ndarray float [n_spots] . [s] is the intensity of spot s . Source code in coppafish/call_spots/qual_check.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_spot_intensity ( spot_colors : np . ndarray ) -> np . ndarray : \"\"\" Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Spot colors normalised to equalise intensities between channels (and rounds). Returns: ```float [n_spots]```. ```[s]``` is the intensity of spot ```s```. \"\"\" check_spot = np . random . randint ( spot_colors . shape [ 0 ]) diff_to_int = np . round ( spot_colors [ check_spot ]) . astype ( int ) - spot_colors [ check_spot ] if np . abs ( diff_to_int ) . max () == 0 : raise ValueError ( f \"spot_intensities should be found using normalised spot_colors.\" f \" \\n But for spot { check_spot } , spot_colors given are integers indicating they are \" f \"the raw intensities.\" ) round_max_color = np . max ( spot_colors , axis = 2 ) return np . median ( round_max_color , axis = 1 )","title":"get_spot_intensity()"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.omp_spot_score","text":"Score for omp gene assignment Parameters: Name Type Description Default nbp NotebookPage OMP Notebook page required score_multiplier float score = score_multiplier * n_pos_neighb + n_neg_neighb . So this influences the importance of positive coefficient neighbours vs negative. required spot_no Optional [ Union [ int , List , np . ndarray ]] Which spots to get score for. If None , all scores will be found. None Returns: Type Description Union [ float , np . ndarray ] Score for each spot in spot_no if given, otherwise all spot scores. Source code in coppafish/call_spots/qual_check.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def omp_spot_score ( nbp : NotebookPage , score_multiplier : float , spot_no : Optional [ Union [ int , List , np . ndarray ]] = None , n_neighbours_pos : Optional [ Union [ np . ndarray , int ]] = None , n_neighbours_neg : Optional [ Union [ np . ndarray , int ]] = None ) -> Union [ float , np . ndarray ]: \"\"\" Score for omp gene assignment Args: nbp: OMP Notebook page score_multiplier: `score = score_multiplier * n_pos_neighb + n_neg_neighb`. So this influences the importance of positive coefficient neighbours vs negative. spot_no: Which spots to get score for. If `None`, all scores will be found. Returns: Score for each spot in spot_no if given, otherwise all spot scores. \"\"\" max_score = score_multiplier * np . sum ( nbp . spot_shape == 1 ) + np . sum ( nbp . spot_shape == - 1 ) if n_neighbours_pos is None : n_neighbours_pos = nbp . n_neighbours_pos if n_neighbours_neg is None : n_neighbours_neg = nbp . n_neighbours_neg if spot_no is None : score = ( score_multiplier * n_neighbours_pos + n_neighbours_neg ) / max_score else : score = ( score_multiplier * n_neighbours_pos [ spot_no ] + n_neighbours_neg [ spot_no ]) / max_score return score","title":"omp_spot_score()"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check.quality_threshold","text":"Indicates which spots pass both the score and intensity quality thresholding. Parameters: Name Type Description Default nb Notebook Notebook containing at least the ref_spots page. required method str 'ref' or 'omp' indicating which spots to consider. 'omp' Returns: Type Description np . ndarray bool [n_spots] indicating which spots pass quality thresholding. Source code in coppafish/call_spots/qual_check.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def quality_threshold ( nb : Notebook , method : str = 'omp' ) -> np . ndarray : \"\"\" Indicates which spots pass both the score and intensity quality thresholding. Args: nb: Notebook containing at least the `ref_spots` page. method: `'ref'` or `'omp'` indicating which spots to consider. Returns: `bool [n_spots]` indicating which spots pass quality thresholding. \"\"\" if method . lower () != 'omp' and method . lower () != 'ref' and method . lower () != 'anchor' : raise ValueError ( f \"method must be 'omp' or 'anchor' but { method } given.\" ) intensity_thresh = get_intensity_thresh ( nb ) if nb . has_page ( 'thresholds' ): if method . lower () == 'omp' : score_thresh = nb . thresholds . score_omp score_multiplier = nb . thresholds . score_omp_multiplier else : score_thresh = nb . thresholds . score_ref else : config = nb . get_config ()[ 'thresholds' ] if method . lower () == 'omp' : score_thresh = config [ 'score_omp' ] score_multiplier = config [ 'score_omp_multiplier' ] else : score_thresh = config [ 'score_ref' ] if method . lower () == 'omp' : intensity = nb . omp . intensity score = omp_spot_score ( nb . omp , score_multiplier ) else : intensity = nb . ref_spots . intensity score = nb . ref_spots . score qual_ok = np . array ([ score > score_thresh , intensity > intensity_thresh ]) . all ( axis = 0 ) return qual_ok","title":"quality_threshold()"},{"location":"code/call_spots/qual_check/#optimised","text":"","title":"Optimised"},{"location":"code/call_spots/qual_check/#coppafish.call_spots.qual_check_optimised.get_spot_intensity","text":"Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Parameters: Name Type Description Default spot_colors jnp . ndarray float [n_spots x n_rounds x n_channels] . Spot colors normalised to equalise intensities between channels (and rounds). required Returns: Type Description jnp . ndarray float [n_spots] . [s] is the intensity of spot s . Source code in coppafish/call_spots/qual_check_optimised.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @jax . jit def get_spot_intensity ( spot_colors : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds the max intensity for each imaging round across all imaging channels for each spot. Then median of these max round intensities is returned. Logic is that we expect spots that are genes to have at least one large intensity value in each round so high spot intensity is more indicative of a gene. Args: spot_colors: ```float [n_spots x n_rounds x n_channels]```. Spot colors normalised to equalise intensities between channels (and rounds). Returns: ```float [n_spots]```. ```[s]``` is the intensity of spot ```s```. \"\"\" return jax . vmap ( lambda x : jnp . median ( jnp . max ( x , axis = 1 )), in_axes = 0 , out_axes = 0 )( spot_colors )","title":"get_spot_intensity()"},{"location":"code/extract/base/","text":"get_extract_info ( image , auto_thresh_multiplier , hist_bin_edges , max_pixel_value , scale , z_info = None ) Gets information from filtered scaled images useful for later in the pipeline. If 3D image, only z-plane used for auto_thresh and hist_counts calculation for speed and the that the exact value of these is not that important, just want a rough idea. Parameters: Name Type Description Default image np . ndarray int [n_y x n_x (x n_z)] Image of tile after filtering and scaling. required auto_thresh_multiplier float auto_thresh is set to auto_thresh_multiplier * median(abs(image)) so that pixel values above this are likely spots. Typical = 10 required hist_bin_edges np . ndarray float [len(nbp['hist_values']) + 1] hist_values shifted by 0.5 to give bin edges not centres. required max_pixel_value int Maximum pixel value that image can contain when saving as tiff file. If no shift was applied, this would be np.iinfo(np.uint16).max . required scale float Factor by which, image has been multiplied in order to fill out available values in tiff file. required z_info Optional [ int ] z-plane to get auto_thresh and hist_counts from. None Returns: Type Description float auto_thresh - int Pixel values above auto_thresh in image are likely spots. np . ndarray hist_counts - int [len(nbp['hist_values'])] . hist_counts[i] is the number of pixels found in image with value equal to hist_values[i] . int n_clip_pixels - int Number of pixels in image with value more than max_pixel_value . float clip_scale - float Suggested scale factor to multiply un-scaled image by in order for n_clip_pixels to be 0. Source code in coppafish/extract/base.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def get_extract_info ( image : np . ndarray , auto_thresh_multiplier : float , hist_bin_edges : np . ndarray , max_pixel_value : int , scale : float , z_info : Optional [ int ] = None ) -> Tuple [ float , np . ndarray , int , float ]: \"\"\" Gets information from filtered scaled images useful for later in the pipeline. If 3D image, only z-plane used for `auto_thresh` and `hist_counts` calculation for speed and the that the exact value of these is not that important, just want a rough idea. Args: image: ```int [n_y x n_x (x n_z)]``` Image of tile after filtering and scaling. auto_thresh_multiplier: ```auto_thresh``` is set to ```auto_thresh_multiplier * median(abs(image))``` so that pixel values above this are likely spots. Typical = 10 hist_bin_edges: ```float [len(nbp['hist_values']) + 1]``` ```hist_values``` shifted by 0.5 to give bin edges not centres. max_pixel_value: Maximum pixel value that image can contain when saving as tiff file. If no shift was applied, this would be ```np.iinfo(np.uint16).max```. scale: Factor by which, ```image``` has been multiplied in order to fill out available values in tiff file. z_info: z-plane to get `auto_thresh` and `hist_counts` from. Returns: - ```auto_thresh``` - ```int``` Pixel values above ```auto_thresh``` in ```image``` are likely spots. - ```hist_counts``` - ```int [len(nbp['hist_values'])]```. ```hist_counts[i]``` is the number of pixels found in ```image``` with value equal to ```hist_values[i]```. - ```n_clip_pixels``` - ```int``` Number of pixels in ```image``` with value more than ```max_pixel_value```. - ```clip_scale``` - ```float``` Suggested scale factor to multiply un-scaled ```image``` by in order for ```n_clip_pixels``` to be 0. \"\"\" if image . ndim == 3 : if z_info is None : raise ValueError ( \"z_info not provided\" ) auto_thresh = np . median ( np . abs ( image [:, :, z_info ])) * auto_thresh_multiplier hist_counts = np . histogram ( image [:, :, z_info ], hist_bin_edges )[ 0 ] else : auto_thresh = np . median ( np . abs ( image )) * auto_thresh_multiplier hist_counts = np . histogram ( image , hist_bin_edges )[ 0 ] n_clip_pixels = np . sum ( image > max_pixel_value ) if n_clip_pixels > 0 : # image has already been multiplied by scale hence inclusion of scale here # max_pixel_value / image.max() is less than 1 so recommended scaling becomes smaller than scale. clip_scale = scale * max_pixel_value / image . max () else : clip_scale = 0 return np . round ( auto_thresh ) . astype ( int ), hist_counts , n_clip_pixels , clip_scale get_pixel_length ( length_microns , pixel_size ) Converts a length in units of microns into a length in units of pixels Parameters: Name Type Description Default length_microns float Length in units of microns (microns) required pixel_size float Size of a pixel in microns (microns/pixels) required Returns: Type Description int Desired length in units of pixels (pixels) Source code in coppafish/extract/base.py 53 54 55 56 57 58 59 60 61 62 63 64 65 def get_pixel_length ( length_microns : float , pixel_size : float ) -> int : \"\"\" Converts a length in units of microns into a length in units of pixels Args: length_microns: Length in units of microns (microns) pixel_size: Size of a pixel in microns (microns/pixels) Returns: Desired length in units of pixels (pixels) \"\"\" return int ( round ( length_microns / pixel_size )) strip_hack ( image ) Finds all columns in image where each row is identical and then sets this column to the nearest normal column. Basically 'repeat padding'. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x (x n_z)] Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z. required Returns: Type Description np . ndarray image - float [n_y x n_x (x n_z)] Input array with change_columns set to nearest np . ndarray change_columns - int [n_changed_columns] Indicates which columns have been changed. Source code in coppafish/extract/base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def strip_hack ( image : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds all columns in image where each row is identical and then sets this column to the nearest normal column. Basically 'repeat padding'. Args: image: ```float [n_y x n_x (x n_z)]``` Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z. Returns: - ```image``` - ```float [n_y x n_x (x n_z)]``` Input array with change_columns set to nearest - ```change_columns``` - ```int [n_changed_columns]``` Indicates which columns have been changed. \"\"\" # all rows identical if standard deviation is 0 if np . ndim ( image ) == 3 : # assume each z-plane of 3d image has same bad columns # seems to always be the case for our data change_columns = np . where ( np . std ( image [:, :, 0 ], 0 ) == 0 )[ 0 ] else : change_columns = np . where ( np . std ( image , 0 ) == 0 )[ 0 ] good_columns = np . setdiff1d ( np . arange ( np . shape ( image )[ 1 ]), change_columns ) for col in change_columns : nearest_good_col = good_columns [ np . argmin ( np . abs ( good_columns - col ))] image [:, col ] = image [:, nearest_good_col ] return image , change_columns wait_for_data ( data_path , wait_time , dir = False ) Waits for wait_time seconds to see if file/directory at data_path becomes available in that time. Parameters: Name Type Description Default data_path str Path to file or directory of interest required wait_time int Time to wait in seconds for file to become available. required dir bool If True, assumes data_path points to a directory, otherwise assumes points to a file. False Source code in coppafish/extract/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def wait_for_data ( data_path : str , wait_time : int , dir : bool = False ): \"\"\" Waits for wait_time seconds to see if file/directory at data_path becomes available in that time. Args: data_path: Path to file or directory of interest wait_time: Time to wait in seconds for file to become available. dir: If True, assumes data_path points to a directory, otherwise assumes points to a file. \"\"\" if dir : check_data_func = lambda x : os . path . isdir ( x ) else : check_data_func = lambda x : os . path . isfile ( x ) if not check_data_func ( data_path ): # wait for file to become available if wait_time > 60 ** 2 : wait_time_print = round ( wait_time / 60 ** 2 , 1 ) wait_time_unit = 'hours' else : wait_time_print = round ( wait_time , 1 ) wait_time_unit = 'seconds' warnings . warn ( f ' \\n No file named \\n { data_path } \\n exists. Waiting for { wait_time_print } { wait_time_unit } ...' ) with tqdm ( total = wait_time , position = 0 ) as pbar : pbar . set_description ( f \"Waiting for { data_path } \" ) for i in range ( wait_time ): time . sleep ( 1 ) if check_data_func ( data_path ): break pbar . update ( 1 ) pbar . close () if not check_data_func ( data_path ): raise utils . errors . NoFileError ( data_path ) print ( \"file found! \\n Waiting for file to fully load...\" ) # wait for file to stop loading old_bytes = 0 new_bytes = 0.00001 while new_bytes > old_bytes : time . sleep ( 5 ) old_bytes = new_bytes new_bytes = os . path . getsize ( data_path ) print ( \"file loaded!\" )","title":"Base"},{"location":"code/extract/base/#coppafish.extract.base.get_extract_info","text":"Gets information from filtered scaled images useful for later in the pipeline. If 3D image, only z-plane used for auto_thresh and hist_counts calculation for speed and the that the exact value of these is not that important, just want a rough idea. Parameters: Name Type Description Default image np . ndarray int [n_y x n_x (x n_z)] Image of tile after filtering and scaling. required auto_thresh_multiplier float auto_thresh is set to auto_thresh_multiplier * median(abs(image)) so that pixel values above this are likely spots. Typical = 10 required hist_bin_edges np . ndarray float [len(nbp['hist_values']) + 1] hist_values shifted by 0.5 to give bin edges not centres. required max_pixel_value int Maximum pixel value that image can contain when saving as tiff file. If no shift was applied, this would be np.iinfo(np.uint16).max . required scale float Factor by which, image has been multiplied in order to fill out available values in tiff file. required z_info Optional [ int ] z-plane to get auto_thresh and hist_counts from. None Returns: Type Description float auto_thresh - int Pixel values above auto_thresh in image are likely spots. np . ndarray hist_counts - int [len(nbp['hist_values'])] . hist_counts[i] is the number of pixels found in image with value equal to hist_values[i] . int n_clip_pixels - int Number of pixels in image with value more than max_pixel_value . float clip_scale - float Suggested scale factor to multiply un-scaled image by in order for n_clip_pixels to be 0. Source code in coppafish/extract/base.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def get_extract_info ( image : np . ndarray , auto_thresh_multiplier : float , hist_bin_edges : np . ndarray , max_pixel_value : int , scale : float , z_info : Optional [ int ] = None ) -> Tuple [ float , np . ndarray , int , float ]: \"\"\" Gets information from filtered scaled images useful for later in the pipeline. If 3D image, only z-plane used for `auto_thresh` and `hist_counts` calculation for speed and the that the exact value of these is not that important, just want a rough idea. Args: image: ```int [n_y x n_x (x n_z)]``` Image of tile after filtering and scaling. auto_thresh_multiplier: ```auto_thresh``` is set to ```auto_thresh_multiplier * median(abs(image))``` so that pixel values above this are likely spots. Typical = 10 hist_bin_edges: ```float [len(nbp['hist_values']) + 1]``` ```hist_values``` shifted by 0.5 to give bin edges not centres. max_pixel_value: Maximum pixel value that image can contain when saving as tiff file. If no shift was applied, this would be ```np.iinfo(np.uint16).max```. scale: Factor by which, ```image``` has been multiplied in order to fill out available values in tiff file. z_info: z-plane to get `auto_thresh` and `hist_counts` from. Returns: - ```auto_thresh``` - ```int``` Pixel values above ```auto_thresh``` in ```image``` are likely spots. - ```hist_counts``` - ```int [len(nbp['hist_values'])]```. ```hist_counts[i]``` is the number of pixels found in ```image``` with value equal to ```hist_values[i]```. - ```n_clip_pixels``` - ```int``` Number of pixels in ```image``` with value more than ```max_pixel_value```. - ```clip_scale``` - ```float``` Suggested scale factor to multiply un-scaled ```image``` by in order for ```n_clip_pixels``` to be 0. \"\"\" if image . ndim == 3 : if z_info is None : raise ValueError ( \"z_info not provided\" ) auto_thresh = np . median ( np . abs ( image [:, :, z_info ])) * auto_thresh_multiplier hist_counts = np . histogram ( image [:, :, z_info ], hist_bin_edges )[ 0 ] else : auto_thresh = np . median ( np . abs ( image )) * auto_thresh_multiplier hist_counts = np . histogram ( image , hist_bin_edges )[ 0 ] n_clip_pixels = np . sum ( image > max_pixel_value ) if n_clip_pixels > 0 : # image has already been multiplied by scale hence inclusion of scale here # max_pixel_value / image.max() is less than 1 so recommended scaling becomes smaller than scale. clip_scale = scale * max_pixel_value / image . max () else : clip_scale = 0 return np . round ( auto_thresh ) . astype ( int ), hist_counts , n_clip_pixels , clip_scale","title":"get_extract_info()"},{"location":"code/extract/base/#coppafish.extract.base.get_pixel_length","text":"Converts a length in units of microns into a length in units of pixels Parameters: Name Type Description Default length_microns float Length in units of microns (microns) required pixel_size float Size of a pixel in microns (microns/pixels) required Returns: Type Description int Desired length in units of pixels (pixels) Source code in coppafish/extract/base.py 53 54 55 56 57 58 59 60 61 62 63 64 65 def get_pixel_length ( length_microns : float , pixel_size : float ) -> int : \"\"\" Converts a length in units of microns into a length in units of pixels Args: length_microns: Length in units of microns (microns) pixel_size: Size of a pixel in microns (microns/pixels) Returns: Desired length in units of pixels (pixels) \"\"\" return int ( round ( length_microns / pixel_size ))","title":"get_pixel_length()"},{"location":"code/extract/base/#coppafish.extract.base.strip_hack","text":"Finds all columns in image where each row is identical and then sets this column to the nearest normal column. Basically 'repeat padding'. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x (x n_z)] Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z. required Returns: Type Description np . ndarray image - float [n_y x n_x (x n_z)] Input array with change_columns set to nearest np . ndarray change_columns - int [n_changed_columns] Indicates which columns have been changed. Source code in coppafish/extract/base.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def strip_hack ( image : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds all columns in image where each row is identical and then sets this column to the nearest normal column. Basically 'repeat padding'. Args: image: ```float [n_y x n_x (x n_z)]``` Image from nd2 file, before filtering (can be after focus stacking) and if 3d, last index must be z. Returns: - ```image``` - ```float [n_y x n_x (x n_z)]``` Input array with change_columns set to nearest - ```change_columns``` - ```int [n_changed_columns]``` Indicates which columns have been changed. \"\"\" # all rows identical if standard deviation is 0 if np . ndim ( image ) == 3 : # assume each z-plane of 3d image has same bad columns # seems to always be the case for our data change_columns = np . where ( np . std ( image [:, :, 0 ], 0 ) == 0 )[ 0 ] else : change_columns = np . where ( np . std ( image , 0 ) == 0 )[ 0 ] good_columns = np . setdiff1d ( np . arange ( np . shape ( image )[ 1 ]), change_columns ) for col in change_columns : nearest_good_col = good_columns [ np . argmin ( np . abs ( good_columns - col ))] image [:, col ] = image [:, nearest_good_col ] return image , change_columns","title":"strip_hack()"},{"location":"code/extract/base/#coppafish.extract.base.wait_for_data","text":"Waits for wait_time seconds to see if file/directory at data_path becomes available in that time. Parameters: Name Type Description Default data_path str Path to file or directory of interest required wait_time int Time to wait in seconds for file to become available. required dir bool If True, assumes data_path points to a directory, otherwise assumes points to a file. False Source code in coppafish/extract/base.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def wait_for_data ( data_path : str , wait_time : int , dir : bool = False ): \"\"\" Waits for wait_time seconds to see if file/directory at data_path becomes available in that time. Args: data_path: Path to file or directory of interest wait_time: Time to wait in seconds for file to become available. dir: If True, assumes data_path points to a directory, otherwise assumes points to a file. \"\"\" if dir : check_data_func = lambda x : os . path . isdir ( x ) else : check_data_func = lambda x : os . path . isfile ( x ) if not check_data_func ( data_path ): # wait for file to become available if wait_time > 60 ** 2 : wait_time_print = round ( wait_time / 60 ** 2 , 1 ) wait_time_unit = 'hours' else : wait_time_print = round ( wait_time , 1 ) wait_time_unit = 'seconds' warnings . warn ( f ' \\n No file named \\n { data_path } \\n exists. Waiting for { wait_time_print } { wait_time_unit } ...' ) with tqdm ( total = wait_time , position = 0 ) as pbar : pbar . set_description ( f \"Waiting for { data_path } \" ) for i in range ( wait_time ): time . sleep ( 1 ) if check_data_func ( data_path ): break pbar . update ( 1 ) pbar . close () if not check_data_func ( data_path ): raise utils . errors . NoFileError ( data_path ) print ( \"file found! \\n Waiting for file to fully load...\" ) # wait for file to stop loading old_bytes = 0 new_bytes = 0.00001 while new_bytes > old_bytes : time . sleep ( 5 ) old_bytes = new_bytes new_bytes = os . path . getsize ( data_path ) print ( \"file loaded!\" )","title":"wait_for_data()"},{"location":"code/extract/deconvolution/","text":"get_psf ( spot_images , annulus_width ) This gets psf, which is average image of spot from individual images of spots. It is normalised so min value is 0 and max value is 1. Parameters: Name Type Description Default spot_images np . ndarray int [n_spots x y_diameter x x_diameter x z_diameter] . spot_images[s] is the small image surrounding spot s . required annulus_width float Within each z-plane, this specifies how big an annulus to use, within which we expect all pixel values to be the same. required Returns: Type Description np . ndarray float [y_diameter x x_diameter x z_diameter] . Average small image about a spot. Normalised so min is 0 and max is 1. Source code in coppafish/extract/deconvolution.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_psf ( spot_images : np . ndarray , annulus_width : float ) -> np . ndarray : \"\"\" This gets psf, which is average image of spot from individual images of spots. It is normalised so min value is 0 and max value is 1. Args: spot_images: ```int [n_spots x y_diameter x x_diameter x z_diameter]```. ```spot_images[s]``` is the small image surrounding spot ```s```. annulus_width: Within each z-plane, this specifies how big an annulus to use, within which we expect all pixel values to be the same. Returns: ```float [y_diameter x x_diameter x z_diameter]```. Average small image about a spot. Normalised so min is 0 and max is 1. \"\"\" # normalise each z plane of each spot image first so each has median of 0 and max of 1. # Found that this works well as taper psf anyway, which gives reduced intensity as move away from centre. spot_images = spot_images - np . expand_dims ( np . nanmedian ( spot_images , axis = [ 1 , 2 ]), [ 1 , 2 ]) spot_images = spot_images / np . expand_dims ( np . nanmax ( spot_images , axis = ( 1 , 2 )), [ 1 , 2 ]) psf = get_average_spot_image ( spot_images , 'median' , 'annulus_2d' , annulus_width ) # normalise psf so min is 0 and max is 1. psf = psf - psf . min () psf = psf / psf . max () return psf get_psf_spots ( nbp_file , nbp_basic , round , use_tiles , channel , use_z , radius_xy , radius_z , min_spots , intensity_thresh , intensity_auto_param , isolation_dist , shape ) Finds spot_shapes about spots found in raw data, average of these then used for psf. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required round int Reference round to get spots from to determine psf. This should be the anchor round (last round) if using. required use_tiles List [ int ] int [n_use_tiles] . tiff tile indices used in experiment. required channel int Reference channel to get spots from to determine psf. required use_z List [ int ] int [n_z] . Z-planes used in the experiment. required radius_xy int Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z int Radius of dilation structuring element in z direction (approximately spot radius) required min_spots int Minimum number of spots required to determine average shape from. Typical: 300 required intensity_thresh Optional [ float ] Spots are local maxima in image with pixel value > intensity_thresh . if intensity_thresh = None , will automatically compute it from mid z-plane of first tile. required intensity_auto_param float If intensity_thresh = None so is automatically computed, it is done using this. required isolation_dist float Spots are isolated if nearest neighbour is further away than this. required shape List [ int ] int [y_diameter, x_diameter, z_diameter] . Desired size of image about each spot. required Returns: Type Description np . ndarray spot_images - int [n_spots x y_diameter x x_diameter x z_diameter] . spot_images[s] is the small image surrounding spot s . float intensity_thresh - float . Only different from input if input was None . List [ int ] tiles_used - int [n_tiles_used] . Tiles the spots were found on. Source code in coppafish/extract/deconvolution.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_psf_spots ( nbp_file : NotebookPage , nbp_basic : NotebookPage , round : int , use_tiles : List [ int ], channel : int , use_z : List [ int ], radius_xy : int , radius_z : int , min_spots : int , intensity_thresh : Optional [ float ], intensity_auto_param : float , isolation_dist : float , shape : List [ int ]) -> Tuple [ np . ndarray , float , List [ int ]]: \"\"\" Finds spot_shapes about spots found in raw data, average of these then used for psf. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page round: Reference round to get spots from to determine psf. This should be the anchor round (last round) if using. use_tiles: ```int [n_use_tiles]```. tiff tile indices used in experiment. channel: Reference channel to get spots from to determine psf. use_z: ```int [n_z]```. Z-planes used in the experiment. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius) min_spots: Minimum number of spots required to determine average shape from. Typical: 300 intensity_thresh: Spots are local maxima in image with ```pixel value > intensity_thresh```. if ```intensity_thresh = None```, will automatically compute it from mid z-plane of first tile. intensity_auto_param: If ```intensity_thresh = None``` so is automatically computed, it is done using this. isolation_dist: Spots are isolated if nearest neighbour is further away than this. shape: ```int [y_diameter, x_diameter, z_diameter]```. Desired size of image about each spot. Returns: - ```spot_images``` - ```int [n_spots x y_diameter x x_diameter x z_diameter]```. ```spot_images[s]``` is the small image surrounding spot ```s```. - ```intensity_thresh``` - ```float```. Only different from input if input was ```None```. - ```tiles_used``` - ```int [n_tiles_used]```. Tiles the spots were found on. \"\"\" n_spots = 0 spot_images = np . zeros (( 0 , shape [ 0 ], shape [ 1 ], shape [ 2 ]), dtype = int ) tiles_used = [] while n_spots < min_spots : t = scale . central_tile ( nbp_basic . tilepos_yx , use_tiles ) # choose tile closet to centre im = utils . raw . load ( nbp_file , nbp_basic , None , round , channel , use_z ) mid_z = np . ceil ( im . shape [ 2 ] / 2 ) . astype ( int ) median_im = np . median ( im [:, :, mid_z ]) if intensity_thresh is None : intensity_thresh = median_im + np . median ( np . abs ( im [:, :, mid_z ] - median_im )) * intensity_auto_param elif intensity_thresh <= median_im or intensity_thresh >= np . iinfo ( np . uint16 ) . max : raise utils . errors . OutOfBoundsError ( \"intensity_thresh\" , intensity_thresh , median_im , np . iinfo ( np . uint16 ) . max ) spot_yxz , _ = detect_spots ( im , intensity_thresh , radius_xy , radius_z , True ) # check fall off in intensity not too large not_single_pixel = check_neighbour_intensity ( im , spot_yxz , median_im ) isolated = get_isolated_points ( spot_yxz , isolation_dist ) spot_yxz = spot_yxz [ np . logical_and ( isolated , not_single_pixel ), :] if n_spots == 0 and np . shape ( spot_yxz )[ 0 ] < min_spots / 4 : # raise error on first tile if looks like we are going to use more than 4 tiles raise ValueError ( f \" \\n First tile, { t } , only found { np . shape ( spot_yxz )[ 0 ] } spots.\" f \" \\n Maybe consider lowering intensity_thresh from current value of { intensity_thresh } .\" ) spot_images = np . append ( spot_images , get_spot_images ( im , spot_yxz , shape ), axis = 0 ) n_spots = np . shape ( spot_images )[ 0 ] use_tiles = np . setdiff1d ( use_tiles , t ) tiles_used . append ( t ) if len ( use_tiles ) == 0 and n_spots < min_spots : raise ValueError ( f \" \\n Required min_spots = { min_spots } , but only found { n_spots } . \\n \" f \"Maybe consider lowering intensity_thresh from current value of { intensity_thresh } .\" ) return spot_images , intensity_thresh . astype ( float ), tiles_used get_wiener_filter ( psf , image_shape , constant ) This tapers the psf so goes to 0 at edges and then computes wiener filter from it. Parameters: Name Type Description Default psf np . ndarray float [y_diameter x x_diameter x z_diameter] . Average small image about a spot. Normalised so min is 0 and max is 1. required image_shape Union [ np . ndarray , List [ int ]] int [n_im_y, n_im_x, n_im_z] . Indicates the shape of the image to be convolved after padding. required constant float Constant used in wiener filter. required Returns: Type Description np . ndarray complex128 [n_im_y x n_im_x x n_im_z] . Wiener filter of same size as image. Source code in coppafish/extract/deconvolution.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def get_wiener_filter ( psf : np . ndarray , image_shape : Union [ np . ndarray , List [ int ]], constant : float ) -> np . ndarray : \"\"\" This tapers the psf so goes to 0 at edges and then computes wiener filter from it. Args: psf: ```float [y_diameter x x_diameter x z_diameter]```. Average small image about a spot. Normalised so min is 0 and max is 1. image_shape: ```int [n_im_y, n_im_x, n_im_z]```. Indicates the shape of the image to be convolved after padding. constant: Constant used in wiener filter. Returns: ```complex128 [n_im_y x n_im_x x n_im_z]```. Wiener filter of same size as image. \"\"\" # taper psf so smoothly goes to 0 at each edge. psf = psf * np . hanning ( psf . shape [ 0 ]) . reshape ( - 1 , 1 , 1 ) * np . hanning ( psf . shape [ 1 ]) . reshape ( 1 , - 1 , 1 ) * \\ np . hanning ( psf . shape [ 2 ]) . reshape ( 1 , 1 , - 1 ) psf = psf_pad ( psf , image_shape ) psf_ft = np . fft . fftn ( np . fft . ifftshift ( psf )) return np . conj ( psf_ft ) / np . real (( psf_ft * np . conj ( psf_ft ) + constant )) psf_pad ( psf , image_shape ) Pads psf with zeros so has same dimensions as image Parameters: Name Type Description Default psf np . ndarray float [y_shape x x_shape (x z_shape)] . Point Spread Function with same shape as small image about each spot. required image_shape Union [ np . ndarray , List [ int ]] int [psf.ndim] . Number of pixels in [y, x, (z)] direction of padded image. required Returns: Type Description np . ndarray float [image_shape[0] x image_shape[1] (x image_shape[2])] . np . ndarray Array same size as image with psf centered on middle pixel. Source code in coppafish/extract/deconvolution.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def psf_pad ( psf : np . ndarray , image_shape : Union [ np . ndarray , List [ int ]]) -> np . ndarray : \"\"\" Pads psf with zeros so has same dimensions as image Args: psf: ```float [y_shape x x_shape (x z_shape)]```. Point Spread Function with same shape as small image about each spot. image_shape: ```int [psf.ndim]```. Number of pixels in ```[y, x, (z)]``` direction of padded image. Returns: ```float [image_shape[0] x image_shape[1] (x image_shape[2])]```. Array same size as image with psf centered on middle pixel. \"\"\" # must pad with ceil first so that ifftshift puts central pixel to (0,0,0). pre_pad = np . ceil (( np . array ( image_shape ) - np . array ( psf . shape )) / 2 ) . astype ( int ) post_pad = np . floor (( np . array ( image_shape ) - np . array ( psf . shape )) / 2 ) . astype ( int ) return np . pad ( psf , [( pre_pad [ i ], post_pad [ i ]) for i in range ( len ( pre_pad ))]) wiener_deconvolve ( image , im_pad_shape , filter ) This pads image so goes to median value of image at each edge. Then deconvolves using wiener filter. Parameters: Name Type Description Default image np . ndarray int [n_im_y x n_im_x x n_im_z] . Image to be deconvolved. required im_pad_shape List [ int ] int [n_pad_y, n_pad_x, n_pad_z] . How much to pad image in [y, x, z] directions. required filter np . ndarray complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z] . Wiener filter to use. required Returns: Type Description np . ndarray int [n_im_y x n_im_x x n_im_z] . Deconvolved image. Source code in coppafish/extract/deconvolution.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def wiener_deconvolve ( image : np . ndarray , im_pad_shape : List [ int ], filter : np . ndarray ) -> np . ndarray : \"\"\" This pads ```image``` so goes to median value of ```image``` at each edge. Then deconvolves using wiener filter. Args: image: ```int [n_im_y x n_im_x x n_im_z]```. Image to be deconvolved. im_pad_shape: ```int [n_pad_y, n_pad_x, n_pad_z]```. How much to pad image in ```[y, x, z]``` directions. filter: ```complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z]```. Wiener filter to use. Returns: ```int [n_im_y x n_im_x x n_im_z]```. Deconvolved image. \"\"\" im_max = image . max () im_min = image . min () im_av = np . median ( image [:, :, 0 ]) image = np . pad ( image , [( im_pad_shape [ i ], im_pad_shape [ i ]) for i in range ( len ( im_pad_shape ))], 'linear_ramp' , end_values = [( im_av , im_av )] * 3 ) im_deconvolved = np . real ( np . fft . ifftn ( np . fft . fftn ( image ) * filter )) im_deconvolved = im_deconvolved [ im_pad_shape [ 0 ]: - im_pad_shape [ 0 ], im_pad_shape [ 1 ]: - im_pad_shape [ 1 ], im_pad_shape [ 2 ]: - im_pad_shape [ 2 ]] # set min and max so it covers same range as input image im_deconvolved = im_deconvolved - im_deconvolved . min () return np . round ( im_deconvolved * ( im_max - im_min ) / im_deconvolved . max () + im_min ) . astype ( int )","title":"Deconvolution"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_psf","text":"This gets psf, which is average image of spot from individual images of spots. It is normalised so min value is 0 and max value is 1. Parameters: Name Type Description Default spot_images np . ndarray int [n_spots x y_diameter x x_diameter x z_diameter] . spot_images[s] is the small image surrounding spot s . required annulus_width float Within each z-plane, this specifies how big an annulus to use, within which we expect all pixel values to be the same. required Returns: Type Description np . ndarray float [y_diameter x x_diameter x z_diameter] . Average small image about a spot. Normalised so min is 0 and max is 1. Source code in coppafish/extract/deconvolution.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_psf ( spot_images : np . ndarray , annulus_width : float ) -> np . ndarray : \"\"\" This gets psf, which is average image of spot from individual images of spots. It is normalised so min value is 0 and max value is 1. Args: spot_images: ```int [n_spots x y_diameter x x_diameter x z_diameter]```. ```spot_images[s]``` is the small image surrounding spot ```s```. annulus_width: Within each z-plane, this specifies how big an annulus to use, within which we expect all pixel values to be the same. Returns: ```float [y_diameter x x_diameter x z_diameter]```. Average small image about a spot. Normalised so min is 0 and max is 1. \"\"\" # normalise each z plane of each spot image first so each has median of 0 and max of 1. # Found that this works well as taper psf anyway, which gives reduced intensity as move away from centre. spot_images = spot_images - np . expand_dims ( np . nanmedian ( spot_images , axis = [ 1 , 2 ]), [ 1 , 2 ]) spot_images = spot_images / np . expand_dims ( np . nanmax ( spot_images , axis = ( 1 , 2 )), [ 1 , 2 ]) psf = get_average_spot_image ( spot_images , 'median' , 'annulus_2d' , annulus_width ) # normalise psf so min is 0 and max is 1. psf = psf - psf . min () psf = psf / psf . max () return psf","title":"get_psf()"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_psf_spots","text":"Finds spot_shapes about spots found in raw data, average of these then used for psf. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required round int Reference round to get spots from to determine psf. This should be the anchor round (last round) if using. required use_tiles List [ int ] int [n_use_tiles] . tiff tile indices used in experiment. required channel int Reference channel to get spots from to determine psf. required use_z List [ int ] int [n_z] . Z-planes used in the experiment. required radius_xy int Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z int Radius of dilation structuring element in z direction (approximately spot radius) required min_spots int Minimum number of spots required to determine average shape from. Typical: 300 required intensity_thresh Optional [ float ] Spots are local maxima in image with pixel value > intensity_thresh . if intensity_thresh = None , will automatically compute it from mid z-plane of first tile. required intensity_auto_param float If intensity_thresh = None so is automatically computed, it is done using this. required isolation_dist float Spots are isolated if nearest neighbour is further away than this. required shape List [ int ] int [y_diameter, x_diameter, z_diameter] . Desired size of image about each spot. required Returns: Type Description np . ndarray spot_images - int [n_spots x y_diameter x x_diameter x z_diameter] . spot_images[s] is the small image surrounding spot s . float intensity_thresh - float . Only different from input if input was None . List [ int ] tiles_used - int [n_tiles_used] . Tiles the spots were found on. Source code in coppafish/extract/deconvolution.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def get_psf_spots ( nbp_file : NotebookPage , nbp_basic : NotebookPage , round : int , use_tiles : List [ int ], channel : int , use_z : List [ int ], radius_xy : int , radius_z : int , min_spots : int , intensity_thresh : Optional [ float ], intensity_auto_param : float , isolation_dist : float , shape : List [ int ]) -> Tuple [ np . ndarray , float , List [ int ]]: \"\"\" Finds spot_shapes about spots found in raw data, average of these then used for psf. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page round: Reference round to get spots from to determine psf. This should be the anchor round (last round) if using. use_tiles: ```int [n_use_tiles]```. tiff tile indices used in experiment. channel: Reference channel to get spots from to determine psf. use_z: ```int [n_z]```. Z-planes used in the experiment. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius) min_spots: Minimum number of spots required to determine average shape from. Typical: 300 intensity_thresh: Spots are local maxima in image with ```pixel value > intensity_thresh```. if ```intensity_thresh = None```, will automatically compute it from mid z-plane of first tile. intensity_auto_param: If ```intensity_thresh = None``` so is automatically computed, it is done using this. isolation_dist: Spots are isolated if nearest neighbour is further away than this. shape: ```int [y_diameter, x_diameter, z_diameter]```. Desired size of image about each spot. Returns: - ```spot_images``` - ```int [n_spots x y_diameter x x_diameter x z_diameter]```. ```spot_images[s]``` is the small image surrounding spot ```s```. - ```intensity_thresh``` - ```float```. Only different from input if input was ```None```. - ```tiles_used``` - ```int [n_tiles_used]```. Tiles the spots were found on. \"\"\" n_spots = 0 spot_images = np . zeros (( 0 , shape [ 0 ], shape [ 1 ], shape [ 2 ]), dtype = int ) tiles_used = [] while n_spots < min_spots : t = scale . central_tile ( nbp_basic . tilepos_yx , use_tiles ) # choose tile closet to centre im = utils . raw . load ( nbp_file , nbp_basic , None , round , channel , use_z ) mid_z = np . ceil ( im . shape [ 2 ] / 2 ) . astype ( int ) median_im = np . median ( im [:, :, mid_z ]) if intensity_thresh is None : intensity_thresh = median_im + np . median ( np . abs ( im [:, :, mid_z ] - median_im )) * intensity_auto_param elif intensity_thresh <= median_im or intensity_thresh >= np . iinfo ( np . uint16 ) . max : raise utils . errors . OutOfBoundsError ( \"intensity_thresh\" , intensity_thresh , median_im , np . iinfo ( np . uint16 ) . max ) spot_yxz , _ = detect_spots ( im , intensity_thresh , radius_xy , radius_z , True ) # check fall off in intensity not too large not_single_pixel = check_neighbour_intensity ( im , spot_yxz , median_im ) isolated = get_isolated_points ( spot_yxz , isolation_dist ) spot_yxz = spot_yxz [ np . logical_and ( isolated , not_single_pixel ), :] if n_spots == 0 and np . shape ( spot_yxz )[ 0 ] < min_spots / 4 : # raise error on first tile if looks like we are going to use more than 4 tiles raise ValueError ( f \" \\n First tile, { t } , only found { np . shape ( spot_yxz )[ 0 ] } spots.\" f \" \\n Maybe consider lowering intensity_thresh from current value of { intensity_thresh } .\" ) spot_images = np . append ( spot_images , get_spot_images ( im , spot_yxz , shape ), axis = 0 ) n_spots = np . shape ( spot_images )[ 0 ] use_tiles = np . setdiff1d ( use_tiles , t ) tiles_used . append ( t ) if len ( use_tiles ) == 0 and n_spots < min_spots : raise ValueError ( f \" \\n Required min_spots = { min_spots } , but only found { n_spots } . \\n \" f \"Maybe consider lowering intensity_thresh from current value of { intensity_thresh } .\" ) return spot_images , intensity_thresh . astype ( float ), tiles_used","title":"get_psf_spots()"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.get_wiener_filter","text":"This tapers the psf so goes to 0 at edges and then computes wiener filter from it. Parameters: Name Type Description Default psf np . ndarray float [y_diameter x x_diameter x z_diameter] . Average small image about a spot. Normalised so min is 0 and max is 1. required image_shape Union [ np . ndarray , List [ int ]] int [n_im_y, n_im_x, n_im_z] . Indicates the shape of the image to be convolved after padding. required constant float Constant used in wiener filter. required Returns: Type Description np . ndarray complex128 [n_im_y x n_im_x x n_im_z] . Wiener filter of same size as image. Source code in coppafish/extract/deconvolution.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def get_wiener_filter ( psf : np . ndarray , image_shape : Union [ np . ndarray , List [ int ]], constant : float ) -> np . ndarray : \"\"\" This tapers the psf so goes to 0 at edges and then computes wiener filter from it. Args: psf: ```float [y_diameter x x_diameter x z_diameter]```. Average small image about a spot. Normalised so min is 0 and max is 1. image_shape: ```int [n_im_y, n_im_x, n_im_z]```. Indicates the shape of the image to be convolved after padding. constant: Constant used in wiener filter. Returns: ```complex128 [n_im_y x n_im_x x n_im_z]```. Wiener filter of same size as image. \"\"\" # taper psf so smoothly goes to 0 at each edge. psf = psf * np . hanning ( psf . shape [ 0 ]) . reshape ( - 1 , 1 , 1 ) * np . hanning ( psf . shape [ 1 ]) . reshape ( 1 , - 1 , 1 ) * \\ np . hanning ( psf . shape [ 2 ]) . reshape ( 1 , 1 , - 1 ) psf = psf_pad ( psf , image_shape ) psf_ft = np . fft . fftn ( np . fft . ifftshift ( psf )) return np . conj ( psf_ft ) / np . real (( psf_ft * np . conj ( psf_ft ) + constant ))","title":"get_wiener_filter()"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.psf_pad","text":"Pads psf with zeros so has same dimensions as image Parameters: Name Type Description Default psf np . ndarray float [y_shape x x_shape (x z_shape)] . Point Spread Function with same shape as small image about each spot. required image_shape Union [ np . ndarray , List [ int ]] int [psf.ndim] . Number of pixels in [y, x, (z)] direction of padded image. required Returns: Type Description np . ndarray float [image_shape[0] x image_shape[1] (x image_shape[2])] . np . ndarray Array same size as image with psf centered on middle pixel. Source code in coppafish/extract/deconvolution.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def psf_pad ( psf : np . ndarray , image_shape : Union [ np . ndarray , List [ int ]]) -> np . ndarray : \"\"\" Pads psf with zeros so has same dimensions as image Args: psf: ```float [y_shape x x_shape (x z_shape)]```. Point Spread Function with same shape as small image about each spot. image_shape: ```int [psf.ndim]```. Number of pixels in ```[y, x, (z)]``` direction of padded image. Returns: ```float [image_shape[0] x image_shape[1] (x image_shape[2])]```. Array same size as image with psf centered on middle pixel. \"\"\" # must pad with ceil first so that ifftshift puts central pixel to (0,0,0). pre_pad = np . ceil (( np . array ( image_shape ) - np . array ( psf . shape )) / 2 ) . astype ( int ) post_pad = np . floor (( np . array ( image_shape ) - np . array ( psf . shape )) / 2 ) . astype ( int ) return np . pad ( psf , [( pre_pad [ i ], post_pad [ i ]) for i in range ( len ( pre_pad ))])","title":"psf_pad()"},{"location":"code/extract/deconvolution/#coppafish.extract.deconvolution.wiener_deconvolve","text":"This pads image so goes to median value of image at each edge. Then deconvolves using wiener filter. Parameters: Name Type Description Default image np . ndarray int [n_im_y x n_im_x x n_im_z] . Image to be deconvolved. required im_pad_shape List [ int ] int [n_pad_y, n_pad_x, n_pad_z] . How much to pad image in [y, x, z] directions. required filter np . ndarray complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z] . Wiener filter to use. required Returns: Type Description np . ndarray int [n_im_y x n_im_x x n_im_z] . Deconvolved image. Source code in coppafish/extract/deconvolution.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 def wiener_deconvolve ( image : np . ndarray , im_pad_shape : List [ int ], filter : np . ndarray ) -> np . ndarray : \"\"\" This pads ```image``` so goes to median value of ```image``` at each edge. Then deconvolves using wiener filter. Args: image: ```int [n_im_y x n_im_x x n_im_z]```. Image to be deconvolved. im_pad_shape: ```int [n_pad_y, n_pad_x, n_pad_z]```. How much to pad image in ```[y, x, z]``` directions. filter: ```complex128 [n_im_y+2*n_pad_y, n_im_x+2*n_pad_x, n_im_z+2*n_pad_z]```. Wiener filter to use. Returns: ```int [n_im_y x n_im_x x n_im_z]```. Deconvolved image. \"\"\" im_max = image . max () im_min = image . min () im_av = np . median ( image [:, :, 0 ]) image = np . pad ( image , [( im_pad_shape [ i ], im_pad_shape [ i ]) for i in range ( len ( im_pad_shape ))], 'linear_ramp' , end_values = [( im_av , im_av )] * 3 ) im_deconvolved = np . real ( np . fft . ifftn ( np . fft . fftn ( image ) * filter )) im_deconvolved = im_deconvolved [ im_pad_shape [ 0 ]: - im_pad_shape [ 0 ], im_pad_shape [ 1 ]: - im_pad_shape [ 1 ], im_pad_shape [ 2 ]: - im_pad_shape [ 2 ]] # set min and max so it covers same range as input image im_deconvolved = im_deconvolved - im_deconvolved . min () return np . round ( im_deconvolved * ( im_max - im_min ) / im_deconvolved . max () + im_min ) . astype ( int )","title":"wiener_deconvolve()"},{"location":"code/extract/fstack/","text":"focus_stack ( im_stack , nhsize = 9 , focus = None , alpha = 0.2 , sth = 13 ) Generate extended depth-of-field image from focus sequence using noise-robust selective all-in-focus algorithm [1]. Input images may be grayscale or color. For color images, the algorithm is applied to each color plane independently. For further details, see: [1] Pertuz et. al. \"Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013. S. Pertuz, Jan/2016 Modified by Josh, 2021 Parameters: Name Type Description Default im_stack np . ndarray Element [:,:,p,:] is greyscale or RGB image at z-plane p . RGB: uint8 [M x N x P x 3] Gray: uint16 [M x N x P] required nhsize int Size of default window. 9 focus Optional [ np . ndarray ] int [P] or None . Vector with focus of each frame. If None , will use np.arange(P) . None alpha float A scalar in [0,1] . See [1] for details. 0.2 sth float A scalar. See [1] for details. 13 Returns: Type Description np . ndarray int [M x N] . All In Focus (AIF) image. Source code in coppafish/extract/fstack.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def focus_stack ( im_stack : np . ndarray , nhsize : int = 9 , focus : Optional [ np . ndarray ] = None , alpha : float = 0.2 , sth : float = 13 ) -> np . ndarray : \"\"\" Generate extended depth-of-field image from focus sequence using noise-robust selective all-in-focus algorithm [1]. Input images may be grayscale or color. For color images, the algorithm is applied to each color plane independently. For further details, see: [1] Pertuz et. al. \"Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013. S. Pertuz, Jan/2016 Modified by Josh, 2021 Args: im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```. RGB: ```uint8 [M x N x P x 3]``` Gray: ```uint16 [M x N x P]``` nhsize: Size of default window. focus: ```int [P]``` or ```None```. Vector with focus of each frame. If ```None```, will use ```np.arange(P)```. alpha: A scalar in ```[0,1]```. See [1] for details. sth: A scalar. See [1] for details. Returns: ```int [M x N]```. All In Focus (AIF) image. \"\"\" rgb = np . ndim ( im_stack ) == 4 if focus is None : focus = np . arange ( im_stack . shape [ 2 ]) if rgb : imagesR = im_stack [:, :, :, 0 ] . astype ( float ) imagesG = im_stack [:, :, :, 1 ] . astype ( float ) imagesB = im_stack [:, :, :, 2 ] . astype ( float ) fm = get_fmeasure ( im_stack , nhsize ) S , fm = get_smeasure ( fm , nhsize , focus ) fm = get_weights ( S , fm , alpha , sth ) '''Fuse Images''' fmn = np . sum ( fm , 2 ) # normalisation factor if rgb : im = np . zeros_like ( im_stack [:, :, 0 ]) im [:, :, 0 ] = np . sum ( imagesR * fm , 2 ) / fmn im [:, :, 1 ] = np . sum ( imagesG * fm , 2 ) / fmn im [:, :, 2 ] = np . sum ( imagesB * fm , 2 ) / fmn im = np . round ( im ) . astype ( np . uint8 ) else : im = np . round (( np . sum ( im_stack . astype ( float ) * fm , 2 ) / fmn )) . astype ( int ) return im gauss3p ( x , y ) Fast 3-point gaussian interpolation. Parameters: Name Type Description Default x np . ndarray int [P] . Vector with focus of each frame. Typical: np.arange(P) . required y np . ndarray float [M x N x P] . Image to interpolate. required Returns: Type Description np . ndarray u - float [M x N] . Mean value of gaussian function. np . ndarray s_squared - float [M x N] . Variance. np . ndarray A - float [M x N] . Max value of gaussian function. np . ndarray y_max - float [M x N] . Max projection of image y . Source code in coppafish/extract/fstack.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def gauss3p ( x : np . ndarray , y : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Fast 3-point gaussian interpolation. Args: x: ```int [P]```. Vector with focus of each frame. Typical: ```np.arange(P)```. y: ```float [M x N x P]```. Image to interpolate. Returns: - ```u``` - ```float [M x N]```. Mean value of gaussian function. - ```s_squared``` - ```float [M x N]```. Variance. - ```A``` - ```float [M x N]```. Max value of gaussian function. - ```y_max``` - ```float [M x N]```. Max projection of image ```y```. \"\"\" step = 2 # internal parameter M , N , P = np . shape ( y ) y_max = np . max ( y , axis = 2 ) I = np . argmax ( y , axis = 2 ) IN , IM = np . meshgrid ( np . arange ( N ), np . arange ( M )) Ic = I . flatten () # transpose before FLATTEN to give same as MATLAB Ic [ Ic <= step - 1 ] = step Ic [ Ic >= P - 1 - step ] = P - 1 - step index1 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic - step ), ( M , N , P )) index2 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic ), ( M , N , P )) index3 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic + step ), ( M , N , P )) index1 [ I . flatten () <= step - 1 ] = index3 [ I . flatten () <= step - 1 ] index3 [ I . flatten () >= step - 1 ] = index1 [ I . flatten () >= step - 1 ] x1 = x [ Ic . flatten () - step ] . reshape ( M , N ) x2 = x [ Ic . flatten ()] . reshape ( M , N ) x3 = x [ Ic . flatten () + step ] . reshape ( M , N ) y1 = np . log ( y [ np . unravel_index ( index1 , np . shape ( y ))] . reshape ( M , N )) y2 = np . log ( y [ np . unravel_index ( index2 , np . shape ( y ))] . reshape ( M , N )) y3 = np . log ( y [ np . unravel_index ( index3 , np . shape ( y ))] . reshape ( M , N )) c = (( y1 - y2 ) * ( x2 - x3 ) - ( y2 - y3 ) * ( x1 - x2 )) / ( ( x1 ** 2 - x2 ** 2 ) * ( x2 - x3 ) - ( x2 ** 2 - x3 ** 2 ) * ( x1 - x2 )) b = (( y2 - y3 ) - c * ( x2 - x3 ) * ( x2 + x3 + 2 )) / ( x2 - x3 ) # +2 is due to python vs matlab indexing s_squared = - 1 / ( 2 * c ) u = b * s_squared a = y1 - b * ( x1 + 1 ) - c * ( x1 + 1 ) ** 2 # +1 is due to python vs matlab indexing A = np . exp ( a + u ** 2. / ( 2 * s_squared )) return u , s_squared , A , y_max get_fmeasure ( im_stack , nhsize ) Returns focus measure value for each pixel. Parameters: Name Type Description Default im_stack np . ndarray Element [:,:,p,:] is greyscale or RGB image at z-plane p . RGB: uint8 [M x N x P x 3] . Gray: uint16 [M x N x P] . required nhsize int Size of focus measure window. Typical: M/200 . required Returns: Type Description np . ndarray float [M x N x P] . Focus measure image. Source code in coppafish/extract/fstack.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_fmeasure ( im_stack : np . ndarray , nhsize : int ) -> np . ndarray : \"\"\" Returns focus measure value for each pixel. Args: im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```. RGB: ```uint8 [M x N x P x 3]```. Gray: ```uint16 [M x N x P]```. nhsize: Size of focus measure window. Typical: ```M/200```. Returns: ```float [M x N x P]```. Focus measure image. \"\"\" rgb = np . ndim ( im_stack ) == 4 im_shape = np . shape ( im_stack ) fm = np . zeros (( im_shape [: 3 ])) for p in range ( im_shape [ 2 ]): if rgb : im = rgb2gray ( im_stack [:, :, p ]) else : im = im_stack [:, :, p ] fm [:, :, p ] = gfocus ( im2double ( im ), nhsize ) return fm get_smeasure ( fm , nhsize , focus ) Returns selectivity measure value for each pixel. Parameters: Name Type Description Default fm np . ndarray float [M x N x P] . Focus Measure Image. required nhsize int Size of focus measure window. Typical: M/200 . required focus np . ndarray int [P] . Vector with focus of each frame. Typical: np.arange(P) . required Returns: Type Description np . ndarray S - float [M x N] . Selectivity measure image. np . ndarray fm - float [M x N x P] . Normalised focus measure image. Source code in coppafish/extract/fstack.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_smeasure ( fm : np . ndarray , nhsize : int , focus : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Returns selectivity measure value for each pixel. Args: fm: ```float [M x N x P]```. Focus Measure Image. nhsize: Size of focus measure window. Typical: ```M/200```. focus: ```int [P]```. Vector with focus of each frame. Typical: ```np.arange(P)```. Returns: - ```S``` - ```float [M x N]```. Selectivity measure image. - ```fm``` - ```float [M x N x P]```. Normalised focus measure image. \"\"\" M , N , P = np . shape ( fm ) u , s_squared , A , fmax = gauss3p ( focus , fm ) # Aprox. RMS of error signal as sum|Signal-Noise| instead of sqrt(sum(Signal-noise)^2): err = np . zeros (( M , N )) for p in range ( P ): # +1 for self.focus is due to python vs matlab indexing # this bit is twice as slow vectorized for 2048 x 2048 x 51 image err = err + np . abs ( fm [:, :, p ] - A * np . exp ( - ( focus [ p ] + 1 - u ) ** 2 / ( 2 * s_squared ))) fm = fm / np . expand_dims ( fmax , 2 ) h = np . ones (( nhsize , nhsize )) / ( nhsize ** 2 ) inv_psnr = ndimage . correlate ( err / ( P * fmax ), h , mode = 'nearest' ) # inv_psnr = cv2.filter2D(err / (P * fmax), -1, h, borderType=cv2.BORDER_REPLICATE) S = 20 * np . log10 ( 1 / inv_psnr ) S [ np . isnan ( S )] = np . nanmin ( S ) return S , fm get_weights ( S , fm , alpha , sth ) Computes sharpening parameter phi and then returns cut off frequency for high pass convolve_2d, omega . Parameters: Name Type Description Default S np . ndarray float [M x N] . Selectivity measure image. required fm np . ndarray float [M x N x P] . Normalised focus measure image. required alpha float A scalar in [0, 1] . Typical: 0.2 . required sth float A scalar. Typical: 13 . required Returns: Type Description np . ndarray float [M x N] . Cut off frequency for high pass convolve_2d. Source code in coppafish/extract/fstack.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def get_weights ( S : np . ndarray , fm : np . ndarray , alpha : float , sth : float ) -> np . ndarray : \"\"\" Computes sharpening parameter phi and then returns cut off frequency for high pass convolve_2d, ```omega```. Args: S: ```float [M x N]```. Selectivity measure image. fm: ```float [M x N x P]```. Normalised focus measure image. alpha: A scalar in ```[0, 1]```. Typical: ```0.2```. sth: A scalar. Typical: ```13```. Returns: ```float [M x N]```. Cut off frequency for high pass convolve_2d. \"\"\" phi = 0.5 * ( 1 + np . tanh ( alpha * ( S - sth ))) / alpha phi = medfilt2d ( phi , 3 ) omega = 0.5 + 0.5 * np . tanh ( np . expand_dims ( phi , 2 ) * ( fm - 1 )) return omega gfocus ( im , w_size ) Compute focus measure using gray level local variance. Parameters: Name Type Description Default im np . ndarray float [M x N] . Gray scale image. required w_size int Size of convolve_2d window. Typical: M/200 . required Returns: Type Description np . ndarray float [M x N] . Focus measure image. Source code in coppafish/extract/fstack.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def gfocus ( im : np . ndarray , w_size : int ) -> np . ndarray : \"\"\" Compute focus measure using gray level local variance. Args: im: ```float [M x N]```. Gray scale image. w_size: Size of convolve_2d window. Typical: ```M/200```. Returns: ```float [M x N]```. Focus measure image. \"\"\" mean_f = np . ones (( w_size , w_size )) / ( w_size ** 2 ) #u = ndimage.correlate(im, mean_f, mode='nearest') u = cv2 . filter2D ( im , - 1 , mean_f , borderType = cv2 . BORDER_REPLICATE ) fm = ( im - u ) ** 2 fm = cv2 . filter2D ( fm , - 1 , mean_f , borderType = cv2 . BORDER_REPLICATE ) #fm = ndimage.correlate(fm, mean_f, mode='nearest') return fm im2double ( im ) Equivalent to Matlab im2double function. Follows answer here . Parameters: Name Type Description Default im np . ndarray int / uint``````[m x n x ...] . Image tom convert. required Returns: Type Description np . ndarray float [m x n x ...] . Converted image. Source code in coppafish/extract/fstack.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def im2double ( im : np . ndarray ) -> np . ndarray : \"\"\" Equivalent to Matlab im2double function. Follows answer [here](https://stackoverflow.com/questions/29100722/equivalent-im2double-function-in-opencv-python/29104511). Args: im: ```int```/```uint``````[m x n x ...]```. Image tom convert. Returns: ```float [m x n x ...]```. Converted image. \"\"\" info = np . iinfo ( im . dtype ) # Get the data type of the input image return im . astype ( float ) / info . max # Divide all values by the largest possible value in the datatype rgb2gray ( im ) Converts RGB m x n x 3 color image into m x n greyscale image. Uses weighted sum indicated here . Parameters: Name Type Description Default im np . ndarray float [m x n x 3] . RGB image required Returns: Type Description np . ndarray float [m x n] . Greyscale image. Source code in coppafish/extract/fstack.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def rgb2gray ( im : np . ndarray ) -> np . ndarray : \"\"\" Converts RGB ```m x n x 3``` color image into ```m x n``` greyscale image. Uses weighted sum indicated [here](https://www.mathworks.com/help/matlab/ref/rgb2gray.html). Args: im: ```float [m x n x 3]```. RGB image Returns: ```float [m x n]```. Greyscale image. \"\"\" im_R = im [:, :, 0 ] im_G = im [:, :, 1 ] im_B = im [:, :, 2 ] if np . issubdtype ( im . dtype , np . integer ): im_output = np . round (( 0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B )) . astype ( im . dtype ) else : im_output = ( 0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B ) . astype ( im . dtype ) return im_output","title":"Fstack"},{"location":"code/extract/fstack/#coppafish.extract.fstack.focus_stack","text":"Generate extended depth-of-field image from focus sequence using noise-robust selective all-in-focus algorithm [1]. Input images may be grayscale or color. For color images, the algorithm is applied to each color plane independently. For further details, see: [1] Pertuz et. al. \"Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013. S. Pertuz, Jan/2016 Modified by Josh, 2021 Parameters: Name Type Description Default im_stack np . ndarray Element [:,:,p,:] is greyscale or RGB image at z-plane p . RGB: uint8 [M x N x P x 3] Gray: uint16 [M x N x P] required nhsize int Size of default window. 9 focus Optional [ np . ndarray ] int [P] or None . Vector with focus of each frame. If None , will use np.arange(P) . None alpha float A scalar in [0,1] . See [1] for details. 0.2 sth float A scalar. See [1] for details. 13 Returns: Type Description np . ndarray int [M x N] . All In Focus (AIF) image. Source code in coppafish/extract/fstack.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def focus_stack ( im_stack : np . ndarray , nhsize : int = 9 , focus : Optional [ np . ndarray ] = None , alpha : float = 0.2 , sth : float = 13 ) -> np . ndarray : \"\"\" Generate extended depth-of-field image from focus sequence using noise-robust selective all-in-focus algorithm [1]. Input images may be grayscale or color. For color images, the algorithm is applied to each color plane independently. For further details, see: [1] Pertuz et. al. \"Generation of all-in-focus images by noise-robust selective fusion of limited depth-of-field images\" IEEE Trans. Image Process, 22(3):1242 - 1251, 2013. S. Pertuz, Jan/2016 Modified by Josh, 2021 Args: im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```. RGB: ```uint8 [M x N x P x 3]``` Gray: ```uint16 [M x N x P]``` nhsize: Size of default window. focus: ```int [P]``` or ```None```. Vector with focus of each frame. If ```None```, will use ```np.arange(P)```. alpha: A scalar in ```[0,1]```. See [1] for details. sth: A scalar. See [1] for details. Returns: ```int [M x N]```. All In Focus (AIF) image. \"\"\" rgb = np . ndim ( im_stack ) == 4 if focus is None : focus = np . arange ( im_stack . shape [ 2 ]) if rgb : imagesR = im_stack [:, :, :, 0 ] . astype ( float ) imagesG = im_stack [:, :, :, 1 ] . astype ( float ) imagesB = im_stack [:, :, :, 2 ] . astype ( float ) fm = get_fmeasure ( im_stack , nhsize ) S , fm = get_smeasure ( fm , nhsize , focus ) fm = get_weights ( S , fm , alpha , sth ) '''Fuse Images''' fmn = np . sum ( fm , 2 ) # normalisation factor if rgb : im = np . zeros_like ( im_stack [:, :, 0 ]) im [:, :, 0 ] = np . sum ( imagesR * fm , 2 ) / fmn im [:, :, 1 ] = np . sum ( imagesG * fm , 2 ) / fmn im [:, :, 2 ] = np . sum ( imagesB * fm , 2 ) / fmn im = np . round ( im ) . astype ( np . uint8 ) else : im = np . round (( np . sum ( im_stack . astype ( float ) * fm , 2 ) / fmn )) . astype ( int ) return im","title":"focus_stack()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.gauss3p","text":"Fast 3-point gaussian interpolation. Parameters: Name Type Description Default x np . ndarray int [P] . Vector with focus of each frame. Typical: np.arange(P) . required y np . ndarray float [M x N x P] . Image to interpolate. required Returns: Type Description np . ndarray u - float [M x N] . Mean value of gaussian function. np . ndarray s_squared - float [M x N] . Variance. np . ndarray A - float [M x N] . Max value of gaussian function. np . ndarray y_max - float [M x N] . Max projection of image y . Source code in coppafish/extract/fstack.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def gauss3p ( x : np . ndarray , y : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Fast 3-point gaussian interpolation. Args: x: ```int [P]```. Vector with focus of each frame. Typical: ```np.arange(P)```. y: ```float [M x N x P]```. Image to interpolate. Returns: - ```u``` - ```float [M x N]```. Mean value of gaussian function. - ```s_squared``` - ```float [M x N]```. Variance. - ```A``` - ```float [M x N]```. Max value of gaussian function. - ```y_max``` - ```float [M x N]```. Max projection of image ```y```. \"\"\" step = 2 # internal parameter M , N , P = np . shape ( y ) y_max = np . max ( y , axis = 2 ) I = np . argmax ( y , axis = 2 ) IN , IM = np . meshgrid ( np . arange ( N ), np . arange ( M )) Ic = I . flatten () # transpose before FLATTEN to give same as MATLAB Ic [ Ic <= step - 1 ] = step Ic [ Ic >= P - 1 - step ] = P - 1 - step index1 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic - step ), ( M , N , P )) index2 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic ), ( M , N , P )) index3 = np . ravel_multi_index (( IM . flatten (), IN . flatten (), Ic + step ), ( M , N , P )) index1 [ I . flatten () <= step - 1 ] = index3 [ I . flatten () <= step - 1 ] index3 [ I . flatten () >= step - 1 ] = index1 [ I . flatten () >= step - 1 ] x1 = x [ Ic . flatten () - step ] . reshape ( M , N ) x2 = x [ Ic . flatten ()] . reshape ( M , N ) x3 = x [ Ic . flatten () + step ] . reshape ( M , N ) y1 = np . log ( y [ np . unravel_index ( index1 , np . shape ( y ))] . reshape ( M , N )) y2 = np . log ( y [ np . unravel_index ( index2 , np . shape ( y ))] . reshape ( M , N )) y3 = np . log ( y [ np . unravel_index ( index3 , np . shape ( y ))] . reshape ( M , N )) c = (( y1 - y2 ) * ( x2 - x3 ) - ( y2 - y3 ) * ( x1 - x2 )) / ( ( x1 ** 2 - x2 ** 2 ) * ( x2 - x3 ) - ( x2 ** 2 - x3 ** 2 ) * ( x1 - x2 )) b = (( y2 - y3 ) - c * ( x2 - x3 ) * ( x2 + x3 + 2 )) / ( x2 - x3 ) # +2 is due to python vs matlab indexing s_squared = - 1 / ( 2 * c ) u = b * s_squared a = y1 - b * ( x1 + 1 ) - c * ( x1 + 1 ) ** 2 # +1 is due to python vs matlab indexing A = np . exp ( a + u ** 2. / ( 2 * s_squared )) return u , s_squared , A , y_max","title":"gauss3p()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_fmeasure","text":"Returns focus measure value for each pixel. Parameters: Name Type Description Default im_stack np . ndarray Element [:,:,p,:] is greyscale or RGB image at z-plane p . RGB: uint8 [M x N x P x 3] . Gray: uint16 [M x N x P] . required nhsize int Size of focus measure window. Typical: M/200 . required Returns: Type Description np . ndarray float [M x N x P] . Focus measure image. Source code in coppafish/extract/fstack.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def get_fmeasure ( im_stack : np . ndarray , nhsize : int ) -> np . ndarray : \"\"\" Returns focus measure value for each pixel. Args: im_stack: Element ```[:,:,p,:]``` is greyscale or RGB image at z-plane ```p```. RGB: ```uint8 [M x N x P x 3]```. Gray: ```uint16 [M x N x P]```. nhsize: Size of focus measure window. Typical: ```M/200```. Returns: ```float [M x N x P]```. Focus measure image. \"\"\" rgb = np . ndim ( im_stack ) == 4 im_shape = np . shape ( im_stack ) fm = np . zeros (( im_shape [: 3 ])) for p in range ( im_shape [ 2 ]): if rgb : im = rgb2gray ( im_stack [:, :, p ]) else : im = im_stack [:, :, p ] fm [:, :, p ] = gfocus ( im2double ( im ), nhsize ) return fm","title":"get_fmeasure()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_smeasure","text":"Returns selectivity measure value for each pixel. Parameters: Name Type Description Default fm np . ndarray float [M x N x P] . Focus Measure Image. required nhsize int Size of focus measure window. Typical: M/200 . required focus np . ndarray int [P] . Vector with focus of each frame. Typical: np.arange(P) . required Returns: Type Description np . ndarray S - float [M x N] . Selectivity measure image. np . ndarray fm - float [M x N x P] . Normalised focus measure image. Source code in coppafish/extract/fstack.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def get_smeasure ( fm : np . ndarray , nhsize : int , focus : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Returns selectivity measure value for each pixel. Args: fm: ```float [M x N x P]```. Focus Measure Image. nhsize: Size of focus measure window. Typical: ```M/200```. focus: ```int [P]```. Vector with focus of each frame. Typical: ```np.arange(P)```. Returns: - ```S``` - ```float [M x N]```. Selectivity measure image. - ```fm``` - ```float [M x N x P]```. Normalised focus measure image. \"\"\" M , N , P = np . shape ( fm ) u , s_squared , A , fmax = gauss3p ( focus , fm ) # Aprox. RMS of error signal as sum|Signal-Noise| instead of sqrt(sum(Signal-noise)^2): err = np . zeros (( M , N )) for p in range ( P ): # +1 for self.focus is due to python vs matlab indexing # this bit is twice as slow vectorized for 2048 x 2048 x 51 image err = err + np . abs ( fm [:, :, p ] - A * np . exp ( - ( focus [ p ] + 1 - u ) ** 2 / ( 2 * s_squared ))) fm = fm / np . expand_dims ( fmax , 2 ) h = np . ones (( nhsize , nhsize )) / ( nhsize ** 2 ) inv_psnr = ndimage . correlate ( err / ( P * fmax ), h , mode = 'nearest' ) # inv_psnr = cv2.filter2D(err / (P * fmax), -1, h, borderType=cv2.BORDER_REPLICATE) S = 20 * np . log10 ( 1 / inv_psnr ) S [ np . isnan ( S )] = np . nanmin ( S ) return S , fm","title":"get_smeasure()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.get_weights","text":"Computes sharpening parameter phi and then returns cut off frequency for high pass convolve_2d, omega . Parameters: Name Type Description Default S np . ndarray float [M x N] . Selectivity measure image. required fm np . ndarray float [M x N x P] . Normalised focus measure image. required alpha float A scalar in [0, 1] . Typical: 0.2 . required sth float A scalar. Typical: 13 . required Returns: Type Description np . ndarray float [M x N] . Cut off frequency for high pass convolve_2d. Source code in coppafish/extract/fstack.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def get_weights ( S : np . ndarray , fm : np . ndarray , alpha : float , sth : float ) -> np . ndarray : \"\"\" Computes sharpening parameter phi and then returns cut off frequency for high pass convolve_2d, ```omega```. Args: S: ```float [M x N]```. Selectivity measure image. fm: ```float [M x N x P]```. Normalised focus measure image. alpha: A scalar in ```[0, 1]```. Typical: ```0.2```. sth: A scalar. Typical: ```13```. Returns: ```float [M x N]```. Cut off frequency for high pass convolve_2d. \"\"\" phi = 0.5 * ( 1 + np . tanh ( alpha * ( S - sth ))) / alpha phi = medfilt2d ( phi , 3 ) omega = 0.5 + 0.5 * np . tanh ( np . expand_dims ( phi , 2 ) * ( fm - 1 )) return omega","title":"get_weights()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.gfocus","text":"Compute focus measure using gray level local variance. Parameters: Name Type Description Default im np . ndarray float [M x N] . Gray scale image. required w_size int Size of convolve_2d window. Typical: M/200 . required Returns: Type Description np . ndarray float [M x N] . Focus measure image. Source code in coppafish/extract/fstack.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def gfocus ( im : np . ndarray , w_size : int ) -> np . ndarray : \"\"\" Compute focus measure using gray level local variance. Args: im: ```float [M x N]```. Gray scale image. w_size: Size of convolve_2d window. Typical: ```M/200```. Returns: ```float [M x N]```. Focus measure image. \"\"\" mean_f = np . ones (( w_size , w_size )) / ( w_size ** 2 ) #u = ndimage.correlate(im, mean_f, mode='nearest') u = cv2 . filter2D ( im , - 1 , mean_f , borderType = cv2 . BORDER_REPLICATE ) fm = ( im - u ) ** 2 fm = cv2 . filter2D ( fm , - 1 , mean_f , borderType = cv2 . BORDER_REPLICATE ) #fm = ndimage.correlate(fm, mean_f, mode='nearest') return fm","title":"gfocus()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.im2double","text":"Equivalent to Matlab im2double function. Follows answer here . Parameters: Name Type Description Default im np . ndarray int / uint``````[m x n x ...] . Image tom convert. required Returns: Type Description np . ndarray float [m x n x ...] . Converted image. Source code in coppafish/extract/fstack.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def im2double ( im : np . ndarray ) -> np . ndarray : \"\"\" Equivalent to Matlab im2double function. Follows answer [here](https://stackoverflow.com/questions/29100722/equivalent-im2double-function-in-opencv-python/29104511). Args: im: ```int```/```uint``````[m x n x ...]```. Image tom convert. Returns: ```float [m x n x ...]```. Converted image. \"\"\" info = np . iinfo ( im . dtype ) # Get the data type of the input image return im . astype ( float ) / info . max # Divide all values by the largest possible value in the datatype","title":"im2double()"},{"location":"code/extract/fstack/#coppafish.extract.fstack.rgb2gray","text":"Converts RGB m x n x 3 color image into m x n greyscale image. Uses weighted sum indicated here . Parameters: Name Type Description Default im np . ndarray float [m x n x 3] . RGB image required Returns: Type Description np . ndarray float [m x n] . Greyscale image. Source code in coppafish/extract/fstack.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def rgb2gray ( im : np . ndarray ) -> np . ndarray : \"\"\" Converts RGB ```m x n x 3``` color image into ```m x n``` greyscale image. Uses weighted sum indicated [here](https://www.mathworks.com/help/matlab/ref/rgb2gray.html). Args: im: ```float [m x n x 3]```. RGB image Returns: ```float [m x n]```. Greyscale image. \"\"\" im_R = im [:, :, 0 ] im_G = im [:, :, 1 ] im_B = im [:, :, 2 ] if np . issubdtype ( im . dtype , np . integer ): im_output = np . round (( 0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B )) . astype ( im . dtype ) else : im_output = ( 0.2989 * im_R + 0.5870 * im_G + 0.1140 * im_B ) . astype ( im . dtype ) return im_output","title":"rgb2gray()"},{"location":"code/extract/scale/","text":"central_tile ( tilepos_yx , use_tiles ) returns tile in use_tiles closest to centre. Parameters: Name Type Description Default tilepos_yx np . ndarray int [n_tiles x 2] . tiff tile positions (index 0 refers to [0,0] ). required use_tiles List [ int ] int [n_use_tiles] . Tiles used in the experiment. required Returns: Type Description int tile in use_tiles closest to centre. Source code in coppafish/extract/scale.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def central_tile ( tilepos_yx : np . ndarray , use_tiles : List [ int ]) -> int : \"\"\" returns tile in use_tiles closest to centre. Args: tilepos_yx: ```int [n_tiles x 2]```. tiff tile positions (index ```0``` refers to ```[0,0]```). use_tiles: ```int [n_use_tiles]```. Tiles used in the experiment. Returns: tile in ```use_tiles``` closest to centre. \"\"\" mean_yx = np . round ( np . mean ( tilepos_yx , 0 )) nearest_t = np . linalg . norm ( tilepos_yx [ use_tiles ] - mean_yx , axis = 1 ) . argmin () return int ( use_tiles [ nearest_t ]) get_scale ( nbp_file , nbp_basic , r , use_tiles , use_channels , use_z , scale_norm , filter_kernel , smooth_kernel = None ) Convolves the image for tile t , channel c , z-plane z with filter_kernel then gets the multiplier to apply to filtered nd2 images by dividing scale_norm by the max value of this filtered image. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required r int Round to get scale from. Should be 0 to determine scale and the anchor round (last round) to determine scale_anchor . required use_tiles List [ int ] int [n_use_tiles] . tiff tile indices to consider when finding tile. required use_channels List [ int ] int [n_use_channels] . Channels to consider when finding channel. required use_z List [ int ] int [n_z] . Z-planes to consider when finding z_plane. required scale_norm int Desired maximum pixel value of npy images. Typical: 40000 . required filter_kernel np . ndarray float [ny_kernel x nx_kernel] . Kernel to convolve nd2 data with to produce npy tiles. Typical shape: [13 x 13] . required smooth_kernel Optional [ np . ndarray ] float [ny_smooth x nx_smooth] . 2D kernel to smooth filtered image with npy with. Typical shape: [3 x 3] . If None, no smoothing is applied None Returns: Type Description int t - int . npy tile index (index 0 refers to tilepos_yx_npy=[MaxY, MaxX] ) scale found from. int c - int . Channel scale found from. int z - int . Z-plane scale found from. float scale - float . Multiplier to apply to filtered nd2 images before saving as npy so full npy uint16 range occupied. Source code in coppafish/extract/scale.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_scale ( nbp_file : NotebookPage , nbp_basic : NotebookPage , r : int , use_tiles : List [ int ], use_channels : List [ int ], use_z : List [ int ], scale_norm : int , filter_kernel : np . ndarray , smooth_kernel : Optional [ np . ndarray ] = None ) -> Tuple [ int , int , int , float ]: \"\"\" Convolves the image for tile ```t```, channel ```c```, z-plane ```z``` with ```filter_kernel``` then gets the multiplier to apply to filtered nd2 images by dividing ```scale_norm``` by the max value of this filtered image. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page r: Round to get `scale` from. Should be 0 to determine `scale` and the anchor round (last round) to determine `scale_anchor`. use_tiles: ```int [n_use_tiles]```. tiff tile indices to consider when finding tile. use_channels: ```int [n_use_channels]```. Channels to consider when finding channel. use_z: ```int [n_z]```. Z-planes to consider when finding z_plane. scale_norm: Desired maximum pixel value of npy images. Typical: ```40000```. filter_kernel: ```float [ny_kernel x nx_kernel]```. Kernel to convolve nd2 data with to produce npy tiles. Typical shape: ```[13 x 13]```. smooth_kernel: ```float [ny_smooth x nx_smooth]```. 2D kernel to smooth filtered image with npy with. Typical shape: ```[3 x 3]```. If None, no smoothing is applied Returns: - ```t``` - ```int```. npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) scale found from. - ```c``` - ```int```. Channel scale found from. - ```z``` - ```int```. Z-plane scale found from. - ```scale``` - ```float```. Multiplier to apply to filtered nd2 images before saving as npy so full npy ```uint16``` range occupied. \"\"\" # tile to get scale from is central tile t = central_tile ( nbp_basic . tilepos_yx , use_tiles ) # find z-plane with max pixel across all channels of tile t c , z , image = get_z_plane ( nbp_file , nbp_basic , r , t , use_channels , use_z ) # convolve_2d image in same way we convolve_2d before saving tiff files im_filtered = utils . morphology . convolve_2d ( image , filter_kernel ) if smooth_kernel is not None : im_filtered = utils . morphology . imfilter ( im_filtered , smooth_kernel , oa = False ) scale = scale_norm / im_filtered . max () return t , c , z , float ( scale ) get_scale_from_txt ( txt_file , scale , scale_anchor , tol = 0.001 ) This checks whether scale and scale_anchor values used for producing npy files in tile_dir match values used and saved to txt_file on previous run. Will raise error if they are different. Parameters: Name Type Description Default txt_file str nb.file_names.scale , path to text file where scale values are saved. File contains two values, scale first and scale_anchor second. Values will be 0 if not used or not yet computed. required scale Optional [ float ] Value of scale used for current run of extract method i.e. config['extract']['scale'] . required scale_anchor Optional [ float ] Value of scale_anchor used for current run of extract method i.e. config['extract']['scale_anchor'] . required tol float Two scale values will be considered the same if they are closer than this. 0.001 Returns: Type Description float scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value. float scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value. Source code in coppafish/extract/scale.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_scale_from_txt ( txt_file : str , scale : Optional [ float ], scale_anchor : Optional [ float ], tol : float = 0.001 ) -> Tuple [ float , float ]: \"\"\" This checks whether `scale` and `scale_anchor` values used for producing npy files in *tile_dir* match values used and saved to `txt_file` on previous run. Will raise error if they are different. Args: txt_file: `nb.file_names.scale`, path to text file where scale values are saved. File contains two values, `scale` first and `scale_anchor` second. Values will be 0 if not used or not yet computed. scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']`. scale_anchor: Value of `scale_anchor` used for current run of extract method i.e. `config['extract']['scale_anchor']`. tol: Two scale values will be considered the same if they are closer than this. Returns: scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value. scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value. \"\"\" if os . path . isfile ( txt_file ): scale_saved , scale_anchor_saved = np . genfromtxt ( txt_file ) if np . abs ( scale_saved ) < tol : pass # 0 means scale not used so do nothing elif scale is None : warnings . warn ( \"Using value of scale = {:.2f} saved in \\n \" . format ( scale_saved ) + txt_file ) scale = float ( scale_saved ) # Set to saved value used up till now if not specified elif np . abs ( scale - scale_saved ) > tol : raise ValueError ( f \" \\n Imaging round (Not anchor) tiles saved so far were calculated with scale = \" f \" { scale_saved } \\n as saved in { txt_file } \\n \" f \"This is different from config['extract']['scale'] = { scale } .\" ) if np . abs ( scale_anchor_saved ) < tol : pass # 0 means scale_anchor not computed yet so do nothing elif scale_anchor is None : warnings . warn ( \"Using value of scale_anchor = {:.2f} saved in \\n \" . format ( scale_anchor_saved ) + txt_file ) scale_anchor = float ( scale_anchor_saved ) # Set to saved value used up till now if not specified elif np . abs ( scale_anchor - scale_anchor_saved ) > tol : raise ValueError ( f \" \\n Anchor round tiles saved so far were calculated with scale_anchor = \" f \" { scale_anchor_saved } \\n as saved in { txt_file } \\n \" f \"This is different from config['extract']['scale_anchor'] = { scale_anchor } .\" ) return scale , scale_anchor get_z_plane ( nbp_file , nbp_basic , r , t , use_channels , use_z ) Finds z plane and channel that has maximum pixel value for given round and tile. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required r int Round to consider. required t int npy tile index (index 0 refers to tilepos_yx_npy=[MaxY, MaxX] ) to find z-plane from. required use_channels List [ int ] int [n_use_channels] . Channels to consider. required use_z List [ int ] int [n_z] . Z-planes to consider. required Returns: Type Description int max_channel - int . Channel to which image with max pixel value corresponds. int max_z - int . Z-plane to which image with max pixel value corresponds. np . ndarray image - int [tile_sz x tile_sz] . Corresponding image. Source code in coppafish/extract/scale.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def get_z_plane ( nbp_file : NotebookPage , nbp_basic : NotebookPage , r : int , t : int , use_channels : List [ int ], use_z : List [ int ]) -> Tuple [ int , int , np . ndarray ]: \"\"\" Finds z plane and channel that has maximum pixel value for given round and tile. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page r: Round to consider. t: npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) to find z-plane from. use_channels: ```int [n_use_channels]```. Channels to consider. use_z: ```int [n_z]```. Z-planes to consider. Returns: - ```max_channel``` - ```int```. Channel to which image with max pixel value corresponds. - ```max_z``` - ```int```. Z-plane to which image with max pixel value corresponds. - ```image``` - ```int [tile_sz x tile_sz]```. Corresponding image. \"\"\" round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) image_max = np . zeros (( len ( use_channels ), len ( use_z ))) for i in range ( len ( use_channels )): image_max [ i , :] = np . max ( np . max ( utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , use_channels [ i ], use_z ), axis = 0 ), axis = 0 ) max_channel = use_channels [ np . max ( image_max , axis = 1 ) . argmax ()] max_z = use_z [ np . max ( image_max , axis = 0 ) . argmax ()] return max_channel , max_z , utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , max_channel , max_z ) save_scale ( txt_file , scale , scale_anchor ) This saves scale and scale_anchor to txt_file . If either scale and scale_anchor are None , they will be set to 0 when saving. Parameters: Name Type Description Default txt_file str nb.file_names.scale , path to text file where scale values are to be saved. File will contain two values, scale first and scale_anchor second. Values will be 0 if not used or not yet computed. required scale Optional [ float ] Value of scale used for current run of extract method i.e. config['extract']['scale'] or value computed from get_scale . required scale_anchor Optional [ float ] Value of scale_anchor used for current run of extract method i.e. config['extract']['scale_anchor'] or value computed from get_scale . required Source code in coppafish/extract/scale.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def save_scale ( txt_file : str , scale : Optional [ float ], scale_anchor : Optional [ float ]): \"\"\" This saves `scale` and `scale_anchor` to `txt_file`. If either `scale` and `scale_anchor` are `None`, they will be set to 0 when saving. Args: txt_file: `nb.file_names.scale`, path to text file where scale values are to be saved. File will contain two values, `scale` first and `scale_anchor` second. Values will be 0 if not used or not yet computed. scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']` or value computed from `get_scale`. scale_anchor: Value of `scale_anchor` used for current run of extract method i.e. `config['extract']['scale_anchor']` or value computed from `get_scale`. \"\"\" scale , scale_anchor = get_scale_from_txt ( txt_file , scale , scale_anchor ) # check if match current saved values if scale is None : scale = 0 if scale_anchor is None : scale_anchor = 0 np . savetxt ( txt_file , [ scale , scale_anchor ], header = 'scale followed by scale_anchor' )","title":"Scale"},{"location":"code/extract/scale/#coppafish.extract.scale.central_tile","text":"returns tile in use_tiles closest to centre. Parameters: Name Type Description Default tilepos_yx np . ndarray int [n_tiles x 2] . tiff tile positions (index 0 refers to [0,0] ). required use_tiles List [ int ] int [n_use_tiles] . Tiles used in the experiment. required Returns: Type Description int tile in use_tiles closest to centre. Source code in coppafish/extract/scale.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def central_tile ( tilepos_yx : np . ndarray , use_tiles : List [ int ]) -> int : \"\"\" returns tile in use_tiles closest to centre. Args: tilepos_yx: ```int [n_tiles x 2]```. tiff tile positions (index ```0``` refers to ```[0,0]```). use_tiles: ```int [n_use_tiles]```. Tiles used in the experiment. Returns: tile in ```use_tiles``` closest to centre. \"\"\" mean_yx = np . round ( np . mean ( tilepos_yx , 0 )) nearest_t = np . linalg . norm ( tilepos_yx [ use_tiles ] - mean_yx , axis = 1 ) . argmin () return int ( use_tiles [ nearest_t ])","title":"central_tile()"},{"location":"code/extract/scale/#coppafish.extract.scale.get_scale","text":"Convolves the image for tile t , channel c , z-plane z with filter_kernel then gets the multiplier to apply to filtered nd2 images by dividing scale_norm by the max value of this filtered image. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required r int Round to get scale from. Should be 0 to determine scale and the anchor round (last round) to determine scale_anchor . required use_tiles List [ int ] int [n_use_tiles] . tiff tile indices to consider when finding tile. required use_channels List [ int ] int [n_use_channels] . Channels to consider when finding channel. required use_z List [ int ] int [n_z] . Z-planes to consider when finding z_plane. required scale_norm int Desired maximum pixel value of npy images. Typical: 40000 . required filter_kernel np . ndarray float [ny_kernel x nx_kernel] . Kernel to convolve nd2 data with to produce npy tiles. Typical shape: [13 x 13] . required smooth_kernel Optional [ np . ndarray ] float [ny_smooth x nx_smooth] . 2D kernel to smooth filtered image with npy with. Typical shape: [3 x 3] . If None, no smoothing is applied None Returns: Type Description int t - int . npy tile index (index 0 refers to tilepos_yx_npy=[MaxY, MaxX] ) scale found from. int c - int . Channel scale found from. int z - int . Z-plane scale found from. float scale - float . Multiplier to apply to filtered nd2 images before saving as npy so full npy uint16 range occupied. Source code in coppafish/extract/scale.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_scale ( nbp_file : NotebookPage , nbp_basic : NotebookPage , r : int , use_tiles : List [ int ], use_channels : List [ int ], use_z : List [ int ], scale_norm : int , filter_kernel : np . ndarray , smooth_kernel : Optional [ np . ndarray ] = None ) -> Tuple [ int , int , int , float ]: \"\"\" Convolves the image for tile ```t```, channel ```c```, z-plane ```z``` with ```filter_kernel``` then gets the multiplier to apply to filtered nd2 images by dividing ```scale_norm``` by the max value of this filtered image. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page r: Round to get `scale` from. Should be 0 to determine `scale` and the anchor round (last round) to determine `scale_anchor`. use_tiles: ```int [n_use_tiles]```. tiff tile indices to consider when finding tile. use_channels: ```int [n_use_channels]```. Channels to consider when finding channel. use_z: ```int [n_z]```. Z-planes to consider when finding z_plane. scale_norm: Desired maximum pixel value of npy images. Typical: ```40000```. filter_kernel: ```float [ny_kernel x nx_kernel]```. Kernel to convolve nd2 data with to produce npy tiles. Typical shape: ```[13 x 13]```. smooth_kernel: ```float [ny_smooth x nx_smooth]```. 2D kernel to smooth filtered image with npy with. Typical shape: ```[3 x 3]```. If None, no smoothing is applied Returns: - ```t``` - ```int```. npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) scale found from. - ```c``` - ```int```. Channel scale found from. - ```z``` - ```int```. Z-plane scale found from. - ```scale``` - ```float```. Multiplier to apply to filtered nd2 images before saving as npy so full npy ```uint16``` range occupied. \"\"\" # tile to get scale from is central tile t = central_tile ( nbp_basic . tilepos_yx , use_tiles ) # find z-plane with max pixel across all channels of tile t c , z , image = get_z_plane ( nbp_file , nbp_basic , r , t , use_channels , use_z ) # convolve_2d image in same way we convolve_2d before saving tiff files im_filtered = utils . morphology . convolve_2d ( image , filter_kernel ) if smooth_kernel is not None : im_filtered = utils . morphology . imfilter ( im_filtered , smooth_kernel , oa = False ) scale = scale_norm / im_filtered . max () return t , c , z , float ( scale )","title":"get_scale()"},{"location":"code/extract/scale/#coppafish.extract.scale.get_scale_from_txt","text":"This checks whether scale and scale_anchor values used for producing npy files in tile_dir match values used and saved to txt_file on previous run. Will raise error if they are different. Parameters: Name Type Description Default txt_file str nb.file_names.scale , path to text file where scale values are saved. File contains two values, scale first and scale_anchor second. Values will be 0 if not used or not yet computed. required scale Optional [ float ] Value of scale used for current run of extract method i.e. config['extract']['scale'] . required scale_anchor Optional [ float ] Value of scale_anchor used for current run of extract method i.e. config['extract']['scale_anchor'] . required tol float Two scale values will be considered the same if they are closer than this. 0.001 Returns: Type Description float scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value. float scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value. Source code in coppafish/extract/scale.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_scale_from_txt ( txt_file : str , scale : Optional [ float ], scale_anchor : Optional [ float ], tol : float = 0.001 ) -> Tuple [ float , float ]: \"\"\" This checks whether `scale` and `scale_anchor` values used for producing npy files in *tile_dir* match values used and saved to `txt_file` on previous run. Will raise error if they are different. Args: txt_file: `nb.file_names.scale`, path to text file where scale values are saved. File contains two values, `scale` first and `scale_anchor` second. Values will be 0 if not used or not yet computed. scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']`. scale_anchor: Value of `scale_anchor` used for current run of extract method i.e. `config['extract']['scale_anchor']`. tol: Two scale values will be considered the same if they are closer than this. Returns: scale - If txt_file exists, this will be the value saved in it otherwise will just be the input value. scale_anchor - If txt_file exists, this will be the value saved in it otherwise will just be the input value. \"\"\" if os . path . isfile ( txt_file ): scale_saved , scale_anchor_saved = np . genfromtxt ( txt_file ) if np . abs ( scale_saved ) < tol : pass # 0 means scale not used so do nothing elif scale is None : warnings . warn ( \"Using value of scale = {:.2f} saved in \\n \" . format ( scale_saved ) + txt_file ) scale = float ( scale_saved ) # Set to saved value used up till now if not specified elif np . abs ( scale - scale_saved ) > tol : raise ValueError ( f \" \\n Imaging round (Not anchor) tiles saved so far were calculated with scale = \" f \" { scale_saved } \\n as saved in { txt_file } \\n \" f \"This is different from config['extract']['scale'] = { scale } .\" ) if np . abs ( scale_anchor_saved ) < tol : pass # 0 means scale_anchor not computed yet so do nothing elif scale_anchor is None : warnings . warn ( \"Using value of scale_anchor = {:.2f} saved in \\n \" . format ( scale_anchor_saved ) + txt_file ) scale_anchor = float ( scale_anchor_saved ) # Set to saved value used up till now if not specified elif np . abs ( scale_anchor - scale_anchor_saved ) > tol : raise ValueError ( f \" \\n Anchor round tiles saved so far were calculated with scale_anchor = \" f \" { scale_anchor_saved } \\n as saved in { txt_file } \\n \" f \"This is different from config['extract']['scale_anchor'] = { scale_anchor } .\" ) return scale , scale_anchor","title":"get_scale_from_txt()"},{"location":"code/extract/scale/#coppafish.extract.scale.get_z_plane","text":"Finds z plane and channel that has maximum pixel value for given round and tile. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required r int Round to consider. required t int npy tile index (index 0 refers to tilepos_yx_npy=[MaxY, MaxX] ) to find z-plane from. required use_channels List [ int ] int [n_use_channels] . Channels to consider. required use_z List [ int ] int [n_z] . Z-planes to consider. required Returns: Type Description int max_channel - int . Channel to which image with max pixel value corresponds. int max_z - int . Z-plane to which image with max pixel value corresponds. np . ndarray image - int [tile_sz x tile_sz] . Corresponding image. Source code in coppafish/extract/scale.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def get_z_plane ( nbp_file : NotebookPage , nbp_basic : NotebookPage , r : int , t : int , use_channels : List [ int ], use_z : List [ int ]) -> Tuple [ int , int , np . ndarray ]: \"\"\" Finds z plane and channel that has maximum pixel value for given round and tile. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page r: Round to consider. t: npy tile index (index ```0``` refers to ```tilepos_yx_npy=[MaxY, MaxX]```) to find z-plane from. use_channels: ```int [n_use_channels]```. Channels to consider. use_z: ```int [n_z]```. Z-planes to consider. Returns: - ```max_channel``` - ```int```. Channel to which image with max pixel value corresponds. - ```max_z``` - ```int```. Z-plane to which image with max pixel value corresponds. - ```image``` - ```int [tile_sz x tile_sz]```. Corresponding image. \"\"\" round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) image_max = np . zeros (( len ( use_channels ), len ( use_z ))) for i in range ( len ( use_channels )): image_max [ i , :] = np . max ( np . max ( utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , use_channels [ i ], use_z ), axis = 0 ), axis = 0 ) max_channel = use_channels [ np . max ( image_max , axis = 1 ) . argmax ()] max_z = use_z [ np . max ( image_max , axis = 0 ) . argmax ()] return max_channel , max_z , utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , max_channel , max_z )","title":"get_z_plane()"},{"location":"code/extract/scale/#coppafish.extract.scale.save_scale","text":"This saves scale and scale_anchor to txt_file . If either scale and scale_anchor are None , they will be set to 0 when saving. Parameters: Name Type Description Default txt_file str nb.file_names.scale , path to text file where scale values are to be saved. File will contain two values, scale first and scale_anchor second. Values will be 0 if not used or not yet computed. required scale Optional [ float ] Value of scale used for current run of extract method i.e. config['extract']['scale'] or value computed from get_scale . required scale_anchor Optional [ float ] Value of scale_anchor used for current run of extract method i.e. config['extract']['scale_anchor'] or value computed from get_scale . required Source code in coppafish/extract/scale.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def save_scale ( txt_file : str , scale : Optional [ float ], scale_anchor : Optional [ float ]): \"\"\" This saves `scale` and `scale_anchor` to `txt_file`. If either `scale` and `scale_anchor` are `None`, they will be set to 0 when saving. Args: txt_file: `nb.file_names.scale`, path to text file where scale values are to be saved. File will contain two values, `scale` first and `scale_anchor` second. Values will be 0 if not used or not yet computed. scale: Value of `scale` used for current run of extract method i.e. `config['extract']['scale']` or value computed from `get_scale`. scale_anchor: Value of `scale_anchor` used for current run of extract method i.e. `config['extract']['scale_anchor']` or value computed from `get_scale`. \"\"\" scale , scale_anchor = get_scale_from_txt ( txt_file , scale , scale_anchor ) # check if match current saved values if scale is None : scale = 0 if scale_anchor is None : scale_anchor = 0 np . savetxt ( txt_file , [ scale , scale_anchor ], header = 'scale followed by scale_anchor' )","title":"save_scale()"},{"location":"code/find_spots/base/","text":"check_neighbour_intensity ( image , spot_yxz , thresh = 0 ) Checks whether a neighbouring pixel to those indicated in spot_yxz has intensity less than thresh . The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. If axis 1 dimension is more than image.ndim , only first image.ndim dimensions used i.e. if supply yxz, with 2d image, only yx position used. required thresh float Spots are indicated as False if intensity at neighbour to spot location is less than this. 0 Returns: Type Description np . ndarray float [n_peaks] . True if no neighbours below thresh. Source code in coppafish/find_spots/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def check_neighbour_intensity ( image : np . ndarray , spot_yxz : np . ndarray , thresh : float = 0 ) -> np . ndarray : \"\"\" Checks whether a neighbouring pixel to those indicated in ```spot_yxz``` has intensity less than ```thresh```. The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot. Args: image: ```float [n_y x n_x x n_z]```. image spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used i.e. if supply yxz, with 2d image, only yx position used. thresh: Spots are indicated as ```False``` if intensity at neighbour to spot location is less than this. Returns: ```float [n_peaks]```. ```True``` if no neighbours below thresh. \"\"\" if image . ndim == 3 : transforms = [[ 1 , 0 , 0 ], [ 0 , 1 , 0 ], [ - 1 , 0 , 0 ], [ 0 , - 1 , 0 ], [ 0 , 0 , 1 ], [ 0 , 0 , - 1 ]] elif image . ndim == 2 : transforms = [[ 1 , 0 ], [ 0 , 1 ], [ - 1 , 0 ], [ 0 , - 1 ]] else : raise ValueError ( f \"image has to have two or three dimensions but given image has { image . ndim } dimensions.\" ) keep = np . zeros (( spot_yxz . shape [ 0 ], len ( transforms )), dtype = bool ) for i , t in enumerate ( transforms ): mod_spot_yx = spot_yxz + t for j in range ( image . ndim ): mod_spot_yx [:, j ] = np . clip ( mod_spot_yx [:, j ], 0 , image . shape [ j ] - 1 ) keep [:, i ] = image [ tuple ([ mod_spot_yx [:, j ] for j in range ( image . ndim )])] > thresh return keep . min ( axis = 1 ) get_isolated ( image , spot_yxz , thresh , radius_inner , radius_xy , radius_z = None ) Determines whether each spot in spot_yxz is isolated by getting the value of image after annular filtering at each location in spot_yxz . Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. If axis 1 dimension is more than image.ndim , only first image.ndim dimensions used i.e. if supply yxz, with 2d image, only yx position used. required thresh float Spots are isolated if annulus filtered image at spot location less than this. required radius_inner float Inner radius of annulus filtering kernel within which values are all zero. required radius_xy float Outer radius of annulus filtering kernel in xy direction. required radius_z Optional [ float ] Outer radius of annulus filtering kernel in z direction. If None , 2D filter is used. None Returns: Type Description np . ndarray bool [n_peaks] . Whether each spot is isolated or not. Source code in coppafish/find_spots/base.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_isolated ( image : np . ndarray , spot_yxz : np . ndarray , thresh : float , radius_inner : float , radius_xy : float , radius_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Determines whether each spot in ```spot_yxz``` is isolated by getting the value of image after annular filtering at each location in ```spot_yxz```. Args: image: ```float [n_y x n_x x n_z]```. image spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used i.e. if supply yxz, with 2d image, only yx position used. thresh: Spots are isolated if annulus filtered image at spot location less than this. radius_inner: Inner radius of annulus filtering kernel within which values are all zero. radius_xy: Outer radius of annulus filtering kernel in xy direction. radius_z: Outer radius of annulus filtering kernel in z direction. If ```None```, 2D filter is used. Returns: ```bool [n_peaks]```. Whether each spot is isolated or not. \"\"\" se = utils . strel . annulus ( radius_inner , radius_xy , radius_z ) # With just coords, takes about 3s for 50 z-planes. isolated = utils . morphology . imfilter_coords ( image , se , spot_yxz , padding = 0 , corr_or_conv = 'corr' ) / np . sum ( se ) return isolated < thresh get_isolated_points ( spot_yxz , isolation_dist ) Get the isolated points in a point cloud as those whose neighbour is far. Parameters: Name Type Description Default spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found in image. required isolation_dist float Spots are isolated if nearest neighbour is further away than this. required Returns: Type Description np . ndarray bool [n_peaks] . True for points far from any other point in spot_yx . Source code in coppafish/find_spots/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def get_isolated_points ( spot_yxz : np . ndarray , isolation_dist : float ) -> np . ndarray : \"\"\" Get the isolated points in a point cloud as those whose neighbour is far. Args: spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found in image. isolation_dist: Spots are isolated if nearest neighbour is further away than this. Returns: ```bool [n_peaks]```. ```True``` for points far from any other point in ```spot_yx```. \"\"\" tree = KDTree ( spot_yxz ) # for distances more than isolation_dist, distances will be set to infinity i.e. will be > isolation_dist. distances = tree . query ( spot_yxz , k = [ 2 ], distance_upper_bound = isolation_dist )[ 0 ] . squeeze () return distances > isolation_dist spot_yxz ( spot_details , tile , round , channel , return_isolated = False ) Function which gets yxz positions (and whether isolated) of spots on a particular tile , round , channel from spot_details in find_spots notebook page. Parameters: Name Type Description Default spot_details np . ndarray int16 [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . required tile int Tile of desired spots. required round int Round of desired spots. required channel int Channel of desired spots. required return_isolated bool Whether to return isolated status of each spot. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_yxz - int16 [n_trc_spots x 3] . yxz coordinates of spots on chosen tile , round and channel . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_isolated - bool [n_trc_spots] (Only returned if return_isolated = True ). Isolated status ( 1 if isolated, 0 if not) of the spots. Source code in coppafish/find_spots/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def spot_yxz ( spot_details : np . ndarray , tile : int , round : int , channel : int , return_isolated : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```, ``` channel``` from ```spot_details``` in find_spots notebook page. Args: spot_details: ```int16 [n_spots x 7]```. ```spot_details[s]``` is ```[tile, round, channel, isolated, y, x, z]``` of spot ```s```. tile: Tile of desired spots. round: Round of desired spots. channel: Channel of desired spots. return_isolated: Whether to return isolated status of each spot. Returns: - ```spot_yxz``` - ```int16 [n_trc_spots x 3]```. yxz coordinates of spots on chosen ```tile```, ```round``` and ```channel```. - ```spot_isolated``` - ```bool [n_trc_spots]``` (Only returned if ```return_isolated = True```). Isolated status (```1``` if isolated, ```0``` if not) of the spots. \"\"\" # Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```, # ```channel``` from ```spot_details``` in find_spots notebook page. use = np . all (( spot_details [:, 0 ] == tile , spot_details [:, 1 ] == round , spot_details [:, 2 ] == channel ), axis = 0 ) if return_isolated : return spot_details [ use , 4 :], spot_details [ use , 3 ] else : return spot_details [ use , 4 :]","title":"Base"},{"location":"code/find_spots/base/#coppafish.find_spots.base.check_neighbour_intensity","text":"Checks whether a neighbouring pixel to those indicated in spot_yxz has intensity less than thresh . The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. If axis 1 dimension is more than image.ndim , only first image.ndim dimensions used i.e. if supply yxz, with 2d image, only yx position used. required thresh float Spots are indicated as False if intensity at neighbour to spot location is less than this. 0 Returns: Type Description np . ndarray float [n_peaks] . True if no neighbours below thresh. Source code in coppafish/find_spots/base.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def check_neighbour_intensity ( image : np . ndarray , spot_yxz : np . ndarray , thresh : float = 0 ) -> np . ndarray : \"\"\" Checks whether a neighbouring pixel to those indicated in ```spot_yxz``` has intensity less than ```thresh```. The idea is that if pixel has very low intensity right next to it, it is probably a spurious spot. Args: image: ```float [n_y x n_x x n_z]```. image spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used i.e. if supply yxz, with 2d image, only yx position used. thresh: Spots are indicated as ```False``` if intensity at neighbour to spot location is less than this. Returns: ```float [n_peaks]```. ```True``` if no neighbours below thresh. \"\"\" if image . ndim == 3 : transforms = [[ 1 , 0 , 0 ], [ 0 , 1 , 0 ], [ - 1 , 0 , 0 ], [ 0 , - 1 , 0 ], [ 0 , 0 , 1 ], [ 0 , 0 , - 1 ]] elif image . ndim == 2 : transforms = [[ 1 , 0 ], [ 0 , 1 ], [ - 1 , 0 ], [ 0 , - 1 ]] else : raise ValueError ( f \"image has to have two or three dimensions but given image has { image . ndim } dimensions.\" ) keep = np . zeros (( spot_yxz . shape [ 0 ], len ( transforms )), dtype = bool ) for i , t in enumerate ( transforms ): mod_spot_yx = spot_yxz + t for j in range ( image . ndim ): mod_spot_yx [:, j ] = np . clip ( mod_spot_yx [:, j ], 0 , image . shape [ j ] - 1 ) keep [:, i ] = image [ tuple ([ mod_spot_yx [:, j ] for j in range ( image . ndim )])] > thresh return keep . min ( axis = 1 )","title":"check_neighbour_intensity()"},{"location":"code/find_spots/base/#coppafish.find_spots.base.get_isolated","text":"Determines whether each spot in spot_yxz is isolated by getting the value of image after annular filtering at each location in spot_yxz . Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. If axis 1 dimension is more than image.ndim , only first image.ndim dimensions used i.e. if supply yxz, with 2d image, only yx position used. required thresh float Spots are isolated if annulus filtered image at spot location less than this. required radius_inner float Inner radius of annulus filtering kernel within which values are all zero. required radius_xy float Outer radius of annulus filtering kernel in xy direction. required radius_z Optional [ float ] Outer radius of annulus filtering kernel in z direction. If None , 2D filter is used. None Returns: Type Description np . ndarray bool [n_peaks] . Whether each spot is isolated or not. Source code in coppafish/find_spots/base.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_isolated ( image : np . ndarray , spot_yxz : np . ndarray , thresh : float , radius_inner : float , radius_xy : float , radius_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Determines whether each spot in ```spot_yxz``` is isolated by getting the value of image after annular filtering at each location in ```spot_yxz```. Args: image: ```float [n_y x n_x x n_z]```. image spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. If axis 1 dimension is more than ```image.ndim```, only first ```image.ndim``` dimensions used i.e. if supply yxz, with 2d image, only yx position used. thresh: Spots are isolated if annulus filtered image at spot location less than this. radius_inner: Inner radius of annulus filtering kernel within which values are all zero. radius_xy: Outer radius of annulus filtering kernel in xy direction. radius_z: Outer radius of annulus filtering kernel in z direction. If ```None```, 2D filter is used. Returns: ```bool [n_peaks]```. Whether each spot is isolated or not. \"\"\" se = utils . strel . annulus ( radius_inner , radius_xy , radius_z ) # With just coords, takes about 3s for 50 z-planes. isolated = utils . morphology . imfilter_coords ( image , se , spot_yxz , padding = 0 , corr_or_conv = 'corr' ) / np . sum ( se ) return isolated < thresh","title":"get_isolated()"},{"location":"code/find_spots/base/#coppafish.find_spots.base.get_isolated_points","text":"Get the isolated points in a point cloud as those whose neighbour is far. Parameters: Name Type Description Default spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found in image. required isolation_dist float Spots are isolated if nearest neighbour is further away than this. required Returns: Type Description np . ndarray bool [n_peaks] . True for points far from any other point in spot_yx . Source code in coppafish/find_spots/base.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def get_isolated_points ( spot_yxz : np . ndarray , isolation_dist : float ) -> np . ndarray : \"\"\" Get the isolated points in a point cloud as those whose neighbour is far. Args: spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found in image. isolation_dist: Spots are isolated if nearest neighbour is further away than this. Returns: ```bool [n_peaks]```. ```True``` for points far from any other point in ```spot_yx```. \"\"\" tree = KDTree ( spot_yxz ) # for distances more than isolation_dist, distances will be set to infinity i.e. will be > isolation_dist. distances = tree . query ( spot_yxz , k = [ 2 ], distance_upper_bound = isolation_dist )[ 0 ] . squeeze () return distances > isolation_dist","title":"get_isolated_points()"},{"location":"code/find_spots/base/#coppafish.find_spots.base.spot_yxz","text":"Function which gets yxz positions (and whether isolated) of spots on a particular tile , round , channel from spot_details in find_spots notebook page. Parameters: Name Type Description Default spot_details np . ndarray int16 [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . required tile int Tile of desired spots. required round int Round of desired spots. required channel int Channel of desired spots. required return_isolated bool Whether to return isolated status of each spot. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_yxz - int16 [n_trc_spots x 3] . yxz coordinates of spots on chosen tile , round and channel . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_isolated - bool [n_trc_spots] (Only returned if return_isolated = True ). Isolated status ( 1 if isolated, 0 if not) of the spots. Source code in coppafish/find_spots/base.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def spot_yxz ( spot_details : np . ndarray , tile : int , round : int , channel : int , return_isolated : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```, ``` channel``` from ```spot_details``` in find_spots notebook page. Args: spot_details: ```int16 [n_spots x 7]```. ```spot_details[s]``` is ```[tile, round, channel, isolated, y, x, z]``` of spot ```s```. tile: Tile of desired spots. round: Round of desired spots. channel: Channel of desired spots. return_isolated: Whether to return isolated status of each spot. Returns: - ```spot_yxz``` - ```int16 [n_trc_spots x 3]```. yxz coordinates of spots on chosen ```tile```, ```round``` and ```channel```. - ```spot_isolated``` - ```bool [n_trc_spots]``` (Only returned if ```return_isolated = True```). Isolated status (```1``` if isolated, ```0``` if not) of the spots. \"\"\" # Function which gets yxz positions (and whether isolated) of spots on a particular ```tile```, ```round```, # ```channel``` from ```spot_details``` in find_spots notebook page. use = np . all (( spot_details [:, 0 ] == tile , spot_details [:, 1 ] == round , spot_details [:, 2 ] == channel ), axis = 0 ) if return_isolated : return spot_details [ use , 4 :], spot_details [ use , 3 ] else : return spot_details [ use , 4 :]","title":"spot_yxz()"},{"location":"code/find_spots/check_spots/","text":"check_n_spots ( nb ) This checks that a decent number of spots are detected on: Each channel across all rounds and tiles. Each tile across all rounds and channels. Each round across all tile and channels. An error will be raised if any of these conditions are violated. config['find_spots']['n_spots_warn_fraction'] and config['find_spots']['n_spots_error_fraction'] are the parameters which determine if warnings/errors will be raised. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required Source code in coppafish/find_spots/check_spots.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def check_n_spots ( nb : Notebook ): \"\"\" This checks that a decent number of spots are detected on: * Each channel across all rounds and tiles. * Each tile across all rounds and channels. * Each round across all tile and channels. An error will be raised if any of these conditions are violated. `config['find_spots']['n_spots_warn_fraction']` and `config['find_spots']['n_spots_error_fraction']` are the parameters which determine if warnings/errors will be raised. Args: nb: *Notebook* containing `find_spots` page. \"\"\" # TODO: show example of what error looks like in the docs config = nb . get_config ()[ 'find_spots' ] if nb . basic_info . is_3d : n_spots_warn = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_3d' ] * nb . basic_info . nz else : n_spots_warn = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_2d' ] n_spots_warn = int ( np . ceil ( n_spots_warn )) use_tiles = np . asarray ( nb . basic_info . use_tiles ) error_message = \"\" if len ( nb . basic_info . use_rounds ) > 0 : use_rounds = np . asarray ( nb . basic_info . use_rounds ) # don't consider anchor in this analysis use_channels = np . asarray ( nb . basic_info . use_channels ) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad channels first as most likely to have consistently low spot counts in a channel n_images = len ( use_tiles ) * len ( use_rounds ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_channels ), dtype = int ) for c in range ( len ( use_channels )): bad_images = np . vstack ( np . where ( spot_no [:, :, c ] < n_spots_warn )) . T n_bad_images [ c ] = bad_images . shape [ 0 ] if n_bad_images [ c ] > 0 : bad_images [:, 0 ] = use_tiles [ bad_images [:, 0 ]] bad_images [:, 1 ] = use_rounds [ bad_images [:, 1 ]] warnings . warn ( f \" \\n Channel { use_channels [ c ] } - { n_bad_images [ c ] } tiles/rounds with n_spots < { n_spots_warn } :\" f \" \\n { bad_images } \" ) fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Channels that failed: { use_channels [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } tiles/rounds, these channels had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n tiles/rounds with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold of \" \\ f \" { n_images_error } . \\n Consider removing these from use_channels.\" # don't consider failed channels for subsequent warnings/errors use_channels = np . setdiff1d ( use_channels , use_channels [ fail_inds ]) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad tiles next as second most likely to have consistently low spot counts in a tile n_images = len ( use_channels ) * len ( use_rounds ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_tiles ), dtype = int ) for t in range ( len ( use_tiles )): bad_images = np . vstack ( np . where ( spot_no [ t ] < n_spots_warn )) . T n_bad_images [ t ] = bad_images . shape [ 0 ] fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Tiles that failed: { use_tiles [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } rounds/channels, these tiles had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n rounds/channels with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold of \" \\ f \" { n_images_error } . \\n Consider removing these from use_tiles.\" # don't consider failed channels for subsequent warnings/errors use_tiles = np . setdiff1d ( use_tiles , use_tiles [ fail_inds ]) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad rounds last as least likely to have consistently low spot counts in a round n_images = len ( use_channels ) * len ( use_tiles ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_rounds ), dtype = int ) for r in range ( len ( use_rounds )): bad_images = np . vstack ( np . where ( spot_no [:, r ] < n_spots_warn )) . T n_bad_images [ r ] = bad_images . shape [ 0 ] fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Rounds that failed: { use_rounds [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } tiles/channels, these tiles had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n tiles/channels with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold \" \\ f \"of { n_images_error } . \\n Consider removing these from use_rounds.\" # Consider anchor if nb . basic_info . use_anchor : spot_no = nb . find_spots . spot_no [ use_tiles , nb . basic_info . anchor_round , nb . basic_info . anchor_channel ] n_images = len ( use_tiles ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) bad_images = np . where ( spot_no < n_spots_warn )[ 0 ] n_bad_images = len ( bad_images ) if n_bad_images > 0 : bad_images = use_tiles [ bad_images ] warnings . warn ( f \" \\n Anchor - { n_bad_images } tiles with n_spots < { n_spots_warn } : \\n \" f \" { bad_images } \" ) if n_bad_images >= n_images_error : error_message = error_message + f \" \\n Anchor - tiles { bad_images } all had n_spots < { n_spots_warn } . \" \\ f \" { n_bad_images } / { n_images } tiles failed which is more than the \" \\ f \"error threshold of { n_images_error } . \\n \" \\ f \"Consider removing these tiles from use_tiles.\" if len ( error_message ) > 0 : error_message = error_message + f \" \\n The function coppafish.plot.view_find_spots may be useful for \" \\ f \"investigating why the above tiles/rounds/channels had so few spots detected.\" raise ValueError ( error_message )","title":"Check Spots"},{"location":"code/find_spots/check_spots/#coppafish.find_spots.check_spots.check_n_spots","text":"This checks that a decent number of spots are detected on: Each channel across all rounds and tiles. Each tile across all rounds and channels. Each round across all tile and channels. An error will be raised if any of these conditions are violated. config['find_spots']['n_spots_warn_fraction'] and config['find_spots']['n_spots_error_fraction'] are the parameters which determine if warnings/errors will be raised. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required Source code in coppafish/find_spots/check_spots.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def check_n_spots ( nb : Notebook ): \"\"\" This checks that a decent number of spots are detected on: * Each channel across all rounds and tiles. * Each tile across all rounds and channels. * Each round across all tile and channels. An error will be raised if any of these conditions are violated. `config['find_spots']['n_spots_warn_fraction']` and `config['find_spots']['n_spots_error_fraction']` are the parameters which determine if warnings/errors will be raised. Args: nb: *Notebook* containing `find_spots` page. \"\"\" # TODO: show example of what error looks like in the docs config = nb . get_config ()[ 'find_spots' ] if nb . basic_info . is_3d : n_spots_warn = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_3d' ] * nb . basic_info . nz else : n_spots_warn = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_2d' ] n_spots_warn = int ( np . ceil ( n_spots_warn )) use_tiles = np . asarray ( nb . basic_info . use_tiles ) error_message = \"\" if len ( nb . basic_info . use_rounds ) > 0 : use_rounds = np . asarray ( nb . basic_info . use_rounds ) # don't consider anchor in this analysis use_channels = np . asarray ( nb . basic_info . use_channels ) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad channels first as most likely to have consistently low spot counts in a channel n_images = len ( use_tiles ) * len ( use_rounds ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_channels ), dtype = int ) for c in range ( len ( use_channels )): bad_images = np . vstack ( np . where ( spot_no [:, :, c ] < n_spots_warn )) . T n_bad_images [ c ] = bad_images . shape [ 0 ] if n_bad_images [ c ] > 0 : bad_images [:, 0 ] = use_tiles [ bad_images [:, 0 ]] bad_images [:, 1 ] = use_rounds [ bad_images [:, 1 ]] warnings . warn ( f \" \\n Channel { use_channels [ c ] } - { n_bad_images [ c ] } tiles/rounds with n_spots < { n_spots_warn } :\" f \" \\n { bad_images } \" ) fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Channels that failed: { use_channels [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } tiles/rounds, these channels had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n tiles/rounds with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold of \" \\ f \" { n_images_error } . \\n Consider removing these from use_channels.\" # don't consider failed channels for subsequent warnings/errors use_channels = np . setdiff1d ( use_channels , use_channels [ fail_inds ]) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad tiles next as second most likely to have consistently low spot counts in a tile n_images = len ( use_channels ) * len ( use_rounds ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_tiles ), dtype = int ) for t in range ( len ( use_tiles )): bad_images = np . vstack ( np . where ( spot_no [ t ] < n_spots_warn )) . T n_bad_images [ t ] = bad_images . shape [ 0 ] fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Tiles that failed: { use_tiles [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } rounds/channels, these tiles had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n rounds/channels with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold of \" \\ f \" { n_images_error } . \\n Consider removing these from use_tiles.\" # don't consider failed channels for subsequent warnings/errors use_tiles = np . setdiff1d ( use_tiles , use_tiles [ fail_inds ]) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad rounds last as least likely to have consistently low spot counts in a round n_images = len ( use_channels ) * len ( use_tiles ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) n_bad_images = np . zeros ( len ( use_rounds ), dtype = int ) for r in range ( len ( use_rounds )): bad_images = np . vstack ( np . where ( spot_no [:, r ] < n_spots_warn )) . T n_bad_images [ r ] = bad_images . shape [ 0 ] fail_inds = np . where ( n_bad_images >= n_images_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Rounds that failed: { use_rounds [ fail_inds ] } \\n \" \\ f \"This is because out of { n_images } tiles/channels, these tiles had \" \\ f \"respectively: \\n { n_bad_images [ fail_inds ] } \\n tiles/channels with \" \\ f \"n_spots < { n_spots_warn } . These are all more than the error threshold \" \\ f \"of { n_images_error } . \\n Consider removing these from use_rounds.\" # Consider anchor if nb . basic_info . use_anchor : spot_no = nb . find_spots . spot_no [ use_tiles , nb . basic_info . anchor_round , nb . basic_info . anchor_channel ] n_images = len ( use_tiles ) n_images_error = int ( np . floor ( n_images * config [ 'n_spots_error_fraction' ])) bad_images = np . where ( spot_no < n_spots_warn )[ 0 ] n_bad_images = len ( bad_images ) if n_bad_images > 0 : bad_images = use_tiles [ bad_images ] warnings . warn ( f \" \\n Anchor - { n_bad_images } tiles with n_spots < { n_spots_warn } : \\n \" f \" { bad_images } \" ) if n_bad_images >= n_images_error : error_message = error_message + f \" \\n Anchor - tiles { bad_images } all had n_spots < { n_spots_warn } . \" \\ f \" { n_bad_images } / { n_images } tiles failed which is more than the \" \\ f \"error threshold of { n_images_error } . \\n \" \\ f \"Consider removing these tiles from use_tiles.\" if len ( error_message ) > 0 : error_message = error_message + f \" \\n The function coppafish.plot.view_find_spots may be useful for \" \\ f \"investigating why the above tiles/rounds/channels had so few spots detected.\" raise ValueError ( error_message )","title":"check_n_spots()"},{"location":"code/find_spots/detect/","text":"detect_spots ( image , intensity_thresh , radius_xy , radius_z = None , remove_duplicates = False , se = None ) Finds local maxima in image exceeding intensity_thresh . This is achieved through a dilation being run on the whole image. Should use for a large se. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image to find spots on. required intensity_thresh float Spots are local maxima in image with pixel_value > intensity_thresh . required radius_xy Optional [ int ] Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If None , 2D filter is used. None remove_duplicates bool Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. False se Optional [ np . ndarray ] int [se_sz_y x se_sz_x x se_sz_z] . Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. None Returns: Type Description np . ndarray peak_yxz - int [n_peaks x image.ndim] . yx or yxz location of spots found. np . ndarray peak_intensity - float [n_peaks] . Pixel value of spots found. Source code in coppafish/find_spots/detect.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def detect_spots ( image : np . ndarray , intensity_thresh : float , radius_xy : Optional [ int ], radius_z : Optional [ int ] = None , remove_duplicates : bool = False , se : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds local maxima in image exceeding ```intensity_thresh```. This is achieved through a dilation being run on the whole image. Should use for a large se. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. intensity_thresh: Spots are local maxima in image with ```pixel_value > intensity_thresh```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If ```None```, 2D filter is used. remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. se: ```int [se_sz_y x se_sz_x x se_sz_z]```. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. Returns: - ```peak_yxz``` - ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. - ```peak_intensity``` - ```float [n_peaks]```. Pixel value of spots found. \"\"\" if se is None : # Default is a cuboid se of all ones as is quicker than disk and very similar results. if radius_z is not None : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 ), dtype = int ) else : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 ), dtype = int ) if image . ndim == 2 and se . ndim == 3 : mid_z = int ( np . floor (( se . shape [ 2 ] - 1 ) / 2 )) warnings . warn ( f \"2D image provided but 3D filter asked for. \\n \" f \"Using the middle plane ( { mid_z } ) of this filter.\" ) se = se [:, :, mid_z ] small = 1e-6 # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image). if remove_duplicates : # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = small * 2 , high = 0.2 , size = image . shape ) image = image + rand_im_shift dilate = utils . morphology . dilate ( image , se ) spots = np . logical_and ( image + small > dilate , image > intensity_thresh ) peak_pos = np . where ( spots ) peak_yxz = np . concatenate ([ coord . reshape ( - 1 , 1 ) for coord in peak_pos ], axis = 1 ) peak_intensity = image [ spots ] return peak_yxz , peak_intensity Optimised detect_spots ( image , intensity_thresh , radius_xy , radius_z = None , remove_duplicates = False , se = None ) Finds local maxima in image exceeding intensity_thresh . This is achieved by looking at neighbours of pixels above intensity_thresh. Should use for a small se and high intensity_thresh . Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image to find spots on. required intensity_thresh float Spots are local maxima in image with pixel_value > intensity_thresh . required radius_xy Optional [ int ] Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If None , 2D filter is used. None remove_duplicates bool Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. False se Optional [ np . ndarray ] int [se_sz_y x se_sz_x x se_sz_z] . Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. None Returns: Type Description np . ndarray peak_yxz - int [n_peaks x image.ndim] . yx or yxz location of spots found. np . ndarray peak_intensity - float [n_peaks] . Pixel value of spots found. Source code in coppafish/find_spots/detect_optimised.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def detect_spots ( image : np . ndarray , intensity_thresh : float , radius_xy : Optional [ int ], radius_z : Optional [ int ] = None , remove_duplicates : bool = False , se : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds local maxima in image exceeding ```intensity_thresh```. This is achieved by looking at neighbours of pixels above intensity_thresh. Should use for a small `se` and high `intensity_thresh`. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. intensity_thresh: Spots are local maxima in image with ```pixel_value > intensity_thresh```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If ```None```, 2D filter is used. remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. se: ```int [se_sz_y x se_sz_x x se_sz_z]```. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. Returns: - ```peak_yxz``` - ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. - ```peak_intensity``` - ```float [n_peaks]```. Pixel value of spots found. \"\"\" if se is None : # Default is a cuboid se of all ones as is quicker than disk and very similar results. if radius_z is not None : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 ), dtype = int ) pad_size_z = radius_z - 1 else : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 ), dtype = int ) pad_size_z = 0 pad_size_y = radius_xy - 1 pad_size_x = radius_xy - 1 else : se = utils . morphology . ensure_odd_kernel ( se ) pad_size_y = int (( se . shape [ 0 ] - 1 ) / 2 ) pad_size_x = int (( se . shape [ 1 ] - 1 ) / 2 ) if se . ndim == 3 : pad_size_z = int (( se . shape [ 2 ] - 1 ) / 2 ) else : pad_size_z = 0 if image . ndim == 2 and se . ndim == 3 : mid_z = int ( np . floor (( se . shape [ 2 ] - 1 ) / 2 )) warnings . warn ( f \"2D image provided but 3D filter asked for. \\n \" f \"Using the middle plane ( { mid_z } ) of this filter.\" ) se = se [:, :, mid_z ] # set central pixel to 0 se [ np . ix_ ( * [( np . floor (( se . shape [ i ] - 1 ) / 2 ) . astype ( int ),) for i in range ( se . ndim )])] = 0 se_shifts = utils . morphology . filter_optimised . get_shifts_from_kernel ( se ) consider_yxz = np . where ( image > intensity_thresh ) n_consider = consider_yxz [ 0 ] . shape [ 0 ] if remove_duplicates : # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = 2e-6 , high = 0.2 , size = n_consider ) . astype ( np . float32 ) image = image . astype ( np . float32 ) image [ consider_yxz ] = image [ consider_yxz ] + rand_im_shift consider_intensity = image [ consider_yxz ] consider_yxz = list ( consider_yxz ) keep = np . asarray ( get_local_maxima_jax ( image , se_shifts , pad_size_y , pad_size_x , pad_size_z , consider_yxz , consider_intensity )) if remove_duplicates : peak_intensity = np . round ( consider_intensity [ keep ]) . astype ( int ) else : peak_intensity = consider_intensity [ keep ] peak_yxz = np . array ( consider_yxz ) . transpose ()[ keep ] return peak_yxz , peak_intensity get_local_maxima_jax ( image , se_shifts , pad_size_y , pad_size_x , pad_size_z , consider_yxz , consider_intensity ) Finds the local maxima from a given set of pixels to consider. Parameters: Name Type Description Default image jnp . ndarray float [n_y x n_x x n_z] . image to find spots on. required se_shifts Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ] (image.ndim x int [n_shifts]) . y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out. required pad_size_y int How much to zero pad image in y. required pad_size_x int How much to zero pad image in x. required pad_size_z int How much to zero pad image in z. required consider_yxz List [ jnp . ndarray ] [3 x int [n_consider]] . All yxz coordinates where value in image is greater than an intensity threshold. required consider_intensity jnp . ndarray float [n_consider] . Value of image at coordinates given by consider_yxz . required Returns: Type Description jnp . ndarray bool [n_consider] Whether each point in consider_yxz is a local maxima or not. Source code in coppafish/find_spots/detect_optimised.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @partial ( jax . jit , static_argnums = ( 2 , 3 , 4 )) def get_local_maxima_jax ( image : jnp . ndarray , se_shifts : Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ], pad_size_y : int , pad_size_x : int , pad_size_z : int , consider_yxz : List [ jnp . ndarray ], consider_intensity : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds the local maxima from a given set of pixels to consider. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. se_shifts: `(image.ndim x int [n_shifts])`. y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out. pad_size_y: How much to zero pad image in y. pad_size_x: How much to zero pad image in x. pad_size_z: How much to zero pad image in z. consider_yxz: `[3 x int [n_consider]]`. All yxz coordinates where value in image is greater than an intensity threshold. consider_intensity: `float [n_consider]`. Value of image at coordinates given by `consider_yxz`. Returns: `bool [n_consider]` Whether each point in `consider_yxz` is a local maxima or not. \"\"\" pad_size = [( pad_size_y , pad_size_y ), ( pad_size_x , pad_size_x ), ( pad_size_z , pad_size_z )][: image . ndim ] image = jnp . pad ( image , pad_size ) for i in range ( len ( pad_size )): consider_yxz [ i ] = consider_yxz [ i ] + pad_size [ i ][ 0 ] keep = jnp . ones ( consider_yxz [ 0 ] . shape [ 0 ], dtype = bool ) for i in range ( se_shifts [ 0 ] . shape [ 0 ]): # Note that in each iteration, only consider coordinates which can still possibly be local maxima. keep = keep * ( image [ tuple ([ consider_yxz [ j ] + se_shifts [ j ][ i ] for j in range ( image . ndim )])] <= consider_intensity ) return keep","title":"Detect"},{"location":"code/find_spots/detect/#coppafish.find_spots.detect.detect_spots","text":"Finds local maxima in image exceeding intensity_thresh . This is achieved through a dilation being run on the whole image. Should use for a large se. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image to find spots on. required intensity_thresh float Spots are local maxima in image with pixel_value > intensity_thresh . required radius_xy Optional [ int ] Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If None , 2D filter is used. None remove_duplicates bool Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. False se Optional [ np . ndarray ] int [se_sz_y x se_sz_x x se_sz_z] . Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. None Returns: Type Description np . ndarray peak_yxz - int [n_peaks x image.ndim] . yx or yxz location of spots found. np . ndarray peak_intensity - float [n_peaks] . Pixel value of spots found. Source code in coppafish/find_spots/detect.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def detect_spots ( image : np . ndarray , intensity_thresh : float , radius_xy : Optional [ int ], radius_z : Optional [ int ] = None , remove_duplicates : bool = False , se : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds local maxima in image exceeding ```intensity_thresh```. This is achieved through a dilation being run on the whole image. Should use for a large se. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. intensity_thresh: Spots are local maxima in image with ```pixel_value > intensity_thresh```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If ```None```, 2D filter is used. remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. se: ```int [se_sz_y x se_sz_x x se_sz_z]```. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. Returns: - ```peak_yxz``` - ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. - ```peak_intensity``` - ```float [n_peaks]```. Pixel value of spots found. \"\"\" if se is None : # Default is a cuboid se of all ones as is quicker than disk and very similar results. if radius_z is not None : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 ), dtype = int ) else : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 ), dtype = int ) if image . ndim == 2 and se . ndim == 3 : mid_z = int ( np . floor (( se . shape [ 2 ] - 1 ) / 2 )) warnings . warn ( f \"2D image provided but 3D filter asked for. \\n \" f \"Using the middle plane ( { mid_z } ) of this filter.\" ) se = se [:, :, mid_z ] small = 1e-6 # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image). if remove_duplicates : # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = small * 2 , high = 0.2 , size = image . shape ) image = image + rand_im_shift dilate = utils . morphology . dilate ( image , se ) spots = np . logical_and ( image + small > dilate , image > intensity_thresh ) peak_pos = np . where ( spots ) peak_yxz = np . concatenate ([ coord . reshape ( - 1 , 1 ) for coord in peak_pos ], axis = 1 ) peak_intensity = image [ spots ] return peak_yxz , peak_intensity","title":"detect_spots()"},{"location":"code/find_spots/detect/#optimised","text":"","title":"Optimised"},{"location":"code/find_spots/detect/#coppafish.find_spots.detect_optimised.detect_spots","text":"Finds local maxima in image exceeding intensity_thresh . This is achieved by looking at neighbours of pixels above intensity_thresh. Should use for a small se and high intensity_thresh . Parameters: Name Type Description Default image np . ndarray float [n_y x n_x x n_z] . image to find spots on. required intensity_thresh float Spots are local maxima in image with pixel_value > intensity_thresh . required radius_xy Optional [ int ] Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If None , 2D filter is used. None remove_duplicates bool Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. False se Optional [ np . ndarray ] int [se_sz_y x se_sz_x x se_sz_z] . Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. None Returns: Type Description np . ndarray peak_yxz - int [n_peaks x image.ndim] . yx or yxz location of spots found. np . ndarray peak_intensity - float [n_peaks] . Pixel value of spots found. Source code in coppafish/find_spots/detect_optimised.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def detect_spots ( image : np . ndarray , intensity_thresh : float , radius_xy : Optional [ int ], radius_z : Optional [ int ] = None , remove_duplicates : bool = False , se : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds local maxima in image exceeding ```intensity_thresh```. This is achieved by looking at neighbours of pixels above intensity_thresh. Should use for a small `se` and high `intensity_thresh`. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. intensity_thresh: Spots are local maxima in image with ```pixel_value > intensity_thresh```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). Must be more than 1 to be 3D. If ```None```, 2D filter is used. remove_duplicates: Whether to only keep one pixel if two or more pixels are local maxima and have same intensity. Only works with integer image. se: ```int [se_sz_y x se_sz_x x se_sz_z]```. Can give structuring element manually rather than using a cuboid element. Must only contain zeros and ones. Returns: - ```peak_yxz``` - ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. - ```peak_intensity``` - ```float [n_peaks]```. Pixel value of spots found. \"\"\" if se is None : # Default is a cuboid se of all ones as is quicker than disk and very similar results. if radius_z is not None : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 ), dtype = int ) pad_size_z = radius_z - 1 else : se = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 ), dtype = int ) pad_size_z = 0 pad_size_y = radius_xy - 1 pad_size_x = radius_xy - 1 else : se = utils . morphology . ensure_odd_kernel ( se ) pad_size_y = int (( se . shape [ 0 ] - 1 ) / 2 ) pad_size_x = int (( se . shape [ 1 ] - 1 ) / 2 ) if se . ndim == 3 : pad_size_z = int (( se . shape [ 2 ] - 1 ) / 2 ) else : pad_size_z = 0 if image . ndim == 2 and se . ndim == 3 : mid_z = int ( np . floor (( se . shape [ 2 ] - 1 ) / 2 )) warnings . warn ( f \"2D image provided but 3D filter asked for. \\n \" f \"Using the middle plane ( { mid_z } ) of this filter.\" ) se = se [:, :, mid_z ] # set central pixel to 0 se [ np . ix_ ( * [( np . floor (( se . shape [ i ] - 1 ) / 2 ) . astype ( int ),) for i in range ( se . ndim )])] = 0 se_shifts = utils . morphology . filter_optimised . get_shifts_from_kernel ( se ) consider_yxz = np . where ( image > intensity_thresh ) n_consider = consider_yxz [ 0 ] . shape [ 0 ] if remove_duplicates : # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = 2e-6 , high = 0.2 , size = n_consider ) . astype ( np . float32 ) image = image . astype ( np . float32 ) image [ consider_yxz ] = image [ consider_yxz ] + rand_im_shift consider_intensity = image [ consider_yxz ] consider_yxz = list ( consider_yxz ) keep = np . asarray ( get_local_maxima_jax ( image , se_shifts , pad_size_y , pad_size_x , pad_size_z , consider_yxz , consider_intensity )) if remove_duplicates : peak_intensity = np . round ( consider_intensity [ keep ]) . astype ( int ) else : peak_intensity = consider_intensity [ keep ] peak_yxz = np . array ( consider_yxz ) . transpose ()[ keep ] return peak_yxz , peak_intensity","title":"detect_spots()"},{"location":"code/find_spots/detect/#coppafish.find_spots.detect_optimised.get_local_maxima_jax","text":"Finds the local maxima from a given set of pixels to consider. Parameters: Name Type Description Default image jnp . ndarray float [n_y x n_x x n_z] . image to find spots on. required se_shifts Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ] (image.ndim x int [n_shifts]) . y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out. required pad_size_y int How much to zero pad image in y. required pad_size_x int How much to zero pad image in x. required pad_size_z int How much to zero pad image in z. required consider_yxz List [ jnp . ndarray ] [3 x int [n_consider]] . All yxz coordinates where value in image is greater than an intensity threshold. required consider_intensity jnp . ndarray float [n_consider] . Value of image at coordinates given by consider_yxz . required Returns: Type Description jnp . ndarray bool [n_consider] Whether each point in consider_yxz is a local maxima or not. Source code in coppafish/find_spots/detect_optimised.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 @partial ( jax . jit , static_argnums = ( 2 , 3 , 4 )) def get_local_maxima_jax ( image : jnp . ndarray , se_shifts : Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ], pad_size_y : int , pad_size_x : int , pad_size_z : int , consider_yxz : List [ jnp . ndarray ], consider_intensity : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds the local maxima from a given set of pixels to consider. Args: image: ```float [n_y x n_x x n_z]```. ```image``` to find spots on. se_shifts: `(image.ndim x int [n_shifts])`. y, x, z shifts which indicate neighbourhood about each spot where local maxima search carried out. pad_size_y: How much to zero pad image in y. pad_size_x: How much to zero pad image in x. pad_size_z: How much to zero pad image in z. consider_yxz: `[3 x int [n_consider]]`. All yxz coordinates where value in image is greater than an intensity threshold. consider_intensity: `float [n_consider]`. Value of image at coordinates given by `consider_yxz`. Returns: `bool [n_consider]` Whether each point in `consider_yxz` is a local maxima or not. \"\"\" pad_size = [( pad_size_y , pad_size_y ), ( pad_size_x , pad_size_x ), ( pad_size_z , pad_size_z )][: image . ndim ] image = jnp . pad ( image , pad_size ) for i in range ( len ( pad_size )): consider_yxz [ i ] = consider_yxz [ i ] + pad_size [ i ][ 0 ] keep = jnp . ones ( consider_yxz [ 0 ] . shape [ 0 ], dtype = bool ) for i in range ( se_shifts [ 0 ] . shape [ 0 ]): # Note that in each iteration, only consider coordinates which can still possibly be local maxima. keep = keep * ( image [ tuple ([ consider_yxz [ j ] + se_shifts [ j ][ i ] for j in range ( image . ndim )])] <= consider_intensity ) return keep","title":"get_local_maxima_jax()"},{"location":"code/omp/base/","text":"get_initial_intensity_thresh ( config , nbp ) Gets absolute intensity threshold from config file. OMP will only be run on pixels with absolute intensity greater than this. Parameters: Name Type Description Default config dict omp section of config file. required nbp NotebookPage call_spots NotebookPage required Returns: Type Description float Either config['initial_intensity_thresh'] or nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']] . Source code in coppafish/omp/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_initial_intensity_thresh ( config : dict , nbp : NotebookPage ) -> float : \"\"\" Gets absolute intensity threshold from config file. OMP will only be run on pixels with absolute intensity greater than this. Args: config: `omp` section of config file. nbp: `call_spots` *NotebookPage* Returns: Either `config['initial_intensity_thresh']` or `nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']]`. \"\"\" initial_intensity_thresh = config [ 'initial_intensity_thresh' ] if initial_intensity_thresh is None : config [ 'initial_intensity_thresh' ] = \\ round_any ( nbp . abs_intensity_percentile [ config [ 'initial_intensity_thresh_percentile' ]], config [ 'initial_intensity_precision' ]) initial_intensity_thresh = \\ float ( np . clip ( config [ 'initial_intensity_thresh' ], config [ 'initial_intensity_thresh_min' ], config [ 'initial_intensity_thresh_max' ])) return initial_intensity_thresh","title":"Base"},{"location":"code/omp/base/#coppafish.omp.base.get_initial_intensity_thresh","text":"Gets absolute intensity threshold from config file. OMP will only be run on pixels with absolute intensity greater than this. Parameters: Name Type Description Default config dict omp section of config file. required nbp NotebookPage call_spots NotebookPage required Returns: Type Description float Either config['initial_intensity_thresh'] or nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']] . Source code in coppafish/omp/base.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_initial_intensity_thresh ( config : dict , nbp : NotebookPage ) -> float : \"\"\" Gets absolute intensity threshold from config file. OMP will only be run on pixels with absolute intensity greater than this. Args: config: `omp` section of config file. nbp: `call_spots` *NotebookPage* Returns: Either `config['initial_intensity_thresh']` or `nbp.abs_intensity_percentile[config['initial_intensity_thresh_percentile']]`. \"\"\" initial_intensity_thresh = config [ 'initial_intensity_thresh' ] if initial_intensity_thresh is None : config [ 'initial_intensity_thresh' ] = \\ round_any ( nbp . abs_intensity_percentile [ config [ 'initial_intensity_thresh_percentile' ]], config [ 'initial_intensity_precision' ]) initial_intensity_thresh = \\ float ( np . clip ( config [ 'initial_intensity_thresh' ], config [ 'initial_intensity_thresh_min' ], config [ 'initial_intensity_thresh_max' ])) return initial_intensity_thresh","title":"get_initial_intensity_thresh()"},{"location":"code/omp/coefs/","text":"fit_coefs ( bled_codes , pixel_colors , genes ) Old method before Jax. This finds the least squared solution for how the n_genes bled_codes can best explain each pixel_color . Can also find weighted least squared solution if weight provided. Parameters: Name Type Description Default bled_codes np . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors np . ndarray float [(n_rounds x n_channels) x n_pixels] if n_genes==1 otherwise float [(n_rounds x n_channels)] . Flattened then transposed pixel colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes np . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required Returns: Type Description np . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coef. np . ndarray coefs - float [n_pixels x n_genes_add] if n_genes == 1 otherwise float [n_genes] if n_pixels == 1. coefficient found through least squares fitting for each gene. Source code in coppafish/omp/coefs.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def fit_coefs ( bled_codes : np . ndarray , pixel_colors : np . ndarray , genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Old method before Jax. This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`. Can also find weighted least squared solution if `weight` provided. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1` otherwise `float [(n_rounds x n_channels)]`. Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coef. - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1. coefficient found through least squares fitting for each gene. \"\"\" n_pixels = pixel_colors . shape [ 1 ] residual = np . zeros (( n_pixels , pixel_colors . shape [ 0 ])) coefs = np . zeros_like ( genes , dtype = float ) for s in range ( n_pixels ): coefs [ s ] = np . linalg . lstsq ( bled_codes [:, genes [ s ]], pixel_colors [:, s ], rcond = None )[ 0 ] residual [ s ] = pixel_colors [:, s ] - bled_codes [:, genes [ s ]] @ coefs [ s ] return residual , coefs fit_coefs_weight ( bled_codes , pixel_colors , genes , weight ) Old method before Jax. This finds the least squared solution for how the n_genes bled_codes can best explain each pixel_color . Can also find weighted least squared solution if weight provided. Parameters: Name Type Description Default bled_codes np . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors np . ndarray float [(n_rounds x n_channels) x n_pixels] if n_genes==1 otherwise float [(n_rounds x n_channels)] . Flattened then transposed pixel colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes np . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required weight np . ndarray float [n_pixels x (n_rounds x n_channels)] . weight[s, i] is the weight to be applied to round_channel i when computing coefficient of each bled_code for pixel s . required Returns: Type Description np . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coef. np . ndarray coefs - float [n_pixels x n_genes_add] if n_genes == 1 otherwise float [n_genes] if n_pixels == 1. coefficient found through least squares fitting for each gene. Source code in coppafish/omp/coefs.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit_coefs_weight ( bled_codes : np . ndarray , pixel_colors : np . ndarray , genes : np . ndarray , weight : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Old method before Jax. This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`. Can also find weighted least squared solution if `weight` provided. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1` otherwise `float [(n_rounds x n_channels)]`. Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. weight: `float [n_pixels x (n_rounds x n_channels)]`. `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code` for pixel `s`. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coef. - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1. coefficient found through least squares fitting for each gene. \"\"\" n_pixels = pixel_colors . shape [ 1 ] residual = np . zeros (( n_pixels , pixel_colors . shape [ 0 ])) coefs = np . zeros_like ( genes , dtype = float ) pixel_colors = pixel_colors * weight . transpose () for s in range ( n_pixels ): bled_codes_s = bled_codes [:, genes [ s ]] * weight [ s ][:, np . newaxis ] coefs [ s ] = np . linalg . lstsq ( bled_codes_s , pixel_colors [:, s ], rcond = None )[ 0 ] residual [ s ] = pixel_colors [:, s ] - bled_codes_s @ coefs [ s ] residual = residual / weight return residual , coefs get_all_coefs ( pixel_colors , bled_codes , background_shift , dp_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit = False , track = False ) This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. Note Background vectors are fitted first and then not updated again. Parameters: Name Type Description Default pixel_colors np . ndarray float [n_pixels x n_rounds x n_channels] . Pixel colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required background_shift float When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. required dp_shift float When finding dot_product_score between residual pixel_colors and bled_codes , this is applied to normalisation of pixel_colors to limit boost of weak pixels. required dp_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added at each iteration. required alpha float Used for fitting_standard_deviation , by how much to increase variance as genes added. required beta float Used for fitting_standard_deviation , the variance with no genes added ( coef=0 ) is beta**2 . required max_genes int Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. required weight_coef_fit bool If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. False track bool If True and one pixel, info about genes added at each step returned. False Returns: Type Description Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] gene_coefs - float32 [n_pixels x n_genes] . gene_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] background_coefs - float32 [n_pixels x n_channels] . coefficient value for each background vector found for each pixel. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] track_info - dictionary containing info about genes added at each step returned if track == True - background_codes - float [n_channels x n_rounds x n_channels] . background_codes[c] is the background vector for channel c with L2 norm of 1. background_coefs - float [n_channels] . background_coefs[c] is the coefficient value for background_codes[c] . gene_added - int [n_genes_added + 2] . gene_added[0] and gene_added[1] are -1. gene_added[2+i] is the ith gene that was added. residual - float [(n_genes_added + 2) x n_rounds x n_channels] . residual[0] is the initial pixel_color . residual[1] is the post background pixel_color . residual[2+i] is the pixel_color after removing gene gene_added[2+i] . coef - float [(n_genes_added + 2) x n_genes] . coef[0] and coef[1] are all 0. coef[2+i] are the coefficients for all genes after the ith gene has been added. dot_product - float [n_genes_added + 2] . dot_product[0] and dot_product[1] are 0. dot_product[2+i] is the dot product for the gene gene_added[2+i] . inverse_var - float [(n_genes_added + 2) x n_rounds x n_channels] . inverse_var[0] and inverse_var[1] are all 0. inverse_var[2+i] is the weighting used to compute dot_product[2+i] , which down-weights rounds/channels for which a gene has already been fitted. Source code in coppafish/omp/coefs.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def get_all_coefs ( pixel_colors : np . ndarray , bled_codes : np . ndarray , background_shift : float , dp_shift : float , dp_thresh : float , alpha : float , beta : float , max_genes : int , weight_coef_fit : bool = False , track : bool = False ) -> Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]]: \"\"\" This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. !!! note Background vectors are fitted first and then not updated again. Args: pixel_colors: `float [n_pixels x n_rounds x n_channels]`. Pixel colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x n_rounds x n_channels]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. background_shift: When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`, this is applied to normalisation of `pixel_colors` to limit boost of weak pixels. dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added at each iteration. alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added. beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`. max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. weight_coef_fit: If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. track: If `True` and one pixel, info about genes added at each step returned. Returns: gene_coefs - `float32 [n_pixels x n_genes]`. `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero. background_coefs - `float32 [n_pixels x n_channels]`. coefficient value for each background vector found for each pixel. track_info - dictionary containing info about genes added at each step returned if `track == True` - - `background_codes` - `float [n_channels x n_rounds x n_channels]`. `background_codes[c]` is the background vector for channel `c` with L2 norm of 1. - `background_coefs` - `float [n_channels]`. `background_coefs[c]` is the coefficient value for `background_codes[c]`. - `gene_added` - `int [n_genes_added + 2]`. `gene_added[0]` and `gene_added[1]` are -1. `gene_added[2+i]` is the `ith` gene that was added. - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `residual[0]` is the initial `pixel_color`. `residual[1]` is the post background `pixel_color`. `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`. - `coef` - `float [(n_genes_added + 2) x n_genes]`. `coef[0]` and `coef[1]` are all 0. `coef[2+i]` are the coefficients for all genes after the ith gene has been added. - `dot_product` - `float [n_genes_added + 2]`. `dot_product[0]` and `dot_product[1]` are 0. `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`. - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `inverse_var[0]` and `inverse_var[1]` are all 0. `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`, which down-weights rounds/channels for which a gene has already been fitted. \"\"\" n_pixels = pixel_colors . shape [ 0 ] n_genes , n_rounds , n_channels = bled_codes . shape no_verbose = n_pixels < 1000 # show progress bar with more than 1000 pixels. if track : return_track = True if n_pixels == 1 : track_info = { 'residual' : np . zeros (( max_genes + 3 , n_rounds , n_channels )), 'background_codes' : None , 'background_coefs' : None , 'inverse_var' : np . zeros (( max_genes + 3 , n_rounds , n_channels )), 'coef' : np . zeros (( max_genes + 3 , n_genes + n_channels )), 'dot_product' : np . zeros ( max_genes + 3 ), 'gene_added' : np . ones ( max_genes + 3 , dtype = int ) * - 1 } track_info [ 'residual' ][ 0 ] = pixel_colors [ 0 ] else : warnings . warn ( f 'Can only get track info if running on one pixel, but there are { n_pixels } pixels ' f 'so not getting track info.' ) track = False track_info = None else : return_track = False # Fit background and override initial pixel_colors gene_coefs = np . zeros (( n_pixels , n_genes ), dtype = np . float32 ) # coefs of all genes and background pixel_colors , background_coefs , background_codes = fit_background ( pixel_colors , background_shift ) if track : track_info [ 'residual' ][ 1 ] = pixel_colors [ 0 ] track_info [ 'background_codes' ] = background_codes track_info [ 'background_coefs' ] = background_coefs [ 0 ] background_genes = np . arange ( n_genes , n_genes + n_channels ) # colors and codes for get_best_gene function # Includes background as if background is the best gene, iteration ends. # uses residual color as used to find next gene to add. bled_codes = bled_codes . reshape (( n_genes , - 1 )) all_codes = np . concatenate (( bled_codes , background_codes . reshape ( n_channels , - 1 ))) bled_codes = bled_codes . transpose () # colors and codes for fit_coefs function (No background as this is not updated again). # always uses post background color as coefficients for all genes re-estimated at each iteration. pixel_colors = pixel_colors . reshape (( n_pixels , - 1 )) continue_pixels = np . arange ( n_pixels ) with tqdm ( total = max_genes , disable = no_verbose ) as pbar : pbar . set_description ( 'Finding OMP coefficients for each pixel' ) for i in range ( max_genes ): if i == 0 : # Background coefs don't change, hence contribution to variance won't either. added_genes , pass_score_thresh , background_variance , best_score = \\ get_best_gene_first_iter ( pixel_colors , all_codes , background_coefs , dp_shift , dp_thresh , alpha , beta , background_genes ) inverse_var = 1 / background_variance pixel_colors = pixel_colors . transpose () else : # only continue with pixels for which dot product score exceeds threshold i_added_genes , pass_score_thresh , inverse_var , best_score = \\ get_best_gene ( residual_pixel_colors , all_codes , i_coefs , added_genes , dp_shift , dp_thresh , alpha , background_genes , background_variance ) # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding. fail_score_thresh = np . invert ( pass_score_thresh ) gene_coefs [ np . asarray ( continue_pixels [ fail_score_thresh ])[:, np . newaxis ], np . asarray ( added_genes [ fail_score_thresh ])] = i_coefs [ fail_score_thresh ] continue_pixels = continue_pixels [ pass_score_thresh ] n_continue = np . size ( continue_pixels ) pbar . set_postfix ({ 'n_pixels' : n_continue }) if n_continue == 0 : if track : track_info [ 'inverse_var' ][ i + 2 ] = inverse_var . reshape ( n_rounds , n_channels ) track_info [ 'dot_product' ][ i + 2 ] = best_score [ 0 ] if i == 0 : track_info [ 'gene_added' ][ i + 2 ] = added_genes else : track_info [ 'gene_added' ][ i + 2 ] = i_added_genes added_genes_fail = np . hstack (( added_genes , i_added_genes [:, np . newaxis ])) # Need to usee all_codes here to deal with case where the best gene is background if weight_coef_fit : residual_pixel_colors_fail , i_coefs_fail = \\ fit_coefs_weight ( all_codes . T , pixel_colors , added_genes_fail , np . sqrt ( inverse_var )) else : residual_pixel_colors_fail , i_coefs_fail = fit_coefs ( all_codes . T , pixel_colors , added_genes_fail ) track_info [ 'residual' ][ i + 2 ] = residual_pixel_colors_fail . reshape ( n_rounds , n_channels ) track_info [ 'coef' ][ i + 2 ][ added_genes_fail ] = i_coefs_fail # Only save info where gene is actually added or for final case where not added. for key in track_info . keys (): if 'background' not in key : track_info [ key ] = track_info [ key ][: i + 3 ] break if i == 0 : added_genes = added_genes [ pass_score_thresh , np . newaxis ] else : added_genes = np . hstack (( added_genes [ pass_score_thresh ], i_added_genes [ pass_score_thresh , np . newaxis ])) pixel_colors = pixel_colors [:, pass_score_thresh ] background_variance = background_variance [ pass_score_thresh ] inverse_var = inverse_var [ pass_score_thresh ] if weight_coef_fit : residual_pixel_colors , i_coefs = fit_coefs_weight ( bled_codes , pixel_colors , added_genes , np . sqrt ( inverse_var )) else : residual_pixel_colors , i_coefs = fit_coefs ( bled_codes , pixel_colors , added_genes ) if i == max_genes - 1 : # Add pixels to final gene_coefs when reach end of iteration. gene_coefs [ continue_pixels [:, np . newaxis ], added_genes ] = i_coefs if track : track_info [ 'residual' ][ i + 2 ] = residual_pixel_colors . reshape ( n_rounds , n_channels ) track_info [ 'inverse_var' ][ i + 2 ] = inverse_var . reshape ( n_rounds , n_channels ) track_info [ 'coef' ][ i + 2 ][ added_genes ] = i_coefs track_info [ 'dot_product' ][ i + 2 ] = best_score [ 0 ] track_info [ 'gene_added' ][ i + 2 ] = added_genes [ 0 ][ - 1 ] pbar . update ( 1 ) pbar . close () if track : # Only return no_gene_add_ind = np . where ( track_info [ 'gene_added' ] == - 1 )[ 0 ] no_gene_add_ind = no_gene_add_ind [ no_gene_add_ind >= 2 ] if len ( no_gene_add_ind ) > 0 : final_ind = no_gene_add_ind . min () if return_track : return gene_coefs . astype ( np . float32 ), background_coefs . astype ( np . float32 ), track_info else : return gene_coefs . astype ( np . float32 ), background_coefs . astype ( np . float32 ) get_best_gene ( residual_pixel_colors , all_bled_codes , coefs , genes_added , norm_shift , score_thresh , alpha , background_genes , background_var ) Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes , already in genes_added[s] or best_score[s] < score_thresh , then pass_score_thresh[s] = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs np . ndarray float [n_pixels x n_genes_added] . coefs[s, g] is the weighting of pixel s for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added np . array int [n_pixels x n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes np . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required background_var np . array float [n_pixels x (n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . np . ndarray inverse_var - float [n_pixels x (n_rounds x n_channels)] . Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_best_gene ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , coefs : np . ndarray , genes_added : np . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : np . ndarray , background_var : np . array ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] < score_thresh`, then `pass_score_thresh[s] = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_pixels x n_genes_added]`. `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_pixels x n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. background_var: `float [n_pixels x (n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" n_pixels , n_genes_added = genes_added . shape n_genes = all_bled_codes . shape [ 0 ] coefs_all = np . zeros (( n_pixels , n_genes )) pixel_ind = np . repeat ( np . arange ( n_pixels ), n_genes_added ) coefs_all [( pixel_ind , genes_added . flatten ())] = coefs . flatten () inverse_var = 1 / ( coefs_all ** 2 @ all_bled_codes ** 2 * alpha + background_var ) ignore_genes = np . concatenate (( genes_added , np . tile ( background_genes , [ n_pixels , 1 ])), axis = 1 ) best_gene , pass_score_thresh , best_score = \\ get_best_gene_base ( residual_pixel_colors , all_bled_codes , norm_shift , score_thresh , inverse_var , ignore_genes ) return best_gene , pass_score_thresh , inverse_var , best_score get_best_gene_base ( residual_pixel_colors , all_bled_codes , norm_shift , score_thresh , inverse_var , ignore_genes ) Computes the dot_product_score between residual_pixel_color and each code in all_bled_codes . If best_score is less than score_thresh or if the corresponding best_gene is in ignore_genes , then pass_score_thresh will be False. Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required inverse_var np . ndarray float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Used as weight_squared when computing dot_product_score . required ignore_genes np . ndarray int [n_pixels x n_genes_ignore] . If best_gene[s] is one of these, pass_score_thresh[s] will be False . required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score[s] > score_thresh and best_gene[s] not in ignore_genes . np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_best_gene_base ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , norm_shift : float , score_thresh : float , inverse_var : np . ndarray , ignore_genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`. If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`, then `pass_score_thresh` will be False. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. inverse_var: `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as `weight_squared` when computing `dot_product_score`. ignore_genes: `int [n_pixels x n_genes_ignore]`. If `best_gene[s]` is one of these, `pass_score_thresh[s]` will be `False`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score[s] > score_thresh` and `best_gene[s]` not in `ignore_genes`. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" # calculate score including background genes as if best gene is background, then stop iteration. all_scores = dot_product_score ( residual_pixel_colors , all_bled_codes , norm_shift , inverse_var ) best_gene = np . argmax ( np . abs ( all_scores ), axis = 1 ) # if best_gene is in ignore_gene, set score below score_thresh. is_ignore_gene = ( best_gene [:, np . newaxis ] == ignore_genes ) . any ( axis = 1 ) best_score = all_scores [( np . arange ( best_gene . shape [ 0 ]), best_gene )] * np . invert ( is_ignore_gene ) pass_score_thresh = np . abs ( best_score ) > score_thresh return best_gene , pass_score_thresh , best_score get_best_gene_first_iter ( residual_pixel_colors , all_bled_codes , background_coefs , norm_shift , score_thresh , alpha , beta , background_genes ) Finds the best_gene to add next based on the dot product score with each bled_code . If best_gene is in background_genes or best_score < score_thresh then pass_score_thresh = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs np . ndarray float [n_pixels x n_channels] . coefs[g] is the weighting for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes np . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . np . ndarray background_var - float [n_pixels x (n_rounds x n_channels)] . Variance in each round/channel based on just the background. np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def get_best_gene_first_iter ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , background_coefs : np . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the `best_gene` to add next based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes` or `best_score < score_thresh` then `pass_score_thresh = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_pixels x n_channels]`. `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - background_var - `float [n_pixels x (n_rounds x n_channels)]`. Variance in each round/channel based on just the background. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" background_var = np . square ( background_coefs ) @ np . square ( all_bled_codes [ background_genes ]) * alpha + beta ** 2 ignore_genes = np . tile ( background_genes , [ background_var . shape [ 0 ], 1 ]) best_gene , pass_score_thresh , best_score = \\ get_best_gene_base ( residual_pixel_colors , all_bled_codes , norm_shift , score_thresh , 1 / background_var , ignore_genes ) return best_gene , pass_score_thresh , background_var , best_score Optimised fit_coefs ( bled_codes , pixel_colors , genes ) This finds the least squared solution for how the n_genes_add bled_codes indicated by genes[s] can best explain pixel_colors[:, s] for each pixel s. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors jnp . ndarray float [(n_rounds x n_channels) x n_pixels] . Flattened then transposed pixel_colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes jnp . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required Returns: Type Description jnp . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_pixels x n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @jax . jit def fit_coefs ( bled_codes : jnp . ndarray , pixel_colors : jnp . ndarray , genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]` can best explain `pixel_colors[:, s]` for each pixel s. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`. Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_pixels x n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" return jax . vmap ( fit_coefs_single , in_axes = ( None , 1 , 0 ), out_axes = ( 0 , 0 ))( bled_codes , pixel_colors , genes ) fit_coefs_single ( bled_codes , pixel_color , genes ) This finds the least squared solution for how the n_genes_add bled_codes indicated by genes can best explain pixel_color . Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Flattened pixel_color which usually has the shape [n_rounds x n_channels] . required genes jnp . ndarray int [n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain pixel_color. required Returns: Type Description jnp . ndarray residual - float [(n_rounds x n_channels)] . Residual pixel_color after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def fit_coefs_single ( bled_codes : jnp . ndarray , pixel_color : jnp . ndarray , genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes` can best explain `pixel_color`. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_color: `float [(n_rounds x n_channels)]`. Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`. genes: `int [n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain pixel_color. Returns: - residual - `float [(n_rounds x n_channels)]`. Residual pixel_color after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" coefs = jnp . linalg . lstsq ( bled_codes [:, genes ], pixel_color )[ 0 ] residual = pixel_color - jnp . matmul ( bled_codes [:, genes ], coefs ) return residual , coefs fit_coefs_weight ( bled_codes , pixel_colors , genes , weight ) This finds the weighted least squared solution for how the n_genes_add bled_codes indicated by genes[s] can best explain pixel_colors[:, s] for each pixel s. The weight indicates which rounds/channels should have more influence when finding the coefficients of each gene. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors jnp . ndarray float [(n_rounds x n_channels) x n_pixels] . Flattened then transposed pixel_colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes jnp . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required weight jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . weight[s, i] is the weight to be applied to round_channel i when computing coefficient of each bled_code for pixel s . required Returns: Type Description jnp . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_pixels x n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 @jax . jit def fit_coefs_weight ( bled_codes : jnp . ndarray , pixel_colors : jnp . ndarray , genes : jnp . ndarray , weight : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]` can best explain `pixel_colors[:, s]` for each pixel s. The `weight` indicates which rounds/channels should have more influence when finding the coefficients of each gene. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`. Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. weight: `float [n_pixels x (n_rounds x n_channels)]`. `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code` for pixel `s`. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_pixels x n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" return jax . vmap ( fit_coefs_weight_single , in_axes = ( None , 1 , 0 , 0 ), out_axes = ( 0 , 0 ))( bled_codes , pixel_colors , genes , weight ) fit_coefs_weight_single ( bled_codes , pixel_color , genes , weight ) This finds the weighted least squared solution for how the n_genes_add bled_codes indicated by genes can best explain pixel_color . The weight indicates which rounds/channels should have more influence when finding the coefficients of each gene. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Flattened pixel_color which usually has the shape [n_rounds x n_channels] . required genes jnp . ndarray int [n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain pixel_color. required weight jnp . ndarray float [(n_rounds x n_channels)] . weight[i] is the weight to be applied to round_channel i when computing coefficient of each bled_code . required Returns: Type Description jnp . ndarray residual - float [(n_rounds x n_channels)] . Residual pixel_color after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def fit_coefs_weight_single ( bled_codes : jnp . ndarray , pixel_color : jnp . ndarray , genes : jnp . ndarray , weight : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes` can best explain `pixel_color`. The `weight` indicates which rounds/channels should have more influence when finding the coefficients of each gene. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_color: `float [(n_rounds x n_channels)]`. Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`. genes: `int [n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain pixel_color. weight: `float [(n_rounds x n_channels)]`. `weight[i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code`. Returns: - residual - `float [(n_rounds x n_channels)]`. Residual pixel_color after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" coefs = jnp . linalg . lstsq ( bled_codes [:, genes ] * weight [:, jnp . newaxis ], pixel_color * weight )[ 0 ] residual = pixel_color * weight - jnp . matmul ( bled_codes [:, genes ] * weight [:, jnp . newaxis ], coefs ) return residual / weight , coefs get_all_coefs ( pixel_colors , bled_codes , background_shift , dp_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit = False ) This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. Note Background vectors are fitted first and then not updated again. Parameters: Name Type Description Default pixel_colors jnp . ndarray float [n_pixels x n_rounds x n_channels] . Pixel colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required background_shift float When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. required dp_shift float When finding dot_product_score between residual pixel_colors and bled_codes , this is applied to normalisation of pixel_colors to limit boost of weak pixels. required dp_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added at each iteration. required alpha float Used for fitting_standard_deviation , by how much to increase variance as genes added. required beta float Used for fitting_standard_deviation , the variance with no genes added ( coef=0 ) is beta**2 . required max_genes int Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. required weight_coef_fit bool If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. False Returns: Type Description np . ndarray gene_coefs - float32 [n_pixels x n_genes] . gene_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero. np . ndarray background_coefs - float32 [n_pixels x n_channels] . coefficient value for each background vector found for each pixel. Source code in coppafish/omp/coefs_optimised.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def get_all_coefs ( pixel_colors : jnp . ndarray , bled_codes : jnp . ndarray , background_shift : float , dp_shift : float , dp_thresh : float , alpha : float , beta : float , max_genes : int , weight_coef_fit : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. !!! note Background vectors are fitted first and then not updated again. Args: pixel_colors: `float [n_pixels x n_rounds x n_channels]`. Pixel colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x n_rounds x n_channels]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. background_shift: When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`, this is applied to normalisation of `pixel_colors` to limit boost of weak pixels. dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added at each iteration. alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added. beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`. max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. weight_coef_fit: If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. Returns: - gene_coefs - `float32 [n_pixels x n_genes]`. `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero. - background_coefs - `float32 [n_pixels x n_channels]`. coefficient value for each background vector found for each pixel. \"\"\" n_pixels = pixel_colors . shape [ 0 ] check_spot = np . random . randint ( n_pixels ) diff_to_int = jnp . round ( pixel_colors [ check_spot ]) . astype ( int ) - pixel_colors [ check_spot ] if jnp . abs ( diff_to_int ) . max () == 0 : raise ValueError ( f \"pixel_coefs should be found using normalised pixel_colors.\" f \" \\n But for pixel { check_spot } , pixel_colors given are integers indicating they are \" f \"the raw intensities.\" ) n_genes , n_rounds , n_channels = bled_codes . shape if not utils . errors . check_shape ( pixel_colors , [ n_pixels , n_rounds , n_channels ]): raise utils . errors . ShapeError ( 'pixel_colors' , pixel_colors . shape , ( n_pixels , n_rounds , n_channels )) no_verbose = n_pixels < 1000 # show progress bar with more than 1000 pixels. # Fit background and override initial pixel_colors gene_coefs = np . zeros (( n_pixels , n_genes ), dtype = np . float32 ) # coefs of all genes and background pixel_colors , background_coefs , background_codes = fit_background ( pixel_colors , background_shift ) background_genes = jnp . arange ( n_genes , n_genes + n_channels ) # colors and codes for get_best_gene function # Includes background as if background is the best gene, iteration ends. # uses residual color as used to find next gene to add. bled_codes = bled_codes . reshape (( n_genes , - 1 )) all_codes = jnp . concatenate (( bled_codes , background_codes . reshape ( n_channels , - 1 ))) bled_codes = bled_codes . transpose () # colors and codes for fit_coefs function (No background as this is not updated again). # always uses post background color as coefficients for all genes re-estimated at each iteration. pixel_colors = pixel_colors . reshape (( n_pixels , - 1 )) continue_pixels = jnp . arange ( n_pixels ) with tqdm ( total = max_genes , disable = no_verbose ) as pbar : pbar . set_description ( 'Finding OMP coefficients for each pixel' ) for i in range ( max_genes ): if i == 0 : # Background coefs don't change, hence contribution to variance won't either. added_genes , pass_score_thresh , background_variance = \\ get_best_gene_first_iter ( pixel_colors , all_codes , background_coefs , dp_shift , dp_thresh , alpha , beta , background_genes ) inverse_var = 1 / background_variance pixel_colors = pixel_colors . transpose () else : # only continue with pixels for which dot product score exceeds threshold i_added_genes , pass_score_thresh , inverse_var = \\ get_best_gene ( residual_pixel_colors , all_codes , i_coefs , added_genes , dp_shift , dp_thresh , alpha , background_genes , background_variance ) # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding. fail_score_thresh = jnp . invert ( pass_score_thresh ) # gene_coefs[np.asarray(continue_pixels[fail_score_thresh])] = np.asarray(i_coefs[fail_score_thresh]) gene_coefs [ np . asarray ( continue_pixels [ fail_score_thresh ])[:, np . newaxis ], np . asarray ( added_genes [ fail_score_thresh ])] = np . asarray ( i_coefs [ fail_score_thresh ]) continue_pixels = continue_pixels [ pass_score_thresh ] n_continue = jnp . size ( continue_pixels ) pbar . set_postfix ({ 'n_pixels' : n_continue }) if n_continue == 0 : break if i == 0 : added_genes = added_genes [ pass_score_thresh , np . newaxis ] else : added_genes = jnp . hstack (( added_genes [ pass_score_thresh ], i_added_genes [ pass_score_thresh , jnp . newaxis ])) pixel_colors = pixel_colors [:, pass_score_thresh ] background_variance = background_variance [ pass_score_thresh ] inverse_var = inverse_var [ pass_score_thresh ] # Maybe add different fit_coefs for i==0 i.e. can do multiple pixels at once for same gene added. if weight_coef_fit : residual_pixel_colors , i_coefs = fit_coefs_weight ( bled_codes , pixel_colors , added_genes , jnp . sqrt ( inverse_var )) else : residual_pixel_colors , i_coefs = fit_coefs ( bled_codes , pixel_colors , added_genes ) if i == max_genes - 1 : # Add pixels to final gene_coefs when reach end of iteration. gene_coefs [ np . asarray ( continue_pixels )[:, np . newaxis ], np . asarray ( added_genes )] = np . asarray ( i_coefs ) pbar . update ( 1 ) pbar . close () return gene_coefs . astype ( np . float32 ), np . asarray ( background_coefs ) . astype ( np . float32 ) get_best_gene ( residual_pixel_colors , all_bled_codes , coefs , genes_added , norm_shift , score_thresh , alpha , background_genes , background_var ) Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes , already in genes_added[s] or best_score[s] < score_thresh , then pass_score_thresh[s] = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_colors jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs jnp . ndarray float [n_pixels x n_genes_added] . coefs[s, g] is the weighting of pixel s for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added jnp . array int [n_pixels x n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required background_var jnp . array float [n_pixels x (n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description jnp . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. jnp . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . jnp . ndarray inverse_var - float [n_pixels x (n_rounds x n_channels)] . Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. Source code in coppafish/omp/coefs_optimised.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @partial ( jax . jit , static_argnums = ( 4 , 5 , 6 )) def get_best_gene ( residual_pixel_colors : jnp . ndarray , all_bled_codes : jnp . ndarray , coefs : jnp . ndarray , genes_added : jnp . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : jnp . ndarray , background_var : jnp . array ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] < score_thresh`, then `pass_score_thresh[s] = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_pixels x n_genes_added]`. `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_pixels x n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. background_var: `float [n_pixels x (n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. \"\"\" return jax . vmap ( get_best_gene_single , in_axes = ( 0 , None , 0 , 0 , None , None , None , None , 0 ), out_axes = ( 0 , 0 , 0 ))( residual_pixel_colors , all_bled_codes , coefs , genes_added , norm_shift , score_thresh , alpha , background_genes , background_var ) get_best_gene_base ( residual_pixel_color , all_bled_codes , norm_shift , score_thresh , inverse_var , ignore_genes ) Computes the dot_product_score between residual_pixel_color and each code in all_bled_codes . If best_score is less than score_thresh or if the corresponding best_gene is in ignore_genes , then pass_score_thresh will be False. Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required inverse_var jnp . ndarray float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Used as weight_squared when computing dot_product_score . required ignore_genes jnp . ndarray int [n_genes_ignore] . If best_gene is one of these, pass_score_thresh will be False . required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh and best_gene not in ignore_genes . Source code in coppafish/omp/coefs_optimised.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_best_gene_base ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , norm_shift : float , score_thresh : float , inverse_var : jnp . ndarray , ignore_genes : jnp . ndarray ) -> Tuple [ int , bool ]: \"\"\" Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`. If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`, then `pass_score_thresh` will be False. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. inverse_var: `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as `weight_squared` when computing `dot_product_score`. ignore_genes: `int [n_genes_ignore]`. If `best_gene` is one of these, `pass_score_thresh` will be `False`. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh` and `best_gene` not in `ignore_genes`. \"\"\" # calculate score including background genes as if best gene is background, then stop iteration. all_scores = dot_product_score_single ( residual_pixel_color , all_bled_codes , norm_shift , inverse_var ) best_gene = jnp . argmax ( jnp . abs ( all_scores )) # if best_gene is background, set score below score_thresh. best_score = all_scores [ best_gene ] * jnp . isin ( best_gene , ignore_genes , invert = True ) pass_score_thresh = jnp . abs ( best_score ) > score_thresh return best_gene , pass_score_thresh get_best_gene_first_iter ( residual_pixel_colors , all_bled_codes , background_coefs , norm_shift , score_thresh , alpha , beta , background_genes ) Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes or best_score[s] < score_thresh then pass_score_thresh[s] = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_colors jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs jnp . ndarray float [n_pixels x n_channels] . coefs[s, g] is the weighting of pixel s for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description jnp . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. jnp . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . jnp . ndarray background_var - float [n_pixels x (n_rounds x n_channels)] . Variance of each pixel in each round/channel based on just the background. Source code in coppafish/omp/coefs_optimised.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @partial ( jax . jit , static_argnums = ( 3 , 4 , 5 , 6 )) def get_best_gene_first_iter ( residual_pixel_colors : jnp . ndarray , all_bled_codes : jnp . ndarray , background_coefs : jnp . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes` or `best_score[s] < score_thresh` then `pass_score_thresh[s] = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_pixels x n_channels]`. `coefs[s, g]` is the weighting of pixel `s` for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - background_var - `float [n_pixels x (n_rounds x n_channels)]`. Variance of each pixel in each round/channel based on just the background. \"\"\" return jax . vmap ( get_best_gene_first_iter_single , in_axes = ( 0 , None , 0 , None , None , None , None , None ), out_axes = ( 0 , 0 , 0 ))( residual_pixel_colors , all_bled_codes , background_coefs , norm_shift , score_thresh , alpha , beta , background_genes ) get_best_gene_first_iter_single ( residual_pixel_color , all_bled_codes , background_coefs , norm_shift , score_thresh , alpha , beta , background_genes ) Finds the best_gene to add next based on the dot product score with each bled_code . If best_gene is in background_genes or best_score < score_thresh then pass_score_thresh = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs jnp . ndarray float [n_channels] . coefs[g] is the weighting for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh . jnp . ndarray background_var - float [(n_rounds x n_channels)] . Variance in each round/channel based on just the background. Source code in coppafish/omp/coefs_optimised.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def get_best_gene_first_iter_single ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , background_coefs : jnp . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : jnp . ndarray ) -> Tuple [ int , bool , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes` or `best_score < score_thresh` then `pass_score_thresh = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_channels]`. `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh`. - background_var - `float [(n_rounds x n_channels)]`. Variance in each round/channel based on just the background. \"\"\" background_var = jnp . square ( background_coefs ) @ jnp . square ( all_bled_codes [ background_genes ]) * alpha + beta ** 2 best_gene , pass_score_thresh = get_best_gene_base ( residual_pixel_color , all_bled_codes , norm_shift , score_thresh , 1 / background_var , background_genes ) return best_gene , pass_score_thresh , background_var get_best_gene_single ( residual_pixel_color , all_bled_codes , coefs , genes_added , norm_shift , score_thresh , alpha , background_genes , background_var ) Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene is in background_genes , already in genes_added or best_score < score_thresh , then pass_score_thresh = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs jnp . ndarray float [n_genes_added] . coefs[g] is the weighting for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added jnp . array int [n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene is set to one of background_genes , pass_score_thresh will be False. required background_var jnp . array float [(n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh . jnp . ndarray inverse_var - float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. Source code in coppafish/omp/coefs_optimised.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def get_best_gene_single ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , coefs : jnp . ndarray , genes_added : jnp . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : jnp . ndarray , background_var : jnp . array ) -> Tuple [ int , bool , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes`, already in `genes_added` or `best_score < score_thresh`, then `pass_score_thresh = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_genes_added]`. `coefs[g]` is the weighting for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene is set to one of `background_genes`, `pass_score_thresh` will be False. background_var: `float [(n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh`. - inverse_var - `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. \"\"\" inverse_var = 1 / ( jnp . square ( coefs ) @ jnp . square ( all_bled_codes [ genes_added ]) * alpha + background_var ) # calculate score including background genes as if best gene is background, then stop iteration. best_gene , pass_score_thresh = get_best_gene_base ( residual_pixel_color , all_bled_codes , norm_shift , score_thresh , inverse_var , jnp . append ( background_genes , genes_added )) return best_gene , pass_score_thresh , inverse_var","title":"Coefficients"},{"location":"code/omp/coefs/#coppafish.omp.coefs.fit_coefs","text":"Old method before Jax. This finds the least squared solution for how the n_genes bled_codes can best explain each pixel_color . Can also find weighted least squared solution if weight provided. Parameters: Name Type Description Default bled_codes np . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors np . ndarray float [(n_rounds x n_channels) x n_pixels] if n_genes==1 otherwise float [(n_rounds x n_channels)] . Flattened then transposed pixel colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes np . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required Returns: Type Description np . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coef. np . ndarray coefs - float [n_pixels x n_genes_add] if n_genes == 1 otherwise float [n_genes] if n_pixels == 1. coefficient found through least squares fitting for each gene. Source code in coppafish/omp/coefs.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def fit_coefs ( bled_codes : np . ndarray , pixel_colors : np . ndarray , genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Old method before Jax. This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`. Can also find weighted least squared solution if `weight` provided. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1` otherwise `float [(n_rounds x n_channels)]`. Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coef. - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1. coefficient found through least squares fitting for each gene. \"\"\" n_pixels = pixel_colors . shape [ 1 ] residual = np . zeros (( n_pixels , pixel_colors . shape [ 0 ])) coefs = np . zeros_like ( genes , dtype = float ) for s in range ( n_pixels ): coefs [ s ] = np . linalg . lstsq ( bled_codes [:, genes [ s ]], pixel_colors [:, s ], rcond = None )[ 0 ] residual [ s ] = pixel_colors [:, s ] - bled_codes [:, genes [ s ]] @ coefs [ s ] return residual , coefs","title":"fit_coefs()"},{"location":"code/omp/coefs/#coppafish.omp.coefs.fit_coefs_weight","text":"Old method before Jax. This finds the least squared solution for how the n_genes bled_codes can best explain each pixel_color . Can also find weighted least squared solution if weight provided. Parameters: Name Type Description Default bled_codes np . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors np . ndarray float [(n_rounds x n_channels) x n_pixels] if n_genes==1 otherwise float [(n_rounds x n_channels)] . Flattened then transposed pixel colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes np . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required weight np . ndarray float [n_pixels x (n_rounds x n_channels)] . weight[s, i] is the weight to be applied to round_channel i when computing coefficient of each bled_code for pixel s . required Returns: Type Description np . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coef. np . ndarray coefs - float [n_pixels x n_genes_add] if n_genes == 1 otherwise float [n_genes] if n_pixels == 1. coefficient found through least squares fitting for each gene. Source code in coppafish/omp/coefs.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def fit_coefs_weight ( bled_codes : np . ndarray , pixel_colors : np . ndarray , genes : np . ndarray , weight : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Old method before Jax. This finds the least squared solution for how the `n_genes` `bled_codes` can best explain each `pixel_color`. Can also find weighted least squared solution if `weight` provided. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]` if `n_genes==1` otherwise `float [(n_rounds x n_channels)]`. Flattened then transposed pixel colors which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. weight: `float [n_pixels x (n_rounds x n_channels)]`. `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code` for pixel `s`. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coef. - coefs - `float [n_pixels x n_genes_add]` if n_genes == 1 otherwise `float [n_genes]` if n_pixels == 1. coefficient found through least squares fitting for each gene. \"\"\" n_pixels = pixel_colors . shape [ 1 ] residual = np . zeros (( n_pixels , pixel_colors . shape [ 0 ])) coefs = np . zeros_like ( genes , dtype = float ) pixel_colors = pixel_colors * weight . transpose () for s in range ( n_pixels ): bled_codes_s = bled_codes [:, genes [ s ]] * weight [ s ][:, np . newaxis ] coefs [ s ] = np . linalg . lstsq ( bled_codes_s , pixel_colors [:, s ], rcond = None )[ 0 ] residual [ s ] = pixel_colors [:, s ] - bled_codes_s @ coefs [ s ] residual = residual / weight return residual , coefs","title":"fit_coefs_weight()"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_all_coefs","text":"This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. Note Background vectors are fitted first and then not updated again. Parameters: Name Type Description Default pixel_colors np . ndarray float [n_pixels x n_rounds x n_channels] . Pixel colors normalised to equalise intensities between channels (and rounds). required bled_codes np . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required background_shift float When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. required dp_shift float When finding dot_product_score between residual pixel_colors and bled_codes , this is applied to normalisation of pixel_colors to limit boost of weak pixels. required dp_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added at each iteration. required alpha float Used for fitting_standard_deviation , by how much to increase variance as genes added. required beta float Used for fitting_standard_deviation , the variance with no genes added ( coef=0 ) is beta**2 . required max_genes int Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. required weight_coef_fit bool If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. False track bool If True and one pixel, info about genes added at each step returned. False Returns: Type Description Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] gene_coefs - float32 [n_pixels x n_genes] . gene_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] background_coefs - float32 [n_pixels x n_channels] . coefficient value for each background vector found for each pixel. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]] track_info - dictionary containing info about genes added at each step returned if track == True - background_codes - float [n_channels x n_rounds x n_channels] . background_codes[c] is the background vector for channel c with L2 norm of 1. background_coefs - float [n_channels] . background_coefs[c] is the coefficient value for background_codes[c] . gene_added - int [n_genes_added + 2] . gene_added[0] and gene_added[1] are -1. gene_added[2+i] is the ith gene that was added. residual - float [(n_genes_added + 2) x n_rounds x n_channels] . residual[0] is the initial pixel_color . residual[1] is the post background pixel_color . residual[2+i] is the pixel_color after removing gene gene_added[2+i] . coef - float [(n_genes_added + 2) x n_genes] . coef[0] and coef[1] are all 0. coef[2+i] are the coefficients for all genes after the ith gene has been added. dot_product - float [n_genes_added + 2] . dot_product[0] and dot_product[1] are 0. dot_product[2+i] is the dot product for the gene gene_added[2+i] . inverse_var - float [(n_genes_added + 2) x n_rounds x n_channels] . inverse_var[0] and inverse_var[1] are all 0. inverse_var[2+i] is the weighting used to compute dot_product[2+i] , which down-weights rounds/channels for which a gene has already been fitted. Source code in coppafish/omp/coefs.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def get_all_coefs ( pixel_colors : np . ndarray , bled_codes : np . ndarray , background_shift : float , dp_shift : float , dp_thresh : float , alpha : float , beta : float , max_genes : int , weight_coef_fit : bool = False , track : bool = False ) -> Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , dict ]]: \"\"\" This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. !!! note Background vectors are fitted first and then not updated again. Args: pixel_colors: `float [n_pixels x n_rounds x n_channels]`. Pixel colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x n_rounds x n_channels]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. background_shift: When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`, this is applied to normalisation of `pixel_colors` to limit boost of weak pixels. dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added at each iteration. alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added. beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`. max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. weight_coef_fit: If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. track: If `True` and one pixel, info about genes added at each step returned. Returns: gene_coefs - `float32 [n_pixels x n_genes]`. `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero. background_coefs - `float32 [n_pixels x n_channels]`. coefficient value for each background vector found for each pixel. track_info - dictionary containing info about genes added at each step returned if `track == True` - - `background_codes` - `float [n_channels x n_rounds x n_channels]`. `background_codes[c]` is the background vector for channel `c` with L2 norm of 1. - `background_coefs` - `float [n_channels]`. `background_coefs[c]` is the coefficient value for `background_codes[c]`. - `gene_added` - `int [n_genes_added + 2]`. `gene_added[0]` and `gene_added[1]` are -1. `gene_added[2+i]` is the `ith` gene that was added. - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `residual[0]` is the initial `pixel_color`. `residual[1]` is the post background `pixel_color`. `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`. - `coef` - `float [(n_genes_added + 2) x n_genes]`. `coef[0]` and `coef[1]` are all 0. `coef[2+i]` are the coefficients for all genes after the ith gene has been added. - `dot_product` - `float [n_genes_added + 2]`. `dot_product[0]` and `dot_product[1]` are 0. `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`. - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `inverse_var[0]` and `inverse_var[1]` are all 0. `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`, which down-weights rounds/channels for which a gene has already been fitted. \"\"\" n_pixels = pixel_colors . shape [ 0 ] n_genes , n_rounds , n_channels = bled_codes . shape no_verbose = n_pixels < 1000 # show progress bar with more than 1000 pixels. if track : return_track = True if n_pixels == 1 : track_info = { 'residual' : np . zeros (( max_genes + 3 , n_rounds , n_channels )), 'background_codes' : None , 'background_coefs' : None , 'inverse_var' : np . zeros (( max_genes + 3 , n_rounds , n_channels )), 'coef' : np . zeros (( max_genes + 3 , n_genes + n_channels )), 'dot_product' : np . zeros ( max_genes + 3 ), 'gene_added' : np . ones ( max_genes + 3 , dtype = int ) * - 1 } track_info [ 'residual' ][ 0 ] = pixel_colors [ 0 ] else : warnings . warn ( f 'Can only get track info if running on one pixel, but there are { n_pixels } pixels ' f 'so not getting track info.' ) track = False track_info = None else : return_track = False # Fit background and override initial pixel_colors gene_coefs = np . zeros (( n_pixels , n_genes ), dtype = np . float32 ) # coefs of all genes and background pixel_colors , background_coefs , background_codes = fit_background ( pixel_colors , background_shift ) if track : track_info [ 'residual' ][ 1 ] = pixel_colors [ 0 ] track_info [ 'background_codes' ] = background_codes track_info [ 'background_coefs' ] = background_coefs [ 0 ] background_genes = np . arange ( n_genes , n_genes + n_channels ) # colors and codes for get_best_gene function # Includes background as if background is the best gene, iteration ends. # uses residual color as used to find next gene to add. bled_codes = bled_codes . reshape (( n_genes , - 1 )) all_codes = np . concatenate (( bled_codes , background_codes . reshape ( n_channels , - 1 ))) bled_codes = bled_codes . transpose () # colors and codes for fit_coefs function (No background as this is not updated again). # always uses post background color as coefficients for all genes re-estimated at each iteration. pixel_colors = pixel_colors . reshape (( n_pixels , - 1 )) continue_pixels = np . arange ( n_pixels ) with tqdm ( total = max_genes , disable = no_verbose ) as pbar : pbar . set_description ( 'Finding OMP coefficients for each pixel' ) for i in range ( max_genes ): if i == 0 : # Background coefs don't change, hence contribution to variance won't either. added_genes , pass_score_thresh , background_variance , best_score = \\ get_best_gene_first_iter ( pixel_colors , all_codes , background_coefs , dp_shift , dp_thresh , alpha , beta , background_genes ) inverse_var = 1 / background_variance pixel_colors = pixel_colors . transpose () else : # only continue with pixels for which dot product score exceeds threshold i_added_genes , pass_score_thresh , inverse_var , best_score = \\ get_best_gene ( residual_pixel_colors , all_codes , i_coefs , added_genes , dp_shift , dp_thresh , alpha , background_genes , background_variance ) # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding. fail_score_thresh = np . invert ( pass_score_thresh ) gene_coefs [ np . asarray ( continue_pixels [ fail_score_thresh ])[:, np . newaxis ], np . asarray ( added_genes [ fail_score_thresh ])] = i_coefs [ fail_score_thresh ] continue_pixels = continue_pixels [ pass_score_thresh ] n_continue = np . size ( continue_pixels ) pbar . set_postfix ({ 'n_pixels' : n_continue }) if n_continue == 0 : if track : track_info [ 'inverse_var' ][ i + 2 ] = inverse_var . reshape ( n_rounds , n_channels ) track_info [ 'dot_product' ][ i + 2 ] = best_score [ 0 ] if i == 0 : track_info [ 'gene_added' ][ i + 2 ] = added_genes else : track_info [ 'gene_added' ][ i + 2 ] = i_added_genes added_genes_fail = np . hstack (( added_genes , i_added_genes [:, np . newaxis ])) # Need to usee all_codes here to deal with case where the best gene is background if weight_coef_fit : residual_pixel_colors_fail , i_coefs_fail = \\ fit_coefs_weight ( all_codes . T , pixel_colors , added_genes_fail , np . sqrt ( inverse_var )) else : residual_pixel_colors_fail , i_coefs_fail = fit_coefs ( all_codes . T , pixel_colors , added_genes_fail ) track_info [ 'residual' ][ i + 2 ] = residual_pixel_colors_fail . reshape ( n_rounds , n_channels ) track_info [ 'coef' ][ i + 2 ][ added_genes_fail ] = i_coefs_fail # Only save info where gene is actually added or for final case where not added. for key in track_info . keys (): if 'background' not in key : track_info [ key ] = track_info [ key ][: i + 3 ] break if i == 0 : added_genes = added_genes [ pass_score_thresh , np . newaxis ] else : added_genes = np . hstack (( added_genes [ pass_score_thresh ], i_added_genes [ pass_score_thresh , np . newaxis ])) pixel_colors = pixel_colors [:, pass_score_thresh ] background_variance = background_variance [ pass_score_thresh ] inverse_var = inverse_var [ pass_score_thresh ] if weight_coef_fit : residual_pixel_colors , i_coefs = fit_coefs_weight ( bled_codes , pixel_colors , added_genes , np . sqrt ( inverse_var )) else : residual_pixel_colors , i_coefs = fit_coefs ( bled_codes , pixel_colors , added_genes ) if i == max_genes - 1 : # Add pixels to final gene_coefs when reach end of iteration. gene_coefs [ continue_pixels [:, np . newaxis ], added_genes ] = i_coefs if track : track_info [ 'residual' ][ i + 2 ] = residual_pixel_colors . reshape ( n_rounds , n_channels ) track_info [ 'inverse_var' ][ i + 2 ] = inverse_var . reshape ( n_rounds , n_channels ) track_info [ 'coef' ][ i + 2 ][ added_genes ] = i_coefs track_info [ 'dot_product' ][ i + 2 ] = best_score [ 0 ] track_info [ 'gene_added' ][ i + 2 ] = added_genes [ 0 ][ - 1 ] pbar . update ( 1 ) pbar . close () if track : # Only return no_gene_add_ind = np . where ( track_info [ 'gene_added' ] == - 1 )[ 0 ] no_gene_add_ind = no_gene_add_ind [ no_gene_add_ind >= 2 ] if len ( no_gene_add_ind ) > 0 : final_ind = no_gene_add_ind . min () if return_track : return gene_coefs . astype ( np . float32 ), background_coefs . astype ( np . float32 ), track_info else : return gene_coefs . astype ( np . float32 ), background_coefs . astype ( np . float32 )","title":"get_all_coefs()"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene","text":"Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes , already in genes_added[s] or best_score[s] < score_thresh , then pass_score_thresh[s] = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs np . ndarray float [n_pixels x n_genes_added] . coefs[s, g] is the weighting of pixel s for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added np . array int [n_pixels x n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes np . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required background_var np . array float [n_pixels x (n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . np . ndarray inverse_var - float [n_pixels x (n_rounds x n_channels)] . Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def get_best_gene ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , coefs : np . ndarray , genes_added : np . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : np . ndarray , background_var : np . array ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] < score_thresh`, then `pass_score_thresh[s] = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_pixels x n_genes_added]`. `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_pixels x n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. background_var: `float [n_pixels x (n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" n_pixels , n_genes_added = genes_added . shape n_genes = all_bled_codes . shape [ 0 ] coefs_all = np . zeros (( n_pixels , n_genes )) pixel_ind = np . repeat ( np . arange ( n_pixels ), n_genes_added ) coefs_all [( pixel_ind , genes_added . flatten ())] = coefs . flatten () inverse_var = 1 / ( coefs_all ** 2 @ all_bled_codes ** 2 * alpha + background_var ) ignore_genes = np . concatenate (( genes_added , np . tile ( background_genes , [ n_pixels , 1 ])), axis = 1 ) best_gene , pass_score_thresh , best_score = \\ get_best_gene_base ( residual_pixel_colors , all_bled_codes , norm_shift , score_thresh , inverse_var , ignore_genes ) return best_gene , pass_score_thresh , inverse_var , best_score","title":"get_best_gene()"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene_base","text":"Computes the dot_product_score between residual_pixel_color and each code in all_bled_codes . If best_score is less than score_thresh or if the corresponding best_gene is in ignore_genes , then pass_score_thresh will be False. Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required inverse_var np . ndarray float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Used as weight_squared when computing dot_product_score . required ignore_genes np . ndarray int [n_pixels x n_genes_ignore] . If best_gene[s] is one of these, pass_score_thresh[s] will be False . required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score[s] > score_thresh and best_gene[s] not in ignore_genes . np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def get_best_gene_base ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , norm_shift : float , score_thresh : float , inverse_var : np . ndarray , ignore_genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`. If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`, then `pass_score_thresh` will be False. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. inverse_var: `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as `weight_squared` when computing `dot_product_score`. ignore_genes: `int [n_pixels x n_genes_ignore]`. If `best_gene[s]` is one of these, `pass_score_thresh[s]` will be `False`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score[s] > score_thresh` and `best_gene[s]` not in `ignore_genes`. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" # calculate score including background genes as if best gene is background, then stop iteration. all_scores = dot_product_score ( residual_pixel_colors , all_bled_codes , norm_shift , inverse_var ) best_gene = np . argmax ( np . abs ( all_scores ), axis = 1 ) # if best_gene is in ignore_gene, set score below score_thresh. is_ignore_gene = ( best_gene [:, np . newaxis ] == ignore_genes ) . any ( axis = 1 ) best_score = all_scores [( np . arange ( best_gene . shape [ 0 ]), best_gene )] * np . invert ( is_ignore_gene ) pass_score_thresh = np . abs ( best_score ) > score_thresh return best_gene , pass_score_thresh , best_score","title":"get_best_gene_base()"},{"location":"code/omp/coefs/#coppafish.omp.coefs.get_best_gene_first_iter","text":"Finds the best_gene to add next based on the dot product score with each bled_code . If best_gene is in background_genes or best_score < score_thresh then pass_score_thresh = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_colors np . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes np . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs np . ndarray float [n_pixels x n_channels] . coefs[g] is the weighting for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes np . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description np . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. np . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . np . ndarray background_var - float [n_pixels x (n_rounds x n_channels)] . Variance in each round/channel based on just the background. np . ndarray best_score - float [n_pixels] . dot_product_score for spot s with gene best_gene[s] . Source code in coppafish/omp/coefs.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def get_best_gene_first_iter ( residual_pixel_colors : np . ndarray , all_bled_codes : np . ndarray , background_coefs : np . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the `best_gene` to add next based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes` or `best_score < score_thresh` then `pass_score_thresh = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_pixels x n_channels]`. `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - background_var - `float [n_pixels x (n_rounds x n_channels)]`. Variance in each round/channel based on just the background. - best_score - `float [n_pixels]`. `dot_product_score` for spot `s` with gene `best_gene[s]`. \"\"\" background_var = np . square ( background_coefs ) @ np . square ( all_bled_codes [ background_genes ]) * alpha + beta ** 2 ignore_genes = np . tile ( background_genes , [ background_var . shape [ 0 ], 1 ]) best_gene , pass_score_thresh , best_score = \\ get_best_gene_base ( residual_pixel_colors , all_bled_codes , norm_shift , score_thresh , 1 / background_var , ignore_genes ) return best_gene , pass_score_thresh , background_var , best_score","title":"get_best_gene_first_iter()"},{"location":"code/omp/coefs/#optimised","text":"","title":"Optimised"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs","text":"This finds the least squared solution for how the n_genes_add bled_codes indicated by genes[s] can best explain pixel_colors[:, s] for each pixel s. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors jnp . ndarray float [(n_rounds x n_channels) x n_pixels] . Flattened then transposed pixel_colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes jnp . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required Returns: Type Description jnp . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_pixels x n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @jax . jit def fit_coefs ( bled_codes : jnp . ndarray , pixel_colors : jnp . ndarray , genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]` can best explain `pixel_colors[:, s]` for each pixel s. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`. Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_pixels x n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" return jax . vmap ( fit_coefs_single , in_axes = ( None , 1 , 0 ), out_axes = ( 0 , 0 ))( bled_codes , pixel_colors , genes )","title":"fit_coefs()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_single","text":"This finds the least squared solution for how the n_genes_add bled_codes indicated by genes can best explain pixel_color . Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Flattened pixel_color which usually has the shape [n_rounds x n_channels] . required genes jnp . ndarray int [n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain pixel_color. required Returns: Type Description jnp . ndarray residual - float [(n_rounds x n_channels)] . Residual pixel_color after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 def fit_coefs_single ( bled_codes : jnp . ndarray , pixel_color : jnp . ndarray , genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes` can best explain `pixel_color`. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_color: `float [(n_rounds x n_channels)]`. Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`. genes: `int [n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain pixel_color. Returns: - residual - `float [(n_rounds x n_channels)]`. Residual pixel_color after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" coefs = jnp . linalg . lstsq ( bled_codes [:, genes ], pixel_color )[ 0 ] residual = pixel_color - jnp . matmul ( bled_codes [:, genes ], coefs ) return residual , coefs","title":"fit_coefs_single()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_weight","text":"This finds the weighted least squared solution for how the n_genes_add bled_codes indicated by genes[s] can best explain pixel_colors[:, s] for each pixel s. The weight indicates which rounds/channels should have more influence when finding the coefficients of each gene. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_colors jnp . ndarray float [(n_rounds x n_channels) x n_pixels] . Flattened then transposed pixel_colors which usually has the shape [n_pixels x n_rounds x n_channels] . required genes jnp . ndarray int [n_pixels x n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. required weight jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . weight[s, i] is the weight to be applied to round_channel i when computing coefficient of each bled_code for pixel s . required Returns: Type Description jnp . ndarray residual - float [n_pixels x (n_rounds x n_channels)] . Residual pixel_colors after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_pixels x n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 @jax . jit def fit_coefs_weight ( bled_codes : jnp . ndarray , pixel_colors : jnp . ndarray , genes : jnp . ndarray , weight : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes[s]` can best explain `pixel_colors[:, s]` for each pixel s. The `weight` indicates which rounds/channels should have more influence when finding the coefficients of each gene. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_colors: `float [(n_rounds x n_channels) x n_pixels]`. Flattened then transposed `pixel_colors` which usually has the shape `[n_pixels x n_rounds x n_channels]`. genes: `int [n_pixels x n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain each pixel_color. weight: `float [n_pixels x (n_rounds x n_channels)]`. `weight[s, i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code` for pixel `s`. Returns: - residual - `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel_colors after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_pixels x n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" return jax . vmap ( fit_coefs_weight_single , in_axes = ( None , 1 , 0 , 0 ), out_axes = ( 0 , 0 ))( bled_codes , pixel_colors , genes , weight )","title":"fit_coefs_weight()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.fit_coefs_weight_single","text":"This finds the weighted least squared solution for how the n_genes_add bled_codes indicated by genes can best explain pixel_color . The weight indicates which rounds/channels should have more influence when finding the coefficients of each gene. Parameters: Name Type Description Default bled_codes jnp . ndarray float [(n_rounds x n_channels) x n_genes] . Flattened then transposed bled codes which usually has the shape [n_genes x n_rounds x n_channels] . required pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Flattened pixel_color which usually has the shape [n_rounds x n_channels] . required genes jnp . ndarray int [n_genes_add] . Indices of codes in bled_codes to find coefficients for which best explain pixel_color. required weight jnp . ndarray float [(n_rounds x n_channels)] . weight[i] is the weight to be applied to round_channel i when computing coefficient of each bled_code . required Returns: Type Description jnp . ndarray residual - float [(n_rounds x n_channels)] . Residual pixel_color after removing bled_codes with coefficients specified by coefs. jnp . ndarray coefs - float [n_genes_add] . Coefficients found through least squares fitting for each gene. Source code in coppafish/omp/coefs_optimised.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def fit_coefs_weight_single ( bled_codes : jnp . ndarray , pixel_color : jnp . ndarray , genes : jnp . ndarray , weight : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This finds the weighted least squared solution for how the `n_genes_add` `bled_codes` indicated by `genes` can best explain `pixel_color`. The `weight` indicates which rounds/channels should have more influence when finding the coefficients of each gene. Args: bled_codes: `float [(n_rounds x n_channels) x n_genes]`. Flattened then transposed bled codes which usually has the shape `[n_genes x n_rounds x n_channels]`. pixel_color: `float [(n_rounds x n_channels)]`. Flattened `pixel_color` which usually has the shape `[n_rounds x n_channels]`. genes: `int [n_genes_add]`. Indices of codes in bled_codes to find coefficients for which best explain pixel_color. weight: `float [(n_rounds x n_channels)]`. `weight[i]` is the weight to be applied to round_channel `i` when computing coefficient of each `bled_code`. Returns: - residual - `float [(n_rounds x n_channels)]`. Residual pixel_color after removing bled_codes with coefficients specified by coefs. - coefs - `float [n_genes_add]`. Coefficients found through least squares fitting for each gene. \"\"\" coefs = jnp . linalg . lstsq ( bled_codes [:, genes ] * weight [:, jnp . newaxis ], pixel_color * weight )[ 0 ] residual = pixel_color * weight - jnp . matmul ( bled_codes [:, genes ] * weight [:, jnp . newaxis ], coefs ) return residual / weight , coefs","title":"fit_coefs_weight_single()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_all_coefs","text":"This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. Note Background vectors are fitted first and then not updated again. Parameters: Name Type Description Default pixel_colors jnp . ndarray float [n_pixels x n_rounds x n_channels] . Pixel colors normalised to equalise intensities between channels (and rounds). required bled_codes jnp . ndarray float [n_genes x n_rounds x n_channels] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . required background_shift float When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. required dp_shift float When finding dot_product_score between residual pixel_colors and bled_codes , this is applied to normalisation of pixel_colors to limit boost of weak pixels. required dp_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added at each iteration. required alpha float Used for fitting_standard_deviation , by how much to increase variance as genes added. required beta float Used for fitting_standard_deviation , the variance with no genes added ( coef=0 ) is beta**2 . required max_genes int Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. required weight_coef_fit bool If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. False Returns: Type Description np . ndarray gene_coefs - float32 [n_pixels x n_genes] . gene_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero. np . ndarray background_coefs - float32 [n_pixels x n_channels] . coefficient value for each background vector found for each pixel. Source code in coppafish/omp/coefs_optimised.py 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 def get_all_coefs ( pixel_colors : jnp . ndarray , bled_codes : jnp . ndarray , background_shift : float , dp_shift : float , dp_thresh : float , alpha : float , beta : float , max_genes : int , weight_coef_fit : bool = False ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" This performs omp on every pixel, the stopping criterion is that the dot_product_score when selecting the next gene to add exceeds dp_thresh or the number of genes added to the pixel exceeds max_genes. !!! note Background vectors are fitted first and then not updated again. Args: pixel_colors: `float [n_pixels x n_rounds x n_channels]`. Pixel colors normalised to equalise intensities between channels (and rounds). bled_codes: `float [n_genes x n_rounds x n_channels]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. background_shift: When fitting background, this is applied to weighting of each background vector to limit boost of weak pixels. dp_shift: When finding `dot_product_score` between residual `pixel_colors` and `bled_codes`, this is applied to normalisation of `pixel_colors` to limit boost of weak pixels. dp_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added at each iteration. alpha: Used for `fitting_standard_deviation`, by how much to increase variance as genes added. beta: Used for `fitting_standard_deviation`, the variance with no genes added (`coef=0`) is `beta**2`. max_genes: Maximum number of genes that can be added to a pixel i.e. number of iterations of OMP. weight_coef_fit: If False, coefs are found through normal least squares fitting. If True, coefs are found through weighted least squares fitting using 1/sigma as the weight factor. Returns: - gene_coefs - `float32 [n_pixels x n_genes]`. `gene_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero. - background_coefs - `float32 [n_pixels x n_channels]`. coefficient value for each background vector found for each pixel. \"\"\" n_pixels = pixel_colors . shape [ 0 ] check_spot = np . random . randint ( n_pixels ) diff_to_int = jnp . round ( pixel_colors [ check_spot ]) . astype ( int ) - pixel_colors [ check_spot ] if jnp . abs ( diff_to_int ) . max () == 0 : raise ValueError ( f \"pixel_coefs should be found using normalised pixel_colors.\" f \" \\n But for pixel { check_spot } , pixel_colors given are integers indicating they are \" f \"the raw intensities.\" ) n_genes , n_rounds , n_channels = bled_codes . shape if not utils . errors . check_shape ( pixel_colors , [ n_pixels , n_rounds , n_channels ]): raise utils . errors . ShapeError ( 'pixel_colors' , pixel_colors . shape , ( n_pixels , n_rounds , n_channels )) no_verbose = n_pixels < 1000 # show progress bar with more than 1000 pixels. # Fit background and override initial pixel_colors gene_coefs = np . zeros (( n_pixels , n_genes ), dtype = np . float32 ) # coefs of all genes and background pixel_colors , background_coefs , background_codes = fit_background ( pixel_colors , background_shift ) background_genes = jnp . arange ( n_genes , n_genes + n_channels ) # colors and codes for get_best_gene function # Includes background as if background is the best gene, iteration ends. # uses residual color as used to find next gene to add. bled_codes = bled_codes . reshape (( n_genes , - 1 )) all_codes = jnp . concatenate (( bled_codes , background_codes . reshape ( n_channels , - 1 ))) bled_codes = bled_codes . transpose () # colors and codes for fit_coefs function (No background as this is not updated again). # always uses post background color as coefficients for all genes re-estimated at each iteration. pixel_colors = pixel_colors . reshape (( n_pixels , - 1 )) continue_pixels = jnp . arange ( n_pixels ) with tqdm ( total = max_genes , disable = no_verbose ) as pbar : pbar . set_description ( 'Finding OMP coefficients for each pixel' ) for i in range ( max_genes ): if i == 0 : # Background coefs don't change, hence contribution to variance won't either. added_genes , pass_score_thresh , background_variance = \\ get_best_gene_first_iter ( pixel_colors , all_codes , background_coefs , dp_shift , dp_thresh , alpha , beta , background_genes ) inverse_var = 1 / background_variance pixel_colors = pixel_colors . transpose () else : # only continue with pixels for which dot product score exceeds threshold i_added_genes , pass_score_thresh , inverse_var = \\ get_best_gene ( residual_pixel_colors , all_codes , i_coefs , added_genes , dp_shift , dp_thresh , alpha , background_genes , background_variance ) # For pixels with at least one non-zero coef, add to final gene_coefs when fail the thresholding. fail_score_thresh = jnp . invert ( pass_score_thresh ) # gene_coefs[np.asarray(continue_pixels[fail_score_thresh])] = np.asarray(i_coefs[fail_score_thresh]) gene_coefs [ np . asarray ( continue_pixels [ fail_score_thresh ])[:, np . newaxis ], np . asarray ( added_genes [ fail_score_thresh ])] = np . asarray ( i_coefs [ fail_score_thresh ]) continue_pixels = continue_pixels [ pass_score_thresh ] n_continue = jnp . size ( continue_pixels ) pbar . set_postfix ({ 'n_pixels' : n_continue }) if n_continue == 0 : break if i == 0 : added_genes = added_genes [ pass_score_thresh , np . newaxis ] else : added_genes = jnp . hstack (( added_genes [ pass_score_thresh ], i_added_genes [ pass_score_thresh , jnp . newaxis ])) pixel_colors = pixel_colors [:, pass_score_thresh ] background_variance = background_variance [ pass_score_thresh ] inverse_var = inverse_var [ pass_score_thresh ] # Maybe add different fit_coefs for i==0 i.e. can do multiple pixels at once for same gene added. if weight_coef_fit : residual_pixel_colors , i_coefs = fit_coefs_weight ( bled_codes , pixel_colors , added_genes , jnp . sqrt ( inverse_var )) else : residual_pixel_colors , i_coefs = fit_coefs ( bled_codes , pixel_colors , added_genes ) if i == max_genes - 1 : # Add pixels to final gene_coefs when reach end of iteration. gene_coefs [ np . asarray ( continue_pixels )[:, np . newaxis ], np . asarray ( added_genes )] = np . asarray ( i_coefs ) pbar . update ( 1 ) pbar . close () return gene_coefs . astype ( np . float32 ), np . asarray ( background_coefs ) . astype ( np . float32 )","title":"get_all_coefs()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene","text":"Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes , already in genes_added[s] or best_score[s] < score_thresh , then pass_score_thresh[s] = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_colors jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs jnp . ndarray float [n_pixels x n_genes_added] . coefs[s, g] is the weighting of pixel s for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added jnp . array int [n_pixels x n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required background_var jnp . array float [n_pixels x (n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description jnp . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. jnp . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . jnp . ndarray inverse_var - float [n_pixels x (n_rounds x n_channels)] . Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. Source code in coppafish/omp/coefs_optimised.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 @partial ( jax . jit , static_argnums = ( 4 , 5 , 6 )) def get_best_gene ( residual_pixel_colors : jnp . ndarray , all_bled_codes : jnp . ndarray , coefs : jnp . ndarray , genes_added : jnp . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : jnp . ndarray , background_var : jnp . array ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes`, already in `genes_added[s]` or `best_score[s] < score_thresh`, then `pass_score_thresh[s] = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_pixels x n_genes_added]`. `coefs[s, g]` is the weighting of pixel `s` for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_pixels x n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. background_var: `float [n_pixels x (n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - inverse_var - `float [n_pixels x (n_rounds x n_channels)]`. Inverse of variance of each pixel in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. \"\"\" return jax . vmap ( get_best_gene_single , in_axes = ( 0 , None , 0 , 0 , None , None , None , None , 0 ), out_axes = ( 0 , 0 , 0 ))( residual_pixel_colors , all_bled_codes , coefs , genes_added , norm_shift , score_thresh , alpha , background_genes , background_var )","title":"get_best_gene()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_base","text":"Computes the dot_product_score between residual_pixel_color and each code in all_bled_codes . If best_score is less than score_thresh or if the corresponding best_gene is in ignore_genes , then pass_score_thresh will be False. Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required inverse_var jnp . ndarray float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Used as weight_squared when computing dot_product_score . required ignore_genes jnp . ndarray int [n_genes_ignore] . If best_gene is one of these, pass_score_thresh will be False . required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh and best_gene not in ignore_genes . Source code in coppafish/omp/coefs_optimised.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def get_best_gene_base ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , norm_shift : float , score_thresh : float , inverse_var : jnp . ndarray , ignore_genes : jnp . ndarray ) -> Tuple [ int , bool ]: \"\"\" Computes the `dot_product_score` between `residual_pixel_color` and each code in `all_bled_codes`. If `best_score` is less than `score_thresh` or if the corresponding `best_gene` is in `ignore_genes`, then `pass_score_thresh` will be False. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. inverse_var: `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Used as `weight_squared` when computing `dot_product_score`. ignore_genes: `int [n_genes_ignore]`. If `best_gene` is one of these, `pass_score_thresh` will be `False`. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh` and `best_gene` not in `ignore_genes`. \"\"\" # calculate score including background genes as if best gene is background, then stop iteration. all_scores = dot_product_score_single ( residual_pixel_color , all_bled_codes , norm_shift , inverse_var ) best_gene = jnp . argmax ( jnp . abs ( all_scores )) # if best_gene is background, set score below score_thresh. best_score = all_scores [ best_gene ] * jnp . isin ( best_gene , ignore_genes , invert = True ) pass_score_thresh = jnp . abs ( best_score ) > score_thresh return best_gene , pass_score_thresh","title":"get_best_gene_base()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_first_iter","text":"Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene[s] is in background_genes or best_score[s] < score_thresh then pass_score_thresh[s] = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_colors jnp . ndarray float [n_pixels x (n_rounds x n_channels)] . Residual pixel colors from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs jnp . ndarray float [n_pixels x n_channels] . coefs[s, g] is the weighting of pixel s for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description jnp . ndarray best_gene - int [n_pixels] . best_gene[s] is the best gene to add to pixel s next. jnp . ndarray pass_score_thresh - bool [n_pixels] . True if best_score > score_thresh . jnp . ndarray background_var - float [n_pixels x (n_rounds x n_channels)] . Variance of each pixel in each round/channel based on just the background. Source code in coppafish/omp/coefs_optimised.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 @partial ( jax . jit , static_argnums = ( 3 , 4 , 5 , 6 )) def get_best_gene_first_iter ( residual_pixel_colors : jnp . ndarray , all_bled_codes : jnp . ndarray , background_coefs : jnp . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene[s]` is in `background_genes` or `best_score[s] < score_thresh` then `pass_score_thresh[s] = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_colors: `float [n_pixels x (n_rounds x n_channels)]`. Residual pixel colors from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_pixels x n_channels]`. `coefs[s, g]` is the weighting of pixel `s` for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - `int [n_pixels]`. `best_gene[s]` is the best gene to add to pixel `s` next. - pass_score_thresh - `bool [n_pixels]`. `True` if `best_score > score_thresh`. - background_var - `float [n_pixels x (n_rounds x n_channels)]`. Variance of each pixel in each round/channel based on just the background. \"\"\" return jax . vmap ( get_best_gene_first_iter_single , in_axes = ( 0 , None , 0 , None , None , None , None , None ), out_axes = ( 0 , 0 , 0 ))( residual_pixel_colors , all_bled_codes , background_coefs , norm_shift , score_thresh , alpha , beta , background_genes )","title":"get_best_gene_first_iter()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_first_iter_single","text":"Finds the best_gene to add next based on the dot product score with each bled_code . If best_gene is in background_genes or best_score < score_thresh then pass_score_thresh = False . Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause pass_score_thresh to be False . Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required background_coefs jnp . ndarray float [n_channels] . coefs[g] is the weighting for gene background_genes[g] found by the omp algorithm. All are non-zero. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required beta float Used for fitting_variance , the variance with no genes added ( coef=0 ) is beta**2 . required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel s is set to one of background_genes , pass_score_thresh[s] will be False. required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh . jnp . ndarray background_var - float [(n_rounds x n_channels)] . Variance in each round/channel based on just the background. Source code in coppafish/omp/coefs_optimised.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def get_best_gene_first_iter_single ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , background_coefs : jnp . ndarray , norm_shift : float , score_thresh : float , alpha : float , beta : float , background_genes : jnp . ndarray ) -> Tuple [ int , bool , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes` or `best_score < score_thresh` then `pass_score_thresh = False`. Different for first iteration as no actual non-zero gene coefficients to consider when computing variance or genes that can be added which will cause `pass_score_thresh` to be `False`. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. background_coefs: `float [n_channels]`. `coefs[g]` is the weighting for gene `background_genes[g]` found by the omp algorithm. All are non-zero. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. beta: Used for `fitting_variance`, the variance with no genes added (`coef=0`) is `beta**2`. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene for pixel `s` is set to one of `background_genes`, `pass_score_thresh[s]` will be False. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh`. - background_var - `float [(n_rounds x n_channels)]`. Variance in each round/channel based on just the background. \"\"\" background_var = jnp . square ( background_coefs ) @ jnp . square ( all_bled_codes [ background_genes ]) * alpha + beta ** 2 best_gene , pass_score_thresh = get_best_gene_base ( residual_pixel_color , all_bled_codes , norm_shift , score_thresh , 1 / background_var , background_genes ) return best_gene , pass_score_thresh , background_var","title":"get_best_gene_first_iter_single()"},{"location":"code/omp/coefs/#coppafish.omp.coefs_optimised.get_best_gene_single","text":"Finds the best_gene to add next to each pixel based on the dot product score with each bled_code . If best_gene is in background_genes , already in genes_added or best_score < score_thresh , then pass_score_thresh = False . Note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Parameters: Name Type Description Default residual_pixel_color jnp . ndarray float [(n_rounds x n_channels)] . Residual pixel color from previous iteration of omp. required all_bled_codes jnp . ndarray float [n_genes x (n_rounds x n_channels)] . bled_codes such that spot_color of a gene g in round r is expected to be a constant multiple of bled_codes[g, r] . Includes codes of genes and background. required coefs jnp . ndarray float [n_genes_added] . coefs[g] is the weighting for gene genes_added[g] found by the omp algorithm on previous iteration. All are non-zero. required genes_added jnp . array int [n_genes_added] Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel s is set to one of genes_added[s] , pass_score_thresh[s] will be False. required norm_shift float shift to apply to normalisation of spot_colors to limit boost of weak spots. required score_thresh float dot_product_score of the best gene for a pixel must exceed this for that gene to be added in the current iteration. required alpha float Used for fitting_variance , by how much to increase variance as genes added. required background_genes jnp . ndarray int [n_channels] . Indices of codes in all_bled_codes which correspond to background. If the best gene is set to one of background_genes , pass_score_thresh will be False. required background_var jnp . array float [(n_rounds x n_channels)] . Contribution of background genes to variance (which does not change throughout omp iterations) i.e. background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2 . required Returns: Type Description int best_gene - The best gene to add next. bool pass_score_thresh - True if best_score > score_thresh . jnp . ndarray inverse_var - float [(n_rounds x n_channels)] . Inverse of variance in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. Source code in coppafish/omp/coefs_optimised.py 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def get_best_gene_single ( residual_pixel_color : jnp . ndarray , all_bled_codes : jnp . ndarray , coefs : jnp . ndarray , genes_added : jnp . array , norm_shift : float , score_thresh : float , alpha : float , background_genes : jnp . ndarray , background_var : jnp . array ) -> Tuple [ int , bool , jnp . ndarray ]: \"\"\" Finds the `best_gene` to add next to each pixel based on the dot product score with each `bled_code`. If `best_gene` is in `background_genes`, already in `genes_added` or `best_score < score_thresh`, then `pass_score_thresh = False`. !!!note The variance computed is based on maximum likelihood estimation - it accounts for all genes and background fit in each round/channel. The more genes added, the greater the variance so if the inverse is used as a weighting for omp fitting or choosing the next gene, the rounds/channels which already have genes in will contribute less. Args: residual_pixel_color: `float [(n_rounds x n_channels)]`. Residual pixel color from previous iteration of omp. all_bled_codes: `float [n_genes x (n_rounds x n_channels)]`. `bled_codes` such that `spot_color` of a gene `g` in round `r` is expected to be a constant multiple of `bled_codes[g, r]`. Includes codes of genes and background. coefs: `float [n_genes_added]`. `coefs[g]` is the weighting for gene `genes_added[g]` found by the omp algorithm on previous iteration. All are non-zero. genes_added: `int [n_genes_added]` Indices of genes added to each pixel from previous iteration of omp. If the best gene for pixel `s` is set to one of `genes_added[s]`, `pass_score_thresh[s]` will be False. norm_shift: shift to apply to normalisation of spot_colors to limit boost of weak spots. score_thresh: `dot_product_score` of the best gene for a pixel must exceed this for that gene to be added in the current iteration. alpha: Used for `fitting_variance`, by how much to increase variance as genes added. background_genes: `int [n_channels]`. Indices of codes in all_bled_codes which correspond to background. If the best gene is set to one of `background_genes`, `pass_score_thresh` will be False. background_var: `float [(n_rounds x n_channels)]`. Contribution of background genes to variance (which does not change throughout omp iterations) i.e. `background_coefs**2 @ all_bled_codes[background_genes]**2 * alpha + beta ** 2`. Returns: - best_gene - The best gene to add next. - pass_score_thresh - `True` if `best_score > score_thresh`. - inverse_var - `float [(n_rounds x n_channels)]`. Inverse of variance in each round/channel based on genes fit on previous iteration. Includes both background and gene contribution. \"\"\" inverse_var = 1 / ( jnp . square ( coefs ) @ jnp . square ( all_bled_codes [ genes_added ]) * alpha + background_var ) # calculate score including background genes as if best gene is background, then stop iteration. best_gene , pass_score_thresh = get_best_gene_base ( residual_pixel_color , all_bled_codes , norm_shift , score_thresh , inverse_var , jnp . append ( background_genes , genes_added )) return best_gene , pass_score_thresh , inverse_var","title":"get_best_gene_single()"},{"location":"code/omp/spots/","text":"count_spot_neighbours ( image , spot_yxz , kernel ) Counts the number of positive (and negative) pixels in a neighbourhood about each spot. If filter contains only 1 and 0, then number of positive pixels returned near each spot. If filter contains only -1 and 0, then number of negative pixels returned near each spot. If filter contains -1, 0 and 1, then number of positive and negative pixels returned near each spot. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x (x n_z)] . image spots were found on. required spot_yxz np . ndarray int [n_spots x image.ndim] . yx or yxz location of spots found. required kernel np . ndarray int [filter_sz_y x filter_sz_x (x filter_sz_z)] . Number of positive (and negative) pixels counted in this neighbourhood about each spot in image. Only contains values 0 and 1 (and -1). required Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] n_pos_neighbours - int [n_spots] (Only if filter contains 1). Number of positive pixels around each spot in neighbourhood given by pos_filter . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] n_neg_neighbours - int [n_spots] (Only if filter contains -1). Number of negative pixels around each spot in neighbourhood given by neg_filter . Source code in coppafish/omp/spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def count_spot_neighbours ( image : np . ndarray , spot_yxz : np . ndarray , kernel : np . ndarray ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Counts the number of positive (and negative) pixels in a neighbourhood about each spot. If `filter` contains only 1 and 0, then number of positive pixels returned near each spot. If `filter` contains only -1 and 0, then number of negative pixels returned near each spot. If `filter` contains -1, 0 and 1, then number of positive and negative pixels returned near each spot. Args: image: `float [n_y x n_x (x n_z)]`. image spots were found on. spot_yxz: `int [n_spots x image.ndim]`. yx or yxz location of spots found. kernel: `int [filter_sz_y x filter_sz_x (x filter_sz_z)]`. Number of positive (and negative) pixels counted in this neighbourhood about each spot in image. Only contains values 0 and 1 (and -1). Returns: - n_pos_neighbours - `int [n_spots]` (Only if `filter` contains 1). Number of positive pixels around each spot in neighbourhood given by `pos_filter`. - n_neg_neighbours - `int [n_spots]` (Only if `filter` contains -1). Number of negative pixels around each spot in neighbourhood given by `neg_filter`. \"\"\" # Correct for 2d cases where an empty dimension has been used for some variables. if all ([ image . ndim == spot_yxz . shape [ 1 ] - 1 , np . max ( np . abs ( spot_yxz [:, - 1 ])) == 0 ]): # Image 2D but spots 3D spot_yxz = spot_yxz [:, : image . ndim ] if all ([ image . ndim == spot_yxz . shape [ 1 ] + 1 , image . shape [ - 1 ] == 1 ]): # Image 3D but spots 2D image = np . mean ( image , axis = image . ndim - 1 ) # average over last dimension just means removing it. if all ([ image . ndim == kernel . ndim - 1 , kernel . shape [ - 1 ] == 1 ]): # Image 2D but pos_filter 3D kernel = np . mean ( kernel , axis = kernel . ndim - 1 ) # Check kernel contains right values. kernel_vals = np . unique ( kernel ) if not np . isin ( kernel_vals , [ - 1 , 0 , 1 ]) . all (): raise ValueError ( 'filter contains values other than -1, 0 or 1.' ) # Check all spots in image max_yxz = np . array ( image . shape ) - 1 spot_oob = [ val for val in spot_yxz if val . min () < 0 or any ( val > max_yxz )] if len ( spot_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"spot_yxz\" , spot_oob [ 0 ], [ 0 ] * image . ndim , max_yxz ) if np . isin ([ - 1 , 1 ], kernel_vals ) . all (): # Return positive and negative counts n_pos = utils . morphology . imfilter_coords ( image > 0 , kernel > 0 , spot_yxz ) n_neg = utils . morphology . imfilter_coords ( image < 0 , kernel < 0 , spot_yxz ) return n_pos , n_neg elif np . isin ( - 1 , kernel_vals ): # Return negative counts return utils . morphology . imfilter_coords ( image < 0 , kernel < 0 , spot_yxz ) . astype ( int ) elif np . isin ( 1 , kernel_vals ): # Return positive counts return utils . morphology . imfilter_coords ( image > 0 , kernel > 0 , spot_yxz ) . astype ( int ) else : raise ValueError ( 'filter contains only 0.' ) cropped_coef_image ( pixel_yxz , pixel_coefs ) Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included. Parameters: Name Type Description Default pixel_yxz np . ndarray int [n_pixels x 3] pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required pixel_coefs Union [ csr_matrix , np . array ] float32 [n_pixels x 1] . pixel_coefs[s] is the weighting of pixel s for a given gene found by the omp algorithm. Most are zero hence sparse form used. required Returns: Type Description Optional [ np . ndarray ] coef_image - float32 [im_size_y x im_size_x x im_size_z] cropped omp coefficient. Will be None if there are no non-zero coefficients. Optional [ np . ndarray ] coord_shift - int [3] . yxz shift subtracted from pixel_yxz to build coef_image. Will be None if there are no non-zero coefficients. Source code in coppafish/omp/spots.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def cropped_coef_image ( pixel_yxz : np . ndarray , pixel_coefs : Union [ csr_matrix , np . array ]) -> Tuple [ Optional [ np . ndarray ], Optional [ np . ndarray ]]: \"\"\" Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included. Args: pixel_yxz: `int [n_pixels x 3]` ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. pixel_coefs: `float32 [n_pixels x 1]`. `pixel_coefs[s]` is the weighting of pixel `s` for a given gene found by the omp algorithm. Most are zero hence sparse form used. Returns: - coef_image - `float32 [im_size_y x im_size_x x im_size_z]` cropped omp coefficient. Will be `None` if there are no non-zero coefficients. - coord_shift - `int [3]`. yxz shift subtracted from pixel_yxz to build coef_image. Will be `None` if there are no non-zero coefficients. \"\"\" if isinstance ( pixel_coefs , csr_matrix ): nz_ind = pixel_coefs . nonzero ()[ 0 ] nz_pixel_coefs = pixel_coefs [ nz_ind ] . toarray () . flatten () else : nz_ind = pixel_coefs != 0 nz_pixel_coefs = pixel_coefs [ nz_ind ] if nz_pixel_coefs . size == 0 : # If no non-zero coefficients, return nothing return None , None else : nz_pixel_yxz = pixel_yxz [ nz_ind , :] # shift nz_pixel_yxz so min is 0 in each axis so smaller image can be formed. coord_shift = nz_pixel_yxz . min ( axis = 0 ) nz_pixel_yxz = nz_pixel_yxz - coord_shift n_y , n_x , n_z = nz_pixel_yxz . max ( axis = 0 ) + 1 # coef_image at pixels other than nz_pixel_yxz is set to 0. if n_z == 1 : coef_image = np . zeros (( n_y , n_x ), dtype = np . float32 ) else : coef_image = np . zeros (( n_y , n_x , n_z ), dtype = np . float32 ) coef_image [ tuple ([ nz_pixel_yxz [:, j ] for j in range ( coef_image . ndim )])] = nz_pixel_coefs return coef_image , coord_shift get_spots ( pixel_coefs , pixel_yxz , radius_xy , radius_z , coef_thresh = 0 , spot_shape = None , pos_neighbour_thresh = 0 , spot_yxzg = None ) Finds all local maxima in coef_image of each gene with coefficient exceeding coef_thresh and returns corresponding yxz position and gene_no . If provide spot_shape , also counts number of positive and negative pixels in neighbourhood of each spot. Parameters: Name Type Description Default pixel_coefs Union [ csr_matrix , np . array ] float [n_pixels x n_genes] . pixel_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero hence sparse form used. required pixel_yxz np . ndarray int [n_pixels x 3] . pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required radius_xy int Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). If None , 2D filter is used. required coef_thresh float Local maxima in coef_image exceeding this value are considered spots. 0 spot_shape Optional [ np . ndarray ] int [shape_size_y x shape_size_x x shape_size_z] or None . Indicates expected sign of coefficients in neighbourhood of spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign so ignore. None pos_neighbour_thresh int Only spots with number of positive neighbours exceeding this will be kept if spot_shape provided. 0 spot_yxzg Optional [ np . ndarray ] float [n_spots x 4] . Can provide location and gene identity of spots if already computed. Where spots are local maxima above coef_thresh in pixel_coefs image for each gene. If None, spots are determined from pixel_coefs . spot_yxzg[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxzg[s, 2] is the local z coordinate in z_pixels for spot s . spot_yxzg[s, 3] is the gene number of spot s . None Returns: Type Description Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] spot_yxz - int [n_spots x 3] spot_yxz[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxz[s, 2] is the local z coordinate in z_pixels for spot s . Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] spot_gene_no - int [n_spots] . spot_gene_no[s] is the gene that spot s is assigned to. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] n_pos_neighbours - int [n_spots] (Only if spot_shape given). Number of positive pixels around each spot in neighbourhood given by spot_shape==1 . Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] n_neg_neighbours - int [n_spots] (Only if spot_shape given). Number of negative pixels around each spot in neighbourhood given by spot_shape==-1 . Source code in coppafish/omp/spots.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def get_spots ( pixel_coefs : Union [ csr_matrix , np . array ], pixel_yxz : np . ndarray , radius_xy : int , radius_z : Optional [ int ], coef_thresh : float = 0 , spot_shape : Optional [ np . ndarray ] = None , pos_neighbour_thresh : int = 0 , spot_yxzg : Optional [ np . ndarray ] = None ) -> Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]]: \"\"\" Finds all local maxima in `coef_image` of each gene with coefficient exceeding `coef_thresh` and returns corresponding `yxz` position and `gene_no`. If provide `spot_shape`, also counts number of positive and negative pixels in neighbourhood of each spot. Args: pixel_coefs: `float [n_pixels x n_genes]`. `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero hence sparse form used. pixel_yxz: ```int [n_pixels x 3]```. ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). If ```None```, 2D filter is used. coef_thresh: Local maxima in `coef_image` exceeding this value are considered spots. spot_shape: `int [shape_size_y x shape_size_x x shape_size_z]` or `None`. Indicates expected sign of coefficients in neighbourhood of spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign so ignore. pos_neighbour_thresh: Only spots with number of positive neighbours exceeding this will be kept if `spot_shape` provided. spot_yxzg: `float [n_spots x 4]`. Can provide location and gene identity of spots if already computed. Where spots are local maxima above `coef_thresh` in `pixel_coefs` image for each gene. If None, spots are determined from `pixel_coefs`. ```spot_yxzg[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxzg[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. ```spot_yxzg[s, 3]``` is the gene number of spot ```s```. Returns: - spot_yxz - `int [n_spots x 3]` ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. - spot_gene_no - `int [n_spots]`. ```spot_gene_no[s]``` is the gene that spot s is assigned to. - n_pos_neighbours - `int [n_spots]` (Only if `spot_shape` given). Number of positive pixels around each spot in neighbourhood given by `spot_shape==1`. - n_neg_neighbours - `int [n_spots]` (Only if `spot_shape` given). Number of negative pixels around each spot in neighbourhood given by `spot_shape==-1`. \"\"\" n_pixels , n_genes = pixel_coefs . shape if not utils . errors . check_shape ( pixel_yxz , [ n_pixels , 3 ]): raise utils . errors . ShapeError ( 'pixel_yxz' , pixel_yxz . shape , ( n_pixels , 3 )) if spot_shape is None : spot_info = np . zeros (( 0 , 4 ), dtype = int ) else : if np . sum ( spot_shape == 1 ) == 0 : raise ValueError ( f \"spot_shape contains no pixels with a value of 1 which indicates the \" f \"neighbourhood about a spot where we expect a positive coefficient.\" ) if np . sum ( spot_shape == - 1 ) == 0 : raise ValueError ( f \"spot_shape contains no pixels with a value of -1 which indicates the \" f \"neighbourhood about a spot where we expect a negative coefficient.\" ) if pos_neighbour_thresh < 0 or pos_neighbour_thresh >= np . sum ( spot_shape > 0 ): # Out of bounds if threshold for positive neighbours is above the maximum possible. raise utils . errors . OutOfBoundsError ( \"pos_neighbour_thresh\" , pos_neighbour_thresh , 0 , np . sum ( spot_shape > 0 ) - 1 ) spot_info = np . zeros (( 0 , 6 ), dtype = int ) if spot_yxzg is not None : # check pixel coefficient is positive for random subset of 500 spots. spots_to_check = np . random . choice ( range ( spot_yxzg . shape [ 0 ]), np . clip ( 500 , 0 , spot_yxzg . shape [ 0 ]), replace = False ) pixel_index = numpy_indexed . indices ( pixel_yxz , spot_yxzg [ spots_to_check , : 3 ] . astype ( pixel_yxz . dtype )) spot_coefs_check = pixel_coefs [ pixel_index , spot_yxzg [ spots_to_check , 3 ]] if spot_coefs_check . min () <= coef_thresh : bad_spot = spots_to_check [ spot_coefs_check . argmin ()] raise ValueError ( f \"spot_yxzg provided but gene { spot_yxzg [ bad_spot , 3 ] } coefficient for spot { bad_spot } \\n \" f \"at yxz = { spot_yxzg [ bad_spot , : 3 ] } is { spot_coefs_check . min () } \\n \" f \"whereas it should be more than coef_thresh = { coef_thresh } as it is listed as a spot.\" ) with tqdm ( total = n_genes ) as pbar : # TODO: if 2D can do all genes together. pbar . set_description ( f \"Finding spots for all { n_genes } genes from omp_coef images.\" ) for g in range ( n_genes ): # shift nzg_pixel_yxz so min is 0 in each axis so smaller image can be formed. # Note size of image will be different for each gene. coef_image , coord_shift = cropped_coef_image ( pixel_yxz , pixel_coefs [:, g ]) if coef_image is None : # If no non-zero coefficients, go to next gene continue if spot_yxzg is None : spot_yxz = detect_spots ( coef_image , coef_thresh , radius_xy , radius_z , False )[ 0 ] else : # spot_yxz match pixel_yxz so if crop pixel_yxz need to crop spot_yxz too. spot_yxz = spot_yxzg [ spot_yxzg [:, 3 ] == g , : coef_image . ndim ] - coord_shift [: coef_image . ndim ] if spot_yxz . shape [ 0 ] > 0 : if spot_shape is None : keep = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) spot_info_g = np . zeros (( np . sum ( keep ), 4 ), dtype = int ) else : n_pos_neighb , n_neg_neighb = count_spot_neighbours ( coef_image , spot_yxz , spot_shape ) keep = n_pos_neighb > pos_neighbour_thresh spot_info_g = np . zeros (( np . sum ( keep ), 6 ), dtype = int ) spot_info_g [:, 4 ] = n_pos_neighb [ keep ] spot_info_g [:, 5 ] = n_neg_neighb [ keep ] spot_info_g [:, : coef_image . ndim ] = spot_yxz [ keep ] spot_info_g [:, : 3 ] = spot_info_g [:, : 3 ] + coord_shift # shift spot_yxz back spot_info_g [:, 3 ] = g spot_info = np . append ( spot_info , spot_info_g , axis = 0 ) pbar . update ( 1 ) pbar . close () if spot_shape is None : return spot_info [:, : 3 ], spot_info [:, 3 ] else : return spot_info [:, : 3 ], spot_info [:, 3 ], spot_info [:, 4 ], spot_info [:, 5 ] spot_neighbourhood ( pixel_coefs , pixel_yxz , spot_yxz , spot_gene_no , max_size , pos_neighbour_thresh , isolation_dist , z_scale , mean_sign_thresh ) Finds the expected sign the coefficient should have in the neighbourhood about a spot. Parameters: Name Type Description Default pixel_coefs Union [ csr_matrix , np . array ] float [n_pixels x n_genes] . pixel_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero hence sparse form used. required pixel_yxz np . ndarray int [n_pixels x 3] . pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required spot_yxz np . ndarray int [n_spots x 3] . spot_yxz[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxz[s, 2] is the local z coordinate in z_pixels for spot s . required spot_gene_no np . ndarray int [n_spots] . spot_gene_no[s] is the gene that this spot is assigned to. required max_size Union [ np . ndarray , List ] int [3] . max YXZ size of spot shape returned. Zeros at extremities will be cropped in av_spot_image . required pos_neighbour_thresh int For spot to be used to find av_spot_image, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Typical = 9. required isolation_dist float Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find av_spot_image. required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required mean_sign_thresh float If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in av_spot_image to be 0. required Returns: Type Description np . ndarray av_spot_image - int8 [av_shape_y x av_shape_x x av_shape_z] Expected sign of omp coefficient in neighbourhood centered on spot. np . ndarray spot_indices_used - int [n_spots_used] . indices of spots in spot_yxzg used to make av_spot_image. np . ndarray av_spot_image_float - float [max_size[0] x max_size[1] x max_size[2]] Mean of omp coefficient sign in neighbourhood centered on spot. This is before cropping and thresholding. Source code in coppafish/omp/spots.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def spot_neighbourhood ( pixel_coefs : Union [ csr_matrix , np . array ], pixel_yxz : np . ndarray , spot_yxz : np . ndarray , spot_gene_no : np . ndarray , max_size : Union [ np . ndarray , List ], pos_neighbour_thresh : int , isolation_dist : float , z_scale : float , mean_sign_thresh : float ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the expected sign the coefficient should have in the neighbourhood about a spot. Args: pixel_coefs: `float [n_pixels x n_genes]`. `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero hence sparse form used. pixel_yxz: ```int [n_pixels x 3]```. ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. spot_yxz: ```int [n_spots x 3]```. ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. spot_gene_no: ```int [n_spots]```. ```spot_gene_no[s]``` is the gene that this spot is assigned to. max_size: `int [3]`. max YXZ size of spot shape returned. Zeros at extremities will be cropped in `av_spot_image`. pos_neighbour_thresh: For spot to be used to find av_spot_image, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Typical = 9. isolation_dist: Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find av_spot_image. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. mean_sign_thresh: If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in av_spot_image to be 0. Returns: - av_spot_image - `int8 [av_shape_y x av_shape_x x av_shape_z]` Expected sign of omp coefficient in neighbourhood centered on spot. - spot_indices_used - `int [n_spots_used]`. indices of spots in `spot_yxzg` used to make av_spot_image. - av_spot_image_float - `float [max_size[0] x max_size[1] x max_size[2]]` Mean of omp coefficient sign in neighbourhood centered on spot. This is before cropping and thresholding. \"\"\" # TODO: Maybe provide pixel_coef_sign instead of pixel_coef as less memory or use csr_matrix. n_pixels , n_genes = pixel_coefs . shape if not utils . errors . check_shape ( pixel_yxz , [ n_pixels , 3 ]): raise utils . errors . ShapeError ( 'pixel_yxz' , pixel_yxz . shape , ( n_pixels , 3 )) n_spots = spot_gene_no . shape [ 0 ] if not utils . errors . check_shape ( spot_yxz , [ n_spots , 3 ]): raise utils . errors . ShapeError ( 'spot_yxz' , spot_yxz . shape , ( n_spots , 3 )) n_z = pixel_yxz . max ( axis = 0 )[ 2 ] + 1 pos_filter_shape_yx = np . ceil ( np . sqrt ( pos_neighbour_thresh )) . astype ( int ) if pos_filter_shape_yx % 2 == 0 : # Shape must be odd pos_filter_shape_yx = pos_filter_shape_yx + 1 if n_z <= 2 : pos_filter_shape_z = 1 else : pos_filter_shape_z = 3 pos_filter = np . zeros (( pos_filter_shape_yx , pos_filter_shape_yx , pos_filter_shape_z ), dtype = int ) pos_filter [:, :, np . floor ( pos_filter_shape_z / 2 ) . astype ( int )] = 1 if pos_filter_shape_z == 3 : mid_yx = np . floor ( pos_filter_shape_yx / 2 ) . astype ( int ) pos_filter [ mid_yx , mid_yx , 0 ] = 1 pos_filter [ mid_yx , mid_yx , 2 ] = 1 max_size = np . array ( max_size ) if n_z == 1 : max_size [ 2 ] = 1 max_size_odd_loc = np . where ( np . array ( max_size ) % 2 == 0 )[ 0 ] if max_size_odd_loc . size > 0 : max_size [ max_size_odd_loc ] += 1 # ensure shape is odd # get image centred on each spot. # Big image shape which will be cropped later. spot_images = np . zeros (( 0 , * max_size ), dtype = int ) spots_used = np . zeros ( n_spots , dtype = bool ) for g in range ( n_genes ): use = spot_gene_no == g if use . any (): # Note size of image will be different for each gene. coef_sign_image , coord_shift = cropped_coef_image ( pixel_yxz , pixel_coefs [:, g ]) if coef_sign_image is None : # Go to next gene if no non-zero coefficients for this gene continue coef_sign_image = np . sign ( coef_sign_image ) . astype ( int ) g_spot_yxz = spot_yxz [ use ] - coord_shift # Only keep spots with all neighbourhood having positive coefficient. n_pos_neighb = count_spot_neighbours ( coef_sign_image , g_spot_yxz , pos_filter ) g_use = n_pos_neighb == pos_filter . sum () use [ np . where ( use )[ 0 ][ np . invert ( g_use )]] = False if coef_sign_image . ndim == 2 : coef_sign_image = coef_sign_image [:, :, np . newaxis ] if use . any (): # nan_to_num sets nan to zero i.e. if out of range of coef_sign_image, coef assumed zero. # This is what we want as have cropped coef_sign_image to exclude zero coefficients. spot_images = np . append ( spot_images , np . nan_to_num ( get_spot_images ( coef_sign_image , g_spot_yxz [ g_use ], max_size ) ) . astype ( int ), axis = 0 ) spots_used [ use ] = True if not spots_used . any (): raise ValueError ( \"No spots found to make average spot image from.\" ) # Compute average spot image from all isolated spots isolated = get_isolated_points ( spot_yxz [ spots_used ] * [ 1 , 1 , z_scale ], isolation_dist ) # get_average below ignores the nan values. av_spot_image = get_average_spot_image ( spot_images [ isolated ] . astype ( float ), 'mean' , 'annulus_3d' ) av_spot_image_float = av_spot_image . copy () spot_indices_used = np . where ( spots_used )[ 0 ][ isolated ] # Where mean sign is low, set to 0. av_spot_image [ np . abs ( av_spot_image ) < mean_sign_thresh ] = 0 av_spot_image = np . sign ( av_spot_image ) . astype ( np . int8 ) # Crop image to remove zeros at extremities # may get issue here if there is a positive sign pixel further away than negative but think unlikely. av_spot_image = av_spot_image [:, :, ~ np . all ( av_spot_image == 0 , axis = ( 0 , 1 ))] av_spot_image = av_spot_image [:, ~ np . all ( av_spot_image == 0 , axis = ( 0 , 2 )), :] av_spot_image = av_spot_image [ ~ np . all ( av_spot_image == 0 , axis = ( 1 , 2 )), :, :] if np . sum ( av_spot_image == 1 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of 1. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too high.\" ) if np . sum ( av_spot_image == - 1 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of -1. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too high.\" ) if np . sum ( av_spot_image == 0 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of 0. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too low.\" ) return av_spot_image , spot_indices_used , av_spot_image_float","title":"Spots"},{"location":"code/omp/spots/#coppafish.omp.spots.count_spot_neighbours","text":"Counts the number of positive (and negative) pixels in a neighbourhood about each spot. If filter contains only 1 and 0, then number of positive pixels returned near each spot. If filter contains only -1 and 0, then number of negative pixels returned near each spot. If filter contains -1, 0 and 1, then number of positive and negative pixels returned near each spot. Parameters: Name Type Description Default image np . ndarray float [n_y x n_x (x n_z)] . image spots were found on. required spot_yxz np . ndarray int [n_spots x image.ndim] . yx or yxz location of spots found. required kernel np . ndarray int [filter_sz_y x filter_sz_x (x filter_sz_z)] . Number of positive (and negative) pixels counted in this neighbourhood about each spot in image. Only contains values 0 and 1 (and -1). required Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] n_pos_neighbours - int [n_spots] (Only if filter contains 1). Number of positive pixels around each spot in neighbourhood given by pos_filter . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] n_neg_neighbours - int [n_spots] (Only if filter contains -1). Number of negative pixels around each spot in neighbourhood given by neg_filter . Source code in coppafish/omp/spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def count_spot_neighbours ( image : np . ndarray , spot_yxz : np . ndarray , kernel : np . ndarray ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Counts the number of positive (and negative) pixels in a neighbourhood about each spot. If `filter` contains only 1 and 0, then number of positive pixels returned near each spot. If `filter` contains only -1 and 0, then number of negative pixels returned near each spot. If `filter` contains -1, 0 and 1, then number of positive and negative pixels returned near each spot. Args: image: `float [n_y x n_x (x n_z)]`. image spots were found on. spot_yxz: `int [n_spots x image.ndim]`. yx or yxz location of spots found. kernel: `int [filter_sz_y x filter_sz_x (x filter_sz_z)]`. Number of positive (and negative) pixels counted in this neighbourhood about each spot in image. Only contains values 0 and 1 (and -1). Returns: - n_pos_neighbours - `int [n_spots]` (Only if `filter` contains 1). Number of positive pixels around each spot in neighbourhood given by `pos_filter`. - n_neg_neighbours - `int [n_spots]` (Only if `filter` contains -1). Number of negative pixels around each spot in neighbourhood given by `neg_filter`. \"\"\" # Correct for 2d cases where an empty dimension has been used for some variables. if all ([ image . ndim == spot_yxz . shape [ 1 ] - 1 , np . max ( np . abs ( spot_yxz [:, - 1 ])) == 0 ]): # Image 2D but spots 3D spot_yxz = spot_yxz [:, : image . ndim ] if all ([ image . ndim == spot_yxz . shape [ 1 ] + 1 , image . shape [ - 1 ] == 1 ]): # Image 3D but spots 2D image = np . mean ( image , axis = image . ndim - 1 ) # average over last dimension just means removing it. if all ([ image . ndim == kernel . ndim - 1 , kernel . shape [ - 1 ] == 1 ]): # Image 2D but pos_filter 3D kernel = np . mean ( kernel , axis = kernel . ndim - 1 ) # Check kernel contains right values. kernel_vals = np . unique ( kernel ) if not np . isin ( kernel_vals , [ - 1 , 0 , 1 ]) . all (): raise ValueError ( 'filter contains values other than -1, 0 or 1.' ) # Check all spots in image max_yxz = np . array ( image . shape ) - 1 spot_oob = [ val for val in spot_yxz if val . min () < 0 or any ( val > max_yxz )] if len ( spot_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"spot_yxz\" , spot_oob [ 0 ], [ 0 ] * image . ndim , max_yxz ) if np . isin ([ - 1 , 1 ], kernel_vals ) . all (): # Return positive and negative counts n_pos = utils . morphology . imfilter_coords ( image > 0 , kernel > 0 , spot_yxz ) n_neg = utils . morphology . imfilter_coords ( image < 0 , kernel < 0 , spot_yxz ) return n_pos , n_neg elif np . isin ( - 1 , kernel_vals ): # Return negative counts return utils . morphology . imfilter_coords ( image < 0 , kernel < 0 , spot_yxz ) . astype ( int ) elif np . isin ( 1 , kernel_vals ): # Return positive counts return utils . morphology . imfilter_coords ( image > 0 , kernel > 0 , spot_yxz ) . astype ( int ) else : raise ValueError ( 'filter contains only 0.' )","title":"count_spot_neighbours()"},{"location":"code/omp/spots/#coppafish.omp.spots.cropped_coef_image","text":"Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included. Parameters: Name Type Description Default pixel_yxz np . ndarray int [n_pixels x 3] pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required pixel_coefs Union [ csr_matrix , np . array ] float32 [n_pixels x 1] . pixel_coefs[s] is the weighting of pixel s for a given gene found by the omp algorithm. Most are zero hence sparse form used. required Returns: Type Description Optional [ np . ndarray ] coef_image - float32 [im_size_y x im_size_x x im_size_z] cropped omp coefficient. Will be None if there are no non-zero coefficients. Optional [ np . ndarray ] coord_shift - int [3] . yxz shift subtracted from pixel_yxz to build coef_image. Will be None if there are no non-zero coefficients. Source code in coppafish/omp/spots.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def cropped_coef_image ( pixel_yxz : np . ndarray , pixel_coefs : Union [ csr_matrix , np . array ]) -> Tuple [ Optional [ np . ndarray ], Optional [ np . ndarray ]]: \"\"\" Make cropped coef_image which is smallest possible image such that all non-zero pixel_coefs included. Args: pixel_yxz: `int [n_pixels x 3]` ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. pixel_coefs: `float32 [n_pixels x 1]`. `pixel_coefs[s]` is the weighting of pixel `s` for a given gene found by the omp algorithm. Most are zero hence sparse form used. Returns: - coef_image - `float32 [im_size_y x im_size_x x im_size_z]` cropped omp coefficient. Will be `None` if there are no non-zero coefficients. - coord_shift - `int [3]`. yxz shift subtracted from pixel_yxz to build coef_image. Will be `None` if there are no non-zero coefficients. \"\"\" if isinstance ( pixel_coefs , csr_matrix ): nz_ind = pixel_coefs . nonzero ()[ 0 ] nz_pixel_coefs = pixel_coefs [ nz_ind ] . toarray () . flatten () else : nz_ind = pixel_coefs != 0 nz_pixel_coefs = pixel_coefs [ nz_ind ] if nz_pixel_coefs . size == 0 : # If no non-zero coefficients, return nothing return None , None else : nz_pixel_yxz = pixel_yxz [ nz_ind , :] # shift nz_pixel_yxz so min is 0 in each axis so smaller image can be formed. coord_shift = nz_pixel_yxz . min ( axis = 0 ) nz_pixel_yxz = nz_pixel_yxz - coord_shift n_y , n_x , n_z = nz_pixel_yxz . max ( axis = 0 ) + 1 # coef_image at pixels other than nz_pixel_yxz is set to 0. if n_z == 1 : coef_image = np . zeros (( n_y , n_x ), dtype = np . float32 ) else : coef_image = np . zeros (( n_y , n_x , n_z ), dtype = np . float32 ) coef_image [ tuple ([ nz_pixel_yxz [:, j ] for j in range ( coef_image . ndim )])] = nz_pixel_coefs return coef_image , coord_shift","title":"cropped_coef_image()"},{"location":"code/omp/spots/#coppafish.omp.spots.get_spots","text":"Finds all local maxima in coef_image of each gene with coefficient exceeding coef_thresh and returns corresponding yxz position and gene_no . If provide spot_shape , also counts number of positive and negative pixels in neighbourhood of each spot. Parameters: Name Type Description Default pixel_coefs Union [ csr_matrix , np . array ] float [n_pixels x n_genes] . pixel_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero hence sparse form used. required pixel_yxz np . ndarray int [n_pixels x 3] . pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required radius_xy int Radius of dilation structuring element in xy plane (approximately spot radius). required radius_z Optional [ int ] Radius of dilation structuring element in z direction (approximately spot radius). If None , 2D filter is used. required coef_thresh float Local maxima in coef_image exceeding this value are considered spots. 0 spot_shape Optional [ np . ndarray ] int [shape_size_y x shape_size_x x shape_size_z] or None . Indicates expected sign of coefficients in neighbourhood of spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign so ignore. None pos_neighbour_thresh int Only spots with number of positive neighbours exceeding this will be kept if spot_shape provided. 0 spot_yxzg Optional [ np . ndarray ] float [n_spots x 4] . Can provide location and gene identity of spots if already computed. Where spots are local maxima above coef_thresh in pixel_coefs image for each gene. If None, spots are determined from pixel_coefs . spot_yxzg[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxzg[s, 2] is the local z coordinate in z_pixels for spot s . spot_yxzg[s, 3] is the gene number of spot s . None Returns: Type Description Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] spot_yxz - int [n_spots x 3] spot_yxz[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxz[s, 2] is the local z coordinate in z_pixels for spot s . Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] spot_gene_no - int [n_spots] . spot_gene_no[s] is the gene that spot s is assigned to. Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] n_pos_neighbours - int [n_spots] (Only if spot_shape given). Number of positive pixels around each spot in neighbourhood given by spot_shape==1 . Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]] n_neg_neighbours - int [n_spots] (Only if spot_shape given). Number of negative pixels around each spot in neighbourhood given by spot_shape==-1 . Source code in coppafish/omp/spots.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def get_spots ( pixel_coefs : Union [ csr_matrix , np . array ], pixel_yxz : np . ndarray , radius_xy : int , radius_z : Optional [ int ], coef_thresh : float = 0 , spot_shape : Optional [ np . ndarray ] = None , pos_neighbour_thresh : int = 0 , spot_yxzg : Optional [ np . ndarray ] = None ) -> Union [ Tuple [ np . ndarray , np . ndarray ], Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]]: \"\"\" Finds all local maxima in `coef_image` of each gene with coefficient exceeding `coef_thresh` and returns corresponding `yxz` position and `gene_no`. If provide `spot_shape`, also counts number of positive and negative pixels in neighbourhood of each spot. Args: pixel_coefs: `float [n_pixels x n_genes]`. `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero hence sparse form used. pixel_yxz: ```int [n_pixels x 3]```. ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. radius_xy: Radius of dilation structuring element in xy plane (approximately spot radius). radius_z: Radius of dilation structuring element in z direction (approximately spot radius). If ```None```, 2D filter is used. coef_thresh: Local maxima in `coef_image` exceeding this value are considered spots. spot_shape: `int [shape_size_y x shape_size_x x shape_size_z]` or `None`. Indicates expected sign of coefficients in neighbourhood of spot. 1 means expected positive coefficient. -1 means expected negative coefficient. 0 means unsure of expected sign so ignore. pos_neighbour_thresh: Only spots with number of positive neighbours exceeding this will be kept if `spot_shape` provided. spot_yxzg: `float [n_spots x 4]`. Can provide location and gene identity of spots if already computed. Where spots are local maxima above `coef_thresh` in `pixel_coefs` image for each gene. If None, spots are determined from `pixel_coefs`. ```spot_yxzg[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxzg[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. ```spot_yxzg[s, 3]``` is the gene number of spot ```s```. Returns: - spot_yxz - `int [n_spots x 3]` ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. - spot_gene_no - `int [n_spots]`. ```spot_gene_no[s]``` is the gene that spot s is assigned to. - n_pos_neighbours - `int [n_spots]` (Only if `spot_shape` given). Number of positive pixels around each spot in neighbourhood given by `spot_shape==1`. - n_neg_neighbours - `int [n_spots]` (Only if `spot_shape` given). Number of negative pixels around each spot in neighbourhood given by `spot_shape==-1`. \"\"\" n_pixels , n_genes = pixel_coefs . shape if not utils . errors . check_shape ( pixel_yxz , [ n_pixels , 3 ]): raise utils . errors . ShapeError ( 'pixel_yxz' , pixel_yxz . shape , ( n_pixels , 3 )) if spot_shape is None : spot_info = np . zeros (( 0 , 4 ), dtype = int ) else : if np . sum ( spot_shape == 1 ) == 0 : raise ValueError ( f \"spot_shape contains no pixels with a value of 1 which indicates the \" f \"neighbourhood about a spot where we expect a positive coefficient.\" ) if np . sum ( spot_shape == - 1 ) == 0 : raise ValueError ( f \"spot_shape contains no pixels with a value of -1 which indicates the \" f \"neighbourhood about a spot where we expect a negative coefficient.\" ) if pos_neighbour_thresh < 0 or pos_neighbour_thresh >= np . sum ( spot_shape > 0 ): # Out of bounds if threshold for positive neighbours is above the maximum possible. raise utils . errors . OutOfBoundsError ( \"pos_neighbour_thresh\" , pos_neighbour_thresh , 0 , np . sum ( spot_shape > 0 ) - 1 ) spot_info = np . zeros (( 0 , 6 ), dtype = int ) if spot_yxzg is not None : # check pixel coefficient is positive for random subset of 500 spots. spots_to_check = np . random . choice ( range ( spot_yxzg . shape [ 0 ]), np . clip ( 500 , 0 , spot_yxzg . shape [ 0 ]), replace = False ) pixel_index = numpy_indexed . indices ( pixel_yxz , spot_yxzg [ spots_to_check , : 3 ] . astype ( pixel_yxz . dtype )) spot_coefs_check = pixel_coefs [ pixel_index , spot_yxzg [ spots_to_check , 3 ]] if spot_coefs_check . min () <= coef_thresh : bad_spot = spots_to_check [ spot_coefs_check . argmin ()] raise ValueError ( f \"spot_yxzg provided but gene { spot_yxzg [ bad_spot , 3 ] } coefficient for spot { bad_spot } \\n \" f \"at yxz = { spot_yxzg [ bad_spot , : 3 ] } is { spot_coefs_check . min () } \\n \" f \"whereas it should be more than coef_thresh = { coef_thresh } as it is listed as a spot.\" ) with tqdm ( total = n_genes ) as pbar : # TODO: if 2D can do all genes together. pbar . set_description ( f \"Finding spots for all { n_genes } genes from omp_coef images.\" ) for g in range ( n_genes ): # shift nzg_pixel_yxz so min is 0 in each axis so smaller image can be formed. # Note size of image will be different for each gene. coef_image , coord_shift = cropped_coef_image ( pixel_yxz , pixel_coefs [:, g ]) if coef_image is None : # If no non-zero coefficients, go to next gene continue if spot_yxzg is None : spot_yxz = detect_spots ( coef_image , coef_thresh , radius_xy , radius_z , False )[ 0 ] else : # spot_yxz match pixel_yxz so if crop pixel_yxz need to crop spot_yxz too. spot_yxz = spot_yxzg [ spot_yxzg [:, 3 ] == g , : coef_image . ndim ] - coord_shift [: coef_image . ndim ] if spot_yxz . shape [ 0 ] > 0 : if spot_shape is None : keep = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) spot_info_g = np . zeros (( np . sum ( keep ), 4 ), dtype = int ) else : n_pos_neighb , n_neg_neighb = count_spot_neighbours ( coef_image , spot_yxz , spot_shape ) keep = n_pos_neighb > pos_neighbour_thresh spot_info_g = np . zeros (( np . sum ( keep ), 6 ), dtype = int ) spot_info_g [:, 4 ] = n_pos_neighb [ keep ] spot_info_g [:, 5 ] = n_neg_neighb [ keep ] spot_info_g [:, : coef_image . ndim ] = spot_yxz [ keep ] spot_info_g [:, : 3 ] = spot_info_g [:, : 3 ] + coord_shift # shift spot_yxz back spot_info_g [:, 3 ] = g spot_info = np . append ( spot_info , spot_info_g , axis = 0 ) pbar . update ( 1 ) pbar . close () if spot_shape is None : return spot_info [:, : 3 ], spot_info [:, 3 ] else : return spot_info [:, : 3 ], spot_info [:, 3 ], spot_info [:, 4 ], spot_info [:, 5 ]","title":"get_spots()"},{"location":"code/omp/spots/#coppafish.omp.spots.spot_neighbourhood","text":"Finds the expected sign the coefficient should have in the neighbourhood about a spot. Parameters: Name Type Description Default pixel_coefs Union [ csr_matrix , np . array ] float [n_pixels x n_genes] . pixel_coefs[s, g] is the weighting of pixel s for gene g found by the omp algorithm. Most are zero hence sparse form used. required pixel_yxz np . ndarray int [n_pixels x 3] . pixel_yxz[s, :2] are the local yx coordinates in yx_pixels for pixel s . pixel_yxz[s, 2] is the local z coordinate in z_pixels for pixel s . required spot_yxz np . ndarray int [n_spots x 3] . spot_yxz[s, :2] are the local yx coordinates in yx_pixels for spot s . spot_yxz[s, 2] is the local z coordinate in z_pixels for spot s . required spot_gene_no np . ndarray int [n_spots] . spot_gene_no[s] is the gene that this spot is assigned to. required max_size Union [ np . ndarray , List ] int [3] . max YXZ size of spot shape returned. Zeros at extremities will be cropped in av_spot_image . required pos_neighbour_thresh int For spot to be used to find av_spot_image, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Typical = 9. required isolation_dist float Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find av_spot_image. required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required mean_sign_thresh float If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in av_spot_image to be 0. required Returns: Type Description np . ndarray av_spot_image - int8 [av_shape_y x av_shape_x x av_shape_z] Expected sign of omp coefficient in neighbourhood centered on spot. np . ndarray spot_indices_used - int [n_spots_used] . indices of spots in spot_yxzg used to make av_spot_image. np . ndarray av_spot_image_float - float [max_size[0] x max_size[1] x max_size[2]] Mean of omp coefficient sign in neighbourhood centered on spot. This is before cropping and thresholding. Source code in coppafish/omp/spots.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def spot_neighbourhood ( pixel_coefs : Union [ csr_matrix , np . array ], pixel_yxz : np . ndarray , spot_yxz : np . ndarray , spot_gene_no : np . ndarray , max_size : Union [ np . ndarray , List ], pos_neighbour_thresh : int , isolation_dist : float , z_scale : float , mean_sign_thresh : float ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Finds the expected sign the coefficient should have in the neighbourhood about a spot. Args: pixel_coefs: `float [n_pixels x n_genes]`. `pixel_coefs[s, g]` is the weighting of pixel `s` for gene `g` found by the omp algorithm. Most are zero hence sparse form used. pixel_yxz: ```int [n_pixels x 3]```. ```pixel_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for pixel ```s```. ```pixel_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for pixel ```s```. spot_yxz: ```int [n_spots x 3]```. ```spot_yxz[s, :2]``` are the local yx coordinates in ```yx_pixels``` for spot ```s```. ```spot_yxz[s, 2]``` is the local z coordinate in ```z_pixels``` for spot ```s```. spot_gene_no: ```int [n_spots]```. ```spot_gene_no[s]``` is the gene that this spot is assigned to. max_size: `int [3]`. max YXZ size of spot shape returned. Zeros at extremities will be cropped in `av_spot_image`. pos_neighbour_thresh: For spot to be used to find av_spot_image, it must have this many pixels around it on the same z-plane that have a positive coefficient. If 3D, also, require 1 positive pixel on each neighbouring plane (i.e. 2 is added to this value). Typical = 9. isolation_dist: Spots are isolated if nearest neighbour (across all genes) is further away than this. Only isolated spots are used to find av_spot_image. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. mean_sign_thresh: If the mean absolute coefficient sign is less than this in a region near a spot, we set the expected coefficient in av_spot_image to be 0. Returns: - av_spot_image - `int8 [av_shape_y x av_shape_x x av_shape_z]` Expected sign of omp coefficient in neighbourhood centered on spot. - spot_indices_used - `int [n_spots_used]`. indices of spots in `spot_yxzg` used to make av_spot_image. - av_spot_image_float - `float [max_size[0] x max_size[1] x max_size[2]]` Mean of omp coefficient sign in neighbourhood centered on spot. This is before cropping and thresholding. \"\"\" # TODO: Maybe provide pixel_coef_sign instead of pixel_coef as less memory or use csr_matrix. n_pixels , n_genes = pixel_coefs . shape if not utils . errors . check_shape ( pixel_yxz , [ n_pixels , 3 ]): raise utils . errors . ShapeError ( 'pixel_yxz' , pixel_yxz . shape , ( n_pixels , 3 )) n_spots = spot_gene_no . shape [ 0 ] if not utils . errors . check_shape ( spot_yxz , [ n_spots , 3 ]): raise utils . errors . ShapeError ( 'spot_yxz' , spot_yxz . shape , ( n_spots , 3 )) n_z = pixel_yxz . max ( axis = 0 )[ 2 ] + 1 pos_filter_shape_yx = np . ceil ( np . sqrt ( pos_neighbour_thresh )) . astype ( int ) if pos_filter_shape_yx % 2 == 0 : # Shape must be odd pos_filter_shape_yx = pos_filter_shape_yx + 1 if n_z <= 2 : pos_filter_shape_z = 1 else : pos_filter_shape_z = 3 pos_filter = np . zeros (( pos_filter_shape_yx , pos_filter_shape_yx , pos_filter_shape_z ), dtype = int ) pos_filter [:, :, np . floor ( pos_filter_shape_z / 2 ) . astype ( int )] = 1 if pos_filter_shape_z == 3 : mid_yx = np . floor ( pos_filter_shape_yx / 2 ) . astype ( int ) pos_filter [ mid_yx , mid_yx , 0 ] = 1 pos_filter [ mid_yx , mid_yx , 2 ] = 1 max_size = np . array ( max_size ) if n_z == 1 : max_size [ 2 ] = 1 max_size_odd_loc = np . where ( np . array ( max_size ) % 2 == 0 )[ 0 ] if max_size_odd_loc . size > 0 : max_size [ max_size_odd_loc ] += 1 # ensure shape is odd # get image centred on each spot. # Big image shape which will be cropped later. spot_images = np . zeros (( 0 , * max_size ), dtype = int ) spots_used = np . zeros ( n_spots , dtype = bool ) for g in range ( n_genes ): use = spot_gene_no == g if use . any (): # Note size of image will be different for each gene. coef_sign_image , coord_shift = cropped_coef_image ( pixel_yxz , pixel_coefs [:, g ]) if coef_sign_image is None : # Go to next gene if no non-zero coefficients for this gene continue coef_sign_image = np . sign ( coef_sign_image ) . astype ( int ) g_spot_yxz = spot_yxz [ use ] - coord_shift # Only keep spots with all neighbourhood having positive coefficient. n_pos_neighb = count_spot_neighbours ( coef_sign_image , g_spot_yxz , pos_filter ) g_use = n_pos_neighb == pos_filter . sum () use [ np . where ( use )[ 0 ][ np . invert ( g_use )]] = False if coef_sign_image . ndim == 2 : coef_sign_image = coef_sign_image [:, :, np . newaxis ] if use . any (): # nan_to_num sets nan to zero i.e. if out of range of coef_sign_image, coef assumed zero. # This is what we want as have cropped coef_sign_image to exclude zero coefficients. spot_images = np . append ( spot_images , np . nan_to_num ( get_spot_images ( coef_sign_image , g_spot_yxz [ g_use ], max_size ) ) . astype ( int ), axis = 0 ) spots_used [ use ] = True if not spots_used . any (): raise ValueError ( \"No spots found to make average spot image from.\" ) # Compute average spot image from all isolated spots isolated = get_isolated_points ( spot_yxz [ spots_used ] * [ 1 , 1 , z_scale ], isolation_dist ) # get_average below ignores the nan values. av_spot_image = get_average_spot_image ( spot_images [ isolated ] . astype ( float ), 'mean' , 'annulus_3d' ) av_spot_image_float = av_spot_image . copy () spot_indices_used = np . where ( spots_used )[ 0 ][ isolated ] # Where mean sign is low, set to 0. av_spot_image [ np . abs ( av_spot_image ) < mean_sign_thresh ] = 0 av_spot_image = np . sign ( av_spot_image ) . astype ( np . int8 ) # Crop image to remove zeros at extremities # may get issue here if there is a positive sign pixel further away than negative but think unlikely. av_spot_image = av_spot_image [:, :, ~ np . all ( av_spot_image == 0 , axis = ( 0 , 1 ))] av_spot_image = av_spot_image [:, ~ np . all ( av_spot_image == 0 , axis = ( 0 , 2 )), :] av_spot_image = av_spot_image [ ~ np . all ( av_spot_image == 0 , axis = ( 1 , 2 )), :, :] if np . sum ( av_spot_image == 1 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of 1. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too high.\" ) if np . sum ( av_spot_image == - 1 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of -1. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too high.\" ) if np . sum ( av_spot_image == 0 ) == 0 : warnings . warn ( f \"In av_spot_image, no pixels have a value of 0. \\n \" f \"Maybe mean_sign_thresh = { mean_sign_thresh } is too low.\" ) return av_spot_image , spot_indices_used , av_spot_image_float","title":"spot_neighbourhood()"},{"location":"code/pipeline/basic_info/","text":"set_basic_info ( config_file , config_basic ) Adds info from 'basic_info' section of config file to notebook page. To basic_info page, the following is also added: anchor_round , n_rounds , n_extra_rounds , n_tiles , n_channels , nz , tile_sz , tilepos_yx , tilepos_yx_nd2 , pixel_size_xy , pixel_size_z , tile_centre , use_anchor . See 'basic_info' sections of notebook_comments.json file for description of the variables. Parameters: Name Type Description Default config_file dict Dictionary obtained from 'file_names' section of config file. required config_basic dict Dictionary obtained from 'basic_info' section of config file. required Returns: Type Description NotebookPage NotebookPage[basic_info] - Page contains information that is used at all stages of the pipeline. Source code in coppafish/pipeline/basic_info.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def set_basic_info ( config_file : dict , config_basic : dict ) -> NotebookPage : \"\"\" Adds info from `'basic_info'` section of config file to notebook page. To `basic_info` page, the following is also added: `anchor_round`, `n_rounds`, `n_extra_rounds`, `n_tiles`, `n_channels`, `nz`, `tile_sz`, `tilepos_yx`, `tilepos_yx_nd2`, `pixel_size_xy`, `pixel_size_z`, `tile_centre`, `use_anchor`. See `'basic_info'` sections of `notebook_comments.json` file for description of the variables. Args: config_file: Dictionary obtained from `'file_names'` section of config file. config_basic: Dictionary obtained from `'basic_info'` section of config file. Returns: - `NotebookPage[basic_info]` - Page contains information that is used at all stages of the pipeline. \"\"\" nbp = NotebookPage ( 'basic_info' ) nbp . is_3d = config_basic [ 'is_3d' ] # Deal with case where no imaging rounds, just want to run anchor round. if config_file [ 'round' ] is None : if config_file [ 'anchor' ] is None : raise ValueError ( f 'Neither imaging rounds nor anchor_round provided' ) config_file [ 'round' ] = [] n_rounds = len ( config_file [ 'round' ]) # Set ref/anchor round/channel if config_file [ 'anchor' ] is None : config_basic [ 'anchor_channel' ] = None # set anchor channel to None if no anchor round config_basic [ 'dapi_channel' ] = None # set dapi channel to None if no anchor round if config_basic [ 'ref_round' ] is None : raise ValueError ( 'No anchor round used, but ref_round not specified' ) if config_basic [ 'ref_channel' ] is None : raise ValueError ( 'No anchor round used, but ref_channel not specified' ) nbp . anchor_round = None nbp . n_extra_rounds = 0 nbp . use_anchor = False warnings . warn ( f \"Anchor file not given.\" f \" \\n Will use round { config_basic [ 'ref_round' ] } , \" f \"channel { config_basic [ 'ref_channel' ] } as reference\" ) else : if config_basic [ 'anchor_channel' ] is None : raise ValueError ( 'Using anchor round, but anchor_channel not specified' ) # always have anchor as first round after imaging rounds nbp . anchor_round = n_rounds config_basic [ 'ref_round' ] = nbp . anchor_round config_basic [ 'ref_channel' ] = config_basic [ 'anchor_channel' ] nbp . n_extra_rounds = 1 nbp . use_anchor = True warnings . warn ( f \"Anchor file given and anchor channel specified.\" f \" \\n Will use anchor round, channel { config_basic [ 'anchor_channel' ] } as reference\" ) nbp . anchor_channel = config_basic [ 'anchor_channel' ] nbp . dapi_channel = config_basic [ 'dapi_channel' ] nbp . ref_round = config_basic [ 'ref_round' ] nbp . ref_channel = config_basic [ 'ref_channel' ] if config_basic [ 'use_rounds' ] is None : config_basic [ 'use_rounds' ] = list ( np . arange ( n_rounds )) nbp . use_rounds = config_basic [ 'use_rounds' ] nbp . use_rounds . sort () # ensure ascending use_rounds_oob = [ val for val in nbp . use_rounds if val < 0 or val >= n_rounds ] if len ( use_rounds_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_rounds\" , use_rounds_oob [ 0 ], 0 , n_rounds - 1 ) if len ( config_file [ 'round' ]) > 0 : first_round_raw = os . path . join ( config_file [ 'input_dir' ], config_file [ 'round' ][ 0 ]) else : first_round_raw = os . path . join ( config_file [ 'input_dir' ], config_file [ 'anchor' ]) if config_file [ 'raw_extension' ] == '.nd2' : # load in metadata of nd2 file corresponding to first round # Test for number of rounds in case of separate round registration and load metadata # from anchor round in that case metadata = utils . nd2 . get_metadata ( first_round_raw + config_file [ 'raw_extension' ]) elif config_file [ 'raw_extension' ] == '.npy' : # Load in metadata as dictionary from a json file config_file [ 'raw_metadata' ] = config_file [ 'raw_metadata' ] . replace ( '.json' , '' ) metadata_file = os . path . join ( config_file [ 'input_dir' ], config_file [ 'raw_metadata' ] + '.json' ) metadata = json . load ( open ( metadata_file )) # Check metadata info matches that in first round npy file. use_tiles_nd2 = utils . raw . metadata_sanity_check ( metadata , first_round_raw ) else : raise ValueError ( f \"config_file['raw_extension'] should be either '.nd2' or '.npy' but it is \" f \" { config_file [ 'raw_extension' ] } .\" ) # get channel info n_channels = metadata [ 'sizes' ][ 'c' ] if config_basic [ 'use_channels' ] is None : config_basic [ 'use_channels' ] = list ( np . arange ( n_channels )) nbp . use_channels = config_basic [ 'use_channels' ] nbp . use_channels . sort () use_channels_oob = [ val for val in nbp . use_channels if val < 0 or val >= n_channels ] if len ( use_channels_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_channels\" , use_channels_oob [ 0 ], 0 , n_channels - 1 ) # get z info if config_basic [ 'use_z' ] is None : config_basic [ 'use_z' ] = list ( np . arange ( metadata [ 'sizes' ][ 'z' ])) elif len ( config_basic [ 'use_z' ]) == 2 : # use consecutive values if only 2 given. config_basic [ 'use_z' ] = list ( np . arange ( config_basic [ 'use_z' ][ 0 ], config_basic [ 'use_z' ][ 1 ] + 1 )) if config_basic [ 'ignore_first_z_plane' ] and 0 in config_basic [ 'use_z' ]: config_basic [ 'use_z' ] . remove ( 0 ) nbp . use_z = config_basic [ 'use_z' ] nbp . use_z . sort () use_z_oob = [ val for val in nbp . use_z if val < 0 or val >= metadata [ 'sizes' ][ 'z' ]] if len ( use_z_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_z\" , use_z_oob [ 0 ], 0 , metadata [ 'sizes' ][ 'z' ] - 1 ) # get tile info tile_sz = metadata [ 'sizes' ][ 'x' ] n_tiles = metadata [ 'sizes' ][ 't' ] tilepos_yx_nd2 , tilepos_yx = setup . get_tilepos ( np . asarray ( metadata [ 'xy_pos' ]), tile_sz ) nbp . tilepos_yx_nd2 = tilepos_yx_nd2 # numpy array, yx coordinate of tile with nd2 index. nbp . tilepos_yx = tilepos_yx # and with npy index if config_file [ 'raw_extension' ] == '.npy' : # Read tile indices from raw data folder and set to use_tiles if not specified already. use_tiles_folder = utils . npy . get_npy_tile_ind ( use_tiles_nd2 , tilepos_yx_nd2 , tilepos_yx ) if config_basic [ 'use_tiles' ] is None : config_basic [ 'use_tiles' ] = use_tiles_folder elif np . setdiff1d ( config_basic [ 'use_tiles' ], use_tiles_folder ) . size > 0 : raise ValueError ( f \"config_basic['use_tiles'] = { config_basic [ 'use_tiles' ] } \\n \" f \"But in the folder: \\n { first_round_raw } \\n Tiles Available are { use_tiles_folder } .\" ) if config_basic [ 'use_tiles' ] is None : config_basic [ 'use_tiles' ] = list ( np . arange ( n_tiles )) if config_basic [ 'ignore_tiles' ] is not None : config_basic [ 'use_tiles' ] = list ( np . setdiff1d ( config_basic [ 'use_tiles' ], config_basic [ 'ignore_tiles' ])) nbp . use_tiles = config_basic [ 'use_tiles' ] nbp . use_tiles . sort () use_tiles_oob = [ val for val in nbp . use_tiles if val < 0 or val >= n_tiles ] if len ( use_tiles_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_tiles\" , use_tiles_oob [ 0 ], 0 , n_tiles - 1 ) # get dye info if config_basic [ 'dye_names' ] is None : warnings . warn ( f \"dye_names not specified so assuming separate dye for each channel.\" ) n_dyes = n_channels else : # Ensure channel_camera/channel_laser are correct sizes n_dyes = len ( config_basic [ 'dye_names' ]) if config_basic [ 'channel_camera' ] is None : raise ValueError ( 'dye_names specified but channel_camera is not.' ) elif len ( config_basic [ 'channel_camera' ]) != n_channels : raise ValueError ( f \"channel_camera contains { len ( config_basic [ 'channel_camera' ]) } values. \\n \" f \"But there must be a value for each channel and there are { n_channels } channels.\" ) if config_basic [ 'channel_laser' ] is None : raise ValueError ( 'dye_names specified but channel_laser is not.' ) elif len ( config_basic [ 'channel_laser' ]) != n_channels : raise ValueError ( f \"channel_laser contains { len ( config_basic [ 'channel_camera' ]) } values. \\n \" f \"But there must be a value for each channel and there are { n_channels } channels.\" ) if config_basic [ 'use_dyes' ] is None : if config_basic [ 'dye_names' ] is None : config_basic [ 'use_dyes' ] = nbp . use_channels else : config_basic [ 'use_dyes' ] = list ( np . arange ( n_dyes )) nbp . use_dyes = config_basic [ 'use_dyes' ] nbp . dye_names = config_basic [ 'dye_names' ] nbp . channel_camera = config_basic [ 'channel_camera' ] nbp . channel_laser = config_basic [ 'channel_laser' ] nbp . tile_pixel_value_shift = config_basic [ 'tile_pixel_value_shift' ] # Add size info obtained from raw metadata to notebook page nbp . n_rounds = n_rounds # int, number of imaging rounds nbp . tile_sz = tile_sz # xy dimension of tiles in pixels. nbp . n_tiles = n_tiles # int, number of tiles nbp . n_channels = n_channels # int, number of imaging channels nbp . nz = len ( nbp . use_z ) # number of z planes in npy file (not necessarily the same as in nd2) nbp . n_dyes = n_dyes # int, number of dyes # subtract tile_centre from local pixel coordinates to get centered local tile coordinates if not nbp . is_3d : nz = 1 else : nz = nbp . nz nbp . tile_centre = ( np . array ([ tile_sz , tile_sz , nz ]) - 1 ) / 2 nbp . pixel_size_xy = metadata [ 'pixel_microns' ] # pixel size in microns in xy nbp . pixel_size_z = metadata [ 'pixel_microns_z' ] # and z directions. # Make sure reference rounds/channels are in range of data provided. if nbp . use_anchor : if not 0 <= nbp . ref_channel <= n_channels - 1 : raise utils . errors . OutOfBoundsError ( \"ref_channel\" , nbp . ref_channel , 0 , n_channels - 1 ) if nbp . dapi_channel is not None : if not 0 <= nbp . dapi_channel <= n_channels - 1 : raise utils . errors . OutOfBoundsError ( \"dapi_channel\" , nbp . ref_channel , 0 , n_channels - 1 ) else : # Seen as ref_channel is an imaging channel if anchor not used, ref_channel must be an imaging channels i.e. # must be in use_channels. Same goes for ref_round. if not np . isin ( nbp . ref_channel , nbp . use_channels ): raise ValueError ( f \"ref_channel is { nbp . ref_channel } which is not in use_channels = { nbp . use_channels } .\" ) if not np . isin ( nbp . ref_round , nbp . use_rounds ): raise ValueError ( f \"ref_round is { nbp . ref_round } which is not in use_rounds = { nbp . use_rounds } .\" ) return nbp","title":"Basic Info"},{"location":"code/pipeline/basic_info/#coppafish.pipeline.basic_info.set_basic_info","text":"Adds info from 'basic_info' section of config file to notebook page. To basic_info page, the following is also added: anchor_round , n_rounds , n_extra_rounds , n_tiles , n_channels , nz , tile_sz , tilepos_yx , tilepos_yx_nd2 , pixel_size_xy , pixel_size_z , tile_centre , use_anchor . See 'basic_info' sections of notebook_comments.json file for description of the variables. Parameters: Name Type Description Default config_file dict Dictionary obtained from 'file_names' section of config file. required config_basic dict Dictionary obtained from 'basic_info' section of config file. required Returns: Type Description NotebookPage NotebookPage[basic_info] - Page contains information that is used at all stages of the pipeline. Source code in coppafish/pipeline/basic_info.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 def set_basic_info ( config_file : dict , config_basic : dict ) -> NotebookPage : \"\"\" Adds info from `'basic_info'` section of config file to notebook page. To `basic_info` page, the following is also added: `anchor_round`, `n_rounds`, `n_extra_rounds`, `n_tiles`, `n_channels`, `nz`, `tile_sz`, `tilepos_yx`, `tilepos_yx_nd2`, `pixel_size_xy`, `pixel_size_z`, `tile_centre`, `use_anchor`. See `'basic_info'` sections of `notebook_comments.json` file for description of the variables. Args: config_file: Dictionary obtained from `'file_names'` section of config file. config_basic: Dictionary obtained from `'basic_info'` section of config file. Returns: - `NotebookPage[basic_info]` - Page contains information that is used at all stages of the pipeline. \"\"\" nbp = NotebookPage ( 'basic_info' ) nbp . is_3d = config_basic [ 'is_3d' ] # Deal with case where no imaging rounds, just want to run anchor round. if config_file [ 'round' ] is None : if config_file [ 'anchor' ] is None : raise ValueError ( f 'Neither imaging rounds nor anchor_round provided' ) config_file [ 'round' ] = [] n_rounds = len ( config_file [ 'round' ]) # Set ref/anchor round/channel if config_file [ 'anchor' ] is None : config_basic [ 'anchor_channel' ] = None # set anchor channel to None if no anchor round config_basic [ 'dapi_channel' ] = None # set dapi channel to None if no anchor round if config_basic [ 'ref_round' ] is None : raise ValueError ( 'No anchor round used, but ref_round not specified' ) if config_basic [ 'ref_channel' ] is None : raise ValueError ( 'No anchor round used, but ref_channel not specified' ) nbp . anchor_round = None nbp . n_extra_rounds = 0 nbp . use_anchor = False warnings . warn ( f \"Anchor file not given.\" f \" \\n Will use round { config_basic [ 'ref_round' ] } , \" f \"channel { config_basic [ 'ref_channel' ] } as reference\" ) else : if config_basic [ 'anchor_channel' ] is None : raise ValueError ( 'Using anchor round, but anchor_channel not specified' ) # always have anchor as first round after imaging rounds nbp . anchor_round = n_rounds config_basic [ 'ref_round' ] = nbp . anchor_round config_basic [ 'ref_channel' ] = config_basic [ 'anchor_channel' ] nbp . n_extra_rounds = 1 nbp . use_anchor = True warnings . warn ( f \"Anchor file given and anchor channel specified.\" f \" \\n Will use anchor round, channel { config_basic [ 'anchor_channel' ] } as reference\" ) nbp . anchor_channel = config_basic [ 'anchor_channel' ] nbp . dapi_channel = config_basic [ 'dapi_channel' ] nbp . ref_round = config_basic [ 'ref_round' ] nbp . ref_channel = config_basic [ 'ref_channel' ] if config_basic [ 'use_rounds' ] is None : config_basic [ 'use_rounds' ] = list ( np . arange ( n_rounds )) nbp . use_rounds = config_basic [ 'use_rounds' ] nbp . use_rounds . sort () # ensure ascending use_rounds_oob = [ val for val in nbp . use_rounds if val < 0 or val >= n_rounds ] if len ( use_rounds_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_rounds\" , use_rounds_oob [ 0 ], 0 , n_rounds - 1 ) if len ( config_file [ 'round' ]) > 0 : first_round_raw = os . path . join ( config_file [ 'input_dir' ], config_file [ 'round' ][ 0 ]) else : first_round_raw = os . path . join ( config_file [ 'input_dir' ], config_file [ 'anchor' ]) if config_file [ 'raw_extension' ] == '.nd2' : # load in metadata of nd2 file corresponding to first round # Test for number of rounds in case of separate round registration and load metadata # from anchor round in that case metadata = utils . nd2 . get_metadata ( first_round_raw + config_file [ 'raw_extension' ]) elif config_file [ 'raw_extension' ] == '.npy' : # Load in metadata as dictionary from a json file config_file [ 'raw_metadata' ] = config_file [ 'raw_metadata' ] . replace ( '.json' , '' ) metadata_file = os . path . join ( config_file [ 'input_dir' ], config_file [ 'raw_metadata' ] + '.json' ) metadata = json . load ( open ( metadata_file )) # Check metadata info matches that in first round npy file. use_tiles_nd2 = utils . raw . metadata_sanity_check ( metadata , first_round_raw ) else : raise ValueError ( f \"config_file['raw_extension'] should be either '.nd2' or '.npy' but it is \" f \" { config_file [ 'raw_extension' ] } .\" ) # get channel info n_channels = metadata [ 'sizes' ][ 'c' ] if config_basic [ 'use_channels' ] is None : config_basic [ 'use_channels' ] = list ( np . arange ( n_channels )) nbp . use_channels = config_basic [ 'use_channels' ] nbp . use_channels . sort () use_channels_oob = [ val for val in nbp . use_channels if val < 0 or val >= n_channels ] if len ( use_channels_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_channels\" , use_channels_oob [ 0 ], 0 , n_channels - 1 ) # get z info if config_basic [ 'use_z' ] is None : config_basic [ 'use_z' ] = list ( np . arange ( metadata [ 'sizes' ][ 'z' ])) elif len ( config_basic [ 'use_z' ]) == 2 : # use consecutive values if only 2 given. config_basic [ 'use_z' ] = list ( np . arange ( config_basic [ 'use_z' ][ 0 ], config_basic [ 'use_z' ][ 1 ] + 1 )) if config_basic [ 'ignore_first_z_plane' ] and 0 in config_basic [ 'use_z' ]: config_basic [ 'use_z' ] . remove ( 0 ) nbp . use_z = config_basic [ 'use_z' ] nbp . use_z . sort () use_z_oob = [ val for val in nbp . use_z if val < 0 or val >= metadata [ 'sizes' ][ 'z' ]] if len ( use_z_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_z\" , use_z_oob [ 0 ], 0 , metadata [ 'sizes' ][ 'z' ] - 1 ) # get tile info tile_sz = metadata [ 'sizes' ][ 'x' ] n_tiles = metadata [ 'sizes' ][ 't' ] tilepos_yx_nd2 , tilepos_yx = setup . get_tilepos ( np . asarray ( metadata [ 'xy_pos' ]), tile_sz ) nbp . tilepos_yx_nd2 = tilepos_yx_nd2 # numpy array, yx coordinate of tile with nd2 index. nbp . tilepos_yx = tilepos_yx # and with npy index if config_file [ 'raw_extension' ] == '.npy' : # Read tile indices from raw data folder and set to use_tiles if not specified already. use_tiles_folder = utils . npy . get_npy_tile_ind ( use_tiles_nd2 , tilepos_yx_nd2 , tilepos_yx ) if config_basic [ 'use_tiles' ] is None : config_basic [ 'use_tiles' ] = use_tiles_folder elif np . setdiff1d ( config_basic [ 'use_tiles' ], use_tiles_folder ) . size > 0 : raise ValueError ( f \"config_basic['use_tiles'] = { config_basic [ 'use_tiles' ] } \\n \" f \"But in the folder: \\n { first_round_raw } \\n Tiles Available are { use_tiles_folder } .\" ) if config_basic [ 'use_tiles' ] is None : config_basic [ 'use_tiles' ] = list ( np . arange ( n_tiles )) if config_basic [ 'ignore_tiles' ] is not None : config_basic [ 'use_tiles' ] = list ( np . setdiff1d ( config_basic [ 'use_tiles' ], config_basic [ 'ignore_tiles' ])) nbp . use_tiles = config_basic [ 'use_tiles' ] nbp . use_tiles . sort () use_tiles_oob = [ val for val in nbp . use_tiles if val < 0 or val >= n_tiles ] if len ( use_tiles_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_tiles\" , use_tiles_oob [ 0 ], 0 , n_tiles - 1 ) # get dye info if config_basic [ 'dye_names' ] is None : warnings . warn ( f \"dye_names not specified so assuming separate dye for each channel.\" ) n_dyes = n_channels else : # Ensure channel_camera/channel_laser are correct sizes n_dyes = len ( config_basic [ 'dye_names' ]) if config_basic [ 'channel_camera' ] is None : raise ValueError ( 'dye_names specified but channel_camera is not.' ) elif len ( config_basic [ 'channel_camera' ]) != n_channels : raise ValueError ( f \"channel_camera contains { len ( config_basic [ 'channel_camera' ]) } values. \\n \" f \"But there must be a value for each channel and there are { n_channels } channels.\" ) if config_basic [ 'channel_laser' ] is None : raise ValueError ( 'dye_names specified but channel_laser is not.' ) elif len ( config_basic [ 'channel_laser' ]) != n_channels : raise ValueError ( f \"channel_laser contains { len ( config_basic [ 'channel_camera' ]) } values. \\n \" f \"But there must be a value for each channel and there are { n_channels } channels.\" ) if config_basic [ 'use_dyes' ] is None : if config_basic [ 'dye_names' ] is None : config_basic [ 'use_dyes' ] = nbp . use_channels else : config_basic [ 'use_dyes' ] = list ( np . arange ( n_dyes )) nbp . use_dyes = config_basic [ 'use_dyes' ] nbp . dye_names = config_basic [ 'dye_names' ] nbp . channel_camera = config_basic [ 'channel_camera' ] nbp . channel_laser = config_basic [ 'channel_laser' ] nbp . tile_pixel_value_shift = config_basic [ 'tile_pixel_value_shift' ] # Add size info obtained from raw metadata to notebook page nbp . n_rounds = n_rounds # int, number of imaging rounds nbp . tile_sz = tile_sz # xy dimension of tiles in pixels. nbp . n_tiles = n_tiles # int, number of tiles nbp . n_channels = n_channels # int, number of imaging channels nbp . nz = len ( nbp . use_z ) # number of z planes in npy file (not necessarily the same as in nd2) nbp . n_dyes = n_dyes # int, number of dyes # subtract tile_centre from local pixel coordinates to get centered local tile coordinates if not nbp . is_3d : nz = 1 else : nz = nbp . nz nbp . tile_centre = ( np . array ([ tile_sz , tile_sz , nz ]) - 1 ) / 2 nbp . pixel_size_xy = metadata [ 'pixel_microns' ] # pixel size in microns in xy nbp . pixel_size_z = metadata [ 'pixel_microns_z' ] # and z directions. # Make sure reference rounds/channels are in range of data provided. if nbp . use_anchor : if not 0 <= nbp . ref_channel <= n_channels - 1 : raise utils . errors . OutOfBoundsError ( \"ref_channel\" , nbp . ref_channel , 0 , n_channels - 1 ) if nbp . dapi_channel is not None : if not 0 <= nbp . dapi_channel <= n_channels - 1 : raise utils . errors . OutOfBoundsError ( \"dapi_channel\" , nbp . ref_channel , 0 , n_channels - 1 ) else : # Seen as ref_channel is an imaging channel if anchor not used, ref_channel must be an imaging channels i.e. # must be in use_channels. Same goes for ref_round. if not np . isin ( nbp . ref_channel , nbp . use_channels ): raise ValueError ( f \"ref_channel is { nbp . ref_channel } which is not in use_channels = { nbp . use_channels } .\" ) if not np . isin ( nbp . ref_round , nbp . use_rounds ): raise ValueError ( f \"ref_round is { nbp . ref_round } which is not in use_rounds = { nbp . use_rounds } .\" ) return nbp","title":"set_basic_info()"},{"location":"code/pipeline/call_reference_spots/","text":"call_reference_spots ( config , nbp_file , nbp_basic , nbp_ref_spots , hist_values , hist_counts , transform , overwrite_ref_spots = False ) This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a simple dot product for spots found on the reference round. Returns the call_spots notebook page and adds the following variables to the ref_spots page: gene_no , score , score_diff , intensity . See 'call_spots' and 'ref_spots' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'call_spots' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required nbp_ref_spots NotebookPage ref_spots notebook page containing all variables produced in pipeline/reference_spots.py i.e. local_yxz , isolated , tile , colors . gene_no , score , score_diff , intensity should all be None to add them here, unless overwrite_ref_spots == True . required hist_values np . ndarray int [n_pixel_values] . All possible pixel values in saved tiff images i.e. n_pixel_values is approximately np.iinfo(np.uint16).max because tiffs saved as uint16 images. This is saved in the extract notebook page i.e. nb.extract.hist_values . required hist_counts np . ndarray int [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round r , channel c which had the value hist_values[i] . This is saved in extract notebook page i.e. nb.extract.hist_counts . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required overwrite_ref_spots bool If True , the variables: gene_no score score_diff intensity in nbp_ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Returns: Type Description NotebookPage NotebookPage[call_spots] - Page contains bleed matrix and expected code for each gene. NotebookPage NotebookPage[ref_spots] - Page contains gene assignments and info for spots found on reference round. Parameters added are: intensity, score, gene_no, score_diff Source code in coppafish/pipeline/call_reference_spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def call_reference_spots ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , nbp_ref_spots : NotebookPage , hist_values : np . ndarray , hist_counts : np . ndarray , transform : np . ndarray , overwrite_ref_spots : bool = False ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a simple dot product for spots found on the reference round. Returns the `call_spots` notebook page and adds the following variables to the `ref_spots` page: `gene_no`, `score`, `score_diff`, `intensity`. See `'call_spots'` and `'ref_spots'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'call_spots'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page nbp_ref_spots: `ref_spots` notebook page containing all variables produced in `pipeline/reference_spots.py` i.e. `local_yxz`, `isolated`, `tile`, `colors`. `gene_no`, `score`, `score_diff`, `intensity` should all be `None` to add them here, unless `overwrite_ref_spots == True`. hist_values: `int [n_pixel_values]`. All possible pixel values in saved tiff images i.e. `n_pixel_values` is approximately `np.iinfo(np.uint16).max` because tiffs saved as `uint16` images. This is saved in the extract notebook page i.e. `nb.extract.hist_values`. hist_counts: `int [n_pixel_values x n_rounds x n_channels]`. `hist_counts[i, r, c]` is the number of pixels across all tiles in round `r`, channel `c` which had the value `hist_values[i]`. This is saved in extract notebook page i.e. `nb.extract.hist_counts`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. overwrite_ref_spots: If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nbp_ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. Returns: `NotebookPage[call_spots]` - Page contains bleed matrix and expected code for each gene. `NotebookPage[ref_spots]` - Page contains gene assignments and info for spots found on reference round. Parameters added are: intensity, score, gene_no, score_diff \"\"\" if overwrite_ref_spots : warnings . warn ( \" \\n overwrite_ref_spots = True so will overwrite: \\n gene_no, score, score_diff, intensity\" \" \\n in nbp_ref_spots.\" ) else : # Raise error if data in nbp_ref_spots already exists that will be overwritted in this function. error_message = \"\" for var in [ 'gene_no' , 'score' , 'score_diff' , 'intensity' ]: if hasattr ( nbp_ref_spots , var ) and nbp_ref_spots . __getattribute__ ( var ) is not None : error_message += f \" \\n nbp_ref_spots. { var } is not None but this function will overwrite { var } .\" \\ f \" \\n Run with overwrite_ref_spots = True to get past this error.\" if len ( error_message ) > 0 : raise ValueError ( error_message ) nbp_ref_spots . finalized = False # So we can add and delete ref_spots page variables # delete all variables in ref_spots set to None so can add them later. for var in [ 'gene_no' , 'score' , 'score_diff' , 'intensity' ]: if hasattr ( nbp_ref_spots , var ): nbp_ref_spots . __delattr__ ( var ) nbp = NotebookPage ( \"call_spots\" ) # get color norm factor rc_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels ) hist_counts_use = np . moveaxis ( np . moveaxis ( hist_counts , 0 , - 1 )[ rc_ind ], - 1 , 0 ) color_norm_factor = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels )) * np . nan color_norm_factor [ rc_ind ] = color_normalisation ( hist_values , hist_counts_use , config [ 'color_norm_intensities' ], config [ 'color_norm_probs' ], config [ 'bleed_matrix_method' ]) # get initial bleed matrix initial_raw_bleed_matrix = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , nbp_basic . n_dyes )) * np . nan rcd_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels , nbp_basic . use_dyes ) if nbp_basic . dye_names is not None : # if specify dyes, will initialize bleed matrix using prior data dye_names_use = np . array ( nbp_basic . dye_names )[ nbp_basic . use_dyes ] camera_use = np . array ( nbp_basic . channel_camera )[ nbp_basic . use_channels ] laser_use = np . array ( nbp_basic . channel_laser )[ nbp_basic . use_channels ] initial_raw_bleed_matrix [ rcd_ind ] = get_dye_channel_intensity_guess ( nbp_file . dye_camera_laser , dye_names_use , camera_use , laser_use ) . transpose () initial_bleed_matrix = initial_raw_bleed_matrix / np . expand_dims ( color_norm_factor , 2 ) else : if nbp_basic . n_dyes != nbp_basic . n_channels : raise ValueError ( f \"'dye_names' were not specified so expect each dye to correspond to a different channel.\" f \" \\n But n_channels= { nbp_basic . n_channels } and n_dyes= { nbp_basic . n_dyes } \" ) if nbp_basic . use_channels != nbp_basic . use_dyes : raise ValueError ( f \"'dye_names' were not specified so expect each dye to correspond to a different channel.\" f \" \\n Bleed matrix computation requires use_channels and use_dyes to be the same to work.\" f \" \\n But use_channels= { nbp_basic . use_channels } and use_dyes= { nbp_basic . use_dyes } \" ) initial_bleed_matrix = initial_raw_bleed_matrix . copy () initial_bleed_matrix [ rcd_ind ] = np . tile ( np . expand_dims ( np . eye ( nbp_basic . n_channels ), 0 ), ( nbp_basic . n_rounds , 1 , 1 ))[ rcd_ind ] # Get norm_shift and intensity_thresh from middle tile/ z-plane average intensity # This is because these variables are all a small fraction of a spot_color L2 norm in one round. # Hence, use average pixel as example of low intensity spot. # get central tile nbp . norm_shift_tile = scale . central_tile ( nbp_basic . tilepos_yx , nbp_basic . use_tiles ) if nbp_basic . is_3d : nbp . norm_shift_z = int ( np . floor ( nbp_basic . nz / 2 )) # central z-plane to get info from. else : nbp . norm_shift_z = 0 pixel_colors = get_spot_colors ( all_pixel_yxz ( nbp_basic . tile_sz , nbp_basic . tile_sz , nbp . norm_shift_z ), nbp . norm_shift_tile , transform , nbp_file , nbp_basic , return_in_bounds = True )[ 0 ] pixel_intensity = get_spot_intensity ( np . abs ( pixel_colors ) / color_norm_factor [ rc_ind ]) nbp . abs_intensity_percentile = np . percentile ( pixel_intensity , np . arange ( 1 , 101 )) if config [ 'background_weight_shift' ] is None : # Set to median absolute pixel intensity config [ 'background_weight_shift' ] = float ( round_any ( nbp . abs_intensity_percentile [ 50 ], config [ 'norm_shift_precision' ], 'ceil' )) median_round_l2_norm = np . median ( np . linalg . norm ( pixel_colors / color_norm_factor [ rc_ind ], axis = 2 )) if config [ 'dp_norm_shift' ] is None : config [ 'dp_norm_shift' ] = float ( round_any ( median_round_l2_norm , config [ 'norm_shift_precision' ])) # intensity thresh is just a very low threshold, would basically be the same if set to 0 # but found it to be slightly better on ground truth pixel_intensity = get_spot_intensity ( pixel_colors / color_norm_factor [ rc_ind ]) if config [ 'gene_efficiency_intensity_thresh' ] is None : config [ 'gene_efficiency_intensity_thresh' ] = \\ float ( round_any ( np . percentile ( pixel_intensity , config [ 'gene_efficiency_intensity_thresh_percentile' ]), config [ 'gene_efficiency_intensity_thresh_precision' ])) nbp . dp_norm_shift = float ( np . clip ( config [ 'dp_norm_shift' ], config [ 'norm_shift_min' ], config [ 'norm_shift_max' ])) nbp . background_weight_shift = float ( np . clip ( config [ 'background_weight_shift' ], config [ 'norm_shift_min' ], config [ 'norm_shift_max' ])) nbp . gene_efficiency_intensity_thresh = \\ float ( np . clip ( config [ 'gene_efficiency_intensity_thresh' ], config [ 'gene_efficiency_intensity_thresh_min' ], config [ 'gene_efficiency_intensity_thresh_max' ])) # get bleed matrix spot_colors_use = np . moveaxis ( np . moveaxis ( nbp_ref_spots . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) / color_norm_factor [ rc_ind ] nbp_ref_spots . intensity = np . asarray ( get_spot_intensity ( spot_colors_use ) . astype ( np . float32 )) # Remove background first background_coef = np . ones (( spot_colors_use . shape [ 0 ], nbp_basic . n_channels )) * np . nan background_codes = np . ones (( nbp_basic . n_channels , nbp_basic . n_rounds , nbp_basic . n_channels )) * np . nan crc_ind = np . ix_ ( nbp_basic . use_channels , nbp_basic . use_rounds , nbp_basic . use_channels ) spot_colors_use , background_coef [:, nbp_basic . use_channels ], background_codes [ crc_ind ] = \\ fit_background ( spot_colors_use , nbp . background_weight_shift ) spot_colors_use = np . asarray ( spot_colors_use ) # in case using jax bleed_matrix = initial_raw_bleed_matrix . copy () bleed_matrix [ rcd_ind ] = get_bleed_matrix ( spot_colors_use [ nbp_ref_spots . isolated ], initial_bleed_matrix [ rcd_ind ], config [ 'bleed_matrix_method' ], config [ 'bleed_matrix_score_thresh' ], config [ 'bleed_matrix_min_cluster_size' ], config [ 'bleed_matrix_n_iter' ], config [ 'bleed_matrix_anneal' ]) # get gene codes gene_names , gene_codes = np . genfromtxt ( nbp_file . code_book , dtype = ( str , str )) . transpose () gene_codes = np . array ([[ int ( i ) for i in gene_codes [ j ]] for j in range ( len ( gene_codes ))]) # bled_codes[g,r,c] returned below is nan where r/c/gene_codes[g,r] outside use_rounds/channels/dyes bled_codes = get_bled_codes ( gene_codes , bleed_matrix ) # get bled_codes_use with no nan values and L2 norm=1 bled_codes_use = np . moveaxis ( np . moveaxis ( bled_codes , 0 , - 1 )[ rc_ind ], - 1 , 0 ) bled_codes_use [ np . isnan ( bled_codes_use )] = 0 # set all round vectors where dye is not in use_dyes to 0. # Give all bled codes an L2 norm of 1 across use_rounds and use_channels norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes_use , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes_use = bled_codes_use / norm_factor # bled_codes[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes n_genes = bled_codes_use . shape [ 0 ] bled_codes = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , n_genes )) * np . nan bled_codes [ rc_ind ] = np . moveaxis ( bled_codes_use , 0 , - 1 ) bled_codes = np . moveaxis ( bled_codes , - 1 , 0 ) nbp . gene_names = gene_names nbp . gene_codes = gene_codes nbp . color_norm_factor = color_norm_factor nbp . initial_raw_bleed_matrix = initial_raw_bleed_matrix nbp . initial_bleed_matrix = initial_bleed_matrix nbp . bleed_matrix = bleed_matrix nbp . bled_codes = bled_codes nbp . background_codes = background_codes # shift in config file is just for one round. n_spots , n_rounds_use , n_channels_use = spot_colors_use . shape dp_norm_shift = nbp . dp_norm_shift * np . sqrt ( n_rounds_use ) # Down-weight round/channels with high background when compute dot product alpha = config [ 'alpha' ] beta = config [ 'beta' ] background_codes = background_codes [ crc_ind ] . reshape ( n_channels_use , - 1 ) background_var = background_coef [:, nbp_basic . use_channels ] ** 2 @ background_codes ** 2 * alpha + beta ** 2 # find spot assignments to genes and gene efficiency n_iter = config [ 'gene_efficiency_n_iter' ] + 1 pass_intensity_thresh = nbp_ref_spots . intensity > nbp . gene_efficiency_intensity_thresh use_ge_last = np . zeros ( n_spots ) . astype ( bool ) bled_codes_ge_use = bled_codes_use . copy () for i in range ( n_iter ): scores = np . asarray ( dot_product_score ( spot_colors_use . reshape ( n_spots , - 1 ), bled_codes_ge_use . reshape ( n_genes , - 1 ), dp_norm_shift , 1 / background_var )) spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( np . shape ( scores )[ 0 ]), spot_gene_no ] pass_score_thresh = spot_score > config [ 'gene_efficiency_score_thresh' ] sort_gene_inds = np . argsort ( scores , axis = 1 ) gene_no_second_best = sort_gene_inds [:, - 2 ] score_second_best = scores [ np . arange ( np . shape ( scores )[ 0 ]), gene_no_second_best ] spot_score_diff = spot_score - score_second_best pass_score_diff_thresh = spot_score_diff > config [ 'gene_efficiency_score_diff_thresh' ] # only use isolated spots which pass strict thresholding to compute gene_efficiencies use_ge = np . array ([ nbp_ref_spots . isolated , pass_intensity_thresh , pass_score_thresh , pass_score_diff_thresh ]) . all ( axis = 0 ) # nan_to_num line below converts nan in bleed_matrix to 0. # This basically just says that for dyes not in use_dyes, we expect intensity to be 0. gene_efficiency_use = get_gene_efficiency ( spot_colors_use [ use_ge ], spot_gene_no [ use_ge ], gene_codes [:, nbp_basic . use_rounds ], np . nan_to_num ( bleed_matrix [ rc_ind ]), config [ 'gene_efficiency_min_spots' ], config [ 'gene_efficiency_max' ], config [ 'gene_efficiency_min' ], config [ 'gene_efficiency_min_factor' ]) # get new bled codes using gene efficiency with L2 norm = 1. multiplier_ge = np . tile ( np . expand_dims ( gene_efficiency_use , 2 ), [ 1 , 1 , n_channels_use ]) bled_codes_ge_use = bled_codes_use * multiplier_ge norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes_ge_use , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes_ge_use = bled_codes_ge_use / norm_factor if np . sum ( use_ge != use_ge_last ) < 3 : # if less than 3 spots different in spots used for ge computation, end. break use_ge_last = use_ge . copy () if config [ 'gene_efficiency_n_iter' ] > 0 : # Compute score with final gene efficiency scores = np . asarray ( dot_product_score ( spot_colors_use . reshape ( n_spots , - 1 ), bled_codes_ge_use . reshape ( n_genes , - 1 ), dp_norm_shift , 1 / background_var )) spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( np . shape ( scores )[ 0 ]), spot_gene_no ] else : bled_codes_ge_use = bled_codes_use . copy () # save score using the latest gene efficiency and diff to second best gene nbp_ref_spots . score = spot_score . astype ( np . float32 ) nbp_ref_spots . gene_no = spot_gene_no . astype ( np . int16 ) sort_gene_inds = np . argsort ( scores , axis = 1 ) gene_no_second_best = sort_gene_inds [:, - 2 ] score_second_best = scores [ np . arange ( np . shape ( scores )[ 0 ]), gene_no_second_best ] nbp_ref_spots . score_diff = ( nbp_ref_spots . score - score_second_best ) . astype ( np . float16 ) # save gene_efficiency[g,r] with nan when r outside use_rounds and 1 when gene_codes[g,r] outside use_dyes. gene_efficiency = np . ones (( n_genes , nbp_basic . n_rounds )) * np . nan gene_efficiency [:, nbp_basic . use_rounds ] = gene_efficiency_use nbp . gene_efficiency = gene_efficiency # bled_codes_ge[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes bled_codes_ge = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , n_genes )) * np . nan bled_codes_ge [ rc_ind ] = np . moveaxis ( bled_codes_ge_use , 0 , - 1 ) bled_codes_ge = np . moveaxis ( bled_codes_ge , - 1 , 0 ) nbp . bled_codes_ge = bled_codes_ge ge_fail_genes = np . where ( np . min ( gene_efficiency_use , axis = 1 ) == 1 )[ 0 ] n_fail_ge = len ( ge_fail_genes ) if n_fail_ge > 0 : fail_genes_str = [ str ( ge_fail_genes [ i ]) + ': ' + gene_names [ ge_fail_genes ][ i ] for i in range ( n_fail_ge )] fail_genes_str = ' \\n ' . join ( fail_genes_str ) warnings . warn ( f \" \\n Gene Efficiency could not be calculated for { n_fail_ge } / { n_genes } \" f \"genes: \\n { fail_genes_str } \" ) nbp_ref_spots . finalized = True return nbp , nbp_ref_spots","title":"Call Reference Spots"},{"location":"code/pipeline/call_reference_spots/#coppafish.pipeline.call_reference_spots.call_reference_spots","text":"This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a simple dot product for spots found on the reference round. Returns the call_spots notebook page and adds the following variables to the ref_spots page: gene_no , score , score_diff , intensity . See 'call_spots' and 'ref_spots' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'call_spots' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required nbp_ref_spots NotebookPage ref_spots notebook page containing all variables produced in pipeline/reference_spots.py i.e. local_yxz , isolated , tile , colors . gene_no , score , score_diff , intensity should all be None to add them here, unless overwrite_ref_spots == True . required hist_values np . ndarray int [n_pixel_values] . All possible pixel values in saved tiff images i.e. n_pixel_values is approximately np.iinfo(np.uint16).max because tiffs saved as uint16 images. This is saved in the extract notebook page i.e. nb.extract.hist_values . required hist_counts np . ndarray int [n_pixel_values x n_rounds x n_channels] . hist_counts[i, r, c] is the number of pixels across all tiles in round r , channel c which had the value hist_values[i] . This is saved in extract notebook page i.e. nb.extract.hist_counts . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required overwrite_ref_spots bool If True , the variables: gene_no score score_diff intensity in nbp_ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Returns: Type Description NotebookPage NotebookPage[call_spots] - Page contains bleed matrix and expected code for each gene. NotebookPage NotebookPage[ref_spots] - Page contains gene assignments and info for spots found on reference round. Parameters added are: intensity, score, gene_no, score_diff Source code in coppafish/pipeline/call_reference_spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def call_reference_spots ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , nbp_ref_spots : NotebookPage , hist_values : np . ndarray , hist_counts : np . ndarray , transform : np . ndarray , overwrite_ref_spots : bool = False ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This produces the bleed matrix and expected code for each gene as well as producing a gene assignment based on a simple dot product for spots found on the reference round. Returns the `call_spots` notebook page and adds the following variables to the `ref_spots` page: `gene_no`, `score`, `score_diff`, `intensity`. See `'call_spots'` and `'ref_spots'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'call_spots'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page nbp_ref_spots: `ref_spots` notebook page containing all variables produced in `pipeline/reference_spots.py` i.e. `local_yxz`, `isolated`, `tile`, `colors`. `gene_no`, `score`, `score_diff`, `intensity` should all be `None` to add them here, unless `overwrite_ref_spots == True`. hist_values: `int [n_pixel_values]`. All possible pixel values in saved tiff images i.e. `n_pixel_values` is approximately `np.iinfo(np.uint16).max` because tiffs saved as `uint16` images. This is saved in the extract notebook page i.e. `nb.extract.hist_values`. hist_counts: `int [n_pixel_values x n_rounds x n_channels]`. `hist_counts[i, r, c]` is the number of pixels across all tiles in round `r`, channel `c` which had the value `hist_values[i]`. This is saved in extract notebook page i.e. `nb.extract.hist_counts`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. overwrite_ref_spots: If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nbp_ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. Returns: `NotebookPage[call_spots]` - Page contains bleed matrix and expected code for each gene. `NotebookPage[ref_spots]` - Page contains gene assignments and info for spots found on reference round. Parameters added are: intensity, score, gene_no, score_diff \"\"\" if overwrite_ref_spots : warnings . warn ( \" \\n overwrite_ref_spots = True so will overwrite: \\n gene_no, score, score_diff, intensity\" \" \\n in nbp_ref_spots.\" ) else : # Raise error if data in nbp_ref_spots already exists that will be overwritted in this function. error_message = \"\" for var in [ 'gene_no' , 'score' , 'score_diff' , 'intensity' ]: if hasattr ( nbp_ref_spots , var ) and nbp_ref_spots . __getattribute__ ( var ) is not None : error_message += f \" \\n nbp_ref_spots. { var } is not None but this function will overwrite { var } .\" \\ f \" \\n Run with overwrite_ref_spots = True to get past this error.\" if len ( error_message ) > 0 : raise ValueError ( error_message ) nbp_ref_spots . finalized = False # So we can add and delete ref_spots page variables # delete all variables in ref_spots set to None so can add them later. for var in [ 'gene_no' , 'score' , 'score_diff' , 'intensity' ]: if hasattr ( nbp_ref_spots , var ): nbp_ref_spots . __delattr__ ( var ) nbp = NotebookPage ( \"call_spots\" ) # get color norm factor rc_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels ) hist_counts_use = np . moveaxis ( np . moveaxis ( hist_counts , 0 , - 1 )[ rc_ind ], - 1 , 0 ) color_norm_factor = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels )) * np . nan color_norm_factor [ rc_ind ] = color_normalisation ( hist_values , hist_counts_use , config [ 'color_norm_intensities' ], config [ 'color_norm_probs' ], config [ 'bleed_matrix_method' ]) # get initial bleed matrix initial_raw_bleed_matrix = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , nbp_basic . n_dyes )) * np . nan rcd_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels , nbp_basic . use_dyes ) if nbp_basic . dye_names is not None : # if specify dyes, will initialize bleed matrix using prior data dye_names_use = np . array ( nbp_basic . dye_names )[ nbp_basic . use_dyes ] camera_use = np . array ( nbp_basic . channel_camera )[ nbp_basic . use_channels ] laser_use = np . array ( nbp_basic . channel_laser )[ nbp_basic . use_channels ] initial_raw_bleed_matrix [ rcd_ind ] = get_dye_channel_intensity_guess ( nbp_file . dye_camera_laser , dye_names_use , camera_use , laser_use ) . transpose () initial_bleed_matrix = initial_raw_bleed_matrix / np . expand_dims ( color_norm_factor , 2 ) else : if nbp_basic . n_dyes != nbp_basic . n_channels : raise ValueError ( f \"'dye_names' were not specified so expect each dye to correspond to a different channel.\" f \" \\n But n_channels= { nbp_basic . n_channels } and n_dyes= { nbp_basic . n_dyes } \" ) if nbp_basic . use_channels != nbp_basic . use_dyes : raise ValueError ( f \"'dye_names' were not specified so expect each dye to correspond to a different channel.\" f \" \\n Bleed matrix computation requires use_channels and use_dyes to be the same to work.\" f \" \\n But use_channels= { nbp_basic . use_channels } and use_dyes= { nbp_basic . use_dyes } \" ) initial_bleed_matrix = initial_raw_bleed_matrix . copy () initial_bleed_matrix [ rcd_ind ] = np . tile ( np . expand_dims ( np . eye ( nbp_basic . n_channels ), 0 ), ( nbp_basic . n_rounds , 1 , 1 ))[ rcd_ind ] # Get norm_shift and intensity_thresh from middle tile/ z-plane average intensity # This is because these variables are all a small fraction of a spot_color L2 norm in one round. # Hence, use average pixel as example of low intensity spot. # get central tile nbp . norm_shift_tile = scale . central_tile ( nbp_basic . tilepos_yx , nbp_basic . use_tiles ) if nbp_basic . is_3d : nbp . norm_shift_z = int ( np . floor ( nbp_basic . nz / 2 )) # central z-plane to get info from. else : nbp . norm_shift_z = 0 pixel_colors = get_spot_colors ( all_pixel_yxz ( nbp_basic . tile_sz , nbp_basic . tile_sz , nbp . norm_shift_z ), nbp . norm_shift_tile , transform , nbp_file , nbp_basic , return_in_bounds = True )[ 0 ] pixel_intensity = get_spot_intensity ( np . abs ( pixel_colors ) / color_norm_factor [ rc_ind ]) nbp . abs_intensity_percentile = np . percentile ( pixel_intensity , np . arange ( 1 , 101 )) if config [ 'background_weight_shift' ] is None : # Set to median absolute pixel intensity config [ 'background_weight_shift' ] = float ( round_any ( nbp . abs_intensity_percentile [ 50 ], config [ 'norm_shift_precision' ], 'ceil' )) median_round_l2_norm = np . median ( np . linalg . norm ( pixel_colors / color_norm_factor [ rc_ind ], axis = 2 )) if config [ 'dp_norm_shift' ] is None : config [ 'dp_norm_shift' ] = float ( round_any ( median_round_l2_norm , config [ 'norm_shift_precision' ])) # intensity thresh is just a very low threshold, would basically be the same if set to 0 # but found it to be slightly better on ground truth pixel_intensity = get_spot_intensity ( pixel_colors / color_norm_factor [ rc_ind ]) if config [ 'gene_efficiency_intensity_thresh' ] is None : config [ 'gene_efficiency_intensity_thresh' ] = \\ float ( round_any ( np . percentile ( pixel_intensity , config [ 'gene_efficiency_intensity_thresh_percentile' ]), config [ 'gene_efficiency_intensity_thresh_precision' ])) nbp . dp_norm_shift = float ( np . clip ( config [ 'dp_norm_shift' ], config [ 'norm_shift_min' ], config [ 'norm_shift_max' ])) nbp . background_weight_shift = float ( np . clip ( config [ 'background_weight_shift' ], config [ 'norm_shift_min' ], config [ 'norm_shift_max' ])) nbp . gene_efficiency_intensity_thresh = \\ float ( np . clip ( config [ 'gene_efficiency_intensity_thresh' ], config [ 'gene_efficiency_intensity_thresh_min' ], config [ 'gene_efficiency_intensity_thresh_max' ])) # get bleed matrix spot_colors_use = np . moveaxis ( np . moveaxis ( nbp_ref_spots . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) / color_norm_factor [ rc_ind ] nbp_ref_spots . intensity = np . asarray ( get_spot_intensity ( spot_colors_use ) . astype ( np . float32 )) # Remove background first background_coef = np . ones (( spot_colors_use . shape [ 0 ], nbp_basic . n_channels )) * np . nan background_codes = np . ones (( nbp_basic . n_channels , nbp_basic . n_rounds , nbp_basic . n_channels )) * np . nan crc_ind = np . ix_ ( nbp_basic . use_channels , nbp_basic . use_rounds , nbp_basic . use_channels ) spot_colors_use , background_coef [:, nbp_basic . use_channels ], background_codes [ crc_ind ] = \\ fit_background ( spot_colors_use , nbp . background_weight_shift ) spot_colors_use = np . asarray ( spot_colors_use ) # in case using jax bleed_matrix = initial_raw_bleed_matrix . copy () bleed_matrix [ rcd_ind ] = get_bleed_matrix ( spot_colors_use [ nbp_ref_spots . isolated ], initial_bleed_matrix [ rcd_ind ], config [ 'bleed_matrix_method' ], config [ 'bleed_matrix_score_thresh' ], config [ 'bleed_matrix_min_cluster_size' ], config [ 'bleed_matrix_n_iter' ], config [ 'bleed_matrix_anneal' ]) # get gene codes gene_names , gene_codes = np . genfromtxt ( nbp_file . code_book , dtype = ( str , str )) . transpose () gene_codes = np . array ([[ int ( i ) for i in gene_codes [ j ]] for j in range ( len ( gene_codes ))]) # bled_codes[g,r,c] returned below is nan where r/c/gene_codes[g,r] outside use_rounds/channels/dyes bled_codes = get_bled_codes ( gene_codes , bleed_matrix ) # get bled_codes_use with no nan values and L2 norm=1 bled_codes_use = np . moveaxis ( np . moveaxis ( bled_codes , 0 , - 1 )[ rc_ind ], - 1 , 0 ) bled_codes_use [ np . isnan ( bled_codes_use )] = 0 # set all round vectors where dye is not in use_dyes to 0. # Give all bled codes an L2 norm of 1 across use_rounds and use_channels norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes_use , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes_use = bled_codes_use / norm_factor # bled_codes[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes n_genes = bled_codes_use . shape [ 0 ] bled_codes = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , n_genes )) * np . nan bled_codes [ rc_ind ] = np . moveaxis ( bled_codes_use , 0 , - 1 ) bled_codes = np . moveaxis ( bled_codes , - 1 , 0 ) nbp . gene_names = gene_names nbp . gene_codes = gene_codes nbp . color_norm_factor = color_norm_factor nbp . initial_raw_bleed_matrix = initial_raw_bleed_matrix nbp . initial_bleed_matrix = initial_bleed_matrix nbp . bleed_matrix = bleed_matrix nbp . bled_codes = bled_codes nbp . background_codes = background_codes # shift in config file is just for one round. n_spots , n_rounds_use , n_channels_use = spot_colors_use . shape dp_norm_shift = nbp . dp_norm_shift * np . sqrt ( n_rounds_use ) # Down-weight round/channels with high background when compute dot product alpha = config [ 'alpha' ] beta = config [ 'beta' ] background_codes = background_codes [ crc_ind ] . reshape ( n_channels_use , - 1 ) background_var = background_coef [:, nbp_basic . use_channels ] ** 2 @ background_codes ** 2 * alpha + beta ** 2 # find spot assignments to genes and gene efficiency n_iter = config [ 'gene_efficiency_n_iter' ] + 1 pass_intensity_thresh = nbp_ref_spots . intensity > nbp . gene_efficiency_intensity_thresh use_ge_last = np . zeros ( n_spots ) . astype ( bool ) bled_codes_ge_use = bled_codes_use . copy () for i in range ( n_iter ): scores = np . asarray ( dot_product_score ( spot_colors_use . reshape ( n_spots , - 1 ), bled_codes_ge_use . reshape ( n_genes , - 1 ), dp_norm_shift , 1 / background_var )) spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( np . shape ( scores )[ 0 ]), spot_gene_no ] pass_score_thresh = spot_score > config [ 'gene_efficiency_score_thresh' ] sort_gene_inds = np . argsort ( scores , axis = 1 ) gene_no_second_best = sort_gene_inds [:, - 2 ] score_second_best = scores [ np . arange ( np . shape ( scores )[ 0 ]), gene_no_second_best ] spot_score_diff = spot_score - score_second_best pass_score_diff_thresh = spot_score_diff > config [ 'gene_efficiency_score_diff_thresh' ] # only use isolated spots which pass strict thresholding to compute gene_efficiencies use_ge = np . array ([ nbp_ref_spots . isolated , pass_intensity_thresh , pass_score_thresh , pass_score_diff_thresh ]) . all ( axis = 0 ) # nan_to_num line below converts nan in bleed_matrix to 0. # This basically just says that for dyes not in use_dyes, we expect intensity to be 0. gene_efficiency_use = get_gene_efficiency ( spot_colors_use [ use_ge ], spot_gene_no [ use_ge ], gene_codes [:, nbp_basic . use_rounds ], np . nan_to_num ( bleed_matrix [ rc_ind ]), config [ 'gene_efficiency_min_spots' ], config [ 'gene_efficiency_max' ], config [ 'gene_efficiency_min' ], config [ 'gene_efficiency_min_factor' ]) # get new bled codes using gene efficiency with L2 norm = 1. multiplier_ge = np . tile ( np . expand_dims ( gene_efficiency_use , 2 ), [ 1 , 1 , n_channels_use ]) bled_codes_ge_use = bled_codes_use * multiplier_ge norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes_ge_use , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes_ge_use = bled_codes_ge_use / norm_factor if np . sum ( use_ge != use_ge_last ) < 3 : # if less than 3 spots different in spots used for ge computation, end. break use_ge_last = use_ge . copy () if config [ 'gene_efficiency_n_iter' ] > 0 : # Compute score with final gene efficiency scores = np . asarray ( dot_product_score ( spot_colors_use . reshape ( n_spots , - 1 ), bled_codes_ge_use . reshape ( n_genes , - 1 ), dp_norm_shift , 1 / background_var )) spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( np . shape ( scores )[ 0 ]), spot_gene_no ] else : bled_codes_ge_use = bled_codes_use . copy () # save score using the latest gene efficiency and diff to second best gene nbp_ref_spots . score = spot_score . astype ( np . float32 ) nbp_ref_spots . gene_no = spot_gene_no . astype ( np . int16 ) sort_gene_inds = np . argsort ( scores , axis = 1 ) gene_no_second_best = sort_gene_inds [:, - 2 ] score_second_best = scores [ np . arange ( np . shape ( scores )[ 0 ]), gene_no_second_best ] nbp_ref_spots . score_diff = ( nbp_ref_spots . score - score_second_best ) . astype ( np . float16 ) # save gene_efficiency[g,r] with nan when r outside use_rounds and 1 when gene_codes[g,r] outside use_dyes. gene_efficiency = np . ones (( n_genes , nbp_basic . n_rounds )) * np . nan gene_efficiency [:, nbp_basic . use_rounds ] = gene_efficiency_use nbp . gene_efficiency = gene_efficiency # bled_codes_ge[g,r,c] so nan when r/c outside use_rounds/channels and 0 when gene_codes[g,r] outside use_dyes bled_codes_ge = np . ones (( nbp_basic . n_rounds , nbp_basic . n_channels , n_genes )) * np . nan bled_codes_ge [ rc_ind ] = np . moveaxis ( bled_codes_ge_use , 0 , - 1 ) bled_codes_ge = np . moveaxis ( bled_codes_ge , - 1 , 0 ) nbp . bled_codes_ge = bled_codes_ge ge_fail_genes = np . where ( np . min ( gene_efficiency_use , axis = 1 ) == 1 )[ 0 ] n_fail_ge = len ( ge_fail_genes ) if n_fail_ge > 0 : fail_genes_str = [ str ( ge_fail_genes [ i ]) + ': ' + gene_names [ ge_fail_genes ][ i ] for i in range ( n_fail_ge )] fail_genes_str = ' \\n ' . join ( fail_genes_str ) warnings . warn ( f \" \\n Gene Efficiency could not be calculated for { n_fail_ge } / { n_genes } \" f \"genes: \\n { fail_genes_str } \" ) nbp_ref_spots . finalized = True return nbp , nbp_ref_spots","title":"call_reference_spots()"},{"location":"code/pipeline/extract_run/","text":"extract_and_filter ( config , nbp_file , nbp_basic ) This reads in images from the raw nd2 files, filters them and then saves them as npy files in the tile directory. Also gets auto_thresh for use in turning images to point clouds and hist_values , hist_counts required for normalisation between channels. Returns the extract and extract_debug notebook pages. See 'extract' and 'extract_debug' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'extract' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required Returns: Type Description NotebookPage NotebookPage[extract] - Page containing auto_thresh for use in turning images to point clouds and hist_values , hist_counts required for normalisation between channels. NotebookPage NotebookPage[extract_debug] - Page containing variables which are not needed later in the pipeline but may be useful for debugging purposes. Source code in coppafish/pipeline/extract_run.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def extract_and_filter ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This reads in images from the raw `nd2` files, filters them and then saves them as npy files in the tile directory. Also gets `auto_thresh` for use in turning images to point clouds and `hist_values`, `hist_counts` required for normalisation between channels. Returns the `extract` and `extract_debug` notebook pages. See `'extract'` and `'extract_debug'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'extract'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page Returns: - `NotebookPage[extract]` - Page containing `auto_thresh` for use in turning images to point clouds and `hist_values`, `hist_counts` required for normalisation between channels. - `NotebookPage[extract_debug]` - Page containing variables which are not needed later in the pipeline but may be useful for debugging purposes. \"\"\" # Check scaling won't cause clipping when saving as uint16 scale_norm_max = np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift if config [ 'scale_norm' ] >= scale_norm_max : raise ValueError ( f \" \\n config['extract']['scale_norm'] = { config [ 'scale_norm' ] } but it must be below \" f \" { scale_norm_max } \" ) # initialise notebook pages if not nbp_basic . is_3d : config [ 'deconvolve' ] = False # only deconvolve if 3d pipeline nbp = NotebookPage ( \"extract\" ) nbp_debug = NotebookPage ( \"extract_debug\" ) # initialise output of this part of pipeline as 'vars' key nbp . auto_thresh = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds + nbp_basic . n_extra_rounds , nbp_basic . n_channels ), dtype = int ) nbp . hist_values = np . arange ( - nbp_basic . tile_pixel_value_shift , np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift + 2 , 1 ) nbp . hist_counts = np . zeros (( len ( nbp . hist_values ), nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = int ) hist_bin_edges = np . concatenate (( nbp . hist_values - 0.5 , nbp . hist_values [ - 1 :] + 0.5 )) # initialise debugging info as 'debug' page nbp_debug . n_clip_pixels = np . zeros_like ( nbp . auto_thresh , dtype = int ) nbp_debug . clip_extract_scale = np . zeros_like ( nbp . auto_thresh , dtype = np . float32 ) if nbp_basic . is_3d : nbp_debug . z_info = int ( np . floor ( nbp_basic . nz / 2 )) # central z-plane to get info from. else : nbp_debug . z_info = 0 # update config params in notebook. All optional parameters in config are added to debug page if config [ 'r1' ] is None : config [ 'r1' ] = extract . get_pixel_length ( config [ 'r1_auto_microns' ], nbp_basic . pixel_size_xy ) if config [ 'r2' ] is None : config [ 'r2' ] = config [ 'r1' ] * 2 if config [ 'r_dapi' ] is None : if config [ 'r_dapi_auto_microns' ] is not None : config [ 'r_dapi' ] = extract . get_pixel_length ( config [ 'r_dapi_auto_microns' ], nbp_basic . pixel_size_xy ) nbp_debug . r1 = config [ 'r1' ] nbp_debug . r2 = config [ 'r2' ] nbp_debug . r_dapi = config [ 'r_dapi' ] filter_kernel = utils . morphology . hanning_diff ( nbp_debug . r1 , nbp_debug . r2 ) if nbp_debug . r_dapi is not None : filter_kernel_dapi = utils . strel . disk ( nbp_debug . r_dapi ) else : filter_kernel_dapi = None if config [ 'r_smooth' ] is not None : if len ( config [ 'r_smooth' ]) == 2 : if nbp_basic . is_3d : warnings . warn ( f \"Running 3D pipeline but only 2D smoothing requested with r_smooth\" f \" = { config [ 'r_smooth' ] } .\" ) elif len ( config [ 'r_smooth' ]) == 3 : if not nbp_basic . is_3d : raise ValueError ( \"Running 2D pipeline but 3D smoothing requested.\" ) else : raise ValueError ( f \"r_smooth provided was { config [ 'r_smooth' ] } . \\n \" f \"But it needs to be a 2 radii for 2D smoothing or 3 radii for 3D smoothing. \\n \" f \"I.e. it is the wrong shape.\" ) if config [ 'r_smooth' ][ 0 ] > config [ 'r2' ]: raise ValueError ( f \"Smoothing radius, { config [ 'r_smooth' ][ 0 ] } , is larger than the outer radius of the \\n \" f \"hanning filter, { config [ 'r2' ] } , making the filtering step redundant.\" ) # smooth_kernel = utils.strel.fspecial(*tuple(config['r_smooth'])) smooth_kernel = np . ones ( tuple ( np . array ( config [ 'r_smooth' ], dtype = int ) * 2 - 1 )) smooth_kernel = smooth_kernel / np . sum ( smooth_kernel ) if np . max ( config [ 'r_smooth' ]) == 1 : warnings . warn ( 'Max radius of smooth filter was 1, so not using.' ) config [ 'r_smooth' ] = None if config [ 'deconvolve' ]: if not os . path . isfile ( nbp_file . psf ): spot_images , config [ 'psf_intensity_thresh' ], psf_tiles_used = \\ extract . get_psf_spots ( nbp_file , nbp_basic , nbp_basic . ref_round , nbp_basic . use_tiles , nbp_basic . ref_channel , nbp_basic . use_z , config [ 'psf_detect_radius_xy' ], config [ 'psf_detect_radius_z' ], config [ 'psf_min_spots' ], config [ 'psf_intensity_thresh' ], config [ 'auto_thresh_multiplier' ], config [ 'psf_isolation_dist' ], config [ 'psf_shape' ]) psf = extract . get_psf ( spot_images , config [ 'psf_annulus_width' ]) np . save ( nbp_file . psf , np . moveaxis ( psf , 2 , 0 )) # save with z as first axis else : # Know psf only computed for 3D pipeline hence know ndim=3 psf = np . moveaxis ( np . load ( nbp_file . psf ), 0 , 2 ) # Put z to last index psf_tiles_used = None # normalise psf so min is 0 and max is 1. psf = psf - psf . min () psf = psf / psf . max () pad_im_shape = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ]) + \\ np . array ( config [ 'wiener_pad_shape' ]) * 2 wiener_filter = extract . get_wiener_filter ( psf , pad_im_shape , config [ 'wiener_constant' ]) nbp_debug . psf = psf nbp_debug . psf_intensity_thresh = config [ 'psf_intensity_thresh' ] nbp_debug . psf_tiles_used = psf_tiles_used else : nbp_debug . psf = None nbp_debug . psf_intensity_thresh = None nbp_debug . psf_tiles_used = None # check to see if scales have already been computed config [ 'scale' ], config [ 'scale_anchor' ] = extract . get_scale_from_txt ( nbp_file . scale , config [ 'scale' ], config [ 'scale_anchor' ]) if config [ 'scale' ] is None and len ( nbp_file . round ) > 0 : # ensure scale_norm value is reasonable so max pixel value in tiff file is significant factor of max of uint16 scale_norm_min = ( np . iinfo ( 'uint16' ) . max - nbp_basic . tile_pixel_value_shift ) / 5 scale_norm_max = np . iinfo ( 'uint16' ) . max - nbp_basic . tile_pixel_value_shift if not scale_norm_min <= config [ 'scale_norm' ] <= scale_norm_max : raise utils . errors . OutOfBoundsError ( \"scale_norm\" , config [ 'scale_norm' ], scale_norm_min , scale_norm_max ) # If using smoothing, apply this as well before scal if config [ 'r_smooth' ] is None : smooth_kernel_2d = None else : if smooth_kernel . ndim == 3 : # take central plane of smooth filter if 3D as scale is found from a single z-plane. smooth_kernel_2d = smooth_kernel [:, :, config [ 'r_smooth' ][ 2 ] - 1 ] else : smooth_kernel_2d = smooth_kernel . copy () # smoothing is averaging so to average in 2D, need to re normalise filter smooth_kernel_2d = smooth_kernel_2d / np . sum ( smooth_kernel_2d ) if np . max ( config [ 'r_smooth' ][: 2 ]) <= 1 : smooth_kernel_2d = None # If dimensions of 2D kernel are [1, 1] is equivalent to no smoothing nbp_debug . scale_tile , nbp_debug . scale_channel , nbp_debug . scale_z , config [ 'scale' ] = \\ extract . get_scale ( nbp_file , nbp_basic , 0 , nbp_basic . use_tiles , nbp_basic . use_channels , nbp_basic . use_z , config [ 'scale_norm' ], filter_kernel , smooth_kernel_2d ) else : nbp_debug . scale_tile = None nbp_debug . scale_channel = None nbp_debug . scale_z = None smooth_kernel_2d = None nbp_debug . scale = config [ 'scale' ] print ( f \"Scale: { nbp_debug . scale } \" ) # save scale values incase need to re-run extract . save_scale ( nbp_file . scale , nbp_debug . scale , config [ 'scale_anchor' ]) # get rounds to iterate over use_channels_anchor = [ c for c in [ nbp_basic . dapi_channel , nbp_basic . anchor_channel ] if c is not None ] use_channels_anchor . sort () if filter_kernel_dapi is None : # If not filtering DAPI, skip over the DAPI channel. use_channels_anchor = np . setdiff1d ( use_channels_anchor , nbp_basic . dapi_channel ) if nbp_basic . use_anchor : # always have anchor as first round after imaging rounds round_files = nbp_file . round + [ nbp_file . anchor ] use_rounds = nbp_basic . use_rounds + [ nbp_basic . n_rounds ] n_images = ( len ( use_rounds ) - 1 ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) + \\ len ( nbp_basic . use_tiles ) * len ( use_channels_anchor ) else : round_files = nbp_file . round use_rounds = nbp_basic . use_rounds n_images = len ( use_rounds ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) n_clip_error_images = 0 if config [ 'n_clip_error' ] is None : # default is 1% of pixels on single z-plane config [ 'n_clip_error' ] = int ( nbp_basic . tile_sz * nbp_basic . tile_sz / 100 ) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f 'Loading in tiles from { nbp_file . raw_extension } , filtering and saving as .npy' ) for r in use_rounds : # set scale and channels to use im_file = os . path . join ( nbp_file . input_dir , round_files [ r ]) if nbp_file . raw_extension == '.npy' : extract . wait_for_data ( im_file , config [ 'wait_time' ], dir = True ) else : extract . wait_for_data ( im_file + nbp_file . raw_extension , config [ 'wait_time' ]) round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) if r == nbp_basic . anchor_round : n_clip_error_images = 0 # reset for anchor as different scale used. if config [ 'scale_anchor' ] is None : nbp_debug . scale_anchor_tile , _ , nbp_debug . scale_anchor_z , config [ 'scale_anchor' ] = \\ extract . get_scale ( nbp_file , nbp_basic , nbp_basic . anchor_round , nbp_basic . use_tiles , [ nbp_basic . anchor_channel ], nbp_basic . use_z , config [ 'scale_norm' ], filter_kernel , smooth_kernel_2d ) # save scale values incase need to re-run extract . save_scale ( nbp_file . scale , nbp_debug . scale , config [ 'scale_anchor' ]) else : nbp_debug . scale_anchor_tile = None nbp_debug . scale_anchor_z = None nbp_debug . scale_anchor = config [ 'scale_anchor' ] print ( f \"Scale_anchor: { nbp_debug . scale_anchor } \" ) scale = nbp_debug . scale_anchor use_channels = use_channels_anchor else : scale = nbp_debug . scale use_channels = nbp_basic . use_channels # convolve_2d each image for t in nbp_basic . use_tiles : if not nbp_basic . is_3d : # for 2d all channels in same file file_exists = os . path . isfile ( nbp_file . tile [ t ][ r ]) if file_exists : # mmap load in image for all channels if tiff exists im_all_channels_2d = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' ) else : # Only save 2d data when all channels collected # For channels not used, keep all pixels 0. im_all_channels_2d = np . zeros (( nbp_basic . n_channels , nbp_basic . tile_sz , nbp_basic . tile_sz ), dtype = np . int32 ) for c in use_channels : if r == nbp_basic . anchor_round and c == nbp_basic . anchor_channel : # max value that can be saved and no shifting done for DAPI max_npy_pixel_value = np . iinfo ( np . uint16 ) . max else : max_npy_pixel_value = np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift if nbp_basic . is_3d : file_exists = os . path . isfile ( nbp_file . tile [ t ][ r ][ c ]) pbar . set_postfix ({ 'round' : r , 'tile' : t , 'channel' : c , 'exists' : str ( file_exists )}) if file_exists : if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : pass else : # Only need to load in mid-z plane if 3D. if nbp_basic . is_3d : im = utils . npy . load_tile ( nbp_file , nbp_basic , t , r , c , yxz = [ None , None , nbp_debug . z_info ]) else : im = im_all_channels_2d [ c ] . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift nbp . auto_thresh [ t , r , c ], hist_counts_trc , nbp_debug . n_clip_pixels [ t , r , c ], \\ nbp_debug . clip_extract_scale [ t , r , c ] = \\ extract . get_extract_info ( im , config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale ) if r != nbp_basic . anchor_round : nbp . hist_counts [:, r , c ] += hist_counts_trc else : im = utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , c , nbp_basic . use_z ) if not nbp_basic . is_3d : im = extract . focus_stack ( im ) im , bad_columns = extract . strip_hack ( im ) # find faulty columns if config [ 'deconvolve' ]: im = extract . wiener_deconvolve ( im , config [ 'wiener_pad_shape' ], wiener_filter ) if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : im = utils . morphology . top_hat ( im , filter_kernel_dapi ) im [:, bad_columns ] = 0 else : # im converted to float in convolve_2d so no point changing dtype beforehand. im = utils . morphology . convolve_2d ( im , filter_kernel ) * scale if config [ 'r_smooth' ] is not None : # oa convolve uses lots of memory and much slower here. im = utils . morphology . imfilter ( im , smooth_kernel , oa = False ) im [:, bad_columns ] = 0 # get_info is quicker on int32 so do this conversion first. im = np . rint ( im , np . zeros_like ( im , dtype = np . int32 ), casting = 'unsafe' ) # only use image unaffected by strip_hack to get information from tile good_columns = np . setdiff1d ( np . arange ( nbp_basic . tile_sz ), bad_columns ) nbp . auto_thresh [ t , r , c ], hist_counts_trc , nbp_debug . n_clip_pixels [ t , r , c ], \\ nbp_debug . clip_extract_scale [ t , r , c ] = \\ extract . get_extract_info ( im [:, good_columns ], config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale , nbp_debug . z_info ) # Deal with pixels outside uint16 range when saving if nbp_debug . n_clip_pixels [ t , r , c ] > config [ 'n_clip_warn' ]: warnings . warn ( f \" \\n Tile { t } , round { r } , channel { c } has \" f \" { nbp_debug . n_clip_pixels [ t , r , c ] } pixels \\n \" f \"that will be clipped when converting to uint16.\" ) if nbp_debug . n_clip_pixels [ t , r , c ] > config [ 'n_clip_error' ]: n_clip_error_images += 1 message = f \" \\n Number of images for which more than { config [ 'n_clip_error' ] } pixels \" \\ f \"clipped in conversion to uint16 is { n_clip_error_images } .\" if n_clip_error_images >= config [ 'n_clip_error_images_thresh' ]: # create new Notebook to save info obtained so far nb_fail_name = os . path . join ( nbp_file . output_dir , 'notebook_extract_error.npz' ) nb_fail = Notebook ( nb_fail_name , None ) # change names of pages so can add extra properties not in json file. nbp . name = 'extract_fail' nbp_debug . name = 'extract_debug_fail' nbp . fail_trc = np . array ([ t , r , c ]) # record where failure occurred nbp_debug . fail_trc = np . array ([ t , r , c ]) nb_fail += nbp nb_fail += nbp_debug raise ValueError ( f \" { message } \\n Results up till now saved as { nb_fail_name } .\" ) else : warnings . warn ( f \" { message } \\n When this reaches { config [ 'n_clip_error_images_thresh' ] } \" f \", the extract step of the algorithm will be interrupted.\" ) if r != nbp_basic . anchor_round : nbp . hist_counts [:, r , c ] += hist_counts_trc if nbp_basic . is_3d : utils . npy . save_tile ( nbp_file , nbp_basic , im , t , r , c ) else : im_all_channels_2d [ c ] = im pbar . update ( 1 ) if not nbp_basic . is_3d : utils . npy . save_tile ( nbp_file , nbp_basic , im_all_channels_2d , t , r ) pbar . close () if not nbp_basic . use_anchor : nbp_debug . scale_anchor_tile = None nbp_debug . scale_anchor_z = None nbp_debug . scale_anchor = None return nbp , nbp_debug","title":"Extract"},{"location":"code/pipeline/extract_run/#coppafish.pipeline.extract_run.extract_and_filter","text":"This reads in images from the raw nd2 files, filters them and then saves them as npy files in the tile directory. Also gets auto_thresh for use in turning images to point clouds and hist_values , hist_counts required for normalisation between channels. Returns the extract and extract_debug notebook pages. See 'extract' and 'extract_debug' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'extract' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required Returns: Type Description NotebookPage NotebookPage[extract] - Page containing auto_thresh for use in turning images to point clouds and hist_values , hist_counts required for normalisation between channels. NotebookPage NotebookPage[extract_debug] - Page containing variables which are not needed later in the pipeline but may be useful for debugging purposes. Source code in coppafish/pipeline/extract_run.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def extract_and_filter ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This reads in images from the raw `nd2` files, filters them and then saves them as npy files in the tile directory. Also gets `auto_thresh` for use in turning images to point clouds and `hist_values`, `hist_counts` required for normalisation between channels. Returns the `extract` and `extract_debug` notebook pages. See `'extract'` and `'extract_debug'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'extract'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page Returns: - `NotebookPage[extract]` - Page containing `auto_thresh` for use in turning images to point clouds and `hist_values`, `hist_counts` required for normalisation between channels. - `NotebookPage[extract_debug]` - Page containing variables which are not needed later in the pipeline but may be useful for debugging purposes. \"\"\" # Check scaling won't cause clipping when saving as uint16 scale_norm_max = np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift if config [ 'scale_norm' ] >= scale_norm_max : raise ValueError ( f \" \\n config['extract']['scale_norm'] = { config [ 'scale_norm' ] } but it must be below \" f \" { scale_norm_max } \" ) # initialise notebook pages if not nbp_basic . is_3d : config [ 'deconvolve' ] = False # only deconvolve if 3d pipeline nbp = NotebookPage ( \"extract\" ) nbp_debug = NotebookPage ( \"extract_debug\" ) # initialise output of this part of pipeline as 'vars' key nbp . auto_thresh = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds + nbp_basic . n_extra_rounds , nbp_basic . n_channels ), dtype = int ) nbp . hist_values = np . arange ( - nbp_basic . tile_pixel_value_shift , np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift + 2 , 1 ) nbp . hist_counts = np . zeros (( len ( nbp . hist_values ), nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = int ) hist_bin_edges = np . concatenate (( nbp . hist_values - 0.5 , nbp . hist_values [ - 1 :] + 0.5 )) # initialise debugging info as 'debug' page nbp_debug . n_clip_pixels = np . zeros_like ( nbp . auto_thresh , dtype = int ) nbp_debug . clip_extract_scale = np . zeros_like ( nbp . auto_thresh , dtype = np . float32 ) if nbp_basic . is_3d : nbp_debug . z_info = int ( np . floor ( nbp_basic . nz / 2 )) # central z-plane to get info from. else : nbp_debug . z_info = 0 # update config params in notebook. All optional parameters in config are added to debug page if config [ 'r1' ] is None : config [ 'r1' ] = extract . get_pixel_length ( config [ 'r1_auto_microns' ], nbp_basic . pixel_size_xy ) if config [ 'r2' ] is None : config [ 'r2' ] = config [ 'r1' ] * 2 if config [ 'r_dapi' ] is None : if config [ 'r_dapi_auto_microns' ] is not None : config [ 'r_dapi' ] = extract . get_pixel_length ( config [ 'r_dapi_auto_microns' ], nbp_basic . pixel_size_xy ) nbp_debug . r1 = config [ 'r1' ] nbp_debug . r2 = config [ 'r2' ] nbp_debug . r_dapi = config [ 'r_dapi' ] filter_kernel = utils . morphology . hanning_diff ( nbp_debug . r1 , nbp_debug . r2 ) if nbp_debug . r_dapi is not None : filter_kernel_dapi = utils . strel . disk ( nbp_debug . r_dapi ) else : filter_kernel_dapi = None if config [ 'r_smooth' ] is not None : if len ( config [ 'r_smooth' ]) == 2 : if nbp_basic . is_3d : warnings . warn ( f \"Running 3D pipeline but only 2D smoothing requested with r_smooth\" f \" = { config [ 'r_smooth' ] } .\" ) elif len ( config [ 'r_smooth' ]) == 3 : if not nbp_basic . is_3d : raise ValueError ( \"Running 2D pipeline but 3D smoothing requested.\" ) else : raise ValueError ( f \"r_smooth provided was { config [ 'r_smooth' ] } . \\n \" f \"But it needs to be a 2 radii for 2D smoothing or 3 radii for 3D smoothing. \\n \" f \"I.e. it is the wrong shape.\" ) if config [ 'r_smooth' ][ 0 ] > config [ 'r2' ]: raise ValueError ( f \"Smoothing radius, { config [ 'r_smooth' ][ 0 ] } , is larger than the outer radius of the \\n \" f \"hanning filter, { config [ 'r2' ] } , making the filtering step redundant.\" ) # smooth_kernel = utils.strel.fspecial(*tuple(config['r_smooth'])) smooth_kernel = np . ones ( tuple ( np . array ( config [ 'r_smooth' ], dtype = int ) * 2 - 1 )) smooth_kernel = smooth_kernel / np . sum ( smooth_kernel ) if np . max ( config [ 'r_smooth' ]) == 1 : warnings . warn ( 'Max radius of smooth filter was 1, so not using.' ) config [ 'r_smooth' ] = None if config [ 'deconvolve' ]: if not os . path . isfile ( nbp_file . psf ): spot_images , config [ 'psf_intensity_thresh' ], psf_tiles_used = \\ extract . get_psf_spots ( nbp_file , nbp_basic , nbp_basic . ref_round , nbp_basic . use_tiles , nbp_basic . ref_channel , nbp_basic . use_z , config [ 'psf_detect_radius_xy' ], config [ 'psf_detect_radius_z' ], config [ 'psf_min_spots' ], config [ 'psf_intensity_thresh' ], config [ 'auto_thresh_multiplier' ], config [ 'psf_isolation_dist' ], config [ 'psf_shape' ]) psf = extract . get_psf ( spot_images , config [ 'psf_annulus_width' ]) np . save ( nbp_file . psf , np . moveaxis ( psf , 2 , 0 )) # save with z as first axis else : # Know psf only computed for 3D pipeline hence know ndim=3 psf = np . moveaxis ( np . load ( nbp_file . psf ), 0 , 2 ) # Put z to last index psf_tiles_used = None # normalise psf so min is 0 and max is 1. psf = psf - psf . min () psf = psf / psf . max () pad_im_shape = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ]) + \\ np . array ( config [ 'wiener_pad_shape' ]) * 2 wiener_filter = extract . get_wiener_filter ( psf , pad_im_shape , config [ 'wiener_constant' ]) nbp_debug . psf = psf nbp_debug . psf_intensity_thresh = config [ 'psf_intensity_thresh' ] nbp_debug . psf_tiles_used = psf_tiles_used else : nbp_debug . psf = None nbp_debug . psf_intensity_thresh = None nbp_debug . psf_tiles_used = None # check to see if scales have already been computed config [ 'scale' ], config [ 'scale_anchor' ] = extract . get_scale_from_txt ( nbp_file . scale , config [ 'scale' ], config [ 'scale_anchor' ]) if config [ 'scale' ] is None and len ( nbp_file . round ) > 0 : # ensure scale_norm value is reasonable so max pixel value in tiff file is significant factor of max of uint16 scale_norm_min = ( np . iinfo ( 'uint16' ) . max - nbp_basic . tile_pixel_value_shift ) / 5 scale_norm_max = np . iinfo ( 'uint16' ) . max - nbp_basic . tile_pixel_value_shift if not scale_norm_min <= config [ 'scale_norm' ] <= scale_norm_max : raise utils . errors . OutOfBoundsError ( \"scale_norm\" , config [ 'scale_norm' ], scale_norm_min , scale_norm_max ) # If using smoothing, apply this as well before scal if config [ 'r_smooth' ] is None : smooth_kernel_2d = None else : if smooth_kernel . ndim == 3 : # take central plane of smooth filter if 3D as scale is found from a single z-plane. smooth_kernel_2d = smooth_kernel [:, :, config [ 'r_smooth' ][ 2 ] - 1 ] else : smooth_kernel_2d = smooth_kernel . copy () # smoothing is averaging so to average in 2D, need to re normalise filter smooth_kernel_2d = smooth_kernel_2d / np . sum ( smooth_kernel_2d ) if np . max ( config [ 'r_smooth' ][: 2 ]) <= 1 : smooth_kernel_2d = None # If dimensions of 2D kernel are [1, 1] is equivalent to no smoothing nbp_debug . scale_tile , nbp_debug . scale_channel , nbp_debug . scale_z , config [ 'scale' ] = \\ extract . get_scale ( nbp_file , nbp_basic , 0 , nbp_basic . use_tiles , nbp_basic . use_channels , nbp_basic . use_z , config [ 'scale_norm' ], filter_kernel , smooth_kernel_2d ) else : nbp_debug . scale_tile = None nbp_debug . scale_channel = None nbp_debug . scale_z = None smooth_kernel_2d = None nbp_debug . scale = config [ 'scale' ] print ( f \"Scale: { nbp_debug . scale } \" ) # save scale values incase need to re-run extract . save_scale ( nbp_file . scale , nbp_debug . scale , config [ 'scale_anchor' ]) # get rounds to iterate over use_channels_anchor = [ c for c in [ nbp_basic . dapi_channel , nbp_basic . anchor_channel ] if c is not None ] use_channels_anchor . sort () if filter_kernel_dapi is None : # If not filtering DAPI, skip over the DAPI channel. use_channels_anchor = np . setdiff1d ( use_channels_anchor , nbp_basic . dapi_channel ) if nbp_basic . use_anchor : # always have anchor as first round after imaging rounds round_files = nbp_file . round + [ nbp_file . anchor ] use_rounds = nbp_basic . use_rounds + [ nbp_basic . n_rounds ] n_images = ( len ( use_rounds ) - 1 ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) + \\ len ( nbp_basic . use_tiles ) * len ( use_channels_anchor ) else : round_files = nbp_file . round use_rounds = nbp_basic . use_rounds n_images = len ( use_rounds ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) n_clip_error_images = 0 if config [ 'n_clip_error' ] is None : # default is 1% of pixels on single z-plane config [ 'n_clip_error' ] = int ( nbp_basic . tile_sz * nbp_basic . tile_sz / 100 ) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f 'Loading in tiles from { nbp_file . raw_extension } , filtering and saving as .npy' ) for r in use_rounds : # set scale and channels to use im_file = os . path . join ( nbp_file . input_dir , round_files [ r ]) if nbp_file . raw_extension == '.npy' : extract . wait_for_data ( im_file , config [ 'wait_time' ], dir = True ) else : extract . wait_for_data ( im_file + nbp_file . raw_extension , config [ 'wait_time' ]) round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) if r == nbp_basic . anchor_round : n_clip_error_images = 0 # reset for anchor as different scale used. if config [ 'scale_anchor' ] is None : nbp_debug . scale_anchor_tile , _ , nbp_debug . scale_anchor_z , config [ 'scale_anchor' ] = \\ extract . get_scale ( nbp_file , nbp_basic , nbp_basic . anchor_round , nbp_basic . use_tiles , [ nbp_basic . anchor_channel ], nbp_basic . use_z , config [ 'scale_norm' ], filter_kernel , smooth_kernel_2d ) # save scale values incase need to re-run extract . save_scale ( nbp_file . scale , nbp_debug . scale , config [ 'scale_anchor' ]) else : nbp_debug . scale_anchor_tile = None nbp_debug . scale_anchor_z = None nbp_debug . scale_anchor = config [ 'scale_anchor' ] print ( f \"Scale_anchor: { nbp_debug . scale_anchor } \" ) scale = nbp_debug . scale_anchor use_channels = use_channels_anchor else : scale = nbp_debug . scale use_channels = nbp_basic . use_channels # convolve_2d each image for t in nbp_basic . use_tiles : if not nbp_basic . is_3d : # for 2d all channels in same file file_exists = os . path . isfile ( nbp_file . tile [ t ][ r ]) if file_exists : # mmap load in image for all channels if tiff exists im_all_channels_2d = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' ) else : # Only save 2d data when all channels collected # For channels not used, keep all pixels 0. im_all_channels_2d = np . zeros (( nbp_basic . n_channels , nbp_basic . tile_sz , nbp_basic . tile_sz ), dtype = np . int32 ) for c in use_channels : if r == nbp_basic . anchor_round and c == nbp_basic . anchor_channel : # max value that can be saved and no shifting done for DAPI max_npy_pixel_value = np . iinfo ( np . uint16 ) . max else : max_npy_pixel_value = np . iinfo ( np . uint16 ) . max - nbp_basic . tile_pixel_value_shift if nbp_basic . is_3d : file_exists = os . path . isfile ( nbp_file . tile [ t ][ r ][ c ]) pbar . set_postfix ({ 'round' : r , 'tile' : t , 'channel' : c , 'exists' : str ( file_exists )}) if file_exists : if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : pass else : # Only need to load in mid-z plane if 3D. if nbp_basic . is_3d : im = utils . npy . load_tile ( nbp_file , nbp_basic , t , r , c , yxz = [ None , None , nbp_debug . z_info ]) else : im = im_all_channels_2d [ c ] . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift nbp . auto_thresh [ t , r , c ], hist_counts_trc , nbp_debug . n_clip_pixels [ t , r , c ], \\ nbp_debug . clip_extract_scale [ t , r , c ] = \\ extract . get_extract_info ( im , config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale ) if r != nbp_basic . anchor_round : nbp . hist_counts [:, r , c ] += hist_counts_trc else : im = utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , c , nbp_basic . use_z ) if not nbp_basic . is_3d : im = extract . focus_stack ( im ) im , bad_columns = extract . strip_hack ( im ) # find faulty columns if config [ 'deconvolve' ]: im = extract . wiener_deconvolve ( im , config [ 'wiener_pad_shape' ], wiener_filter ) if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : im = utils . morphology . top_hat ( im , filter_kernel_dapi ) im [:, bad_columns ] = 0 else : # im converted to float in convolve_2d so no point changing dtype beforehand. im = utils . morphology . convolve_2d ( im , filter_kernel ) * scale if config [ 'r_smooth' ] is not None : # oa convolve uses lots of memory and much slower here. im = utils . morphology . imfilter ( im , smooth_kernel , oa = False ) im [:, bad_columns ] = 0 # get_info is quicker on int32 so do this conversion first. im = np . rint ( im , np . zeros_like ( im , dtype = np . int32 ), casting = 'unsafe' ) # only use image unaffected by strip_hack to get information from tile good_columns = np . setdiff1d ( np . arange ( nbp_basic . tile_sz ), bad_columns ) nbp . auto_thresh [ t , r , c ], hist_counts_trc , nbp_debug . n_clip_pixels [ t , r , c ], \\ nbp_debug . clip_extract_scale [ t , r , c ] = \\ extract . get_extract_info ( im [:, good_columns ], config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale , nbp_debug . z_info ) # Deal with pixels outside uint16 range when saving if nbp_debug . n_clip_pixels [ t , r , c ] > config [ 'n_clip_warn' ]: warnings . warn ( f \" \\n Tile { t } , round { r } , channel { c } has \" f \" { nbp_debug . n_clip_pixels [ t , r , c ] } pixels \\n \" f \"that will be clipped when converting to uint16.\" ) if nbp_debug . n_clip_pixels [ t , r , c ] > config [ 'n_clip_error' ]: n_clip_error_images += 1 message = f \" \\n Number of images for which more than { config [ 'n_clip_error' ] } pixels \" \\ f \"clipped in conversion to uint16 is { n_clip_error_images } .\" if n_clip_error_images >= config [ 'n_clip_error_images_thresh' ]: # create new Notebook to save info obtained so far nb_fail_name = os . path . join ( nbp_file . output_dir , 'notebook_extract_error.npz' ) nb_fail = Notebook ( nb_fail_name , None ) # change names of pages so can add extra properties not in json file. nbp . name = 'extract_fail' nbp_debug . name = 'extract_debug_fail' nbp . fail_trc = np . array ([ t , r , c ]) # record where failure occurred nbp_debug . fail_trc = np . array ([ t , r , c ]) nb_fail += nbp nb_fail += nbp_debug raise ValueError ( f \" { message } \\n Results up till now saved as { nb_fail_name } .\" ) else : warnings . warn ( f \" { message } \\n When this reaches { config [ 'n_clip_error_images_thresh' ] } \" f \", the extract step of the algorithm will be interrupted.\" ) if r != nbp_basic . anchor_round : nbp . hist_counts [:, r , c ] += hist_counts_trc if nbp_basic . is_3d : utils . npy . save_tile ( nbp_file , nbp_basic , im , t , r , c ) else : im_all_channels_2d [ c ] = im pbar . update ( 1 ) if not nbp_basic . is_3d : utils . npy . save_tile ( nbp_file , nbp_basic , im_all_channels_2d , t , r ) pbar . close () if not nbp_basic . use_anchor : nbp_debug . scale_anchor_tile = None nbp_debug . scale_anchor_z = None nbp_debug . scale_anchor = None return nbp , nbp_debug","title":"extract_and_filter()"},{"location":"code/pipeline/find_spots/","text":"find_spots ( config , nbp_file , nbp_basic , auto_thresh ) This function turns each tiff file in the tile directory into a point cloud, saving the results as spot_details in the find_spots notebook page. See 'find_spots' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'find_spots' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required auto_thresh np . ndarray float [n_tiles x n_rounds x n_channels] . auto_thresh[t, r, c] is the threshold for the tiff file corresponding to tile t , round r , channel c such that all local maxima with pixel values greater than this are considered spots. required Returns: Type Description NotebookPage NotebookPage[find_spots] - Page containing point cloud of all tiles, rounds and channels. Source code in coppafish/pipeline/find_spots.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def find_spots ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , auto_thresh : np . ndarray ) -> NotebookPage : \"\"\" This function turns each tiff file in the tile directory into a point cloud, saving the results as `spot_details` in the `find_spots` notebook page. See `'find_spots'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'find_spots'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page auto_thresh: `float [n_tiles x n_rounds x n_channels]`. `auto_thresh[t, r, c]` is the threshold for the tiff file corresponding to tile `t`, round `r`, channel `c` such that all local maxima with pixel values greater than this are considered spots. Returns: `NotebookPage[find_spots]` - Page containing point cloud of all tiles, rounds and channels. \"\"\" nbp = NotebookPage ( \"find_spots\" ) if nbp_basic . is_3d is False : # set z details to None if using 2d pipeline config [ 'radius_z' ] = None config [ 'isolation_radius_z' ] = None max_spots = config [ 'max_spots_2d' ] else : max_spots = config [ 'max_spots_3d' ] # record threshold for isolated spots in each tile of reference round/channel if config [ 'isolation_thresh' ] is None : nbp . isolation_thresh = auto_thresh [:, nbp_basic . ref_round , nbp_basic . anchor_channel ] * \\ config [ 'auto_isolation_thresh_multiplier' ] else : nbp . isolation_thresh = np . ones_like ( auto_thresh [:, nbp_basic . ref_round , nbp_basic . anchor_channel ]) * \\ config [ 'isolation_thresh' ] # have to save spot_yxz and spot_isolated as table to stop pickle issues associated with numpy object arrays. # columns of spot_details are: tile, channel, round, isolated, y, x, z # max value is y or x coordinate of around 2048 hence can use int16. spot_details = np . empty (( 0 , 7 ), dtype = np . int16 ) nbp . spot_no = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds + nbp_basic . n_extra_rounds , nbp_basic . n_channels ), dtype = np . int32 ) use_rounds = nbp_basic . use_rounds n_images = len ( use_rounds ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) if nbp_basic . use_anchor : use_rounds = use_rounds + [ nbp_basic . anchor_round ] n_images = n_images + len ( nbp_basic . use_tiles ) n_z = np . max ([ 1 , nbp_basic . is_3d * nbp_basic . nz ]) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f \"Detecting spots on filtered images saved as npy\" ) for r in use_rounds : if r == nbp_basic . anchor_round : use_channels = [ nbp_basic . anchor_channel ] else : use_channels = nbp_basic . use_channels for t in nbp_basic . use_tiles : for c in use_channels : pbar . set_postfix ({ 'round' : r , 'tile' : t , 'channel' : c }) # Find local maxima on shifted uint16 images to save time avoiding conversion to int32. # Then need to shift the detect_spots and check_neighb_intensity thresh correspondingly. image = utils . npy . load_tile ( nbp_file , nbp_basic , t , r , c , apply_shift = False ) spot_yxz , spot_intensity = fs . detect_spots ( image , auto_thresh [ t , r , c ] + nbp_basic . tile_pixel_value_shift , config [ 'radius_xy' ], config [ 'radius_z' ], True ) no_negative_neighbour = fs . check_neighbour_intensity ( image , spot_yxz , thresh = nbp_basic . tile_pixel_value_shift ) spot_yxz = spot_yxz [ no_negative_neighbour ] spot_intensity = spot_intensity [ no_negative_neighbour ] if r == nbp_basic . ref_round : spot_isolated = fs . get_isolated ( image . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift , spot_yxz , nbp . isolation_thresh [ t ], config [ 'isolation_radius_inner' ], config [ 'isolation_radius_xy' ], config [ 'isolation_radius_z' ]) else : # if imaging round, only keep the highest intensity spots on each z plane # as only used for registration keep = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) for z in range ( n_z ): if nbp_basic . is_3d : in_z = spot_yxz [:, 2 ] == z else : in_z = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) if np . sum ( in_z ) > max_spots : intensity_thresh = np . sort ( spot_intensity [ in_z ])[ - max_spots ] keep [ np . logical_and ( in_z , spot_intensity < intensity_thresh )] = False spot_yxz = spot_yxz [ keep ] # don't care if these spots isolated so say they are not spot_isolated = np . zeros ( spot_yxz . shape [ 0 ], dtype = bool ) spot_details_trc = np . zeros (( spot_yxz . shape [ 0 ], spot_details . shape [ 1 ]), dtype = np . int16 ) spot_details_trc [:, : 3 ] = [ t , r , c ] spot_details_trc [:, 3 ] = spot_isolated spot_details_trc [:, 4 : 4 + spot_yxz . shape [ 1 ]] = spot_yxz # if 2d pipeline, z coordinate set to 0. spot_details = np . append ( spot_details , spot_details_trc , axis = 0 ) nbp . spot_no [ t , r , c ] = spot_yxz . shape [ 0 ] pbar . update ( 1 ) nbp . spot_details = spot_details return nbp","title":"Find Spots"},{"location":"code/pipeline/find_spots/#coppafish.pipeline.find_spots.find_spots","text":"This function turns each tiff file in the tile directory into a point cloud, saving the results as spot_details in the find_spots notebook page. See 'find_spots' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'find_spots' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required auto_thresh np . ndarray float [n_tiles x n_rounds x n_channels] . auto_thresh[t, r, c] is the threshold for the tiff file corresponding to tile t , round r , channel c such that all local maxima with pixel values greater than this are considered spots. required Returns: Type Description NotebookPage NotebookPage[find_spots] - Page containing point cloud of all tiles, rounds and channels. Source code in coppafish/pipeline/find_spots.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def find_spots ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , auto_thresh : np . ndarray ) -> NotebookPage : \"\"\" This function turns each tiff file in the tile directory into a point cloud, saving the results as `spot_details` in the `find_spots` notebook page. See `'find_spots'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'find_spots'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page auto_thresh: `float [n_tiles x n_rounds x n_channels]`. `auto_thresh[t, r, c]` is the threshold for the tiff file corresponding to tile `t`, round `r`, channel `c` such that all local maxima with pixel values greater than this are considered spots. Returns: `NotebookPage[find_spots]` - Page containing point cloud of all tiles, rounds and channels. \"\"\" nbp = NotebookPage ( \"find_spots\" ) if nbp_basic . is_3d is False : # set z details to None if using 2d pipeline config [ 'radius_z' ] = None config [ 'isolation_radius_z' ] = None max_spots = config [ 'max_spots_2d' ] else : max_spots = config [ 'max_spots_3d' ] # record threshold for isolated spots in each tile of reference round/channel if config [ 'isolation_thresh' ] is None : nbp . isolation_thresh = auto_thresh [:, nbp_basic . ref_round , nbp_basic . anchor_channel ] * \\ config [ 'auto_isolation_thresh_multiplier' ] else : nbp . isolation_thresh = np . ones_like ( auto_thresh [:, nbp_basic . ref_round , nbp_basic . anchor_channel ]) * \\ config [ 'isolation_thresh' ] # have to save spot_yxz and spot_isolated as table to stop pickle issues associated with numpy object arrays. # columns of spot_details are: tile, channel, round, isolated, y, x, z # max value is y or x coordinate of around 2048 hence can use int16. spot_details = np . empty (( 0 , 7 ), dtype = np . int16 ) nbp . spot_no = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds + nbp_basic . n_extra_rounds , nbp_basic . n_channels ), dtype = np . int32 ) use_rounds = nbp_basic . use_rounds n_images = len ( use_rounds ) * len ( nbp_basic . use_tiles ) * len ( nbp_basic . use_channels ) if nbp_basic . use_anchor : use_rounds = use_rounds + [ nbp_basic . anchor_round ] n_images = n_images + len ( nbp_basic . use_tiles ) n_z = np . max ([ 1 , nbp_basic . is_3d * nbp_basic . nz ]) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f \"Detecting spots on filtered images saved as npy\" ) for r in use_rounds : if r == nbp_basic . anchor_round : use_channels = [ nbp_basic . anchor_channel ] else : use_channels = nbp_basic . use_channels for t in nbp_basic . use_tiles : for c in use_channels : pbar . set_postfix ({ 'round' : r , 'tile' : t , 'channel' : c }) # Find local maxima on shifted uint16 images to save time avoiding conversion to int32. # Then need to shift the detect_spots and check_neighb_intensity thresh correspondingly. image = utils . npy . load_tile ( nbp_file , nbp_basic , t , r , c , apply_shift = False ) spot_yxz , spot_intensity = fs . detect_spots ( image , auto_thresh [ t , r , c ] + nbp_basic . tile_pixel_value_shift , config [ 'radius_xy' ], config [ 'radius_z' ], True ) no_negative_neighbour = fs . check_neighbour_intensity ( image , spot_yxz , thresh = nbp_basic . tile_pixel_value_shift ) spot_yxz = spot_yxz [ no_negative_neighbour ] spot_intensity = spot_intensity [ no_negative_neighbour ] if r == nbp_basic . ref_round : spot_isolated = fs . get_isolated ( image . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift , spot_yxz , nbp . isolation_thresh [ t ], config [ 'isolation_radius_inner' ], config [ 'isolation_radius_xy' ], config [ 'isolation_radius_z' ]) else : # if imaging round, only keep the highest intensity spots on each z plane # as only used for registration keep = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) for z in range ( n_z ): if nbp_basic . is_3d : in_z = spot_yxz [:, 2 ] == z else : in_z = np . ones ( spot_yxz . shape [ 0 ], dtype = bool ) if np . sum ( in_z ) > max_spots : intensity_thresh = np . sort ( spot_intensity [ in_z ])[ - max_spots ] keep [ np . logical_and ( in_z , spot_intensity < intensity_thresh )] = False spot_yxz = spot_yxz [ keep ] # don't care if these spots isolated so say they are not spot_isolated = np . zeros ( spot_yxz . shape [ 0 ], dtype = bool ) spot_details_trc = np . zeros (( spot_yxz . shape [ 0 ], spot_details . shape [ 1 ]), dtype = np . int16 ) spot_details_trc [:, : 3 ] = [ t , r , c ] spot_details_trc [:, 3 ] = spot_isolated spot_details_trc [:, 4 : 4 + spot_yxz . shape [ 1 ]] = spot_yxz # if 2d pipeline, z coordinate set to 0. spot_details = np . append ( spot_details , spot_details_trc , axis = 0 ) nbp . spot_no [ t , r , c ] = spot_yxz . shape [ 0 ] pbar . update ( 1 ) nbp . spot_details = spot_details return nbp","title":"find_spots()"},{"location":"code/pipeline/get_reference_spots/","text":"get_reference_spots ( nbp_file , nbp_basic , spot_details , tile_origin , transform ) This takes each spot found on the reference round/channel and computes the corresponding intensity in each of the imaging rounds/channels. See 'ref_spots' section of notebook_comments.json file for description of the variables in the page. The following variables: gene_no score score_diff intensity will be set to None so the page can be added to a Notebook . call_reference_spots should then be run to give their actual values. This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required Returns: Type Description NotebookPage NotebookPage[ref_spots] - Page containing intensity of each reference spot on each imaging round/channel. Source code in coppafish/pipeline/get_reference_spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_reference_spots ( nbp_file : NotebookPage , nbp_basic : NotebookPage , spot_details : np . ndarray , tile_origin : np . ndarray , transform : np . ndarray ) -> NotebookPage : \"\"\" This takes each spot found on the reference round/channel and computes the corresponding intensity in each of the imaging rounds/channels. See `'ref_spots'` section of `notebook_comments.json` file for description of the variables in the page. The following variables: * `gene_no` * `score` * `score_diff` * `intensity` will be set to `None` so the page can be added to a *Notebook*. `call_reference_spots` should then be run to give their actual values. This is so if there is an error in `call_reference_spots`, `get_reference_spots` won't have to be re-run. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. Returns: `NotebookPage[ref_spots]` - Page containing intensity of each reference spot on each imaging round/channel. \"\"\" nbp = NotebookPage ( \"ref_spots\" ) r = nbp_basic . ref_round c = nbp_basic . ref_channel # all means all spots found on the reference round / channel all_local_yxz = np . zeros (( 0 , 3 ), dtype = np . int16 ) all_isolated = np . zeros ( 0 , dtype = bool ) all_local_tile = np . zeros ( 0 , dtype = np . int16 ) for t in nbp_basic . use_tiles : t_local_yxz , t_isolated = spot_yxz ( spot_details , t , r , c , return_isolated = True ) if np . shape ( t_local_yxz )[ 0 ] > 0 : all_local_yxz = np . append ( all_local_yxz , t_local_yxz , axis = 0 ) all_isolated = np . append ( all_isolated , t_isolated . astype ( bool ), axis = 0 ) all_local_tile = np . append ( all_local_tile , np . ones_like ( t_isolated , dtype = np . int16 ) * t ) # find duplicate spots as those detected on a tile which is not tile centre they are closest to not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , all_local_yxz , all_local_tile ) # nd means all spots that are not duplicate nd_local_yxz = all_local_yxz [ not_duplicate ] nd_isolated = all_isolated [ not_duplicate ] nd_local_tile = all_local_tile [ not_duplicate ] invalid_value = - nbp_basic . tile_pixel_value_shift # Only save used rounds/channels initially n_use_rounds = len ( nbp_basic . use_rounds ) n_use_channels = len ( nbp_basic . use_channels ) use_tiles = np . array ( nbp_basic . use_tiles . copy ()) n_use_tiles = len ( use_tiles ) nd_spot_colors_use = np . zeros (( nd_local_tile . shape [ 0 ], n_use_rounds , n_use_channels ), dtype = np . int32 ) transform = jnp . asarray ( transform ) print ( 'Reading in spot_colors for ref_round spots' ) for t in nbp_basic . use_tiles : in_tile = nd_local_tile == t if np . sum ( in_tile ) > 0 : print ( f \"Tile { np . where ( use_tiles == t )[ 0 ][ 0 ] + 1 } / { n_use_tiles } \" ) # this line will return invalid_value for spots outside tile bounds on particular r/c. nd_spot_colors_use [ in_tile ] = get_spot_colors ( jnp . asarray ( nd_local_yxz [ in_tile ]), t , transform , nbp_file , nbp_basic ) # good means all spots that were in bounds of tile on every imaging round and channel that was used. # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use, 0, -1) # use_rc_index = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels) # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use[use_rc_index], -1, 0) good = ~ np . any ( nd_spot_colors_use == invalid_value , axis = ( 1 , 2 )) good_local_yxz = nd_local_yxz [ good ] good_isolated = nd_isolated [ good ] good_local_tile = nd_local_tile [ good ] # add in un-used rounds with invalid_value n_good = np . sum ( good ) good_spot_colors = np . full (( n_good , nbp_basic . n_rounds , nbp_basic . n_channels ), invalid_value , dtype = np . int32 ) good_spot_colors [ np . ix_ ( np . arange ( n_good ), nbp_basic . use_rounds , nbp_basic . use_channels )] = nd_spot_colors_use [ good ] # save spot info to notebook nbp . local_yxz = good_local_yxz nbp . isolated = good_isolated nbp . tile = good_local_tile nbp . colors = good_spot_colors # Set variables added in call_reference_spots to None so can save to Notebook. # I.e. if call_reference_spots hit error, but we did not do this, # we would have to run get_reference_spots again. nbp . gene_no = None nbp . score = None nbp . score_diff = None nbp . intensity = None return nbp","title":"Get Reference Spots"},{"location":"code/pipeline/get_reference_spots/#coppafish.pipeline.get_reference_spots.get_reference_spots","text":"This takes each spot found on the reference round/channel and computes the corresponding intensity in each of the imaging rounds/channels. See 'ref_spots' section of notebook_comments.json file for description of the variables in the page. The following variables: gene_no score score_diff intensity will be set to None so the page can be added to a Notebook . call_reference_spots should then be run to give their actual values. This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required Returns: Type Description NotebookPage NotebookPage[ref_spots] - Page containing intensity of each reference spot on each imaging round/channel. Source code in coppafish/pipeline/get_reference_spots.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def get_reference_spots ( nbp_file : NotebookPage , nbp_basic : NotebookPage , spot_details : np . ndarray , tile_origin : np . ndarray , transform : np . ndarray ) -> NotebookPage : \"\"\" This takes each spot found on the reference round/channel and computes the corresponding intensity in each of the imaging rounds/channels. See `'ref_spots'` section of `notebook_comments.json` file for description of the variables in the page. The following variables: * `gene_no` * `score` * `score_diff` * `intensity` will be set to `None` so the page can be added to a *Notebook*. `call_reference_spots` should then be run to give their actual values. This is so if there is an error in `call_reference_spots`, `get_reference_spots` won't have to be re-run. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. Returns: `NotebookPage[ref_spots]` - Page containing intensity of each reference spot on each imaging round/channel. \"\"\" nbp = NotebookPage ( \"ref_spots\" ) r = nbp_basic . ref_round c = nbp_basic . ref_channel # all means all spots found on the reference round / channel all_local_yxz = np . zeros (( 0 , 3 ), dtype = np . int16 ) all_isolated = np . zeros ( 0 , dtype = bool ) all_local_tile = np . zeros ( 0 , dtype = np . int16 ) for t in nbp_basic . use_tiles : t_local_yxz , t_isolated = spot_yxz ( spot_details , t , r , c , return_isolated = True ) if np . shape ( t_local_yxz )[ 0 ] > 0 : all_local_yxz = np . append ( all_local_yxz , t_local_yxz , axis = 0 ) all_isolated = np . append ( all_isolated , t_isolated . astype ( bool ), axis = 0 ) all_local_tile = np . append ( all_local_tile , np . ones_like ( t_isolated , dtype = np . int16 ) * t ) # find duplicate spots as those detected on a tile which is not tile centre they are closest to not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , all_local_yxz , all_local_tile ) # nd means all spots that are not duplicate nd_local_yxz = all_local_yxz [ not_duplicate ] nd_isolated = all_isolated [ not_duplicate ] nd_local_tile = all_local_tile [ not_duplicate ] invalid_value = - nbp_basic . tile_pixel_value_shift # Only save used rounds/channels initially n_use_rounds = len ( nbp_basic . use_rounds ) n_use_channels = len ( nbp_basic . use_channels ) use_tiles = np . array ( nbp_basic . use_tiles . copy ()) n_use_tiles = len ( use_tiles ) nd_spot_colors_use = np . zeros (( nd_local_tile . shape [ 0 ], n_use_rounds , n_use_channels ), dtype = np . int32 ) transform = jnp . asarray ( transform ) print ( 'Reading in spot_colors for ref_round spots' ) for t in nbp_basic . use_tiles : in_tile = nd_local_tile == t if np . sum ( in_tile ) > 0 : print ( f \"Tile { np . where ( use_tiles == t )[ 0 ][ 0 ] + 1 } / { n_use_tiles } \" ) # this line will return invalid_value for spots outside tile bounds on particular r/c. nd_spot_colors_use [ in_tile ] = get_spot_colors ( jnp . asarray ( nd_local_yxz [ in_tile ]), t , transform , nbp_file , nbp_basic ) # good means all spots that were in bounds of tile on every imaging round and channel that was used. # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use, 0, -1) # use_rc_index = np.ix_(nbp_basic.use_rounds, nbp_basic.use_channels) # nd_spot_colors_use = np.moveaxis(nd_spot_colors_use[use_rc_index], -1, 0) good = ~ np . any ( nd_spot_colors_use == invalid_value , axis = ( 1 , 2 )) good_local_yxz = nd_local_yxz [ good ] good_isolated = nd_isolated [ good ] good_local_tile = nd_local_tile [ good ] # add in un-used rounds with invalid_value n_good = np . sum ( good ) good_spot_colors = np . full (( n_good , nbp_basic . n_rounds , nbp_basic . n_channels ), invalid_value , dtype = np . int32 ) good_spot_colors [ np . ix_ ( np . arange ( n_good ), nbp_basic . use_rounds , nbp_basic . use_channels )] = nd_spot_colors_use [ good ] # save spot info to notebook nbp . local_yxz = good_local_yxz nbp . isolated = good_isolated nbp . tile = good_local_tile nbp . colors = good_spot_colors # Set variables added in call_reference_spots to None so can save to Notebook. # I.e. if call_reference_spots hit error, but we did not do this, # we would have to run get_reference_spots again. nbp . gene_no = None nbp . score = None nbp . score_diff = None nbp . intensity = None return nbp","title":"get_reference_spots()"},{"location":"code/pipeline/omp/","text":"call_spots_omp ( config , nbp_file , nbp_basic , nbp_call_spots , tile_origin , transform , shape_tile ) This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel. From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene. Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate. See 'omp' section of notebook_comments.json file for description of the variables in the omp page. Parameters: Name Type Description Default config dict Dictionary obtained from 'omp' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required nbp_call_spots NotebookPage required tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required shape_tile Optional [ int ] Tile to use to compute the expected shape of a spot in the gene coefficient images. Should be the tile, for which the most spots where found in the call_reference_spots step. If None , will be set to the centre tile. required Returns: Type Description NotebookPage NotebookPage[omp] - Page contains gene assignments and info for spots using omp. Source code in coppafish/pipeline/omp.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def call_spots_omp ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , nbp_call_spots : NotebookPage , tile_origin : np . ndarray , transform : np . ndarray , shape_tile : Optional [ int ]) -> NotebookPage : \"\"\" This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel. From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene. Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate. See `'omp'` section of `notebook_comments.json` file for description of the variables in the omp page. Args: config: Dictionary obtained from `'omp'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page nbp_call_spots: tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. shape_tile: Tile to use to compute the expected shape of a spot in the gene coefficient images. Should be the tile, for which the most spots where found in the `call_reference_spots` step. If `None`, will be set to the centre tile. Returns: `NotebookPage[omp]` - Page contains gene assignments and info for spots using omp. \"\"\" nbp = NotebookPage ( \"omp\" ) # use bled_codes with gene efficiency incorporated and only use_rounds/channels rc_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels ) bled_codes = np . moveaxis ( np . moveaxis ( nbp_call_spots . bled_codes_ge , 0 , - 1 )[ rc_ind ], - 1 , 0 ) utils . errors . check_color_nan ( bled_codes , nbp_basic ) norm_bled_codes = np . linalg . norm ( bled_codes , axis = ( 1 , 2 )) if np . abs ( norm_bled_codes - 1 ) . max () > 1e-6 : raise ValueError ( \"nbp_call_spots.bled_codes_ge don't all have an L2 norm of 1 over \" \"use_rounds and use_channels.\" ) bled_codes = jnp . asarray ( bled_codes ) transform = jnp . asarray ( transform ) color_norm_factor = jnp . asarray ( nbp_call_spots . color_norm_factor [ rc_ind ]) n_genes , n_rounds_use , n_channels_use = bled_codes . shape dp_norm_shift = nbp_call_spots . dp_norm_shift * np . sqrt ( n_rounds_use ) if nbp_basic . is_3d : detect_radius_z = config [ 'radius_z' ] n_z = nbp_basic . nz else : detect_radius_z = None n_z = 1 config [ 'use_z' ] = np . arange ( n_z ) if config [ 'use_z' ] is not None : use_z_oob = [ val for val in config [ 'use_z' ] if val < 0 or val >= n_z ] if len ( use_z_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_z\" , use_z_oob [ 0 ], 0 , n_z - 1 ) if len ( config [ 'use_z' ]) == 2 : # use consecutive values if only 2 given. config [ 'use_z' ] = list ( np . arange ( config [ 'use_z' ][ 0 ], config [ 'use_z' ][ 1 ] + 1 )) use_z = np . array ( config [ 'use_z' ]) else : use_z = np . arange ( n_z ) # determine initial_intensity_thresh from average intensity over all pixels on central z-plane. nbp . initial_intensity_thresh = omp . get_initial_intensity_thresh ( config , nbp_call_spots ) use_tiles = np . array ( nbp_basic . use_tiles . copy ()) if not os . path . isfile ( nbp_file . omp_spot_shape ): # Set tile order so do shape_tile first to compute spot_shape from it. if shape_tile is None : shape_tile = scale . central_tile ( nbp_basic . tilepos_yx , nbp_basic . use_tiles ) if shape_tile not in nbp_basic . use_tiles : raise ValueError ( f \"shape_tile, { shape_tile } is not in nbp_basic.use_tiles, { nbp_basic . use_tiles } \" ) shape_tile_ind = np . where ( np . array ( nbp_basic . use_tiles ) == shape_tile )[ 0 ][ 0 ] use_tiles [ 0 ], use_tiles [ shape_tile_ind ] = use_tiles [ shape_tile_ind ], use_tiles [ 0 ] spot_shape = None else : nbp . shape_tile = None nbp . shape_spot_local_yxz = None nbp . shape_spot_gene_no = None nbp . spot_shape_float = None # -1 because saved as uint16 so convert 0, 1, 2 to -1, 0, 1. spot_shape = np . load ( nbp_file . omp_spot_shape ) # Put z to last index if spot_shape . ndim == 3 : spot_shape = np . moveaxis ( spot_shape , 0 , 2 ) # Put z to last index # Deal with case where algorithm has been run for some tiles and data saved if os . path . isfile ( nbp_file . omp_spot_info ) and os . path . isfile ( nbp_file . omp_spot_coef ): if spot_shape is None : raise ValueError ( f 'OMP information already exists for some tiles but spot_shape tiff file does not: \\n ' f ' { nbp_file . omp_spot_shape } \\n Either add spot_shape tiff or delete the files: \\n ' f ' { nbp_file . omp_spot_info } and { nbp_file . omp_spot_coef } .' ) spot_coefs = sparse . load_npz ( nbp_file . omp_spot_coef ) spot_info = np . load ( nbp_file . omp_spot_info ) if spot_coefs . shape [ 0 ] > spot_info . shape [ 0 ]: # Case where bugged out after saving spot_coefs but before saving spot_info, delete all excess spot_coefs. warnings . warn ( f \"Have spot_coefs for { spot_coefs . shape [ 0 ] } spots but only spot_info for { spot_info . shape [ 0 ] } \" f \" spots. \\n So deleting the excess spot_coefs and re-saving to { nbp_file . omp_spot_coef } .\" ) spot_coefs = spot_coefs [: spot_info . shape [ 0 ]] sparse . save_npz ( nbp_file . omp_spot_coef , spot_coefs ) elif spot_coefs . shape [ 0 ] < spot_info . shape [ 0 ]: # If more spots in info than coefs then likely because duplicates removed from coefs but not spot_info. not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) if not_duplicate . size == spot_info . shape [ 0 ]: warnings . warn ( f 'There were less spots in \\n { nbp_file . omp_spot_info } \\n than \\n { nbp_file . omp_spot_coef } ' f 'because duplicates were deleted for spot_coefs but not for spot_info. \\n ' f 'Now, spot_info duplicates have also been deleted.' ) spot_info = spot_info [ not_duplicate ] np . save ( nbp_file . omp_spot_info , spot_info ) else : raise ValueError ( f \"Have spot_info for { spot_info . shape [ 0 ] } spots but only spot_coefs for \" f \" { spot_coefs . shape [ 0 ] } \\n Need to delete both { nbp_file . omp_spot_coef } and \" f \" { nbp_file . omp_spot_info } to get past this error.\" ) else : prev_found_tiles = np . unique ( spot_info [:, - 1 ]) use_tiles = np . setdiff1d ( use_tiles , prev_found_tiles ) warnings . warn ( f 'Already have OMP results for tiles { prev_found_tiles } so now just running on tiles ' f ' { use_tiles } .' ) del spot_coefs , spot_info elif os . path . isfile ( nbp_file . omp_spot_coef ): # If only have information only file but not the other, need to delete all files and start again. raise ValueError ( f 'The file { nbp_file . omp_spot_coef } exists but the file { nbp_file . omp_spot_info } does not. \\n ' f 'Delete or re-name the file { nbp_file . omp_spot_coef } to run omp part from scratch.' ) elif os . path . isfile ( nbp_file . omp_spot_info ): raise ValueError ( f 'The file { nbp_file . omp_spot_info } exists but the file { nbp_file . omp_spot_coef } does not. \\n ' f 'Delete or re-name the file { nbp_file . omp_spot_info } to run omp part from scratch.' ) print ( f 'Finding OMP coefficients for all pixels on tiles { use_tiles } :' ) initial_pos_neighbour_thresh = config [ 'initial_pos_neighbour_thresh' ] for t in use_tiles : pixel_yxz_t = np . zeros (( 0 , 3 ), dtype = np . int16 ) pixel_coefs_t = sparse . csr_matrix ( np . zeros (( 0 , n_genes ), dtype = np . float32 )) for z in use_z : print ( f \"Tile { np . where ( use_tiles == t )[ 0 ][ 0 ] + 1 } / { len ( use_tiles ) } ,\" f \" Z-plane { np . where ( use_z == z )[ 0 ][ 0 ] + 1 } / { len ( use_z ) } \" ) # While iterating through tiles, only save info for rounds/channels using # - add all rounds/channels back in later. This returns colors in use_rounds/channels only and no invalid. pixel_colors_tz , pixel_yxz_tz = get_spot_colors ( all_pixel_yxz ( nbp_basic . tile_sz , nbp_basic . tile_sz , int ( z )), int ( t ), transform , nbp_file , nbp_basic , return_in_bounds = True ) if pixel_colors_tz . shape [ 0 ] == 0 : continue pixel_colors_tz = pixel_colors_tz / color_norm_factor # Only keep pixels with significant absolute intensity to save memory. # absolute because important to find negative coefficients as well. # pixel_intensity_tz = get_spot_intensity(jnp.abs(pixel_colors_tz)) pixel_intensity_tz = get_spot_intensity ( jnp . abs ( pixel_colors_tz )) keep = pixel_intensity_tz > nbp . initial_intensity_thresh if not keep . any (): continue pixel_colors_tz = pixel_colors_tz [ keep ] pixel_yxz_tz = pixel_yxz_tz [ keep ] del pixel_intensity_tz , keep pixel_coefs_tz = sparse . csr_matrix ( omp . get_all_coefs ( pixel_colors_tz , bled_codes , nbp_call_spots . background_weight_shift , dp_norm_shift , config [ 'dp_thresh' ], config [ 'alpha' ], config [ 'beta' ], config [ 'max_genes' ], config [ 'weight_coef_fit' ])[ 0 ]) del pixel_colors_tz # Only keep pixels for which at least one gene has non-zero coefficient. keep = ( np . abs ( pixel_coefs_tz ) . max ( axis = 1 ) > 0 ) . nonzero ()[ 0 ] # nonzero as is sparse matrix. if len ( keep ) == 0 : continue # TODO: check order of np.asarray and keep, which is quicker - think this is quickest though pixel_yxz_t = np . append ( pixel_yxz_t , np . asarray ( pixel_yxz_tz [ keep ]), axis = 0 ) del pixel_yxz_tz pixel_coefs_t = sparse . vstack (( pixel_coefs_t , pixel_coefs_tz [ keep ])) del pixel_coefs_tz , keep if spot_shape is None : nbp . shape_tile = int ( t ) spot_yxz , spot_gene_no = omp . get_spots ( pixel_coefs_t , pixel_yxz_t , config [ 'radius_xy' ], detect_radius_z ) z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy spot_shape , spots_used , spot_shape_float = \\ omp . spot_neighbourhood ( pixel_coefs_t , pixel_yxz_t , spot_yxz , spot_gene_no , config [ 'shape_max_size' ], config [ 'shape_pos_neighbour_thresh' ], config [ 'shape_isolation_dist' ], z_scale , config [ 'shape_sign_thresh' ]) if not np . isin ( - 1 , spot_shape ): # Rase error if computed average spot shape has no negative values error_file = nbp_file . omp_spot_shape . replace ( '.npy' , '_float_ERROR.npy' ) if spot_shape_float . ndim == 3 : # put z axis to front before saving if 3D np . save ( error_file , np . moveaxis ( spot_shape_float , 2 , 0 )) else : np . save ( error_file , spot_shape_float ) shape_neg_values = spot_shape_float [ spot_shape_float < 0 ] message = f \"Error when computing nb.omp.spot_shape: \\n \" \\ f \"Average spot_shape in OMP Coefficient images was found with { spots_used . size } spots. \\n \" \\ f \"However, it contains no pixels with a value of -1. \\n \" \\ f \"nb.omp.spot_shape_float was saved as \\n { error_file } \\n \" if len ( shape_neg_values ) == 0 : message += \"This contains no negative values either, so OMP section needs re-running with \" \\ \"config['file_names']['omp_spot_shape'] specified\" else : max_neg_value = round_any ( np . abs ( shape_neg_values ) . max (), 0.001 , 'floor' ) message += f \"OMP section needs re-running with \\n \" \\ f \"config['omp']['shape_sign_thresh'] < { max_neg_value } or \" \\ f \"config['file_names']['omp_spot_shape'] specified\" raise ValueError ( message ) nbp . spot_shape_float = spot_shape_float nbp . shape_spot_local_yxz = spot_yxz [ spots_used ] nbp . shape_spot_gene_no = spot_gene_no [ spots_used ] if spot_shape . ndim == 3 : # put z axis to front before saving if 3D np . save ( nbp_file . omp_spot_shape , np . moveaxis ( spot_shape , 2 , 0 )) else : np . save ( nbp_file . omp_spot_shape , spot_shape ) # already found spots so don't find again. spot_yxzg = np . append ( spot_yxz , spot_gene_no . reshape ( - 1 , 1 ), axis = 1 ) del spot_yxz , spot_gene_no , spots_used else : spot_yxzg = None if initial_pos_neighbour_thresh is None : # Only save spots which have 10% of max possible number of positive neighbours initial_pos_neighbour_thresh = config [ 'initial_pos_neighbour_thresh_param' ] * np . sum ( spot_shape > 0 ) initial_pos_neighbour_thresh = np . floor ( initial_pos_neighbour_thresh ) initial_pos_neighbour_thresh = int ( np . clip ( initial_pos_neighbour_thresh , config [ 'initial_pos_neighbour_thresh_min' ], config [ 'initial_pos_neighbour_thresh_max' ])) spot_info_t = \\ omp . get_spots ( pixel_coefs_t , pixel_yxz_t , config [ 'radius_xy' ], detect_radius_z , 0 , spot_shape , initial_pos_neighbour_thresh , spot_yxzg ) del spot_yxzg n_spots = spot_info_t [ 0 ] . shape [ 0 ] spot_info_t = np . concatenate ([ spot_var . reshape ( n_spots , - 1 ) . astype ( np . int16 ) for spot_var in spot_info_t ], axis = 1 ) spot_info_t = np . append ( spot_info_t , np . ones (( n_spots , 1 ), dtype = np . int16 ) * t , axis = 1 ) # find index of each spot in pixel array to add colors and coefs pixel_index = numpy_indexed . indices ( pixel_yxz_t , spot_info_t [:, : 3 ]) # append this tile info to all tile info if os . path . isfile ( nbp_file . omp_spot_info ) and os . path . isfile ( nbp_file . omp_spot_coef ): # After ran on one tile, need to load in spot_coefs and spot_info, append and then save again. spot_coefs = sparse . load_npz ( nbp_file . omp_spot_coef ) spot_coefs = sparse . vstack (( spot_coefs , pixel_coefs_t [ pixel_index ])) del pixel_coefs_t , pixel_index sparse . save_npz ( nbp_file . omp_spot_coef , spot_coefs ) del spot_coefs spot_info = np . load ( nbp_file . omp_spot_info ) spot_info = np . append ( spot_info , spot_info_t , axis = 0 ) del spot_info_t np . save ( nbp_file . omp_spot_info , spot_info ) del spot_info else : # 1st tile, need to create files to save to sparse . save_npz ( nbp_file . omp_spot_coef , pixel_coefs_t [ pixel_index ]) del pixel_coefs_t , pixel_index np . save ( nbp_file . omp_spot_info , spot_info_t . astype ( np . int16 )) del spot_info_t nbp . spot_shape = spot_shape nbp . initial_pos_neighbour_thresh = initial_pos_neighbour_thresh spot_info = np . load ( nbp_file . omp_spot_info ) # find duplicate spots as those detected on a tile which is not tile centre they are closest to not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) # Add spot info to notebook page nbp . local_yxz = spot_info [ not_duplicate , : 3 ] nbp . tile = spot_info [ not_duplicate , 6 ] # Get colors, background_coef and intensity of final spots. n_spots = np . sum ( not_duplicate ) invalid_value = - nbp_basic . tile_pixel_value_shift # Only read in used colors first for background/intensity calculation. nd_spot_colors_use = np . ones (( n_spots , n_rounds_use , n_channels_use ), dtype = np . int32 ) * invalid_value for t in nbp_basic . use_tiles : in_tile = nbp . tile == t if np . sum ( in_tile ) > 0 : nd_spot_colors_use [ in_tile ] = get_spot_colors ( jnp . asarray ( nbp . local_yxz [ in_tile ]), t , transform , nbp_file , nbp_basic ) spot_colors_norm = jnp . array ( nd_spot_colors_use ) / color_norm_factor nbp . intensity = np . asarray ( get_spot_intensity ( spot_colors_norm )) del spot_colors_norm # When saving to notebook, include unused rounds/channels. nd_spot_colors = np . ones (( n_spots , nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = np . int32 ) * invalid_value nd_spot_colors [ np . ix_ ( np . arange ( n_spots ), nbp_basic . use_rounds , nbp_basic . use_channels )] = nd_spot_colors_use nbp . colors = nd_spot_colors del nd_spot_colors_use nbp . gene_no = spot_info [ not_duplicate , 3 ] nbp . n_neighbours_pos = spot_info [ not_duplicate , 4 ] nbp . n_neighbours_neg = spot_info [ not_duplicate , 5 ] return nbp","title":"OMP"},{"location":"code/pipeline/omp/#coppafish.pipeline.omp.call_spots_omp","text":"This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel. From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene. Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate. See 'omp' section of notebook_comments.json file for description of the variables in the omp page. Parameters: Name Type Description Default config dict Dictionary obtained from 'omp' section of config file. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required nbp_call_spots NotebookPage required tile_origin np . ndarray float [n_tiles x 3] . tile_origin[t,:] is the bottom left yxz coordinate of tile t . yx coordinates in yx_pixels and z coordinate in z_pixels . This is saved in the stitch notebook page i.e. nb.stitch.tile_origin . required transform np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transform[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . This is saved in the register notebook page i.e. nb.register.transform . required shape_tile Optional [ int ] Tile to use to compute the expected shape of a spot in the gene coefficient images. Should be the tile, for which the most spots where found in the call_reference_spots step. If None , will be set to the centre tile. required Returns: Type Description NotebookPage NotebookPage[omp] - Page contains gene assignments and info for spots using omp. Source code in coppafish/pipeline/omp.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def call_spots_omp ( config : dict , nbp_file : NotebookPage , nbp_basic : NotebookPage , nbp_call_spots : NotebookPage , tile_origin : np . ndarray , transform : np . ndarray , shape_tile : Optional [ int ]) -> NotebookPage : \"\"\" This runs orthogonal matching pursuit (omp) on every pixel to determine a coefficient for each gene at each pixel. From these gene coefficient images, a local maxima search is performed to find the position of spots for each gene. Various properties of the spots are then saved to determine the likelihood that the gene assignment is legitimate. See `'omp'` section of `notebook_comments.json` file for description of the variables in the omp page. Args: config: Dictionary obtained from `'omp'` section of config file. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page nbp_call_spots: tile_origin: `float [n_tiles x 3]`. `tile_origin[t,:]` is the bottom left yxz coordinate of tile `t`. yx coordinates in `yx_pixels` and z coordinate in `z_pixels`. This is saved in the `stitch` notebook page i.e. `nb.stitch.tile_origin`. transform: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transform[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. This is saved in the register notebook page i.e. `nb.register.transform`. shape_tile: Tile to use to compute the expected shape of a spot in the gene coefficient images. Should be the tile, for which the most spots where found in the `call_reference_spots` step. If `None`, will be set to the centre tile. Returns: `NotebookPage[omp]` - Page contains gene assignments and info for spots using omp. \"\"\" nbp = NotebookPage ( \"omp\" ) # use bled_codes with gene efficiency incorporated and only use_rounds/channels rc_ind = np . ix_ ( nbp_basic . use_rounds , nbp_basic . use_channels ) bled_codes = np . moveaxis ( np . moveaxis ( nbp_call_spots . bled_codes_ge , 0 , - 1 )[ rc_ind ], - 1 , 0 ) utils . errors . check_color_nan ( bled_codes , nbp_basic ) norm_bled_codes = np . linalg . norm ( bled_codes , axis = ( 1 , 2 )) if np . abs ( norm_bled_codes - 1 ) . max () > 1e-6 : raise ValueError ( \"nbp_call_spots.bled_codes_ge don't all have an L2 norm of 1 over \" \"use_rounds and use_channels.\" ) bled_codes = jnp . asarray ( bled_codes ) transform = jnp . asarray ( transform ) color_norm_factor = jnp . asarray ( nbp_call_spots . color_norm_factor [ rc_ind ]) n_genes , n_rounds_use , n_channels_use = bled_codes . shape dp_norm_shift = nbp_call_spots . dp_norm_shift * np . sqrt ( n_rounds_use ) if nbp_basic . is_3d : detect_radius_z = config [ 'radius_z' ] n_z = nbp_basic . nz else : detect_radius_z = None n_z = 1 config [ 'use_z' ] = np . arange ( n_z ) if config [ 'use_z' ] is not None : use_z_oob = [ val for val in config [ 'use_z' ] if val < 0 or val >= n_z ] if len ( use_z_oob ) > 0 : raise utils . errors . OutOfBoundsError ( \"use_z\" , use_z_oob [ 0 ], 0 , n_z - 1 ) if len ( config [ 'use_z' ]) == 2 : # use consecutive values if only 2 given. config [ 'use_z' ] = list ( np . arange ( config [ 'use_z' ][ 0 ], config [ 'use_z' ][ 1 ] + 1 )) use_z = np . array ( config [ 'use_z' ]) else : use_z = np . arange ( n_z ) # determine initial_intensity_thresh from average intensity over all pixels on central z-plane. nbp . initial_intensity_thresh = omp . get_initial_intensity_thresh ( config , nbp_call_spots ) use_tiles = np . array ( nbp_basic . use_tiles . copy ()) if not os . path . isfile ( nbp_file . omp_spot_shape ): # Set tile order so do shape_tile first to compute spot_shape from it. if shape_tile is None : shape_tile = scale . central_tile ( nbp_basic . tilepos_yx , nbp_basic . use_tiles ) if shape_tile not in nbp_basic . use_tiles : raise ValueError ( f \"shape_tile, { shape_tile } is not in nbp_basic.use_tiles, { nbp_basic . use_tiles } \" ) shape_tile_ind = np . where ( np . array ( nbp_basic . use_tiles ) == shape_tile )[ 0 ][ 0 ] use_tiles [ 0 ], use_tiles [ shape_tile_ind ] = use_tiles [ shape_tile_ind ], use_tiles [ 0 ] spot_shape = None else : nbp . shape_tile = None nbp . shape_spot_local_yxz = None nbp . shape_spot_gene_no = None nbp . spot_shape_float = None # -1 because saved as uint16 so convert 0, 1, 2 to -1, 0, 1. spot_shape = np . load ( nbp_file . omp_spot_shape ) # Put z to last index if spot_shape . ndim == 3 : spot_shape = np . moveaxis ( spot_shape , 0 , 2 ) # Put z to last index # Deal with case where algorithm has been run for some tiles and data saved if os . path . isfile ( nbp_file . omp_spot_info ) and os . path . isfile ( nbp_file . omp_spot_coef ): if spot_shape is None : raise ValueError ( f 'OMP information already exists for some tiles but spot_shape tiff file does not: \\n ' f ' { nbp_file . omp_spot_shape } \\n Either add spot_shape tiff or delete the files: \\n ' f ' { nbp_file . omp_spot_info } and { nbp_file . omp_spot_coef } .' ) spot_coefs = sparse . load_npz ( nbp_file . omp_spot_coef ) spot_info = np . load ( nbp_file . omp_spot_info ) if spot_coefs . shape [ 0 ] > spot_info . shape [ 0 ]: # Case where bugged out after saving spot_coefs but before saving spot_info, delete all excess spot_coefs. warnings . warn ( f \"Have spot_coefs for { spot_coefs . shape [ 0 ] } spots but only spot_info for { spot_info . shape [ 0 ] } \" f \" spots. \\n So deleting the excess spot_coefs and re-saving to { nbp_file . omp_spot_coef } .\" ) spot_coefs = spot_coefs [: spot_info . shape [ 0 ]] sparse . save_npz ( nbp_file . omp_spot_coef , spot_coefs ) elif spot_coefs . shape [ 0 ] < spot_info . shape [ 0 ]: # If more spots in info than coefs then likely because duplicates removed from coefs but not spot_info. not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) if not_duplicate . size == spot_info . shape [ 0 ]: warnings . warn ( f 'There were less spots in \\n { nbp_file . omp_spot_info } \\n than \\n { nbp_file . omp_spot_coef } ' f 'because duplicates were deleted for spot_coefs but not for spot_info. \\n ' f 'Now, spot_info duplicates have also been deleted.' ) spot_info = spot_info [ not_duplicate ] np . save ( nbp_file . omp_spot_info , spot_info ) else : raise ValueError ( f \"Have spot_info for { spot_info . shape [ 0 ] } spots but only spot_coefs for \" f \" { spot_coefs . shape [ 0 ] } \\n Need to delete both { nbp_file . omp_spot_coef } and \" f \" { nbp_file . omp_spot_info } to get past this error.\" ) else : prev_found_tiles = np . unique ( spot_info [:, - 1 ]) use_tiles = np . setdiff1d ( use_tiles , prev_found_tiles ) warnings . warn ( f 'Already have OMP results for tiles { prev_found_tiles } so now just running on tiles ' f ' { use_tiles } .' ) del spot_coefs , spot_info elif os . path . isfile ( nbp_file . omp_spot_coef ): # If only have information only file but not the other, need to delete all files and start again. raise ValueError ( f 'The file { nbp_file . omp_spot_coef } exists but the file { nbp_file . omp_spot_info } does not. \\n ' f 'Delete or re-name the file { nbp_file . omp_spot_coef } to run omp part from scratch.' ) elif os . path . isfile ( nbp_file . omp_spot_info ): raise ValueError ( f 'The file { nbp_file . omp_spot_info } exists but the file { nbp_file . omp_spot_coef } does not. \\n ' f 'Delete or re-name the file { nbp_file . omp_spot_info } to run omp part from scratch.' ) print ( f 'Finding OMP coefficients for all pixels on tiles { use_tiles } :' ) initial_pos_neighbour_thresh = config [ 'initial_pos_neighbour_thresh' ] for t in use_tiles : pixel_yxz_t = np . zeros (( 0 , 3 ), dtype = np . int16 ) pixel_coefs_t = sparse . csr_matrix ( np . zeros (( 0 , n_genes ), dtype = np . float32 )) for z in use_z : print ( f \"Tile { np . where ( use_tiles == t )[ 0 ][ 0 ] + 1 } / { len ( use_tiles ) } ,\" f \" Z-plane { np . where ( use_z == z )[ 0 ][ 0 ] + 1 } / { len ( use_z ) } \" ) # While iterating through tiles, only save info for rounds/channels using # - add all rounds/channels back in later. This returns colors in use_rounds/channels only and no invalid. pixel_colors_tz , pixel_yxz_tz = get_spot_colors ( all_pixel_yxz ( nbp_basic . tile_sz , nbp_basic . tile_sz , int ( z )), int ( t ), transform , nbp_file , nbp_basic , return_in_bounds = True ) if pixel_colors_tz . shape [ 0 ] == 0 : continue pixel_colors_tz = pixel_colors_tz / color_norm_factor # Only keep pixels with significant absolute intensity to save memory. # absolute because important to find negative coefficients as well. # pixel_intensity_tz = get_spot_intensity(jnp.abs(pixel_colors_tz)) pixel_intensity_tz = get_spot_intensity ( jnp . abs ( pixel_colors_tz )) keep = pixel_intensity_tz > nbp . initial_intensity_thresh if not keep . any (): continue pixel_colors_tz = pixel_colors_tz [ keep ] pixel_yxz_tz = pixel_yxz_tz [ keep ] del pixel_intensity_tz , keep pixel_coefs_tz = sparse . csr_matrix ( omp . get_all_coefs ( pixel_colors_tz , bled_codes , nbp_call_spots . background_weight_shift , dp_norm_shift , config [ 'dp_thresh' ], config [ 'alpha' ], config [ 'beta' ], config [ 'max_genes' ], config [ 'weight_coef_fit' ])[ 0 ]) del pixel_colors_tz # Only keep pixels for which at least one gene has non-zero coefficient. keep = ( np . abs ( pixel_coefs_tz ) . max ( axis = 1 ) > 0 ) . nonzero ()[ 0 ] # nonzero as is sparse matrix. if len ( keep ) == 0 : continue # TODO: check order of np.asarray and keep, which is quicker - think this is quickest though pixel_yxz_t = np . append ( pixel_yxz_t , np . asarray ( pixel_yxz_tz [ keep ]), axis = 0 ) del pixel_yxz_tz pixel_coefs_t = sparse . vstack (( pixel_coefs_t , pixel_coefs_tz [ keep ])) del pixel_coefs_tz , keep if spot_shape is None : nbp . shape_tile = int ( t ) spot_yxz , spot_gene_no = omp . get_spots ( pixel_coefs_t , pixel_yxz_t , config [ 'radius_xy' ], detect_radius_z ) z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy spot_shape , spots_used , spot_shape_float = \\ omp . spot_neighbourhood ( pixel_coefs_t , pixel_yxz_t , spot_yxz , spot_gene_no , config [ 'shape_max_size' ], config [ 'shape_pos_neighbour_thresh' ], config [ 'shape_isolation_dist' ], z_scale , config [ 'shape_sign_thresh' ]) if not np . isin ( - 1 , spot_shape ): # Rase error if computed average spot shape has no negative values error_file = nbp_file . omp_spot_shape . replace ( '.npy' , '_float_ERROR.npy' ) if spot_shape_float . ndim == 3 : # put z axis to front before saving if 3D np . save ( error_file , np . moveaxis ( spot_shape_float , 2 , 0 )) else : np . save ( error_file , spot_shape_float ) shape_neg_values = spot_shape_float [ spot_shape_float < 0 ] message = f \"Error when computing nb.omp.spot_shape: \\n \" \\ f \"Average spot_shape in OMP Coefficient images was found with { spots_used . size } spots. \\n \" \\ f \"However, it contains no pixels with a value of -1. \\n \" \\ f \"nb.omp.spot_shape_float was saved as \\n { error_file } \\n \" if len ( shape_neg_values ) == 0 : message += \"This contains no negative values either, so OMP section needs re-running with \" \\ \"config['file_names']['omp_spot_shape'] specified\" else : max_neg_value = round_any ( np . abs ( shape_neg_values ) . max (), 0.001 , 'floor' ) message += f \"OMP section needs re-running with \\n \" \\ f \"config['omp']['shape_sign_thresh'] < { max_neg_value } or \" \\ f \"config['file_names']['omp_spot_shape'] specified\" raise ValueError ( message ) nbp . spot_shape_float = spot_shape_float nbp . shape_spot_local_yxz = spot_yxz [ spots_used ] nbp . shape_spot_gene_no = spot_gene_no [ spots_used ] if spot_shape . ndim == 3 : # put z axis to front before saving if 3D np . save ( nbp_file . omp_spot_shape , np . moveaxis ( spot_shape , 2 , 0 )) else : np . save ( nbp_file . omp_spot_shape , spot_shape ) # already found spots so don't find again. spot_yxzg = np . append ( spot_yxz , spot_gene_no . reshape ( - 1 , 1 ), axis = 1 ) del spot_yxz , spot_gene_no , spots_used else : spot_yxzg = None if initial_pos_neighbour_thresh is None : # Only save spots which have 10% of max possible number of positive neighbours initial_pos_neighbour_thresh = config [ 'initial_pos_neighbour_thresh_param' ] * np . sum ( spot_shape > 0 ) initial_pos_neighbour_thresh = np . floor ( initial_pos_neighbour_thresh ) initial_pos_neighbour_thresh = int ( np . clip ( initial_pos_neighbour_thresh , config [ 'initial_pos_neighbour_thresh_min' ], config [ 'initial_pos_neighbour_thresh_max' ])) spot_info_t = \\ omp . get_spots ( pixel_coefs_t , pixel_yxz_t , config [ 'radius_xy' ], detect_radius_z , 0 , spot_shape , initial_pos_neighbour_thresh , spot_yxzg ) del spot_yxzg n_spots = spot_info_t [ 0 ] . shape [ 0 ] spot_info_t = np . concatenate ([ spot_var . reshape ( n_spots , - 1 ) . astype ( np . int16 ) for spot_var in spot_info_t ], axis = 1 ) spot_info_t = np . append ( spot_info_t , np . ones (( n_spots , 1 ), dtype = np . int16 ) * t , axis = 1 ) # find index of each spot in pixel array to add colors and coefs pixel_index = numpy_indexed . indices ( pixel_yxz_t , spot_info_t [:, : 3 ]) # append this tile info to all tile info if os . path . isfile ( nbp_file . omp_spot_info ) and os . path . isfile ( nbp_file . omp_spot_coef ): # After ran on one tile, need to load in spot_coefs and spot_info, append and then save again. spot_coefs = sparse . load_npz ( nbp_file . omp_spot_coef ) spot_coefs = sparse . vstack (( spot_coefs , pixel_coefs_t [ pixel_index ])) del pixel_coefs_t , pixel_index sparse . save_npz ( nbp_file . omp_spot_coef , spot_coefs ) del spot_coefs spot_info = np . load ( nbp_file . omp_spot_info ) spot_info = np . append ( spot_info , spot_info_t , axis = 0 ) del spot_info_t np . save ( nbp_file . omp_spot_info , spot_info ) del spot_info else : # 1st tile, need to create files to save to sparse . save_npz ( nbp_file . omp_spot_coef , pixel_coefs_t [ pixel_index ]) del pixel_coefs_t , pixel_index np . save ( nbp_file . omp_spot_info , spot_info_t . astype ( np . int16 )) del spot_info_t nbp . spot_shape = spot_shape nbp . initial_pos_neighbour_thresh = initial_pos_neighbour_thresh spot_info = np . load ( nbp_file . omp_spot_info ) # find duplicate spots as those detected on a tile which is not tile centre they are closest to not_duplicate = get_non_duplicate ( tile_origin , nbp_basic . use_tiles , nbp_basic . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) # Add spot info to notebook page nbp . local_yxz = spot_info [ not_duplicate , : 3 ] nbp . tile = spot_info [ not_duplicate , 6 ] # Get colors, background_coef and intensity of final spots. n_spots = np . sum ( not_duplicate ) invalid_value = - nbp_basic . tile_pixel_value_shift # Only read in used colors first for background/intensity calculation. nd_spot_colors_use = np . ones (( n_spots , n_rounds_use , n_channels_use ), dtype = np . int32 ) * invalid_value for t in nbp_basic . use_tiles : in_tile = nbp . tile == t if np . sum ( in_tile ) > 0 : nd_spot_colors_use [ in_tile ] = get_spot_colors ( jnp . asarray ( nbp . local_yxz [ in_tile ]), t , transform , nbp_file , nbp_basic ) spot_colors_norm = jnp . array ( nd_spot_colors_use ) / color_norm_factor nbp . intensity = np . asarray ( get_spot_intensity ( spot_colors_norm )) del spot_colors_norm # When saving to notebook, include unused rounds/channels. nd_spot_colors = np . ones (( n_spots , nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = np . int32 ) * invalid_value nd_spot_colors [ np . ix_ ( np . arange ( n_spots ), nbp_basic . use_rounds , nbp_basic . use_channels )] = nd_spot_colors_use nbp . colors = nd_spot_colors del nd_spot_colors_use nbp . gene_no = spot_info [ not_duplicate , 3 ] nbp . n_neighbours_pos = spot_info [ not_duplicate , 4 ] nbp . n_neighbours_neg = spot_info [ not_duplicate , 5 ] return nbp","title":"call_spots_omp()"},{"location":"code/pipeline/register/","text":"register ( config , nbp_basic , spot_details , initial_shift ) This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. It uses iterative closest point and the starting shifts found in pipeline/register_initial.py . See 'register' and 'register_debug' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'register' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required initial_shift np . ndarray int [n_tiles x n_rounds x 3] . initial_shift[t, r] is the yxz shift found that is applied to tile t , ref_round to take it to tile t , round r . Units: [yx_pixels, yx_pixels, z_pixels] . This is saved in the register_initial_debug notebook page i.e. nb.register_initial_debug.shift . required Returns: Type Description NotebookPage NotebookPage[register] - Page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. NotebookPage NotebookPage[register_debug] - Page contains information on how the affine transforms were calculated. Source code in coppafish/pipeline/register.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def register ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray , initial_shift : np . ndarray ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. It uses iterative closest point and the starting shifts found in `pipeline/register_initial.py`. See `'register'` and `'register_debug'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'register'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. initial_shift: `int [n_tiles x n_rounds x 3]`. `initial_shift[t, r]` is the yxz shift found that is applied to tile `t`, `ref_round` to take it to tile `t`, round `r`. Units: `[yx_pixels, yx_pixels, z_pixels]`. This is saved in the `register_initial_debug` notebook page i.e. `nb.register_initial_debug.shift`. Returns: - `NotebookPage[register]` - Page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. - `NotebookPage[register_debug]` - Page contains information on how the affine transforms were calculated. \"\"\" nbp = NotebookPage ( \"register\" ) nbp_debug = NotebookPage ( \"register_debug\" ) nbp . initial_shift = initial_shift . copy () if nbp_basic . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] # centre and scale spot yxz coordinates z_scale = [ 1 , 1 , nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy ] spot_yxz_ref = np . zeros ( nbp_basic . n_tiles , dtype = object ) spot_yxz_imaging = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = object ) n_matches_thresh = np . zeros_like ( spot_yxz_imaging , dtype = float ) initial_shift = initial_shift . astype ( float ) for t in nbp_basic . use_tiles : spot_yxz_ref [ t ] = spot_yxz ( spot_details , t , nbp_basic . ref_round , nbp_basic . ref_channel ) spot_yxz_ref [ t ] = ( spot_yxz_ref [ t ] - nbp_basic . tile_centre ) * z_scale for r in nbp_basic . use_rounds : initial_shift [ t , r ] = initial_shift [ t , r ] * z_scale # put z initial shift into xy pixel units for c in nbp_basic . use_channels : spot_yxz_imaging [ t , r , c ] = spot_yxz ( spot_details , t , r , c ) spot_yxz_imaging [ t , r , c ] = ( spot_yxz_imaging [ t , r , c ] - nbp_basic . tile_centre ) * z_scale if neighb_dist_thresh < 50 : # only keep isolated spots, those whose second neighbour is far away isolated = get_isolated_points ( spot_yxz_imaging [ t , r , c ], 2 * neighb_dist_thresh ) spot_yxz_imaging [ t , r , c ] = spot_yxz_imaging [ t , r , c ][ isolated , :] n_matches_thresh [ t , r , c ] = ( config [ 'matches_thresh_fract' ] * np . min ([ spot_yxz_ref [ t ] . shape [ 0 ], spot_yxz_imaging [ t , r , c ] . shape [ 0 ]])) # get indices of tiles/rounds/channels used trc_ind = np . ix_ ( nbp_basic . use_tiles , nbp_basic . use_rounds , nbp_basic . use_channels ) tr_ind = np . ix_ ( nbp_basic . use_tiles , nbp_basic . use_rounds ) # needed for av_shifts as no channel index n_matches_thresh [ trc_ind ] = np . clip ( n_matches_thresh [ trc_ind ], config [ 'matches_thresh_min' ], config [ 'matches_thresh_max' ]) n_matches_thresh = n_matches_thresh . astype ( int ) # Initialise variables obtain from PCR algorithm. This includes spaces for tiles/rounds/channels not used start_transform = transform_from_scale_shift ( np . ones (( nbp_basic . n_channels , 3 )), initial_shift ) final_transform = np . zeros_like ( start_transform ) n_matches = np . zeros_like ( spot_yxz_imaging , dtype = int ) error = np . zeros_like ( spot_yxz_imaging , dtype = float ) failed = np . zeros_like ( spot_yxz_imaging , dtype = bool ) converged = np . zeros_like ( spot_yxz_imaging , dtype = bool ) av_scaling = np . zeros (( nbp_basic . n_channels , 3 ), dtype = float ) av_shifts = np . zeros_like ( initial_shift ) transform_outliers = np . zeros_like ( start_transform ) # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant. reg_constant_scale = np . sqrt ( 0.5 * config [ 'regularize_constant' ] * config [ 'regularize_factor' ]) reg_constant_shift = np . sqrt ( 0.5 * config [ 'regularize_constant' ]) # get ICP output only for tiles/rounds/channels that we are using final_transform [ trc_ind ], pcr_debug = \\ icp ( spot_yxz_ref [ nbp_basic . use_tiles ], spot_yxz_imaging [ trc_ind ], start_transform [ trc_ind ], config [ 'n_iter' ], neighb_dist_thresh , n_matches_thresh [ trc_ind ], config [ 'scale_dev_thresh' ], config [ 'shift_dev_thresh' ], reg_constant_scale , reg_constant_shift ) # save debug info at correct tile, round, channel index n_matches [ trc_ind ] = pcr_debug [ 'n_matches' ] error [ trc_ind ] = pcr_debug [ 'error' ] failed [ trc_ind ] = pcr_debug [ 'failed' ] converged [ trc_ind ] = pcr_debug [ 'is_converged' ] av_scaling [ nbp_basic . use_channels ] = pcr_debug [ 'av_scaling' ] av_shifts [ tr_ind ] = pcr_debug [ 'av_shifts' ] transform_outliers [ trc_ind ] = pcr_debug [ 'transforms_outlier' ] # add to notebook nbp . transform = final_transform nbp_debug . n_matches = n_matches nbp_debug . n_matches_thresh = n_matches_thresh nbp_debug . error = error nbp_debug . failed = failed nbp_debug . converged = converged nbp_debug . av_scaling = av_scaling nbp_debug . av_shifts = av_shifts nbp_debug . transform_outlier = transform_outliers return nbp , nbp_debug","title":"Register"},{"location":"code/pipeline/register/#coppafish.pipeline.register.register","text":"This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. It uses iterative closest point and the starting shifts found in pipeline/register_initial.py . See 'register' and 'register_debug' sections of notebook_comments.json file for description of the variables in each page. Parameters: Name Type Description Default config dict Dictionary obtained from 'register' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required initial_shift np . ndarray int [n_tiles x n_rounds x 3] . initial_shift[t, r] is the yxz shift found that is applied to tile t , ref_round to take it to tile t , round r . Units: [yx_pixels, yx_pixels, z_pixels] . This is saved in the register_initial_debug notebook page i.e. nb.register_initial_debug.shift . required Returns: Type Description NotebookPage NotebookPage[register] - Page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. NotebookPage NotebookPage[register_debug] - Page contains information on how the affine transforms were calculated. Source code in coppafish/pipeline/register.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def register ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray , initial_shift : np . ndarray ) -> Tuple [ NotebookPage , NotebookPage ]: \"\"\" This finds the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. It uses iterative closest point and the starting shifts found in `pipeline/register_initial.py`. See `'register'` and `'register_debug'` sections of `notebook_comments.json` file for description of the variables in each page. Args: config: Dictionary obtained from `'register'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. initial_shift: `int [n_tiles x n_rounds x 3]`. `initial_shift[t, r]` is the yxz shift found that is applied to tile `t`, `ref_round` to take it to tile `t`, round `r`. Units: `[yx_pixels, yx_pixels, z_pixels]`. This is saved in the `register_initial_debug` notebook page i.e. `nb.register_initial_debug.shift`. Returns: - `NotebookPage[register]` - Page contains the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. - `NotebookPage[register_debug]` - Page contains information on how the affine transforms were calculated. \"\"\" nbp = NotebookPage ( \"register\" ) nbp_debug = NotebookPage ( \"register_debug\" ) nbp . initial_shift = initial_shift . copy () if nbp_basic . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] # centre and scale spot yxz coordinates z_scale = [ 1 , 1 , nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy ] spot_yxz_ref = np . zeros ( nbp_basic . n_tiles , dtype = object ) spot_yxz_imaging = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds , nbp_basic . n_channels ), dtype = object ) n_matches_thresh = np . zeros_like ( spot_yxz_imaging , dtype = float ) initial_shift = initial_shift . astype ( float ) for t in nbp_basic . use_tiles : spot_yxz_ref [ t ] = spot_yxz ( spot_details , t , nbp_basic . ref_round , nbp_basic . ref_channel ) spot_yxz_ref [ t ] = ( spot_yxz_ref [ t ] - nbp_basic . tile_centre ) * z_scale for r in nbp_basic . use_rounds : initial_shift [ t , r ] = initial_shift [ t , r ] * z_scale # put z initial shift into xy pixel units for c in nbp_basic . use_channels : spot_yxz_imaging [ t , r , c ] = spot_yxz ( spot_details , t , r , c ) spot_yxz_imaging [ t , r , c ] = ( spot_yxz_imaging [ t , r , c ] - nbp_basic . tile_centre ) * z_scale if neighb_dist_thresh < 50 : # only keep isolated spots, those whose second neighbour is far away isolated = get_isolated_points ( spot_yxz_imaging [ t , r , c ], 2 * neighb_dist_thresh ) spot_yxz_imaging [ t , r , c ] = spot_yxz_imaging [ t , r , c ][ isolated , :] n_matches_thresh [ t , r , c ] = ( config [ 'matches_thresh_fract' ] * np . min ([ spot_yxz_ref [ t ] . shape [ 0 ], spot_yxz_imaging [ t , r , c ] . shape [ 0 ]])) # get indices of tiles/rounds/channels used trc_ind = np . ix_ ( nbp_basic . use_tiles , nbp_basic . use_rounds , nbp_basic . use_channels ) tr_ind = np . ix_ ( nbp_basic . use_tiles , nbp_basic . use_rounds ) # needed for av_shifts as no channel index n_matches_thresh [ trc_ind ] = np . clip ( n_matches_thresh [ trc_ind ], config [ 'matches_thresh_min' ], config [ 'matches_thresh_max' ]) n_matches_thresh = n_matches_thresh . astype ( int ) # Initialise variables obtain from PCR algorithm. This includes spaces for tiles/rounds/channels not used start_transform = transform_from_scale_shift ( np . ones (( nbp_basic . n_channels , 3 )), initial_shift ) final_transform = np . zeros_like ( start_transform ) n_matches = np . zeros_like ( spot_yxz_imaging , dtype = int ) error = np . zeros_like ( spot_yxz_imaging , dtype = float ) failed = np . zeros_like ( spot_yxz_imaging , dtype = bool ) converged = np . zeros_like ( spot_yxz_imaging , dtype = bool ) av_scaling = np . zeros (( nbp_basic . n_channels , 3 ), dtype = float ) av_shifts = np . zeros_like ( initial_shift ) transform_outliers = np . zeros_like ( start_transform ) # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant. reg_constant_scale = np . sqrt ( 0.5 * config [ 'regularize_constant' ] * config [ 'regularize_factor' ]) reg_constant_shift = np . sqrt ( 0.5 * config [ 'regularize_constant' ]) # get ICP output only for tiles/rounds/channels that we are using final_transform [ trc_ind ], pcr_debug = \\ icp ( spot_yxz_ref [ nbp_basic . use_tiles ], spot_yxz_imaging [ trc_ind ], start_transform [ trc_ind ], config [ 'n_iter' ], neighb_dist_thresh , n_matches_thresh [ trc_ind ], config [ 'scale_dev_thresh' ], config [ 'shift_dev_thresh' ], reg_constant_scale , reg_constant_shift ) # save debug info at correct tile, round, channel index n_matches [ trc_ind ] = pcr_debug [ 'n_matches' ] error [ trc_ind ] = pcr_debug [ 'error' ] failed [ trc_ind ] = pcr_debug [ 'failed' ] converged [ trc_ind ] = pcr_debug [ 'is_converged' ] av_scaling [ nbp_basic . use_channels ] = pcr_debug [ 'av_scaling' ] av_shifts [ tr_ind ] = pcr_debug [ 'av_shifts' ] transform_outliers [ trc_ind ] = pcr_debug [ 'transforms_outlier' ] # add to notebook nbp . transform = final_transform nbp_debug . n_matches = n_matches nbp_debug . n_matches_thresh = n_matches_thresh nbp_debug . error = error nbp_debug . failed = failed nbp_debug . converged = converged nbp_debug . av_scaling = av_scaling nbp_debug . av_shifts = av_shifts nbp_debug . transform_outlier = transform_outliers return nbp , nbp_debug","title":"register()"},{"location":"code/pipeline/register_initial/","text":"register_initial ( config , nbp_basic , spot_details ) This finds the shift between ref round/channel to each imaging round for each tile. These are then used as the starting point for determining the affine transforms in pipeline/register.py . See 'register_initial' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'register_initial' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required Returns: Type Description NotebookPage NotebookPage[register_initial] - Page contains information about how shift between ref round/channel to each imaging round for each tile was found. Source code in coppafish/pipeline/register_initial.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def register_initial ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray ) -> NotebookPage : \"\"\" This finds the shift between ref round/channel to each imaging round for each tile. These are then used as the starting point for determining the affine transforms in `pipeline/register.py`. See `'register_initial'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'register_initial'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. Returns: `NotebookPage[register_initial]` - Page contains information about how shift between ref round/channel to each imaging round for each tile was found. \"\"\" nbp_debug = NotebookPage ( \"register_initial\" ) if config [ 'shift_channel' ] is None : config [ 'shift_channel' ] = nbp_basic . ref_channel if not np . isin ( config [ 'shift_channel' ], nbp_basic . use_channels ): raise ValueError ( f \"config['shift_channel'] should be in nb.basic_info.use_channels, but value given is \\n \" f \" { config [ 'shift_channel' ] } which is not in use_channels = { nbp_basic . use_channels } .\" ) nbp_debug . shift_channel = config [ 'shift_channel' ] coords = [ 'y' , 'x' , 'z' ] shifts = [{}] start_shift_search = np . zeros (( nbp_basic . n_rounds , 3 , 3 ), dtype = int ) for i in range ( len ( coords )): shifts [ 0 ][ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) start_shift_search [ nbp_basic . use_rounds , i , :] = [ config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ], config [ 'shift_step' ][ i ]] if not nbp_basic . is_3d : config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 0 ][ 'z' ] = np . array ([ 0 ], dtype = int ) start_shift_search [:, 2 , : 2 ] = 0 shifts = shifts * nbp_basic . n_rounds # get one set of shifts for each round shift = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds , 3 ), dtype = int ) shift_score = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds ), dtype = float ) shift_score_thresh = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds ), dtype = float ) c_ref = nbp_basic . ref_channel r_ref = nbp_basic . ref_round c_imaging = config [ 'shift_channel' ] # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy with tqdm ( total = len ( nbp_basic . use_rounds ) * len ( nbp_basic . use_tiles )) as pbar : pbar . set_description ( f \"Finding shift from ref_round( { r_ref } )/ref_channel( { c_ref } ) to channel { c_imaging } \" f \"of all imaging rounds\" ) for r in nbp_basic . use_rounds : for t in nbp_basic . use_tiles : pbar . set_postfix ({ 'round' : r , 'tile' : t }) shift [ t , r ], shift_score [ t , r ], shift_score_thresh [ t , r ] = \\ compute_shift ( spot_yxz ( spot_details , t , r_ref , c_ref ), spot_yxz ( spot_details , t , r , c_imaging ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 3 ] good_shifts = shift_score [:, r ] > shift_score_thresh [:, r ] if np . sum ( good_shifts ) >= 3 : # once found shifts, refine shifts to be searched around these for i in range ( len ( coords )): shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [ good_shifts , r , i ]) pbar . update ( 1 ) pbar . close () # amend shifts for which score fell below score_thresh shift_outlier = shift . copy () shift_score_outlier = shift_score . copy () n_shifts = len ( nbp_basic . use_tiles ) final_shift_search = np . zeros_like ( start_shift_search ) final_shift_search [:, :, 2 ] = start_shift_search [:, :, 2 ] # spacing does not change for r in nbp_basic . use_rounds : good_shifts = shift_score [:, r ] > shift_score_thresh [:, r ] for i in range ( len ( coords )): # change shift search to be near good shifts found # this will only do something if 3>sum(good_shifts)>0, otherwise will have been done in previous loop. if np . sum ( good_shifts ) > 0 : shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [ good_shifts , r , i ]) elif good_shifts . size > 0 : shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [:, r , i ]) final_shift_search [ r , :, 0 ] = [ np . min ( shifts [ r ][ key ]) for key in shifts [ r ] . keys ()] final_shift_search [ r , :, 1 ] = [ np . max ( shifts [ r ][ key ]) for key in shifts [ r ] . keys ()] shift_outlier [ good_shifts , r ] = 0 # only keep outlier information for not good shifts shift_score_outlier [ good_shifts , r ] = 0 if ( np . sum ( good_shifts ) < 2 and n_shifts > 4 ) or ( np . sum ( good_shifts ) == 0 and n_shifts > 0 ): warnings . warn ( f \"Round { r } : { n_shifts - np . sum ( good_shifts ) } / { n_shifts } \" f \"of shifts fell below score threshold\" ) for t in np . where ( good_shifts == False )[ 0 ]: if t not in nbp_basic . use_tiles : continue # re-find shifts that fell below threshold by only looking at shifts near to others found # score set to 0 so will find do refined search no matter what. shift [ t , r ], shift_score [ t , r ] = compute_shift ( spot_yxz ( spot_details , t , r_ref , c_ref ), spot_yxz ( spot_details , t , r , c_imaging ), 0 , None , None , None , config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], None , None , z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 2 ] warnings . warn ( f \" \\n Shift for tile { t } to round { r } changed from \\n \" f \" { shift_outlier [ t , r ] } to { shift [ t , r ] } .\" ) nbp_debug . shift = shift nbp_debug . start_shift_search = start_shift_search nbp_debug . final_shift_search = final_shift_search nbp_debug . shift_score = shift_score nbp_debug . shift_score_thresh = shift_score_thresh nbp_debug . shift_outlier = shift_outlier nbp_debug . shift_score_outlier = shift_score_outlier return nbp_debug","title":"Register Initial"},{"location":"code/pipeline/register_initial/#coppafish.pipeline.register_initial.register_initial","text":"This finds the shift between ref round/channel to each imaging round for each tile. These are then used as the starting point for determining the affine transforms in pipeline/register.py . See 'register_initial' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'register_initial' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required Returns: Type Description NotebookPage NotebookPage[register_initial] - Page contains information about how shift between ref round/channel to each imaging round for each tile was found. Source code in coppafish/pipeline/register_initial.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def register_initial ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray ) -> NotebookPage : \"\"\" This finds the shift between ref round/channel to each imaging round for each tile. These are then used as the starting point for determining the affine transforms in `pipeline/register.py`. See `'register_initial'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'register_initial'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. Returns: `NotebookPage[register_initial]` - Page contains information about how shift between ref round/channel to each imaging round for each tile was found. \"\"\" nbp_debug = NotebookPage ( \"register_initial\" ) if config [ 'shift_channel' ] is None : config [ 'shift_channel' ] = nbp_basic . ref_channel if not np . isin ( config [ 'shift_channel' ], nbp_basic . use_channels ): raise ValueError ( f \"config['shift_channel'] should be in nb.basic_info.use_channels, but value given is \\n \" f \" { config [ 'shift_channel' ] } which is not in use_channels = { nbp_basic . use_channels } .\" ) nbp_debug . shift_channel = config [ 'shift_channel' ] coords = [ 'y' , 'x' , 'z' ] shifts = [{}] start_shift_search = np . zeros (( nbp_basic . n_rounds , 3 , 3 ), dtype = int ) for i in range ( len ( coords )): shifts [ 0 ][ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) start_shift_search [ nbp_basic . use_rounds , i , :] = [ config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ], config [ 'shift_step' ][ i ]] if not nbp_basic . is_3d : config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 0 ][ 'z' ] = np . array ([ 0 ], dtype = int ) start_shift_search [:, 2 , : 2 ] = 0 shifts = shifts * nbp_basic . n_rounds # get one set of shifts for each round shift = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds , 3 ), dtype = int ) shift_score = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds ), dtype = float ) shift_score_thresh = np . zeros (( nbp_basic . n_tiles , nbp_basic . n_rounds ), dtype = float ) c_ref = nbp_basic . ref_channel r_ref = nbp_basic . ref_round c_imaging = config [ 'shift_channel' ] # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy with tqdm ( total = len ( nbp_basic . use_rounds ) * len ( nbp_basic . use_tiles )) as pbar : pbar . set_description ( f \"Finding shift from ref_round( { r_ref } )/ref_channel( { c_ref } ) to channel { c_imaging } \" f \"of all imaging rounds\" ) for r in nbp_basic . use_rounds : for t in nbp_basic . use_tiles : pbar . set_postfix ({ 'round' : r , 'tile' : t }) shift [ t , r ], shift_score [ t , r ], shift_score_thresh [ t , r ] = \\ compute_shift ( spot_yxz ( spot_details , t , r_ref , c_ref ), spot_yxz ( spot_details , t , r , c_imaging ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 3 ] good_shifts = shift_score [:, r ] > shift_score_thresh [:, r ] if np . sum ( good_shifts ) >= 3 : # once found shifts, refine shifts to be searched around these for i in range ( len ( coords )): shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [ good_shifts , r , i ]) pbar . update ( 1 ) pbar . close () # amend shifts for which score fell below score_thresh shift_outlier = shift . copy () shift_score_outlier = shift_score . copy () n_shifts = len ( nbp_basic . use_tiles ) final_shift_search = np . zeros_like ( start_shift_search ) final_shift_search [:, :, 2 ] = start_shift_search [:, :, 2 ] # spacing does not change for r in nbp_basic . use_rounds : good_shifts = shift_score [:, r ] > shift_score_thresh [:, r ] for i in range ( len ( coords )): # change shift search to be near good shifts found # this will only do something if 3>sum(good_shifts)>0, otherwise will have been done in previous loop. if np . sum ( good_shifts ) > 0 : shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [ good_shifts , r , i ]) elif good_shifts . size > 0 : shifts [ r ][ coords [ i ]] = update_shifts ( shifts [ r ][ coords [ i ]], shift [:, r , i ]) final_shift_search [ r , :, 0 ] = [ np . min ( shifts [ r ][ key ]) for key in shifts [ r ] . keys ()] final_shift_search [ r , :, 1 ] = [ np . max ( shifts [ r ][ key ]) for key in shifts [ r ] . keys ()] shift_outlier [ good_shifts , r ] = 0 # only keep outlier information for not good shifts shift_score_outlier [ good_shifts , r ] = 0 if ( np . sum ( good_shifts ) < 2 and n_shifts > 4 ) or ( np . sum ( good_shifts ) == 0 and n_shifts > 0 ): warnings . warn ( f \"Round { r } : { n_shifts - np . sum ( good_shifts ) } / { n_shifts } \" f \"of shifts fell below score threshold\" ) for t in np . where ( good_shifts == False )[ 0 ]: if t not in nbp_basic . use_tiles : continue # re-find shifts that fell below threshold by only looking at shifts near to others found # score set to 0 so will find do refined search no matter what. shift [ t , r ], shift_score [ t , r ] = compute_shift ( spot_yxz ( spot_details , t , r_ref , c_ref ), spot_yxz ( spot_details , t , r , c_imaging ), 0 , None , None , None , config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], None , None , z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 2 ] warnings . warn ( f \" \\n Shift for tile { t } to round { r } changed from \\n \" f \" { shift_outlier [ t , r ] } to { shift [ t , r ] } .\" ) nbp_debug . shift = shift nbp_debug . start_shift_search = start_shift_search nbp_debug . final_shift_search = final_shift_search nbp_debug . shift_score = shift_score nbp_debug . shift_score_thresh = shift_score_thresh nbp_debug . shift_outlier = shift_outlier nbp_debug . shift_score_outlier = shift_score_outlier return nbp_debug","title":"register_initial()"},{"location":"code/pipeline/run/","text":"initialize_nb ( config_file ) Quick function which creates a Notebook and adds basic_info page before saving. file_names page will be added automatically as soon as basic_info page is added. If Notebook already exists and contains these pages, it will just be returned. Parameters: Name Type Description Default config_file str Path to config file. required Returns: Type Description setup . Notebook Notebook containing file_names and basic_info pages. Source code in coppafish/pipeline/run.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def initialize_nb ( config_file : str ) -> setup . Notebook : \"\"\" Quick function which creates a `Notebook` and adds `basic_info` page before saving. `file_names` page will be added automatically as soon as `basic_info` page is added. If `Notebook` already exists and contains these pages, it will just be returned. Args: config_file: Path to config file. Returns: `Notebook` containing `file_names` and `basic_info` pages. \"\"\" nb = setup . Notebook ( config_file = config_file ) config = nb . get_config () if not nb . has_page ( \"basic_info\" ): nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb += nbp_basic else : warnings . warn ( 'basic_info' , utils . warnings . NotebookPageWarning ) return nb run_extract ( nb ) This runs the extract_and_filter step of the pipeline to produce the tiff files in the tile directory. extract and extract_debug pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing file_names and basic_info pages. required Source code in coppafish/pipeline/run.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def run_extract ( nb : setup . Notebook ): \"\"\" This runs the `extract_and_filter` step of the pipeline to produce the tiff files in the tile directory. `extract` and `extract_debug` pages are added to the `Notebook` before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `file_names` and `basic_info` pages. \"\"\" if not all ( nb . has_page ([ \"extract\" , \"extract_debug\" ])): config = nb . get_config () nbp , nbp_debug = extract_and_filter ( config [ 'extract' ], nb . file_names , nb . basic_info ) nb += nbp nb += nbp_debug else : warnings . warn ( 'extract' , utils . warnings . NotebookPageWarning ) warnings . warn ( 'extract_debug' , utils . warnings . NotebookPageWarning ) run_find_spots ( nb ) This runs the find_spots step of the pipeline to produce point cloud from each tiff file in the tile directory. find_spots page added to the Notebook before saving. If Notebook already contains this page, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing extract page. required Source code in coppafish/pipeline/run.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def run_find_spots ( nb : setup . Notebook ): \"\"\" This runs the `find_spots` step of the pipeline to produce point cloud from each tiff file in the tile directory. `find_spots` page added to the `Notebook` before saving. If `Notebook` already contains this page, it will just be returned. Args: nb: `Notebook` containing `extract` page. \"\"\" if not nb . has_page ( \"find_spots\" ): config = nb . get_config () nbp = find_spots ( config [ 'find_spots' ], nb . file_names , nb . basic_info , nb . extract . auto_thresh ) nb += nbp check_n_spots ( nb ) # error if too few spots - may indicate tile or channel which should not be included else : warnings . warn ( 'find_spots' , utils . warnings . NotebookPageWarning ) run_omp ( nb ) This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of spots and their gene identity. It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at every pixel. Spots are then local maxima in these gene coefficient images. omp page is added to the Notebook before saving. Parameters: Name Type Description Default nb setup . Notebook Notebook containing call_spots page. required Source code in coppafish/pipeline/run.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def run_omp ( nb : setup . Notebook ): \"\"\" This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of spots and their gene identity. It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at every pixel. Spots are then local maxima in these gene coefficient images. `omp` page is added to the Notebook before saving. Args: nb: `Notebook` containing `call_spots` page. \"\"\" if not nb . has_page ( \"omp\" ): config = nb . get_config () # Use tile with most spots on to find spot shape in omp tile_most_spots = int ( stats . mode ( nb . ref_spots . tile [ quality_threshold ( nb , 'ref' )])[ 0 ][ 0 ]) nbp = call_spots_omp ( config [ 'omp' ], nb . file_names , nb . basic_info , nb . call_spots , nb . stitch . tile_origin , nb . register . transform , tile_most_spots ) nb += nbp # Update omp_info files after omp notebook page saved into notebook # Save only non-duplicates - important spot_coefs saved first for exception at start of call_spots_omp # which can deal with case where duplicates removed from spot_coefs but not spot_info. # After re-saving here, spot_coefs[s] should be the coefficients for gene at nb.omp.local_yxz[s] # i.e. indices should match up. spot_info = np . load ( nb . file_names . omp_spot_info ) not_duplicate = get_non_duplicate ( nb . stitch . tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) spot_coefs = sparse . load_npz ( nb . file_names . omp_spot_coef ) sparse . save_npz ( nb . file_names . omp_spot_coef , spot_coefs [ not_duplicate ]) np . save ( nb . file_names . omp_spot_info , spot_info [ not_duplicate ]) # only raise error after saving to notebook if spot_colors have nan in wrong places. utils . errors . check_color_nan ( nbp . colors , nb . basic_info ) else : warnings . warn ( 'omp' , utils . warnings . NotebookPageWarning ) run_pipeline ( config_file , overwrite_ref_spots = False ) Bridge function to run every step of the pipeline. Parameters: Name Type Description Default config_file str Path to config file. required overwrite_ref_spots bool Only used if Notebook contains ref_spots but not call_spots page. If True , the variables: gene_no score score_diff intensity in nb.ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Returns: Type Description setup . Notebook Notebook containing all information gathered during the pipeline. Source code in coppafish/pipeline/run.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def run_pipeline ( config_file : str , overwrite_ref_spots : bool = False ) -> setup . Notebook : \"\"\" Bridge function to run every step of the pipeline. Args: config_file: Path to config file. overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page. If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. Returns: `Notebook` containing all information gathered during the pipeline. \"\"\" nb = initialize_nb ( config_file ) run_extract ( nb ) run_find_spots ( nb ) run_stitch ( nb ) run_register ( nb ) run_reference_spots ( nb , overwrite_ref_spots ) run_omp ( nb ) return nb run_reference_spots ( nb , overwrite_ref_spots = False ) This runs the reference_spots step of the pipeline to get the intensity of each spot on the reference round/channel in each imaging round/channel. The call_spots step of the pipeline is then run to produce the bleed_matrix , bled_code for each gene and the gene assignments of the spots on the reference round. ref_spots and call_spots pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing stitch and register pages. required overwrite_ref_spots bool Only used if Notebook contains ref_spots but not call_spots page. If True , the variables: gene_no score score_diff intensity in nb.ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Source code in coppafish/pipeline/run.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run_reference_spots ( nb : setup . Notebook , overwrite_ref_spots : bool = False ): \"\"\" This runs the `reference_spots` step of the pipeline to get the intensity of each spot on the reference round/channel in each imaging round/channel. The `call_spots` step of the pipeline is then run to produce the `bleed_matrix`, `bled_code` for each gene and the gene assignments of the spots on the reference round. `ref_spots` and `call_spots` pages are added to the Notebook before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `stitch` and `register` pages. overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page. If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. \"\"\" if not nb . has_page ( 'ref_spots' ): nbp = get_reference_spots ( nb . file_names , nb . basic_info , nb . find_spots . spot_details , nb . stitch . tile_origin , nb . register . transform ) nb += nbp # save to Notebook with gene_no, score, score_diff, intensity = None. # These will be added in call_reference_spots else : warnings . warn ( 'ref_spots' , utils . warnings . NotebookPageWarning ) if not nb . has_page ( \"call_spots\" ): config = nb . get_config () nbp , nbp_ref_spots = call_reference_spots ( config [ 'call_spots' ], nb . file_names , nb . basic_info , nb . ref_spots , nb . extract . hist_values , nb . extract . hist_counts , nb . register . transform , overwrite_ref_spots ) nb += nbp # Raise errors if stitch, register_initial or register section failed # Do that at this stage, so can still run viewer to see what spots look like check_shifts_stitch ( nb ) # error if too many bad shifts between tiles check_shifts_register ( nb ) # error if too many bad shifts between rounds check_transforms ( nb ) # error if affine transforms found have low number of matches # only raise error after saving to notebook if spot_colors have nan in wrong places. utils . errors . check_color_nan ( nb . ref_spots . colors , nb . basic_info ) else : warnings . warn ( 'call_spots' , utils . warnings . NotebookPageWarning ) run_register ( nb ) This runs the register_initial step of the pipeline to find shift between ref round/channel to each imaging round for each tile. It then runs the register step of the pipeline which uses this as a starting point to get the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. register_initial , register and register_debug pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing extract page. required Source code in coppafish/pipeline/run.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def run_register ( nb : setup . Notebook ): \"\"\" This runs the `register_initial` step of the pipeline to find shift between ref round/channel to each imaging round for each tile. It then runs the `register` step of the pipeline which uses this as a starting point to get the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. `register_initial`, `register` and `register_debug` pages are added to the `Notebook` before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `extract` page. \"\"\" config = nb . get_config () if nb . has_page ( \"register_initial_debug\" ): # deal with old notebook files where page was called \"register_initial_debug\" instead of # \"register_initial\". This will trigger a save after name change too. nb . change_page_name ( \"register_initial_debug\" , \"register_initial\" ) if not nb . has_page ( \"register_initial\" ): nbp_initial = register_initial ( config [ 'register_initial' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_initial else : warnings . warn ( 'register_initial' , utils . warnings . NotebookPageWarning ) if not all ( nb . has_page ([ \"register\" , \"register_debug\" ])): nbp , nbp_debug = register ( config [ 'register' ], nb . basic_info , nb . find_spots . spot_details , nb . register_initial . shift ) nb += nbp nb += nbp_debug else : warnings . warn ( 'register' , utils . warnings . NotebookPageWarning ) warnings . warn ( 'register_debug' , utils . warnings . NotebookPageWarning ) run_stitch ( nb ) This runs the stitch step of the pipeline to produce origin of each tile such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images. stitch page added to the Notebook before saving. If Notebook already contains this page, it will just be returned. If stitched images already exist, they won't be created again. Parameters: Name Type Description Default nb setup . Notebook Notebook containing find_spots page. required Source code in coppafish/pipeline/run.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def run_stitch ( nb : setup . Notebook ): \"\"\" This runs the `stitch` step of the pipeline to produce origin of each tile such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images. `stitch` page added to the `Notebook` before saving. If `Notebook` already contains this page, it will just be returned. If stitched images already exist, they won't be created again. Args: nb: `Notebook` containing `find_spots` page. \"\"\" config = nb . get_config () if not nb . has_page ( \"stitch\" ): nbp_debug = stitch ( config [ 'stitch' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_debug else : warnings . warn ( 'stitch' , utils . warnings . NotebookPageWarning ) if nb . file_names . big_dapi_image is not None and not os . path . isfile ( nb . file_names . big_dapi_image ): # save stitched dapi # Will load in from nd2 file if nb.extract_debug.r_dapi is None i.e. if no DAPI filtering performed. utils . npy . save_stitched ( nb . file_names . big_dapi_image , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . anchor_round , nb . basic_info . dapi_channel , nb . extract_debug . r_dapi is None , config [ 'stitch' ][ 'save_image_zero_thresh' ]) if nb . file_names . big_anchor_image is not None and not os . path . isfile ( nb . file_names . big_anchor_image ): # save stitched reference round/channel utils . npy . save_stitched ( nb . file_names . big_anchor_image , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . ref_round , nb . basic_info . ref_channel , False , config [ 'stitch' ][ 'save_image_zero_thresh' ])","title":"Run"},{"location":"code/pipeline/run/#coppafish.pipeline.run.initialize_nb","text":"Quick function which creates a Notebook and adds basic_info page before saving. file_names page will be added automatically as soon as basic_info page is added. If Notebook already exists and contains these pages, it will just be returned. Parameters: Name Type Description Default config_file str Path to config file. required Returns: Type Description setup . Notebook Notebook containing file_names and basic_info pages. Source code in coppafish/pipeline/run.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def initialize_nb ( config_file : str ) -> setup . Notebook : \"\"\" Quick function which creates a `Notebook` and adds `basic_info` page before saving. `file_names` page will be added automatically as soon as `basic_info` page is added. If `Notebook` already exists and contains these pages, it will just be returned. Args: config_file: Path to config file. Returns: `Notebook` containing `file_names` and `basic_info` pages. \"\"\" nb = setup . Notebook ( config_file = config_file ) config = nb . get_config () if not nb . has_page ( \"basic_info\" ): nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb += nbp_basic else : warnings . warn ( 'basic_info' , utils . warnings . NotebookPageWarning ) return nb","title":"initialize_nb()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_extract","text":"This runs the extract_and_filter step of the pipeline to produce the tiff files in the tile directory. extract and extract_debug pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing file_names and basic_info pages. required Source code in coppafish/pipeline/run.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def run_extract ( nb : setup . Notebook ): \"\"\" This runs the `extract_and_filter` step of the pipeline to produce the tiff files in the tile directory. `extract` and `extract_debug` pages are added to the `Notebook` before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `file_names` and `basic_info` pages. \"\"\" if not all ( nb . has_page ([ \"extract\" , \"extract_debug\" ])): config = nb . get_config () nbp , nbp_debug = extract_and_filter ( config [ 'extract' ], nb . file_names , nb . basic_info ) nb += nbp nb += nbp_debug else : warnings . warn ( 'extract' , utils . warnings . NotebookPageWarning ) warnings . warn ( 'extract_debug' , utils . warnings . NotebookPageWarning )","title":"run_extract()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_find_spots","text":"This runs the find_spots step of the pipeline to produce point cloud from each tiff file in the tile directory. find_spots page added to the Notebook before saving. If Notebook already contains this page, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing extract page. required Source code in coppafish/pipeline/run.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def run_find_spots ( nb : setup . Notebook ): \"\"\" This runs the `find_spots` step of the pipeline to produce point cloud from each tiff file in the tile directory. `find_spots` page added to the `Notebook` before saving. If `Notebook` already contains this page, it will just be returned. Args: nb: `Notebook` containing `extract` page. \"\"\" if not nb . has_page ( \"find_spots\" ): config = nb . get_config () nbp = find_spots ( config [ 'find_spots' ], nb . file_names , nb . basic_info , nb . extract . auto_thresh ) nb += nbp check_n_spots ( nb ) # error if too few spots - may indicate tile or channel which should not be included else : warnings . warn ( 'find_spots' , utils . warnings . NotebookPageWarning )","title":"run_find_spots()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_omp","text":"This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of spots and their gene identity. It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at every pixel. Spots are then local maxima in these gene coefficient images. omp page is added to the Notebook before saving. Parameters: Name Type Description Default nb setup . Notebook Notebook containing call_spots page. required Source code in coppafish/pipeline/run.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def run_omp ( nb : setup . Notebook ): \"\"\" This runs the orthogonal matching pursuit section of the pipeline as an alternate method to determine location of spots and their gene identity. It achieves this by fitting multiple gene bled codes to each pixel to find a coefficient for every gene at every pixel. Spots are then local maxima in these gene coefficient images. `omp` page is added to the Notebook before saving. Args: nb: `Notebook` containing `call_spots` page. \"\"\" if not nb . has_page ( \"omp\" ): config = nb . get_config () # Use tile with most spots on to find spot shape in omp tile_most_spots = int ( stats . mode ( nb . ref_spots . tile [ quality_threshold ( nb , 'ref' )])[ 0 ][ 0 ]) nbp = call_spots_omp ( config [ 'omp' ], nb . file_names , nb . basic_info , nb . call_spots , nb . stitch . tile_origin , nb . register . transform , tile_most_spots ) nb += nbp # Update omp_info files after omp notebook page saved into notebook # Save only non-duplicates - important spot_coefs saved first for exception at start of call_spots_omp # which can deal with case where duplicates removed from spot_coefs but not spot_info. # After re-saving here, spot_coefs[s] should be the coefficients for gene at nb.omp.local_yxz[s] # i.e. indices should match up. spot_info = np . load ( nb . file_names . omp_spot_info ) not_duplicate = get_non_duplicate ( nb . stitch . tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , spot_info [:, : 3 ], spot_info [:, 6 ]) spot_coefs = sparse . load_npz ( nb . file_names . omp_spot_coef ) sparse . save_npz ( nb . file_names . omp_spot_coef , spot_coefs [ not_duplicate ]) np . save ( nb . file_names . omp_spot_info , spot_info [ not_duplicate ]) # only raise error after saving to notebook if spot_colors have nan in wrong places. utils . errors . check_color_nan ( nbp . colors , nb . basic_info ) else : warnings . warn ( 'omp' , utils . warnings . NotebookPageWarning )","title":"run_omp()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_pipeline","text":"Bridge function to run every step of the pipeline. Parameters: Name Type Description Default config_file str Path to config file. required overwrite_ref_spots bool Only used if Notebook contains ref_spots but not call_spots page. If True , the variables: gene_no score score_diff intensity in nb.ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Returns: Type Description setup . Notebook Notebook containing all information gathered during the pipeline. Source code in coppafish/pipeline/run.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def run_pipeline ( config_file : str , overwrite_ref_spots : bool = False ) -> setup . Notebook : \"\"\" Bridge function to run every step of the pipeline. Args: config_file: Path to config file. overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page. If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. Returns: `Notebook` containing all information gathered during the pipeline. \"\"\" nb = initialize_nb ( config_file ) run_extract ( nb ) run_find_spots ( nb ) run_stitch ( nb ) run_register ( nb ) run_reference_spots ( nb , overwrite_ref_spots ) run_omp ( nb ) return nb","title":"run_pipeline()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_reference_spots","text":"This runs the reference_spots step of the pipeline to get the intensity of each spot on the reference round/channel in each imaging round/channel. The call_spots step of the pipeline is then run to produce the bleed_matrix , bled_code for each gene and the gene assignments of the spots on the reference round. ref_spots and call_spots pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing stitch and register pages. required overwrite_ref_spots bool Only used if Notebook contains ref_spots but not call_spots page. If True , the variables: gene_no score score_diff intensity in nb.ref_spots will be overwritten if they exist. If this is False , they will only be overwritten if they are all set to None , otherwise an error will occur. False Source code in coppafish/pipeline/run.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run_reference_spots ( nb : setup . Notebook , overwrite_ref_spots : bool = False ): \"\"\" This runs the `reference_spots` step of the pipeline to get the intensity of each spot on the reference round/channel in each imaging round/channel. The `call_spots` step of the pipeline is then run to produce the `bleed_matrix`, `bled_code` for each gene and the gene assignments of the spots on the reference round. `ref_spots` and `call_spots` pages are added to the Notebook before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `stitch` and `register` pages. overwrite_ref_spots: Only used if *Notebook* contains *ref_spots* but not *call_spots* page. If `True`, the variables: * `gene_no` * `score` * `score_diff` * `intensity` in `nb.ref_spots` will be overwritten if they exist. If this is `False`, they will only be overwritten if they are all set to `None`, otherwise an error will occur. \"\"\" if not nb . has_page ( 'ref_spots' ): nbp = get_reference_spots ( nb . file_names , nb . basic_info , nb . find_spots . spot_details , nb . stitch . tile_origin , nb . register . transform ) nb += nbp # save to Notebook with gene_no, score, score_diff, intensity = None. # These will be added in call_reference_spots else : warnings . warn ( 'ref_spots' , utils . warnings . NotebookPageWarning ) if not nb . has_page ( \"call_spots\" ): config = nb . get_config () nbp , nbp_ref_spots = call_reference_spots ( config [ 'call_spots' ], nb . file_names , nb . basic_info , nb . ref_spots , nb . extract . hist_values , nb . extract . hist_counts , nb . register . transform , overwrite_ref_spots ) nb += nbp # Raise errors if stitch, register_initial or register section failed # Do that at this stage, so can still run viewer to see what spots look like check_shifts_stitch ( nb ) # error if too many bad shifts between tiles check_shifts_register ( nb ) # error if too many bad shifts between rounds check_transforms ( nb ) # error if affine transforms found have low number of matches # only raise error after saving to notebook if spot_colors have nan in wrong places. utils . errors . check_color_nan ( nb . ref_spots . colors , nb . basic_info ) else : warnings . warn ( 'call_spots' , utils . warnings . NotebookPageWarning )","title":"run_reference_spots()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_register","text":"This runs the register_initial step of the pipeline to find shift between ref round/channel to each imaging round for each tile. It then runs the register step of the pipeline which uses this as a starting point to get the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. register_initial , register and register_debug pages are added to the Notebook before saving. If Notebook already contains these pages, it will just be returned. Parameters: Name Type Description Default nb setup . Notebook Notebook containing extract page. required Source code in coppafish/pipeline/run.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def run_register ( nb : setup . Notebook ): \"\"\" This runs the `register_initial` step of the pipeline to find shift between ref round/channel to each imaging round for each tile. It then runs the `register` step of the pipeline which uses this as a starting point to get the affine transforms to go from the ref round/channel to each imaging round/channel for every tile. `register_initial`, `register` and `register_debug` pages are added to the `Notebook` before saving. If `Notebook` already contains these pages, it will just be returned. Args: nb: `Notebook` containing `extract` page. \"\"\" config = nb . get_config () if nb . has_page ( \"register_initial_debug\" ): # deal with old notebook files where page was called \"register_initial_debug\" instead of # \"register_initial\". This will trigger a save after name change too. nb . change_page_name ( \"register_initial_debug\" , \"register_initial\" ) if not nb . has_page ( \"register_initial\" ): nbp_initial = register_initial ( config [ 'register_initial' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_initial else : warnings . warn ( 'register_initial' , utils . warnings . NotebookPageWarning ) if not all ( nb . has_page ([ \"register\" , \"register_debug\" ])): nbp , nbp_debug = register ( config [ 'register' ], nb . basic_info , nb . find_spots . spot_details , nb . register_initial . shift ) nb += nbp nb += nbp_debug else : warnings . warn ( 'register' , utils . warnings . NotebookPageWarning ) warnings . warn ( 'register_debug' , utils . warnings . NotebookPageWarning )","title":"run_register()"},{"location":"code/pipeline/run/#coppafish.pipeline.run.run_stitch","text":"This runs the stitch step of the pipeline to produce origin of each tile such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images. stitch page added to the Notebook before saving. If Notebook already contains this page, it will just be returned. If stitched images already exist, they won't be created again. Parameters: Name Type Description Default nb setup . Notebook Notebook containing find_spots page. required Source code in coppafish/pipeline/run.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def run_stitch ( nb : setup . Notebook ): \"\"\" This runs the `stitch` step of the pipeline to produce origin of each tile such that a global coordinate system can be built. Also saves stitched DAPI and reference channel images. `stitch` page added to the `Notebook` before saving. If `Notebook` already contains this page, it will just be returned. If stitched images already exist, they won't be created again. Args: nb: `Notebook` containing `find_spots` page. \"\"\" config = nb . get_config () if not nb . has_page ( \"stitch\" ): nbp_debug = stitch ( config [ 'stitch' ], nb . basic_info , nb . find_spots . spot_details ) nb += nbp_debug else : warnings . warn ( 'stitch' , utils . warnings . NotebookPageWarning ) if nb . file_names . big_dapi_image is not None and not os . path . isfile ( nb . file_names . big_dapi_image ): # save stitched dapi # Will load in from nd2 file if nb.extract_debug.r_dapi is None i.e. if no DAPI filtering performed. utils . npy . save_stitched ( nb . file_names . big_dapi_image , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . anchor_round , nb . basic_info . dapi_channel , nb . extract_debug . r_dapi is None , config [ 'stitch' ][ 'save_image_zero_thresh' ]) if nb . file_names . big_anchor_image is not None and not os . path . isfile ( nb . file_names . big_anchor_image ): # save stitched reference round/channel utils . npy . save_stitched ( nb . file_names . big_anchor_image , nb . file_names , nb . basic_info , nb . stitch . tile_origin , nb . basic_info . ref_round , nb . basic_info . ref_channel , False , config [ 'stitch' ][ 'save_image_zero_thresh' ])","title":"run_stitch()"},{"location":"code/pipeline/stitch/","text":"stitch ( config , nbp_basic , spot_details ) This gets the origin of each tile such that a global coordinate system can be built. See 'stitch' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'stitch' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required Returns: Type Description NotebookPage NotebookPage[stitch] - Page contains information about how tiles were stitched together to give global coordinates. Source code in coppafish/pipeline/stitch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def stitch ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray ) -> NotebookPage : \"\"\" This gets the origin of each tile such that a global coordinate system can be built. See `'stitch'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'stitch'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. Returns: `NotebookPage[stitch]` - Page contains information about how tiles were stitched together to give global coordinates. \"\"\" nbp_debug = NotebookPage ( \"stitch\" ) # NOTE that directions should actually be 'north' and 'east' directions = [ 'south' , 'west' ] coords = [ 'y' , 'x' , 'z' ] shifts = get_shifts_to_search ( config , nbp_basic , nbp_debug ) if not nbp_basic . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 # initialise variables to store shift info shift_info = { 'south' : {}, 'west' : {}} for j in directions : shift_info [ j ][ 'pairs' ] = np . zeros (( 0 , 2 ), dtype = int ) shift_info [ j ][ 'shifts' ] = np . zeros (( 0 , 3 ), dtype = int ) shift_info [ j ][ 'score' ] = np . zeros (( 0 , 1 ), dtype = float ) shift_info [ j ][ 'score_thresh' ] = np . zeros (( 0 , 1 ), dtype = float ) # find shifts between overlapping tiles c = nbp_basic . ref_channel r = nbp_basic . ref_round t_neighb = { 'south' : [], 'west' : []} # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy with tqdm ( total = 2 * len ( nbp_basic . use_tiles )) as pbar : pbar . set_description ( f \"Finding overlap between tiles in round { r } (ref_round)\" ) for t in nbp_basic . use_tiles : # align to south neighbour followed by west neighbour t_neighb [ 'south' ] = np . where ( np . sum ( nbp_basic . tilepos_yx == nbp_basic . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] t_neighb [ 'west' ] = np . where ( np . sum ( nbp_basic . tilepos_yx == nbp_basic . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] for j in directions : pbar . set_postfix ({ 'tile' : t , 'direction' : j }) if t_neighb [ j ] in nbp_basic . use_tiles : shift , score , score_thresh = compute_shift ( spot_yxz ( spot_details , t , r , c ), spot_yxz ( spot_details , t_neighb [ j ][ 0 ], r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 3 ] shift_info [ j ][ 'pairs' ] = np . append ( shift_info [ j ][ 'pairs' ], np . array ([ t , t_neighb [ j ][ 0 ]]) . reshape ( 1 , 2 ), axis = 0 ) shift_info [ j ][ 'shifts' ] = np . append ( shift_info [ j ][ 'shifts' ], np . array ( shift ) . reshape ( 1 , 3 ), axis = 0 ) shift_info [ j ][ 'score' ] = np . append ( shift_info [ j ][ 'score' ], np . array ( score ) . reshape ( 1 , 1 ), axis = 0 ) shift_info [ j ][ 'score_thresh' ] = np . append ( shift_info [ j ][ 'score_thresh' ], np . array ( score_thresh ) . reshape ( 1 , 1 ), axis = 0 ) good_shifts = ( shift_info [ j ][ 'score' ] > shift_info [ j ][ 'score_thresh' ]) . flatten () if np . sum ( good_shifts ) >= 3 : # once found shifts, refine shifts to be searched around these for i in range ( len ( coords )): shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][ good_shifts , i ]) pbar . update ( 1 ) pbar . close () # amend shifts for which score fell below score_thresh for j in directions : good_shifts = ( shift_info [ j ][ 'score' ] > shift_info [ j ][ 'score_thresh' ]) . flatten () for i in range ( len ( coords )): # change shift search to be near good shifts found # this will only do something if 3>sum(good_shifts)>0, otherwise will have been done in previous loop. if np . sum ( good_shifts ) > 0 : shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][ good_shifts , i ]) elif good_shifts . size > 0 : shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][:, i ]) # add outlier variable to shift_info to keep track of those shifts which are changed. shift_info [ j ][ 'outlier_shifts' ] = shift_info [ j ][ 'shifts' ] . copy () shift_info [ j ][ 'outlier_score' ] = shift_info [ j ][ 'score' ] . copy () shift_info [ j ][ 'outlier_shifts' ][ good_shifts , :] = 0 shift_info [ j ][ 'outlier_score' ][ good_shifts , :] = 0 if ( np . sum ( good_shifts ) < 2 and len ( good_shifts ) > 4 ) or ( np . sum ( good_shifts ) == 0 and len ( good_shifts ) > 0 ): warnings . warn ( f \" { len ( good_shifts ) - np . sum ( good_shifts ) } / { len ( good_shifts ) } \" f \" of shifts fell below score threshold\" ) for i in np . where ( good_shifts == False )[ 0 ]: t = shift_info [ j ][ 'pairs' ][ i , 0 ] t_neighb = shift_info [ j ][ 'pairs' ][ i , 1 ] # re-find shifts that fell below threshold by only looking at shifts near to others found # Don't allow any widening so shift found must be in this range. # score_thresh given is 0, so it is not re-computed. shift_info [ j ][ 'shifts' ][ i ], shift_info [ j ][ 'score' ][ i ] = \\ compute_shift ( spot_yxz ( spot_details , t , r , c ), spot_yxz ( spot_details , t_neighb , r , c ), 0 , None , None , None , config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], None , None , z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 2 ] warnings . warn ( f \" \\n Shift from tile { t } to tile { t_neighb } changed from \\n \" f \" { shift_info [ j ][ 'outlier_shifts' ][ i ] } to { shift_info [ j ][ 'shifts' ][ i ] } .\" ) # get tile origins in global coordinates. # global coordinates are built about central tile so found this first tile_dist_to_centre = np . linalg . norm ( nbp_basic . tilepos_yx [ nbp_basic . use_tiles ] - np . mean ( nbp_basic . tilepos_yx , axis = 0 ), axis = 1 ) centre_tile = nbp_basic . use_tiles [ tile_dist_to_centre . argmin ()] tile_origin = get_tile_origin ( shift_info [ 'south' ][ 'pairs' ], shift_info [ 'south' ][ 'shifts' ], shift_info [ 'west' ][ 'pairs' ], shift_info [ 'west' ][ 'shifts' ], nbp_basic . n_tiles , centre_tile ) if nbp_basic . is_3d is False : tile_origin [:, 2 ] = 0 # set z coordinate to 0 for all tiles if 2d # add tile origin to debugging notebook so don't have whole page for one variable, # and need to add other rounds to it in registration stage anyway. nbp_debug . tile_origin = tile_origin # save all shift info to debugging page for j in directions : nbp_debug . __setattr__ ( j + '_' + 'final_shift_search' , np . zeros (( 3 , 3 ), dtype = int )) nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 0 ] = \\ [ np . min ( shifts [ j ][ key ]) for key in shifts [ j ] . keys ()] nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 1 ] = [ np . max ( shifts [ j ][ key ]) for key in shifts [ j ] . keys ()] nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 2 ] = \\ nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[:, 2 ] for var in shift_info [ j ] . keys (): nbp_debug . __setattr__ ( j + '_' + var , shift_info [ j ][ var ]) return nbp_debug","title":"Stitch"},{"location":"code/pipeline/stitch/#coppafish.pipeline.stitch.stitch","text":"This gets the origin of each tile such that a global coordinate system can be built. See 'stitch' section of notebook_comments.json file for description of the variables in the page. Parameters: Name Type Description Default config dict Dictionary obtained from 'stitch' section of config file. required nbp_basic NotebookPage basic_info notebook page required spot_details np . ndarray int [n_spots x 7] . spot_details[s] is [tile, round, channel, isolated, y, x, z] of spot s . This is saved in the find_spots notebook page i.e. nb.find_spots.spot_details . required Returns: Type Description NotebookPage NotebookPage[stitch] - Page contains information about how tiles were stitched together to give global coordinates. Source code in coppafish/pipeline/stitch.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def stitch ( config : dict , nbp_basic : NotebookPage , spot_details : np . ndarray ) -> NotebookPage : \"\"\" This gets the origin of each tile such that a global coordinate system can be built. See `'stitch'` section of `notebook_comments.json` file for description of the variables in the page. Args: config: Dictionary obtained from `'stitch'` section of config file. nbp_basic: `basic_info` notebook page spot_details: `int [n_spots x 7]`. `spot_details[s]` is `[tile, round, channel, isolated, y, x, z]` of spot `s`. This is saved in the find_spots notebook page i.e. `nb.find_spots.spot_details`. Returns: `NotebookPage[stitch]` - Page contains information about how tiles were stitched together to give global coordinates. \"\"\" nbp_debug = NotebookPage ( \"stitch\" ) # NOTE that directions should actually be 'north' and 'east' directions = [ 'south' , 'west' ] coords = [ 'y' , 'x' , 'z' ] shifts = get_shifts_to_search ( config , nbp_basic , nbp_debug ) if not nbp_basic . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 # initialise variables to store shift info shift_info = { 'south' : {}, 'west' : {}} for j in directions : shift_info [ j ][ 'pairs' ] = np . zeros (( 0 , 2 ), dtype = int ) shift_info [ j ][ 'shifts' ] = np . zeros (( 0 , 3 ), dtype = int ) shift_info [ j ][ 'score' ] = np . zeros (( 0 , 1 ), dtype = float ) shift_info [ j ][ 'score_thresh' ] = np . zeros (( 0 , 1 ), dtype = float ) # find shifts between overlapping tiles c = nbp_basic . ref_channel r = nbp_basic . ref_round t_neighb = { 'south' : [], 'west' : []} # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy with tqdm ( total = 2 * len ( nbp_basic . use_tiles )) as pbar : pbar . set_description ( f \"Finding overlap between tiles in round { r } (ref_round)\" ) for t in nbp_basic . use_tiles : # align to south neighbour followed by west neighbour t_neighb [ 'south' ] = np . where ( np . sum ( nbp_basic . tilepos_yx == nbp_basic . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] t_neighb [ 'west' ] = np . where ( np . sum ( nbp_basic . tilepos_yx == nbp_basic . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] for j in directions : pbar . set_postfix ({ 'tile' : t , 'direction' : j }) if t_neighb [ j ] in nbp_basic . use_tiles : shift , score , score_thresh = compute_shift ( spot_yxz ( spot_details , t , r , c ), spot_yxz ( spot_details , t_neighb [ j ][ 0 ], r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 3 ] shift_info [ j ][ 'pairs' ] = np . append ( shift_info [ j ][ 'pairs' ], np . array ([ t , t_neighb [ j ][ 0 ]]) . reshape ( 1 , 2 ), axis = 0 ) shift_info [ j ][ 'shifts' ] = np . append ( shift_info [ j ][ 'shifts' ], np . array ( shift ) . reshape ( 1 , 3 ), axis = 0 ) shift_info [ j ][ 'score' ] = np . append ( shift_info [ j ][ 'score' ], np . array ( score ) . reshape ( 1 , 1 ), axis = 0 ) shift_info [ j ][ 'score_thresh' ] = np . append ( shift_info [ j ][ 'score_thresh' ], np . array ( score_thresh ) . reshape ( 1 , 1 ), axis = 0 ) good_shifts = ( shift_info [ j ][ 'score' ] > shift_info [ j ][ 'score_thresh' ]) . flatten () if np . sum ( good_shifts ) >= 3 : # once found shifts, refine shifts to be searched around these for i in range ( len ( coords )): shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][ good_shifts , i ]) pbar . update ( 1 ) pbar . close () # amend shifts for which score fell below score_thresh for j in directions : good_shifts = ( shift_info [ j ][ 'score' ] > shift_info [ j ][ 'score_thresh' ]) . flatten () for i in range ( len ( coords )): # change shift search to be near good shifts found # this will only do something if 3>sum(good_shifts)>0, otherwise will have been done in previous loop. if np . sum ( good_shifts ) > 0 : shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][ good_shifts , i ]) elif good_shifts . size > 0 : shifts [ j ][ coords [ i ]] = update_shifts ( shifts [ j ][ coords [ i ]], shift_info [ j ][ 'shifts' ][:, i ]) # add outlier variable to shift_info to keep track of those shifts which are changed. shift_info [ j ][ 'outlier_shifts' ] = shift_info [ j ][ 'shifts' ] . copy () shift_info [ j ][ 'outlier_score' ] = shift_info [ j ][ 'score' ] . copy () shift_info [ j ][ 'outlier_shifts' ][ good_shifts , :] = 0 shift_info [ j ][ 'outlier_score' ][ good_shifts , :] = 0 if ( np . sum ( good_shifts ) < 2 and len ( good_shifts ) > 4 ) or ( np . sum ( good_shifts ) == 0 and len ( good_shifts ) > 0 ): warnings . warn ( f \" { len ( good_shifts ) - np . sum ( good_shifts ) } / { len ( good_shifts ) } \" f \" of shifts fell below score threshold\" ) for i in np . where ( good_shifts == False )[ 0 ]: t = shift_info [ j ][ 'pairs' ][ i , 0 ] t_neighb = shift_info [ j ][ 'pairs' ][ i , 1 ] # re-find shifts that fell below threshold by only looking at shifts near to others found # Don't allow any widening so shift found must be in this range. # score_thresh given is 0, so it is not re-computed. shift_info [ j ][ 'shifts' ][ i ], shift_info [ j ][ 'score' ][ i ] = \\ compute_shift ( spot_yxz ( spot_details , t , r , c ), spot_yxz ( spot_details , t_neighb , r , c ), 0 , None , None , None , config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], None , None , z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ])[: 2 ] warnings . warn ( f \" \\n Shift from tile { t } to tile { t_neighb } changed from \\n \" f \" { shift_info [ j ][ 'outlier_shifts' ][ i ] } to { shift_info [ j ][ 'shifts' ][ i ] } .\" ) # get tile origins in global coordinates. # global coordinates are built about central tile so found this first tile_dist_to_centre = np . linalg . norm ( nbp_basic . tilepos_yx [ nbp_basic . use_tiles ] - np . mean ( nbp_basic . tilepos_yx , axis = 0 ), axis = 1 ) centre_tile = nbp_basic . use_tiles [ tile_dist_to_centre . argmin ()] tile_origin = get_tile_origin ( shift_info [ 'south' ][ 'pairs' ], shift_info [ 'south' ][ 'shifts' ], shift_info [ 'west' ][ 'pairs' ], shift_info [ 'west' ][ 'shifts' ], nbp_basic . n_tiles , centre_tile ) if nbp_basic . is_3d is False : tile_origin [:, 2 ] = 0 # set z coordinate to 0 for all tiles if 2d # add tile origin to debugging notebook so don't have whole page for one variable, # and need to add other rounds to it in registration stage anyway. nbp_debug . tile_origin = tile_origin # save all shift info to debugging page for j in directions : nbp_debug . __setattr__ ( j + '_' + 'final_shift_search' , np . zeros (( 3 , 3 ), dtype = int )) nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 0 ] = \\ [ np . min ( shifts [ j ][ key ]) for key in shifts [ j ] . keys ()] nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 1 ] = [ np . max ( shifts [ j ][ key ]) for key in shifts [ j ] . keys ()] nbp_debug . __getattribute__ ( j + '_' + 'final_shift_search' )[:, 2 ] = \\ nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[:, 2 ] for var in shift_info [ j ] . keys (): nbp_debug . __setattr__ ( j + '_' + var , shift_info [ j ][ var ]) return nbp_debug","title":"stitch()"},{"location":"code/plot/call_spots/","text":"Bleed Matrix view_bleed_matrix Diagnostic to plot bleed_matrix . If config['call_spots']['bleed_matrix_method'] is 'single' , a single bleed_matrix will be plotted. If it is 'separate' , one will be shown for each round. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required Source code in coppafish/plot/call_spots/bleed_matrix.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , nb : Notebook ): \"\"\" Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`, a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape single_bm = ( color_norm == color_norm [ 0 ]) . all () if single_bm : bleed_matrix = [ nb . call_spots . bleed_matrix [ 0 ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )]] subplot_row_columns = [ 1 , 1 ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] fig_size = ( 9 , 5 ) else : bleed_matrix = [ nb . call_spots . bleed_matrix [ r ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )] for r in range ( n_use_rounds )] if n_use_rounds <= 3 : subplot_row_columns = [ n_use_rounds , 1 ] else : n_cols = int ( np . ceil ( n_use_rounds / 4 )) # at most 4 rows subplot_row_columns = [ int ( np . ceil ( n_use_rounds / n_cols )), n_cols ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.92 ] fig_size = ( 12 , 7 ) n_use_dyes = bleed_matrix [ 0 ] . shape [ 1 ] # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes # i.e. normalisation just affected by channel not by dye. color_norm = [ np . expand_dims ( color_norm [ r ], 1 ) for r in range ( n_use_rounds )] super () . __init__ ( bleed_matrix , color_norm , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) if nb . basic_info . dye_names is None : self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = nb . basic_info . use_dyes ) else : self . fig . subplots_adjust ( bottom = 0.15 ) self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) if single_bm : self . ax [ 0 ] . set_title ( 'Bleed Matrix' ) self . ax [ 0 ] . set_ylabel ( 'Color Channel' ) self . ax [ 0 ] . set_xlabel ( 'Dyes' ) else : for i in range ( n_use_rounds ): self . ax [ i ] . set_title ( f 'Round { nb . basic_info . use_rounds [ i ] } ' , size = 8 ) plt . suptitle ( \"Bleed Matrices\" , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) self . fig . supxlabel ( 'Dyes' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . change_norm () # initialise with method = 'norm' plt . show () view_bled_codes Diagnostic to plot bleed_matrix . If config['call_spots']['bleed_matrix_method'] is 'single' , a single bleed_matrix will be plotted. If it is 'separate' , one will be shown for each round. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required Source code in coppafish/plot/call_spots/bleed_matrix.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , nb : Notebook ): \"\"\" Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`, a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape single_bm = ( color_norm == color_norm [ 0 ]) . all () if single_bm : bleed_matrix = [ nb . call_spots . bleed_matrix [ 0 ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )]] subplot_row_columns = [ 1 , 1 ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] fig_size = ( 9 , 5 ) else : bleed_matrix = [ nb . call_spots . bleed_matrix [ r ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )] for r in range ( n_use_rounds )] if n_use_rounds <= 3 : subplot_row_columns = [ n_use_rounds , 1 ] else : n_cols = int ( np . ceil ( n_use_rounds / 4 )) # at most 4 rows subplot_row_columns = [ int ( np . ceil ( n_use_rounds / n_cols )), n_cols ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.92 ] fig_size = ( 12 , 7 ) n_use_dyes = bleed_matrix [ 0 ] . shape [ 1 ] # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes # i.e. normalisation just affected by channel not by dye. color_norm = [ np . expand_dims ( color_norm [ r ], 1 ) for r in range ( n_use_rounds )] super () . __init__ ( bleed_matrix , color_norm , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) if nb . basic_info . dye_names is None : self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = nb . basic_info . use_dyes ) else : self . fig . subplots_adjust ( bottom = 0.15 ) self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) if single_bm : self . ax [ 0 ] . set_title ( 'Bleed Matrix' ) self . ax [ 0 ] . set_ylabel ( 'Color Channel' ) self . ax [ 0 ] . set_xlabel ( 'Dyes' ) else : for i in range ( n_use_rounds ): self . ax [ i ] . set_title ( f 'Round { nb . basic_info . use_rounds [ i ] } ' , size = 8 ) plt . suptitle ( \"Bleed Matrices\" , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) self . fig . supxlabel ( 'Dyes' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . change_norm () # initialise with method = 'norm' plt . show () Spot Colors view_codes Diagnostic to compare spot_color to bled_code of predicted gene. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' Source code in coppafish/plot/call_spots/spot_colors.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' ): \"\"\" Diagnostic to compare `spot_color` to `bled_code` of predicted gene. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] self . spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () / color_norm # Get spot color after background fitting self . background_removed = False self . spot_color_pb = fit_background ( self . spot_color . T [ np . newaxis ], nb . call_spots . background_weight_shift )[ 0 ][ 0 ] . T gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] gene_color = nb . call_spots . bled_codes_ge [ gene_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () super () . __init__ ([ self . spot_color , gene_color ], color_norm , slider_pos = [ 0.85 , 0.2 , 0.01 , 0.75 ], cbar_pos = [ 0.9 , 0.2 , 0.03 , 0.75 ]) self . ax [ 0 ] . set_title ( f 'Spot { spot_no } : match { str ( np . around ( spot_score , 2 )) } ' f 'to { gene_name } ' ) self . ax [ 1 ] . set_title ( f 'Predicted code for Gene { gene_no } : { gene_name } ' ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 1 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ])) self . ax [ 1 ] . set_xticklabels ([ ' {:.0f} ( {:.2f} )' . format ( r , nb . call_spots . gene_efficiency [ gene_no , r ]) for r in nb . basic_info . use_rounds ]) self . ax [ 1 ] . set_xlabel ( 'Round (Gene Efficiency)' ) self . fig . supylabel ( 'Color Channel' ) intense_gene_cr = np . where ( gene_color > self . intense_gene_thresh ) for i in range ( len ( intense_gene_cr [ 0 ])): for j in range ( 2 ): # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( intense_gene_cr [ 1 ][ i ] - 0.5 , intense_gene_cr [ 0 ][ i ] - 0.5 ), 1 , 1 , fill = False , ec = \"lime\" , linestyle = ':' , lw = 2 ) self . ax [ j ] . add_patch ( rectangle ) self . background_button_ax = self . fig . add_axes ([ 0.85 , 0.1 , 0.1 , 0.05 ]) self . background_button = Button ( self . background_button_ax , 'Background' , hovercolor = '0.275' ) self . background_button . label . set_color ( self . norm_button_color ) self . background_button . on_clicked ( self . change_background ) self . change_norm () # initialise with method = 'norm' plt . show () view_spot Diagnostic to show intensity of each color channel / round in neighbourhood of spot. Will show a grid of n_use_channels x n_use_rounds subplots. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' im_size int Radius of image to be plotted for each channel/round. 8 Source code in coppafish/plot/call_spots/spot_colors.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' , im_size : int = 8 ): \"\"\" Diagnostic to show intensity of each color channel / round in neighbourhood of spot. Will show a grid of `n_use_channels x n_use_rounds` subplots. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: Radius of image to be plotted for each channel/round. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : config = nb . get_config ()[ 'thresholds' ] page_name = 'omp' spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] gene_color = nb . call_spots . bled_codes_ge [ gene_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () . flatten () n_use_channels , n_use_rounds = color_norm . shape color_norm = [ val for val in color_norm . flatten ()] spot_yxz_global = spot_yxz + nb . stitch . tile_origin [ t ] im_size = [ im_size , im_size ] # Useful for debugging to have different im_size_y, im_size_x. # Subtlety here, may have y-axis flipped, but I think it is correct: # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct. # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5]) # will show the value 1 at max_y, min_x+1. im_yxz = np . array ( np . meshgrid ( np . arange ( spot_yxz [ 0 ] - im_size [ 0 ], spot_yxz [ 0 ] + im_size [ 0 ] + 1 )[:: - 1 ], np . arange ( spot_yxz [ 1 ] - im_size [ 1 ], spot_yxz [ 1 ] + im_size [ 1 ] + 1 ), spot_yxz [ 2 ]), dtype = np . int16 ) . T . reshape ( - 1 , 3 ) im_diameter = [ 2 * im_size [ 0 ] + 1 , 2 * im_size [ 1 ] + 1 ] spot_colors = get_spot_colors ( im_yxz , t , nb . register . transform , nb . file_names , nb . basic_info ) spot_colors = np . moveaxis ( spot_colors , 1 , 2 ) # put round as the last axis to match color_norm spot_colors = spot_colors . reshape ( im_yxz . shape [ 0 ], - 1 ) # reshape cr_images = [ spot_colors [:, i ] . reshape ( im_diameter [ 0 ], im_diameter [ 1 ]) / color_norm [ i ] for i in range ( spot_colors . shape [ 1 ])] subplot_adjust = [ 0.07 , 0.775 , 0.075 , 0.92 ] super () . __init__ ( cr_images , color_norm , subplot_row_columns = [ n_use_channels , n_use_rounds ], subplot_adjust = subplot_adjust , fig_size = ( 13 , 8 )) # set x, y coordinates to be those of the global coordinate system plot_extent = [ im_yxz [:, 1 ] . min () - 0.5 + nb . stitch . tile_origin [ t , 1 ], im_yxz [:, 1 ] . max () + 0.5 + nb . stitch . tile_origin [ t , 1 ], im_yxz [:, 0 ] . min () - 0.5 + nb . stitch . tile_origin [ t , 0 ], im_yxz [:, 0 ] . max () + 0.5 + nb . stitch . tile_origin [ t , 0 ]] for i in range ( self . n_images ): # Add cross-hair if gene_color [ i ] > self . intense_gene_thresh : cross_hair_color = 'lime' # different color if expected large intensity linestyle = '--' self . ax [ i ] . tick_params ( color = 'lime' , labelcolor = 'lime' ) for spine in self . ax [ i ] . spines . values (): spine . set_edgecolor ( 'lime' ) else : cross_hair_color = 'k' linestyle = ':' self . ax [ i ] . axes . plot ([ spot_yxz_global [ 1 ], spot_yxz_global [ 1 ]], [ plot_extent [ 2 ], plot_extent [ 3 ]], cross_hair_color , linestyle = linestyle , lw = 1 ) self . ax [ i ] . axes . plot ([ plot_extent [ 0 ], plot_extent [ 1 ]], [ spot_yxz_global [ 0 ], spot_yxz_global [ 0 ]], cross_hair_color , linestyle = linestyle , lw = 1 ) self . im [ i ] . set_extent ( plot_extent ) self . ax [ i ] . tick_params ( labelbottom = False , labelleft = False ) # Add axis labels to subplots of far left column or bottom row if i % n_use_rounds == 0 : self . ax [ i ] . set_ylabel ( f ' { nb . basic_info . use_channels [ int ( i / n_use_rounds )] } ' ) if i >= self . n_images - n_use_rounds : r = nb . basic_info . use_rounds [ i - ( self . n_images - n_use_rounds )] self . ax [ i ] . set_xlabel ( ' {:.0f} ( {:.2f} )' . format ( r , nb . call_spots . gene_efficiency [ gene_no , r ])) self . ax [ 0 ] . set_xticks ([ spot_yxz_global [ 1 ]]) self . ax [ 0 ] . set_yticks ([ spot_yxz_global [ 0 ]]) self . fig . supylabel ( 'Color Channel' , size = 14 ) self . fig . supxlabel ( 'Round (Gene Efficiency)' , size = 14 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) plt . suptitle ( f 'Spot { spot_no } : match { str ( np . around ( spot_score , decimals = 2 )) } ' f 'to { gene_name } ' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 , size = 16 ) self . change_norm () plt . show () view_intensity Diagnostic to show how intensity is computed from spot_color . Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' Source code in coppafish/plot/call_spots/spot_colors.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' ): \"\"\" Diagnostic to show how intensity is computed from `spot_color`. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] else : page_name = 'ref_spots' intensity_saved = nb . __getattribute__ ( page_name ) . intensity [ spot_no ] intensity_thresh = get_intensity_thresh ( nb ) spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () / color_norm subplot_adjust = [ 0.07 , 0.775 , 0.1 , 0.91 ] super () . __init__ ([ spot_color ], color_norm , subplot_adjust = subplot_adjust ) if intensity_saved > intensity_thresh : color = 'w' else : color = 'r' spot_color_symbol = r \"$\\mathbf{\\zeta_s}$\" intensity_symbol = r \"$\\chi_s$, (median of $\\max_c\\zeta_{s_ {rc} }$ indicated in green)\" self . ax [ 0 ] . set_title ( f 'Spot Color, { spot_color_symbol } , for spot { spot_no } \\n ' f 'Intensity, { intensity_symbol } = { str ( np . around ( intensity_saved , 3 )) } ' , color = color ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 0 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ]), labels = nb . basic_info . use_rounds ) self . ax [ 0 ] . set_xlabel ( 'Round' ) self . fig . supylabel ( 'Color Channel' ) # Highlight max channel in each round which contributes to intensity max_channels = np . argmax ( self . im_data [ 0 ], axis = 0 ) for r in range ( len ( nb . basic_info . use_rounds )): # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( r - 0.5 , max_channels [ r ] - 0.5 ), 1 , 1 , fill = False , ec = 'lime' , linestyle = ':' , lw = 4 ) self . ax [ 0 ] . add_patch ( rectangle ) self . change_norm () # initialise with method = 'norm' plt . show () ColorPlotBase This is the base class for plots with multiple subplots and with a slider to change the color axis and a button to change the normalisation. After initialising, the function change_norm() should be run to plot normalised images. This will change self.method from 'raw' to 'norm' . Parameters: Name Type Description Default images List float [n_images] Each image is n_y x n_x (x n_z) . This is the normalised image. There will be a subplot for each image and if it is 3D, the first z-plane will be set as the starting data, self.im_data while the full 3d data will be saved as self.im_data_3d . required norm_factor Optional [ Union [ np . ndarray , List ]] float [n_images] norm_factor[i] is the value to multiply images[i] to give raw image. norm_factor[i] is either an integer or an array of same dimensions as image[i] . If a single norm_factor given, assume same for each image. required subplot_row_columns Optional [ List ] [n_rows, n_columns] The subplots will be arranged into n_rows and n_columns . If not given, n_columns will be 1. None fig_size Optional [ Tuple ] [width, height] Size of figure to plot in inches. If not given, will be set to (9, 5) . None subplot_adjust Optional [ List ] [left, right, bottom, top] The position of the sides of the subplots in the figure. I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that. If not given, will be set to [0.07, 0.775, 0.095, 0.94] . None cbar_pos Optional [ List ] [left, bottom, width, height] Position of color axis. If not given, will be set to [0.9, 0.15, 0.03, 0.8] . None slider_pos Optional [ List ] [left, bottom, width, height] Position of slider that controls color axis. If not given, will be set to [0.85, 0.15, 0.01, 0.8] . None button_pos Optional [ List ] [left, bottom, width, height] Position of button which triggers change of normalisation. If not given, will be set to [0.85, 0.02, 0.1, 0.05] . None Source code in coppafish/plot/call_spots/spot_colors.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def __init__ ( self , images : List , norm_factor : Optional [ Union [ np . ndarray , List ]], subplot_row_columns : Optional [ List ] = None , fig_size : Optional [ Tuple ] = None , subplot_adjust : Optional [ List ] = None , cbar_pos : Optional [ List ] = None , slider_pos : Optional [ List ] = None , button_pos : Optional [ List ] = None ): \"\"\" This is the base class for plots with multiple subplots and with a slider to change the color axis and a button to change the normalisation. After initialising, the function `change_norm()` should be run to plot normalised images. This will change `self.method` from `'raw'` to `'norm'`. Args: images: `float [n_images]` Each image is `n_y x n_x (x n_z)`. This is the normalised image. There will be a subplot for each image and if it is 3D, the first z-plane will be set as the starting data, `self.im_data` while the full 3d data will be saved as `self.im_data_3d`. norm_factor: `float [n_images]` `norm_factor[i]` is the value to multiply `images[i]` to give raw image. `norm_factor[i]` is either an integer or an array of same dimensions as `image[i]`. If a single `norm_factor` given, assume same for each image. subplot_row_columns: `[n_rows, n_columns]` The subplots will be arranged into `n_rows` and `n_columns`. If not given, `n_columns` will be 1. fig_size: `[width, height]` Size of figure to plot in inches. If not given, will be set to `(9, 5)`. subplot_adjust: `[left, right, bottom, top]` The position of the sides of the subplots in the figure. I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that. If not given, will be set to `[0.07, 0.775, 0.095, 0.94]`. cbar_pos: `[left, bottom, width, height]` Position of color axis. If not given, will be set to `[0.9, 0.15, 0.03, 0.8]`. slider_pos: `[left, bottom, width, height]` Position of slider that controls color axis. If not given, will be set to `[0.85, 0.15, 0.01, 0.8]`. button_pos: `[left, bottom, width, height]` Position of button which triggers change of normalisation. If not given, will be set to `[0.85, 0.02, 0.1, 0.05]`. \"\"\" # When bled code of a gene more than this, that particular round/channel will be highlighted in plots self . intense_gene_thresh = 0.2 self . n_images = len ( images ) if subplot_row_columns is None : subplot_row_columns = [ self . n_images , 1 ] # Default positions if fig_size is None : fig_size = ( 9 , 5 ) if subplot_adjust is None : subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] if cbar_pos is None : cbar_pos = [ 0.9 , 0.15 , 0.03 , 0.8 ] if slider_pos is None : self . slider_pos = [ 0.85 , 0.15 , 0.01 , 0.8 ] else : self . slider_pos = slider_pos if button_pos is None : button_pos = [ 0.85 , 0.02 , 0.1 , 0.05 ] if not isinstance ( norm_factor , list ): # allow for different norm for each image if norm_factor is None : self . color_norm = None else : self . color_norm = [ norm_factor , ] * self . n_images else : self . color_norm = norm_factor self . im_data = [ val for val in images ] # put in order channels, rounds self . method = 'raw' if self . color_norm is not None else 'norm' if self . color_norm is None : self . caxis_info = { 'norm' : {}} else : self . caxis_info = { 'norm' : {}, 'raw' : {}} for key in self . caxis_info : if key == 'norm' : im_data = self . im_data self . caxis_info [ key ][ 'format' ] = ' %.2f ' else : im_data = [ self . im_data [ i ] * self . color_norm [ i ] for i in range ( self . n_images )] self . caxis_info [ key ][ 'format' ] = ' %.0f ' self . caxis_info [ key ][ 'min' ] = np . min ([ im . min () for im in im_data ] + [ - 1e-20 ]) self . caxis_info [ key ][ 'max' ] = np . max ([ im . max () for im in im_data ] + [ 1e-20 ]) self . caxis_info [ key ][ 'max' ] = np . max ([ self . caxis_info [ key ][ 'max' ], - self . caxis_info [ key ][ 'min' ]]) # have equal either side of zero so small negatives don't look large self . caxis_info [ key ][ 'min' ] = - self . caxis_info [ key ][ 'max' ] self . caxis_info [ key ][ 'clims' ] = [ self . caxis_info [ key ][ 'min' ], self . caxis_info [ key ][ 'max' ]] # cmap_norm is so cmap is white at 0. self . caxis_info [ key ][ 'cmap_norm' ] = \\ matplotlib . colors . TwoSlopeNorm ( vmin = self . caxis_info [ key ][ 'min' ], vcenter = 0 , vmax = self . caxis_info [ key ][ 'max' ]) self . fig , self . ax = plt . subplots ( subplot_row_columns [ 0 ], subplot_row_columns [ 1 ], figsize = fig_size , sharex = True , sharey = True ) if self . n_images == 1 : self . ax = [ self . ax ] # need it to be a list elif subplot_row_columns [ 0 ] > 1 and subplot_row_columns [ 1 ] > 1 : self . ax = self . ax . flatten () # only have 1 ax index oob_axes = np . arange ( self . n_images , subplot_row_columns [ 0 ] * subplot_row_columns [ 1 ]) if oob_axes . size > 0 : for i in oob_axes : self . fig . delaxes ( self . ax [ i ]) # delete excess subplots self . ax = self . ax [: self . n_images ] self . fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) self . im = [ None ] * self . n_images if self . im_data [ 0 ] . ndim == 3 : # For 3D data, start by showing just the first plane self . im_data_3d = self . im_data . copy () self . im_data = [ val [:, :, 0 ] for val in self . im_data_3d ] if self . color_norm is not None : self . color_norm_3d = self . color_norm . copy () self . color_norm = [ val [:, :, 0 ] for val in self . color_norm_3d ] else : self . im_data_3d = None self . color_norm_3d = None # initialise plots with a zero array for i in range ( self . n_images ): self . im [ i ] = self . ax [ i ] . imshow ( np . zeros ( self . im_data [ 0 ] . shape [: 2 ]), cmap = \"seismic\" , aspect = 'auto' , norm = self . caxis_info [ self . method ][ 'cmap_norm' ]) cbar_ax = self . fig . add_axes ( cbar_pos ) # left, bottom, width, height self . fig . colorbar ( self . im [ 0 ], cax = cbar_ax ) self . slider_ax = self . fig . add_axes ( self . slider_pos ) self . color_slider = None if self . color_norm is not None : self . norm_button_color = 'white' self . norm_button_color_press = 'red' if self . method == 'raw' : current_color = self . norm_button_color_press else : current_color = self . norm_button_color self . norm_button_ax = self . fig . add_axes ( button_pos ) self . norm_button = Button ( self . norm_button_ax , 'Norm' , hovercolor = '0.275' ) self . norm_button . label . set_color ( current_color ) self . norm_button . on_clicked ( self . change_norm ) Dot Product view_score This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically the normalised inverse variance). The bottom row shows the contribution from background and genes to the variance. The iteration as well as the alpha and beta parameters used to compute the weight can be changed through the text boxes. If the weight plot is clicked on, the view_weight plot will open for the current iteration. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' g Optional [ int ] Gene to view dot product calculation for. If left as None , will show the gene with the largest dot product score. None iter int Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted ( iter=0 will have only background fit, iter=1 will have background + 1 gene etc.). The score saved as nb.ref_spots.score can be viewed with iter=0 . 0 omp_fit_info Optional [ List ] This is a list containing [track_info, bled_codes, dp_thresh] . It is only ever used to call this function from view_omp_fit . None check_weight bool When this is True , we raise an error if weight computed here is different to that computed with get_track_info . False Source code in coppafish/plot/call_spots/dot_product.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , g : Optional [ int ] = None , iter : int = 0 , omp_fit_info : Optional [ List ] = None , check_weight : bool = False ): \"\"\" This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically the normalised inverse variance). The bottom row shows the contribution from background and genes to the variance. The iteration as well as the alpha and beta parameters used to compute the weight can be changed through the text boxes. If the weight plot is clicked on, the `view_weight` plot will open for the current iteration. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. g: Gene to view dot product calculation for. If left as `None`, will show the gene with the largest dot product score. iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (`iter=0` will have only background fit, `iter=1` will have background + 1 gene etc.). The score saved as `nb.ref_spots.score` can be viewed with `iter=0`. omp_fit_info: This is a list containing `[track_info, bled_codes, dp_thresh]`. It is only ever used to call this function from `view_omp_fit`. check_weight: When this is `True`, we raise an error if weight computed here is different to that computed with `get_track_info`. \"\"\" self . spot_no = spot_no if omp_fit_info is None : self . track_info , self . bled_codes , self . dp_thresh = get_track_info ( nb , spot_no , method ) else : self . track_info , self . bled_codes , self . dp_thresh = omp_fit_info self . n_genes , self . n_rounds_use , self . n_channels_use = self . bled_codes . shape # allow to view dot product with background self . bled_codes = np . append ( self . bled_codes , self . track_info [ 'background_codes' ], axis = 0 ) self . n_genes_all = self . bled_codes . shape [ 0 ] self . spot_color = self . track_info [ 'residual' ][ 0 ] # Get saved values if anchor method if method . lower () != 'omp' : self . g_saved = nb . ref_spots . gene_no [ spot_no ] if self . track_info [ 'gene_added' ][ 2 ] < self . n_genes : # Possibility best gene will be background here, but impossible for saved best gene to be background if self . track_info [ 'gene_added' ][ 2 ] != self . g_saved and check_weight : raise ValueError ( f \" \\n Best gene saved was { self . g_saved } but with parameters used here, it \" f \"was { self . track_info [ 'gene_added' ][ 2 ] } . \\n Ensure that alpha and beta in \" f \"config['call_spots'] have not been changed. \\n \" f \"Set check_weight=False to skip this error.\" ) self . dp_val_saved = nb . ref_spots . score [ spot_no ] config_name = 'call_spots' else : self . g_saved = None self . dp_val_saved = None config_name = 'omp' # Get default dot product params config = nb . get_config () self . alpha = config [ config_name ][ 'alpha' ] self . beta = config [ config_name ][ 'beta' ] self . dp_norm_shift = nb . call_spots . dp_norm_shift self . check_weight = check_weight self . n_iter = self . track_info [ 'residual' ] . shape [ 0 ] - 2 # first two indices in track is not added gene if iter >= self . n_iter or iter < 0 : warnings . warn ( f \"Only { self . n_iter } iterations for this pixel but iter= { iter } , \" f \"setting iter = { self . n_iter - 1 } .\" ) iter = self . n_iter - 1 self . iter = iter if g is None : g = self . track_info [ 'gene_added' ][ 2 + iter ] self . g = g self . gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds # Initialize data self . dp_val = None self . dp_weight_val = None self . im_data = None self . update_data () # Initialize plot self . title = None self . vmax = None self . get_cax_lim () self . fig = plt . figure ( figsize = ( 16 , 7 )) ax1 = self . fig . add_subplot ( 2 , 4 , 1 ) ax2 = self . fig . add_subplot ( 2 , 4 , 5 ) ax3 = self . fig . add_subplot ( 2 , 4 , 2 ) ax4 = self . fig . add_subplot ( 2 , 4 , 6 ) ax5 = self . fig . add_subplot ( 1 , 4 , 3 ) ax6 = self . fig . add_subplot ( 2 , 4 , 4 ) ax7 = self . fig . add_subplot ( 2 , 4 , 8 ) self . ax = [ ax1 , ax2 , ax3 , ax4 , ax5 , ax6 , ax7 ] self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . subplot_adjust = [ 0.05 , 0.87 , 0.05 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * len ( self . ax ) self . set_up_plots () self . set_titles () self . add_rectangles () # Text boxes to change parameters text_box_labels = [ 'Gene' , 'Iteration' , r '$\\alpha$' , r '$\\beta$' , r 'dp_shift, $\\lambda_d$' ] text_box_values = [ self . g , self . iter , self . alpha , self . beta , self . dp_norm_shift ] text_box_funcs = [ self . update_g , self . update_iter , self . update_alpha , self . update_beta , self . update_dp_norm_shift ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 3 ] - 0.15 * ( i + 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Make so if click on weight plot, it opens view_weight self . nb = nb self . method = method self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_weight ) plt . show () Weight view_weight This produces at least 5 plots which show how the weight used in the dot product score was calculated. The iteration as well as the alpha and beta parameters used to compute the weight can be changed with the text boxes. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' iter int Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted ( iter=0 will have only background fit, iter=1 will have background + 1 gene etc.). The score saved as nb.ref_spots.score can be viewed with iter=0 . 0 score_info Optional [ List ] This is a list containing [track_info, bled_codes, weight_vmax] . It is only ever used to call this function from view_score . None check_weight bool When this is True , we raise an error if weight computed here is different to that computed with get_track_info . True Source code in coppafish/plot/call_spots/weight.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , iter : int = 0 , alpha : Optional [ float ] = None , beta : Optional [ float ] = None , score_info : Optional [ List ] = None , check_weight : bool = True ): \"\"\" This produces at least 5 plots which show how the weight used in the dot product score was calculated. The iteration as well as the alpha and beta parameters used to compute the weight can be changed with the text boxes. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (`iter=0` will have only background fit, `iter=1` will have background + 1 gene etc.). The score saved as `nb.ref_spots.score` can be viewed with `iter=0`. score_info: This is a list containing `[track_info, bled_codes, weight_vmax]`. It is only ever used to call this function from `view_score`. check_weight: When this is `True`, we raise an error if weight computed here is different to that computed with `get_track_info`. \"\"\" self . spot_no = spot_no if score_info is None : self . track_info , self . bled_codes = get_track_info ( nb , spot_no , method )[: 2 ] self . weight_vmax = None else : self . track_info , self . bled_codes , self . weight_vmax = score_info self . n_genes , self . n_rounds_use , self . n_channels_use = self . bled_codes . shape # allow to view dot product with background self . bled_codes = np . append ( self . bled_codes , self . track_info [ 'background_codes' ], axis = 0 ) self . n_genes_all = self . bled_codes . shape [ 0 ] self . spot_color = self . track_info [ 'residual' ][ 0 ] if method . lower () == 'omp' : config_name = 'omp' else : config_name = 'call_spots' # Get default params config = nb . get_config () if alpha is None : alpha = config [ config_name ][ 'alpha' ] if beta is None : beta = config [ config_name ][ 'beta' ] self . alpha = alpha self . beta = beta self . check_weight = check_weight self . n_iter = self . track_info [ 'residual' ] . shape [ 0 ] - 2 # first two indices in track is not added gene if iter >= self . n_iter or iter < 0 : warnings . warn ( f \"Only { self . n_iter } iterations for this pixel but iter= { iter } , \" f \"setting iter = { self . n_iter - 1 } .\" ) iter = self . n_iter - 1 self . iter = iter self . gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds # Initialize data self . n_plots = self . n_plots_top_row + self . n_iter self . update_data () # Initialize plots self . vmax = None self . get_cax_lim () n_rows = 2 n_cols = int ( np . max ([ self . n_plots_top_row , self . n_iter ])) self . fig = plt . figure ( figsize = ( 16 , 7 )) self . ax = [] for i in range ( self . n_plots ): if i >= self . n_plots_top_row : # So goes to next row self . ax += [ self . fig . add_subplot ( n_rows , n_cols , n_cols + i + 1 - self . n_plots_top_row )] else : self . ax += [ self . fig . add_subplot ( n_rows , n_cols , i + 1 )] # Y and X axis are the same for all plots hence share self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . subplot_adjust = [ 0.05 , 0.87 , 0.07 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * self . n_plots self . variance_cbar = None self . set_up_plots () self . set_titles () self . add_rectangles () # Text boxes to change parameters text_box_labels = [ 'Iteration' , r '$\\alpha$' , r '$\\beta$' ] text_box_values = [ self . iter , self . alpha , self . beta ] text_box_funcs = [ self . update_iter , self . update_alpha , self . update_beta ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 3 ] - 0.15 * ( i + 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) self . nb = nb self . method = method self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_background ) plt . show () Background view_background This shows how the background coefficients were calculated. The weighted dot product is equal to weight multiplied by dot product. Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c. Also shows residual after removing background. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' check bool When this is True , we raise an error if background coefs computed here is different to that computed with get_track_info . True track_info Optional [ List ] To use when calling from view_weight . None Source code in coppafish/plot/call_spots/background.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , check : bool = True , track_info : Optional [ List ] = None ): \"\"\" This shows how the background coefficients were calculated. The weighted dot product is equal to weight multiplied by dot product. Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c. Also shows residual after removing background. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. check: When this is `True`, we raise an error if background coefs computed here is different to that computed with `get_track_info`. track_info: To use when calling from `view_weight`. \"\"\" self . spot_no = spot_no if track_info is None : self . track_info = get_track_info ( nb , spot_no , method )[ 0 ] self . color_vmax = None else : self . track_info , self . color_vmax = track_info self . n_genes_all = self . track_info [ 'coef' ][ 0 ] . size self . spot_color = self . track_info [ 'residual' ][ 0 ] self . n_rounds_use , self . n_channels_use = self . spot_color . shape self . n_genes = self . n_genes_all - self . n_channels_use self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds self . background_weight_shift = nb . call_spots . background_weight_shift self . check = check # Initialize data self . im_data = None self . update_data () hi = 5 # Initialize plots self . vmax = None self . weight_dp_max_initial = None self . get_cax_lim () self . fig = plt . figure ( figsize = ( 16 , 7 )) self . ax = [] ax1 = self . fig . add_subplot ( 2 , 5 , 1 ) ax2 = self . fig . add_subplot ( 2 , 5 , 6 ) ax3 = self . fig . add_subplot ( 1 , 5 , 2 ) ax4 = self . fig . add_subplot ( 2 , 5 , 3 ) ax5 = self . fig . add_subplot ( 2 , 5 , 8 ) ax6 = self . fig . add_subplot ( 2 , 5 , 4 ) ax7 = self . fig . add_subplot ( 2 , 5 , 9 ) ax8 = self . fig . add_subplot ( 2 , 5 , 5 ) ax9 = self . fig . add_subplot ( 2 , 5 , 10 ) self . ax = [ ax1 , ax2 , ax3 , ax4 , ax5 , ax6 , ax7 , ax8 , ax9 ] self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) # Coef plots have different x-axis self . ax [ self . coef_plot_ind [ 0 ]] . get_shared_x_axes () . join ( self . ax [ self . coef_plot_ind [ 0 ]], self . ax [ self . coef_plot_ind [ 1 ]]) # All other plots have same axis self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 : self . coef_plot_ind [ 0 ]]) self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ self . coef_plot_ind [ 1 ] + 1 :]) self . subplot_adjust = [ 0.05 , 0.97 , 0.07 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * len ( self . ax ) self . set_up_plots () self . set_titles () weight_plot_pos = self . ax [ self . weight_plot_ind ] . get_position () box_x = np . mean ([ weight_plot_pos . x0 , weight_plot_pos . x1 ]) box_y = weight_plot_pos . y0 text_ax = self . fig . add_axes ([ box_x , box_y - 0.15 , 0.05 , 0.04 ]) self . text_box = TextBox ( text_ax , r 'background_shift, $\\lambda_b$' , self . background_weight_shift , color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . update_background_weight_shift ) plt . show () Gene Counts gene_counts This shows the number of reference spots assigned to each gene which pass the quality thresholding based on the parameters score_thresh and intensity_thresh . If nb has the OMP page, then the number of omp spots will also be shown, where the quality thresholding is based on score_omp_thresh , score_omp_multiplier and intensity_thresh . There will also be a second reference spots histogram, the difference with this is that the spots were allowed to be assigned to some fake genes with bled_codes specified through fake_bled_codes . Note fake_bled_codes have dimension n_fake x nbp_basic.n_rounds x nbp_basic.n_channels not n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) . Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required fake_bled_codes Optional [ np . ndarray ] float [n_fake_genes x n_rounds x n_channels] . colors of fake genes to find dot product with Will find new gene assignment of anchor spots to new set of bled_codes which include these in addition to bled_codes_ge . By default, will have a fake gene for each round and channel such that it is \\(1\\) in round r, channel \\(c\\) and 0 everywhere else. None fake_gene_names Optional [ List [ str ]] str [n_fake_genes] . Can give name of each fake gene. If None , fake gene \\(i\\) will be called FAKE: \\(i\\) . None score_thresh Optional [ float ] Threshold for score for ref_spots. Can be changed with text box. None intensity_thresh Optional [ float ] Threshold for intensity. Can be changed with text box. None score_omp_thresh Optional [ float ] Threshold for score for omp_spots. Can be changed with text box. None score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. Can be changed with text box. None Source code in coppafish/plot/call_spots/gene_counts.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , nb : Notebook , fake_bled_codes : Optional [ np . ndarray ] = None , fake_gene_names : Optional [ List [ str ]] = None , score_thresh : Optional [ float ] = None , intensity_thresh : Optional [ float ] = None , score_omp_thresh : Optional [ float ] = None , score_omp_multiplier : Optional [ float ] = None ): \"\"\" This shows the number of reference spots assigned to each gene which pass the quality thresholding based on the parameters `score_thresh` and `intensity_thresh`. If `nb` has the *OMP* page, then the number of omp spots will also be shown, where the quality thresholding is based on `score_omp_thresh`, `score_omp_multiplier` and `intensity_thresh`. There will also be a second reference spots histogram, the difference with this is that the spots were allowed to be assigned to some fake genes with `bled_codes` specified through `fake_bled_codes`. !!! note `fake_bled_codes` have dimension `n_fake x nbp_basic.n_rounds x nbp_basic.n_channels` not `n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)`. Args: nb: *Notebook* containing at least `call_spots` page. fake_bled_codes: `float [n_fake_genes x n_rounds x n_channels]`. colors of fake genes to find dot product with Will find new gene assignment of anchor spots to new set of `bled_codes` which include these in addition to `bled_codes_ge`. By default, will have a fake gene for each round and channel such that it is $1$ in round r, channel $c$ and 0 everywhere else. fake_gene_names: `str [n_fake_genes]`. Can give name of each fake gene. If `None`, fake gene $i$ will be called FAKE:$i$. score_thresh: Threshold for score for ref_spots. Can be changed with text box. intensity_thresh: Threshold for intensity. Can be changed with text box. score_omp_thresh: Threshold for score for omp_spots. Can be changed with text box. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. Can be changed with text box. \"\"\" # Add fake genes if fake_bled_codes is None : # Default have binary fake gene for each used round and channel n_fake = len ( nb . basic_info . use_rounds ) * len ( nb . basic_info . use_channels ) fake_bled_codes = np . zeros (( n_fake , nb . basic_info . n_rounds , nb . basic_info . n_channels )) i = 0 # Cluster fake genes by channel because more likely to be a faulty channel for c in nb . basic_info . use_channels : for r in nb . basic_info . use_rounds : fake_bled_codes [ i , r , c ] = 1 i += 1 fake_gene_names = [ f 'r { r } c { c } ' for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] n_fake = fake_bled_codes . shape [ 0 ] if fake_gene_names is None : fake_gene_names = [ f 'FAKE: { i } ' for i in range ( n_fake )] self . gene_names = nb . call_spots . gene_names . tolist () + fake_gene_names self . n_genes = len ( self . gene_names ) self . n_genes_real = self . n_genes - n_fake # Do new gene assignment for anchor spots when fake genes are included spot_colors_pb , background_var = background_fitting ( nb , 'anchor' )[ 1 :] # Fit background before dot product score grc_ind = np . ix_ ( np . arange ( self . n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels ) # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds bled_codes = np . append ( nb . call_spots . bled_codes_ge , fake_bled_codes , axis = 0 ) bled_codes = bled_codes [ grc_ind ] # Ensure L2 norm of 1 for each gene norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes = bled_codes / norm_factor score , gene_no = get_dot_product_score ( spot_colors_pb , bled_codes , None , nb . call_spots . dp_norm_shift , background_var ) # Add quality thresholding info and gene assigned to for method with no fake genes and method with fake genes self . intensity = [ nb . ref_spots . intensity . astype ( np . float16 )] * 2 self . score = [ nb . ref_spots . score . astype ( np . float16 ), score . astype ( np . float16 )] self . gene_no = [ nb . ref_spots . gene_no , gene_no ] # Add current thresholds config = nb . get_config ()[ 'thresholds' ] if config [ 'intensity' ] is None : config [ 'intensity' ] = nb . call_spots . gene_efficiency_intensity_thresh if score_thresh is None : score_thresh = config [ 'score_ref' ] if intensity_thresh is None : intensity_thresh = config [ 'intensity' ] self . score_thresh = [ score_thresh ] * 2 self . intensity_thresh = intensity_thresh # Add omp gene assignment if have page if nb . has_page ( 'omp' ): if score_omp_multiplier is None : score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_omp_multiplier self . nbp_omp = nb . omp if score_omp_thresh is None : score_omp_thresh = config [ 'score_omp' ] self . score_thresh += [ score_omp_thresh ] self . score += [ omp_spot_score ( self . nbp_omp , self . score_multiplier ) . astype ( np . float16 )] self . intensity += [ self . nbp_omp . intensity . astype ( np . float16 )] self . gene_no += [ self . nbp_omp . gene_no ] self . omp = True else : self . omp = False self . n_plots = len ( self . score ) self . use = None self . update_use () # Initialise plot self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 7 )) self . subplot_adjust = [ 0.07 , 0.85 , 0.12 , 0.93 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax . set_ylabel ( r \"Number of Spots\" ) self . ax . set_xlabel ( 'Gene' ) self . ax . set_title ( f \"Number of Spots assigned to each Gene\" ) # record min and max for textbox input self . score_min = np . around ( self . score [ 0 ] . min (), 2 ) # Min score of score[1] cannot be less than this self . score_max = np . around ( self . score [ 1 ] . max (), 2 ) # Max score of score[0] cannot be more than this self . intensity_min = np . around ( self . intensity [ 0 ] . min (), 2 ) self . intensity_max = np . around ( self . intensity [ 0 ] . max (), 2 ) self . plots = [ None ] * self . n_plots default_colors = plt . rcParams [ 'axes.prop_cycle' ] . _left default_colors = default_colors [: 2 ] + default_colors [ 3 : 4 ] # default 3 is better than default 2 for omp plot for i in range ( self . n_plots ): self . plots [ i ], = self . ax . plot ( np . arange ( self . n_genes ), np . histogram ( self . gene_no [ i ][ self . use [ i ]], np . arange ( self . n_genes + 1 ) - 0.5 )[ 0 ], color = default_colors [ i ][ 'color' ]) if i == 1 : self . plots [ i ] . set_visible ( False ) self . ax . set_ylim ( 0 , None ) self . ax . set_xticks ( np . arange ( self . n_genes )) self . ax . set_xticklabels ( self . gene_names , rotation = 90 , size = 7 ) self . ax . set_xlim ( - 0.5 , self . n_genes_real - 0.5 ) # Add text box to change score multiplier text_box_labels = [ r 'Score, $\\Delta_s$' + ' \\n Threshold' , r 'Intensity, $\\chi_s$' + ' \\n Threshold' ] text_box_values = [ np . around ( self . score_thresh [ 0 ], 2 ), np . around ( self . intensity_thresh , 2 )] text_box_funcs = [ self . update_score_thresh , self . update_intensity_thresh ] if self . omp : text_box_labels += [ r 'OMP Score, $\\gamma_s$' + ' \\n Threshold' , 'Score \\n ' + r 'Multiplier, $\\rho$' ] text_box_values += [ np . around ( self . score_thresh [ 2 ], 2 ), np . around ( self . score_multiplier , 2 )] text_box_funcs += [ self . update_score_omp_thresh , self . update_score_multiplier ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 2 ] + 0.15 * ( len ( text_box_labels ) - i - 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 2.75 ]) # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Add buttons to add/remove score_dp histograms self . buttons_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.02 , self . subplot_adjust [ 3 ] - 0.25 , 0.15 , 0.3 ]) plt . axis ( 'off' ) self . button_labels = [ \"Ref Spots\" , \"Ref Spots - Fake Genes\" ] label_checked = [ True , False ] if self . omp : self . button_labels += [ \"OMP Spots\" ] label_checked += [ True ] self . buttons = CheckButtons ( self . buttons_ax , self . button_labels , label_checked ) for i in range ( self . n_plots ): self . buttons . labels [ i ] . set_fontsize ( 7 ) self . buttons . labels [ i ] . set_color ( default_colors [ i ][ 'color' ]) self . buttons . rectangles [ i ] . set_color ( 'w' ) self . buttons . on_clicked ( self . choose_plots ) plt . show () Score Calculation background_fitting ( nb , method ) Computes background using parameters in config file. Then removes this from the spot_colors . Parameters: Name Type Description Default nb Notebook Notebook containing call_spots page required method str 'omp' or 'anchor', indicating which spot_colors to use. required Returns: Type Description np . ndarray spot_colors - float [n_spots x n_rounds_use x n_channels_use] . spot_color after normalised by color_norm_factor but before background fit. np . ndarray spot_colors_pb - float [n_spots x n_rounds_use x n_channels_use] . spot_color after background removed. np . ndarray background_var - float [n_spots x n_rounds_use x n_channels_use] . inverse of the weighting used for dot product score calculation. Source code in coppafish/plot/call_spots/score_calc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def background_fitting ( nb : Notebook , method : str ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Computes background using parameters in config file. Then removes this from the `spot_colors`. Args: nb: Notebook containing call_spots page method: 'omp' or 'anchor', indicating which `spot_colors` to use. Returns: `spot_colors` - `float [n_spots x n_rounds_use x n_channels_use]`. `spot_color` after normalised by `color_norm_factor` but before background fit. `spot_colors_pb` - `float [n_spots x n_rounds_use x n_channels_use]`. `spot_color` after background removed. `background_var` - `float [n_spots x n_rounds_use x n_channels_use]`. inverse of the weighting used for dot product score calculation. \"\"\" rc_ind = np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels ) if method . lower () == 'omp' : spot_colors = np . moveaxis ( np . moveaxis ( nb . omp . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) config = nb . get_config ()[ 'omp' ] else : spot_colors = np . moveaxis ( np . moveaxis ( nb . ref_spots . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) config = nb . get_config ()[ 'call_spots' ] alpha = config [ 'alpha' ] beta = config [ 'beta' ] spot_colors = spot_colors / nb . call_spots . color_norm_factor [ rc_ind ] spot_colors_pb , background_coef , background_codes = \\ fit_background ( spot_colors , nb . call_spots . background_weight_shift ) background_codes = background_codes . reshape ( background_codes . shape [ 0 ], - 1 ) background_var = background_coef ** 2 @ background_codes ** 2 * alpha + beta ** 2 return spot_colors , spot_colors_pb , background_var get_dot_product_score ( spot_colors , bled_codes , spot_gene_no , dp_norm_shift , background_var ) Finds dot product score for each spot_color given to the gene indicated by spot_gene_no . Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds_use x n_channels_use] . colors of spots to find score of. required bled_codes np . ndarray float [n_genes x n_rounds_use x n_channels_use] . colors of genes to find dot product with. required spot_gene_no Optional [ np . ndarray ] int [n_spots] . Gene that each spot was assigned to. If None, will set spot_gene_no[s] to gene for which score was largest. required dp_norm_shift float Normalisation constant for single round used for dot product calculation. I.e. nb.call_spots.dp_norm_shift . required background_var Optional [ np . ndarray ] float [n_spots x n_rounds_use x n_channels_use] . inverse of the weighting used for dot product score calculation. required Returns: Type Description np . ndarray spot_score - float [n_spots] . Dot product score for each spot. np . ndarray spot_gene_no - will be same as input if given, otherwise will be the best gene assigned. Source code in coppafish/plot/call_spots/score_calc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_dot_product_score ( spot_colors : np . ndarray , bled_codes : np . ndarray , spot_gene_no : Optional [ np . ndarray ], dp_norm_shift : float , background_var : Optional [ np . ndarray ]) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds dot product score for each `spot_color` given to the gene indicated by `spot_gene_no`. Args: spot_colors: `float [n_spots x n_rounds_use x n_channels_use]`. colors of spots to find score of. bled_codes: `float [n_genes x n_rounds_use x n_channels_use]`. colors of genes to find dot product with. spot_gene_no: `int [n_spots]`. Gene that each spot was assigned to. If None, will set `spot_gene_no[s]` to gene for which score was largest. dp_norm_shift: Normalisation constant for single round used for dot product calculation. I.e. `nb.call_spots.dp_norm_shift`. background_var: `float [n_spots x n_rounds_use x n_channels_use]`. inverse of the weighting used for dot product score calculation. Returns: `spot_score` - `float [n_spots]`. Dot product score for each spot. `spot_gene_no` - will be same as input if given, otherwise will be the best gene assigned. \"\"\" n_spots , n_rounds_use = spot_colors . shape [: 2 ] n_genes = bled_codes . shape [ 0 ] dp_norm_shift = dp_norm_shift * np . sqrt ( n_rounds_use ) if background_var is None : weight = None else : weight = 1 / background_var scores = np . asarray ( dot_product_score ( spot_colors . reshape ( n_spots , - 1 ), bled_codes . reshape ( n_genes , - 1 ), dp_norm_shift , weight )) if spot_gene_no is None : spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( n_spots ), spot_gene_no ] return spot_score , spot_gene_no Scaled K Means view_scaled_k_means ( nb , r = 0 , check = False ) Plot to show how scaled_k_means was used to compute the bleed matrix. There will be upto 3 columns, each with 2 plots. The vector for dye \\(d\\) in the bleed_matrix is computed from all the spot round vectors whose dot product to the dye \\(d\\) vector was the highest. The boxplots in the first row show these dot product values for each dye. The second row then shows the bleed matrix at each stage of the computation. The first column shows the initial bleed matrix. The second column shows the bleed matrix after running scaled_k_means once with a score threshold of 0. The third column shows the final bleed_matrix after running scaled_k_means a second time with score_thresh for dye \\(d\\) set to the median of the scores assigned to dye \\(d\\) in the first run. Third column only present if config['call_spots']['bleed_matrix_anneal']==True . Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required r int Round of bleed matrix to view. Only relevant if config['call_spots']['bleed_matrix_method'] = 'separate' . 0 check bool If True, will raise error if bleed_matrix computed here is different to that saved in notebook False Source code in coppafish/plot/call_spots/scaled_k_means.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def view_scaled_k_means ( nb : Notebook , r : int = 0 , check : bool = False ): \"\"\" Plot to show how `scaled_k_means` was used to compute the bleed matrix. There will be upto 3 columns, each with 2 plots. The vector for dye $d$ in the `bleed_matrix` is computed from all the spot round vectors whose dot product to the dye $d$ vector was the highest. The boxplots in the first row show these dot product values for each dye. The second row then shows the bleed matrix at each stage of the computation. The first column shows the initial bleed matrix. The second column shows the bleed matrix after running `scaled_k_means` once with a score threshold of 0. The third column shows the final `bleed_matrix` after running `scaled_k_means` a second time with `score_thresh` for dye $d$ set to the median of the scores assigned to dye $d$ in the first run. Third column only present if `config['call_spots']['bleed_matrix_anneal']==True`. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. r: Round of bleed matrix to view. Only relevant if `config['call_spots']['bleed_matrix_method'] = 'separate'`. check: If True, will raise error if `bleed_matrix` computed here is different to that saved in notebook \"\"\" # Fit background to spot_colors as is done in call_reference_spots before bleed_matrix calc spot_colors = background_fitting ( nb , 'ref' )[ 1 ] # Get bleed matrix and plotting info rcd_ind = np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels , nb . basic_info . use_dyes ) initial_bleed_matrix = nb . call_spots . initial_bleed_matrix [ rcd_ind ] config = nb . get_config ()[ 'call_spots' ] if config [ 'bleed_matrix_method' ] . lower () == 'separate' : r_ind = int ( np . where ( np . asarray ( nb . basic_info . use_rounds ) == r )[ 0 ]) title_start = f \"Bleed matrix for round { r } \" else : r_ind = 0 title_start = \"Bleed matrix \" debug_info = get_bleed_matrix ( spot_colors [ nb . ref_spots . isolated ], initial_bleed_matrix , config [ 'bleed_matrix_method' ], config [ 'bleed_matrix_score_thresh' ], config [ 'bleed_matrix_min_cluster_size' ], config [ 'bleed_matrix_n_iter' ], config [ 'bleed_matrix_anneal' ], r_ind )[ 1 ] if check : if np . abs ( nb . call_spots . bleed_matrix [ rcd_ind ][ r_ind ] - debug_info [ 'bleed_matrix' ][ - 1 ]) . max () > 1e-3 : raise ValueError ( \"Bleed Matrix saved to Notebook is different from that computed here \" \"with get_bleed_matrix. \\n Make sure that the background and bleed_matrix\" \"parameters in the config file have not changed.\" ) # Make it so all bleed matrices have same L2 norm as final one bm_norm = np . linalg . norm ( debug_info [ 'bleed_matrix' ][ - 1 ]) bleed_matrix = [ bm * bm_norm / np . linalg . norm ( bm ) for bm in debug_info [ 'bleed_matrix' ]] vmax = np . max ( bleed_matrix ) # Set up plot n_plots , n_use_channels , n_use_dyes = debug_info [ 'bleed_matrix' ] . shape fig , ax = plt . subplots ( 2 , n_plots , figsize = ( 11 , 7 ), sharex = True ) ax [ 0 , 0 ] . get_shared_y_axes () . join ( ax [ 0 , 0 ], ax [ 0 , 1 ]) subplot_adjust = [ 0.075 , 0.92 , 0.08 , 0.88 ] fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) titles = [ 'Initial Bleed Matrix' , 'Bleed Matrix after Scaled K Means 1' , 'Bleed Matrix after Scaled K Means 2' ] if not config [ 'bleed_matrix_anneal' ]: titles [ 1 ] = 'Bleed Matrix after Scaled K Means' for i in range ( n_plots ): box_data = [ debug_info [ 'cluster_score' ][ i ][ debug_info [ 'cluster_ind' ][ i ] == d ] for d in range ( n_use_dyes )] bp = ax [ 0 , i ] . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) for d in range ( n_use_dyes ): ax [ 0 , i ] . text ( d + 1 , np . percentile ( box_data [ d ], 25 ), \" {:.1e} \" . format ( len ( box_data [ d ])), horizontalalignment = 'center' , color = bp [ 'medians' ][ d ] . get_color (), size = 5 , clip_on = True ) im = ax [ 1 , i ] . imshow ( bleed_matrix [ i ], extent = [ 0.5 , n_use_dyes + 0.5 , n_use_channels - 0.5 , - 0.5 ], aspect = 'auto' , vmin = 0 , vmax = vmax ) if nb . basic_info . dye_names is None : ax [ 1 , i ] . set_xticks ( ticks = np . arange ( 1 , n_use_dyes + 1 ), labels = nb . basic_info . use_dyes ) else : subplot_adjust [ 2 ] = 0.15 fig . subplots_adjust ( bottom = subplot_adjust [ 2 ]) ax [ 1 , i ] . set_xticks ( ticks = np . arange ( 1 , n_use_dyes + 1 ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) ax [ 1 , i ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) ax [ 0 , i ] . set_title ( titles [ i ], fontsize = 10 ) if i == 0 : ax [ 0 , i ] . set_ylabel ( 'Dot Product to Best Dye Vector' ) ax [ 1 , i ] . set_ylabel ( 'Channel' ) fig . supxlabel ( 'Dye' , size = 12 ) mid_point = ( subplot_adjust [ 2 ] + subplot_adjust [ 3 ]) / 2 cbar_ax = fig . add_axes ([ subplot_adjust [ 1 ] + 0.01 , subplot_adjust [ 2 ], 0.005 , mid_point - subplot_adjust [ 2 ] - 0.04 ]) # left, bottom, width, height fig . colorbar ( im , cax = cbar_ax ) plt . suptitle ( f ' { title_start } at { n_plots } different stages \\n Box plots showing the dot product of spot round ' f 'vectors with the dye vector they best matched to in the bleed matrix' , size = 11 ) ax [ 1 , 1 ] . set_title ( 'Bleed Matrix where each dye column was computed from all vectors assigned to that dye ' 'in the boxplot above' , size = 11 ) plt . show ()","title":"Call Spots"},{"location":"code/plot/call_spots/#bleed-matrix","text":"","title":"Bleed Matrix"},{"location":"code/plot/call_spots/#view_bleed_matrix","text":"Diagnostic to plot bleed_matrix . If config['call_spots']['bleed_matrix_method'] is 'single' , a single bleed_matrix will be plotted. If it is 'separate' , one will be shown for each round. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required Source code in coppafish/plot/call_spots/bleed_matrix.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , nb : Notebook ): \"\"\" Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`, a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape single_bm = ( color_norm == color_norm [ 0 ]) . all () if single_bm : bleed_matrix = [ nb . call_spots . bleed_matrix [ 0 ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )]] subplot_row_columns = [ 1 , 1 ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] fig_size = ( 9 , 5 ) else : bleed_matrix = [ nb . call_spots . bleed_matrix [ r ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )] for r in range ( n_use_rounds )] if n_use_rounds <= 3 : subplot_row_columns = [ n_use_rounds , 1 ] else : n_cols = int ( np . ceil ( n_use_rounds / 4 )) # at most 4 rows subplot_row_columns = [ int ( np . ceil ( n_use_rounds / n_cols )), n_cols ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.92 ] fig_size = ( 12 , 7 ) n_use_dyes = bleed_matrix [ 0 ] . shape [ 1 ] # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes # i.e. normalisation just affected by channel not by dye. color_norm = [ np . expand_dims ( color_norm [ r ], 1 ) for r in range ( n_use_rounds )] super () . __init__ ( bleed_matrix , color_norm , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) if nb . basic_info . dye_names is None : self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = nb . basic_info . use_dyes ) else : self . fig . subplots_adjust ( bottom = 0.15 ) self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) if single_bm : self . ax [ 0 ] . set_title ( 'Bleed Matrix' ) self . ax [ 0 ] . set_ylabel ( 'Color Channel' ) self . ax [ 0 ] . set_xlabel ( 'Dyes' ) else : for i in range ( n_use_rounds ): self . ax [ i ] . set_title ( f 'Round { nb . basic_info . use_rounds [ i ] } ' , size = 8 ) plt . suptitle ( \"Bleed Matrices\" , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) self . fig . supxlabel ( 'Dyes' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . change_norm () # initialise with method = 'norm' plt . show ()","title":"view_bleed_matrix"},{"location":"code/plot/call_spots/#view_bled_codes","text":"Diagnostic to plot bleed_matrix . If config['call_spots']['bleed_matrix_method'] is 'single' , a single bleed_matrix will be plotted. If it is 'separate' , one will be shown for each round. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required Source code in coppafish/plot/call_spots/bleed_matrix.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def __init__ ( self , nb : Notebook ): \"\"\" Diagnostic to plot `bleed_matrix`. If `config['call_spots']['bleed_matrix_method']` is `'single'`, a single `bleed_matrix` will be plotted. If it is `'separate'`, one will be shown for each round. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape single_bm = ( color_norm == color_norm [ 0 ]) . all () if single_bm : bleed_matrix = [ nb . call_spots . bleed_matrix [ 0 ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )]] subplot_row_columns = [ 1 , 1 ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] fig_size = ( 9 , 5 ) else : bleed_matrix = [ nb . call_spots . bleed_matrix [ r ][ np . ix_ ( nb . basic_info . use_channels , nb . basic_info . use_dyes )] for r in range ( n_use_rounds )] if n_use_rounds <= 3 : subplot_row_columns = [ n_use_rounds , 1 ] else : n_cols = int ( np . ceil ( n_use_rounds / 4 )) # at most 4 rows subplot_row_columns = [ int ( np . ceil ( n_use_rounds / n_cols )), n_cols ] subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.92 ] fig_size = ( 12 , 7 ) n_use_dyes = bleed_matrix [ 0 ] . shape [ 1 ] # different norm for each round, each has dims n_use_channels x 1 whereas BM dims is n_use_channels x n_dyes # i.e. normalisation just affected by channel not by dye. color_norm = [ np . expand_dims ( color_norm [ r ], 1 ) for r in range ( n_use_rounds )] super () . __init__ ( bleed_matrix , color_norm , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) if nb . basic_info . dye_names is None : self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = nb . basic_info . use_dyes ) else : self . fig . subplots_adjust ( bottom = 0.15 ) self . ax [ - 1 ] . set_xticks ( ticks = np . arange ( n_use_dyes ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) if single_bm : self . ax [ 0 ] . set_title ( 'Bleed Matrix' ) self . ax [ 0 ] . set_ylabel ( 'Color Channel' ) self . ax [ 0 ] . set_xlabel ( 'Dyes' ) else : for i in range ( n_use_rounds ): self . ax [ i ] . set_title ( f 'Round { nb . basic_info . use_rounds [ i ] } ' , size = 8 ) plt . suptitle ( \"Bleed Matrices\" , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) self . fig . supxlabel ( 'Dyes' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . change_norm () # initialise with method = 'norm' plt . show ()","title":"view_bled_codes"},{"location":"code/plot/call_spots/#spot-colors","text":"","title":"Spot Colors"},{"location":"code/plot/call_spots/#view_codes","text":"Diagnostic to compare spot_color to bled_code of predicted gene. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' Source code in coppafish/plot/call_spots/spot_colors.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' ): \"\"\" Diagnostic to compare `spot_color` to `bled_code` of predicted gene. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] self . spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () / color_norm # Get spot color after background fitting self . background_removed = False self . spot_color_pb = fit_background ( self . spot_color . T [ np . newaxis ], nb . call_spots . background_weight_shift )[ 0 ][ 0 ] . T gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] gene_color = nb . call_spots . bled_codes_ge [ gene_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () super () . __init__ ([ self . spot_color , gene_color ], color_norm , slider_pos = [ 0.85 , 0.2 , 0.01 , 0.75 ], cbar_pos = [ 0.9 , 0.2 , 0.03 , 0.75 ]) self . ax [ 0 ] . set_title ( f 'Spot { spot_no } : match { str ( np . around ( spot_score , 2 )) } ' f 'to { gene_name } ' ) self . ax [ 1 ] . set_title ( f 'Predicted code for Gene { gene_no } : { gene_name } ' ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 1 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ])) self . ax [ 1 ] . set_xticklabels ([ ' {:.0f} ( {:.2f} )' . format ( r , nb . call_spots . gene_efficiency [ gene_no , r ]) for r in nb . basic_info . use_rounds ]) self . ax [ 1 ] . set_xlabel ( 'Round (Gene Efficiency)' ) self . fig . supylabel ( 'Color Channel' ) intense_gene_cr = np . where ( gene_color > self . intense_gene_thresh ) for i in range ( len ( intense_gene_cr [ 0 ])): for j in range ( 2 ): # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( intense_gene_cr [ 1 ][ i ] - 0.5 , intense_gene_cr [ 0 ][ i ] - 0.5 ), 1 , 1 , fill = False , ec = \"lime\" , linestyle = ':' , lw = 2 ) self . ax [ j ] . add_patch ( rectangle ) self . background_button_ax = self . fig . add_axes ([ 0.85 , 0.1 , 0.1 , 0.05 ]) self . background_button = Button ( self . background_button_ax , 'Background' , hovercolor = '0.275' ) self . background_button . label . set_color ( self . norm_button_color ) self . background_button . on_clicked ( self . change_background ) self . change_norm () # initialise with method = 'norm' plt . show ()","title":"view_codes"},{"location":"code/plot/call_spots/#view_spot","text":"Diagnostic to show intensity of each color channel / round in neighbourhood of spot. Will show a grid of n_use_channels x n_use_rounds subplots. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' im_size int Radius of image to be plotted for each channel/round. 8 Source code in coppafish/plot/call_spots/spot_colors.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' , im_size : int = 8 ): \"\"\" Diagnostic to show intensity of each color channel / round in neighbourhood of spot. Will show a grid of `n_use_channels x n_use_rounds` subplots. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: Radius of image to be plotted for each channel/round. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : config = nb . get_config ()[ 'thresholds' ] page_name = 'omp' spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] gene_color = nb . call_spots . bled_codes_ge [ gene_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () . flatten () n_use_channels , n_use_rounds = color_norm . shape color_norm = [ val for val in color_norm . flatten ()] spot_yxz_global = spot_yxz + nb . stitch . tile_origin [ t ] im_size = [ im_size , im_size ] # Useful for debugging to have different im_size_y, im_size_x. # Subtlety here, may have y-axis flipped, but I think it is correct: # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct. # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5]) # will show the value 1 at max_y, min_x+1. im_yxz = np . array ( np . meshgrid ( np . arange ( spot_yxz [ 0 ] - im_size [ 0 ], spot_yxz [ 0 ] + im_size [ 0 ] + 1 )[:: - 1 ], np . arange ( spot_yxz [ 1 ] - im_size [ 1 ], spot_yxz [ 1 ] + im_size [ 1 ] + 1 ), spot_yxz [ 2 ]), dtype = np . int16 ) . T . reshape ( - 1 , 3 ) im_diameter = [ 2 * im_size [ 0 ] + 1 , 2 * im_size [ 1 ] + 1 ] spot_colors = get_spot_colors ( im_yxz , t , nb . register . transform , nb . file_names , nb . basic_info ) spot_colors = np . moveaxis ( spot_colors , 1 , 2 ) # put round as the last axis to match color_norm spot_colors = spot_colors . reshape ( im_yxz . shape [ 0 ], - 1 ) # reshape cr_images = [ spot_colors [:, i ] . reshape ( im_diameter [ 0 ], im_diameter [ 1 ]) / color_norm [ i ] for i in range ( spot_colors . shape [ 1 ])] subplot_adjust = [ 0.07 , 0.775 , 0.075 , 0.92 ] super () . __init__ ( cr_images , color_norm , subplot_row_columns = [ n_use_channels , n_use_rounds ], subplot_adjust = subplot_adjust , fig_size = ( 13 , 8 )) # set x, y coordinates to be those of the global coordinate system plot_extent = [ im_yxz [:, 1 ] . min () - 0.5 + nb . stitch . tile_origin [ t , 1 ], im_yxz [:, 1 ] . max () + 0.5 + nb . stitch . tile_origin [ t , 1 ], im_yxz [:, 0 ] . min () - 0.5 + nb . stitch . tile_origin [ t , 0 ], im_yxz [:, 0 ] . max () + 0.5 + nb . stitch . tile_origin [ t , 0 ]] for i in range ( self . n_images ): # Add cross-hair if gene_color [ i ] > self . intense_gene_thresh : cross_hair_color = 'lime' # different color if expected large intensity linestyle = '--' self . ax [ i ] . tick_params ( color = 'lime' , labelcolor = 'lime' ) for spine in self . ax [ i ] . spines . values (): spine . set_edgecolor ( 'lime' ) else : cross_hair_color = 'k' linestyle = ':' self . ax [ i ] . axes . plot ([ spot_yxz_global [ 1 ], spot_yxz_global [ 1 ]], [ plot_extent [ 2 ], plot_extent [ 3 ]], cross_hair_color , linestyle = linestyle , lw = 1 ) self . ax [ i ] . axes . plot ([ plot_extent [ 0 ], plot_extent [ 1 ]], [ spot_yxz_global [ 0 ], spot_yxz_global [ 0 ]], cross_hair_color , linestyle = linestyle , lw = 1 ) self . im [ i ] . set_extent ( plot_extent ) self . ax [ i ] . tick_params ( labelbottom = False , labelleft = False ) # Add axis labels to subplots of far left column or bottom row if i % n_use_rounds == 0 : self . ax [ i ] . set_ylabel ( f ' { nb . basic_info . use_channels [ int ( i / n_use_rounds )] } ' ) if i >= self . n_images - n_use_rounds : r = nb . basic_info . use_rounds [ i - ( self . n_images - n_use_rounds )] self . ax [ i ] . set_xlabel ( ' {:.0f} ( {:.2f} )' . format ( r , nb . call_spots . gene_efficiency [ gene_no , r ])) self . ax [ 0 ] . set_xticks ([ spot_yxz_global [ 1 ]]) self . ax [ 0 ] . set_yticks ([ spot_yxz_global [ 0 ]]) self . fig . supylabel ( 'Color Channel' , size = 14 ) self . fig . supxlabel ( 'Round (Gene Efficiency)' , size = 14 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) plt . suptitle ( f 'Spot { spot_no } : match { str ( np . around ( spot_score , decimals = 2 )) } ' f 'to { gene_name } ' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 , size = 16 ) self . change_norm () plt . show ()","title":"view_spot"},{"location":"code/plot/call_spots/#view_intensity","text":"Diagnostic to show how intensity is computed from spot_color . Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'anchor' Source code in coppafish/plot/call_spots/spot_colors.py 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'anchor' ): \"\"\" Diagnostic to show how intensity is computed from `spot_color`. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] else : page_name = 'ref_spots' intensity_saved = nb . __getattribute__ ( page_name ) . intensity [ spot_no ] intensity_thresh = get_intensity_thresh ( nb ) spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] . transpose () / color_norm subplot_adjust = [ 0.07 , 0.775 , 0.1 , 0.91 ] super () . __init__ ([ spot_color ], color_norm , subplot_adjust = subplot_adjust ) if intensity_saved > intensity_thresh : color = 'w' else : color = 'r' spot_color_symbol = r \"$\\mathbf{\\zeta_s}$\" intensity_symbol = r \"$\\chi_s$, (median of $\\max_c\\zeta_{s_ {rc} }$ indicated in green)\" self . ax [ 0 ] . set_title ( f 'Spot Color, { spot_color_symbol } , for spot { spot_no } \\n ' f 'Intensity, { intensity_symbol } = { str ( np . around ( intensity_saved , 3 )) } ' , color = color ) self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 0 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ]), labels = nb . basic_info . use_rounds ) self . ax [ 0 ] . set_xlabel ( 'Round' ) self . fig . supylabel ( 'Color Channel' ) # Highlight max channel in each round which contributes to intensity max_channels = np . argmax ( self . im_data [ 0 ], axis = 0 ) for r in range ( len ( nb . basic_info . use_rounds )): # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( r - 0.5 , max_channels [ r ] - 0.5 ), 1 , 1 , fill = False , ec = 'lime' , linestyle = ':' , lw = 4 ) self . ax [ 0 ] . add_patch ( rectangle ) self . change_norm () # initialise with method = 'norm' plt . show ()","title":"view_intensity"},{"location":"code/plot/call_spots/#colorplotbase","text":"This is the base class for plots with multiple subplots and with a slider to change the color axis and a button to change the normalisation. After initialising, the function change_norm() should be run to plot normalised images. This will change self.method from 'raw' to 'norm' . Parameters: Name Type Description Default images List float [n_images] Each image is n_y x n_x (x n_z) . This is the normalised image. There will be a subplot for each image and if it is 3D, the first z-plane will be set as the starting data, self.im_data while the full 3d data will be saved as self.im_data_3d . required norm_factor Optional [ Union [ np . ndarray , List ]] float [n_images] norm_factor[i] is the value to multiply images[i] to give raw image. norm_factor[i] is either an integer or an array of same dimensions as image[i] . If a single norm_factor given, assume same for each image. required subplot_row_columns Optional [ List ] [n_rows, n_columns] The subplots will be arranged into n_rows and n_columns . If not given, n_columns will be 1. None fig_size Optional [ Tuple ] [width, height] Size of figure to plot in inches. If not given, will be set to (9, 5) . None subplot_adjust Optional [ List ] [left, right, bottom, top] The position of the sides of the subplots in the figure. I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that. If not given, will be set to [0.07, 0.775, 0.095, 0.94] . None cbar_pos Optional [ List ] [left, bottom, width, height] Position of color axis. If not given, will be set to [0.9, 0.15, 0.03, 0.8] . None slider_pos Optional [ List ] [left, bottom, width, height] Position of slider that controls color axis. If not given, will be set to [0.85, 0.15, 0.01, 0.8] . None button_pos Optional [ List ] [left, bottom, width, height] Position of button which triggers change of normalisation. If not given, will be set to [0.85, 0.02, 0.1, 0.05] . None Source code in coppafish/plot/call_spots/spot_colors.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def __init__ ( self , images : List , norm_factor : Optional [ Union [ np . ndarray , List ]], subplot_row_columns : Optional [ List ] = None , fig_size : Optional [ Tuple ] = None , subplot_adjust : Optional [ List ] = None , cbar_pos : Optional [ List ] = None , slider_pos : Optional [ List ] = None , button_pos : Optional [ List ] = None ): \"\"\" This is the base class for plots with multiple subplots and with a slider to change the color axis and a button to change the normalisation. After initialising, the function `change_norm()` should be run to plot normalised images. This will change `self.method` from `'raw'` to `'norm'`. Args: images: `float [n_images]` Each image is `n_y x n_x (x n_z)`. This is the normalised image. There will be a subplot for each image and if it is 3D, the first z-plane will be set as the starting data, `self.im_data` while the full 3d data will be saved as `self.im_data_3d`. norm_factor: `float [n_images]` `norm_factor[i]` is the value to multiply `images[i]` to give raw image. `norm_factor[i]` is either an integer or an array of same dimensions as `image[i]`. If a single `norm_factor` given, assume same for each image. subplot_row_columns: `[n_rows, n_columns]` The subplots will be arranged into `n_rows` and `n_columns`. If not given, `n_columns` will be 1. fig_size: `[width, height]` Size of figure to plot in inches. If not given, will be set to `(9, 5)`. subplot_adjust: `[left, right, bottom, top]` The position of the sides of the subplots in the figure. I.e., we don't want subplot to overlap with cbar, slider or buttom and this ensures that. If not given, will be set to `[0.07, 0.775, 0.095, 0.94]`. cbar_pos: `[left, bottom, width, height]` Position of color axis. If not given, will be set to `[0.9, 0.15, 0.03, 0.8]`. slider_pos: `[left, bottom, width, height]` Position of slider that controls color axis. If not given, will be set to `[0.85, 0.15, 0.01, 0.8]`. button_pos: `[left, bottom, width, height]` Position of button which triggers change of normalisation. If not given, will be set to `[0.85, 0.02, 0.1, 0.05]`. \"\"\" # When bled code of a gene more than this, that particular round/channel will be highlighted in plots self . intense_gene_thresh = 0.2 self . n_images = len ( images ) if subplot_row_columns is None : subplot_row_columns = [ self . n_images , 1 ] # Default positions if fig_size is None : fig_size = ( 9 , 5 ) if subplot_adjust is None : subplot_adjust = [ 0.07 , 0.775 , 0.095 , 0.94 ] if cbar_pos is None : cbar_pos = [ 0.9 , 0.15 , 0.03 , 0.8 ] if slider_pos is None : self . slider_pos = [ 0.85 , 0.15 , 0.01 , 0.8 ] else : self . slider_pos = slider_pos if button_pos is None : button_pos = [ 0.85 , 0.02 , 0.1 , 0.05 ] if not isinstance ( norm_factor , list ): # allow for different norm for each image if norm_factor is None : self . color_norm = None else : self . color_norm = [ norm_factor , ] * self . n_images else : self . color_norm = norm_factor self . im_data = [ val for val in images ] # put in order channels, rounds self . method = 'raw' if self . color_norm is not None else 'norm' if self . color_norm is None : self . caxis_info = { 'norm' : {}} else : self . caxis_info = { 'norm' : {}, 'raw' : {}} for key in self . caxis_info : if key == 'norm' : im_data = self . im_data self . caxis_info [ key ][ 'format' ] = ' %.2f ' else : im_data = [ self . im_data [ i ] * self . color_norm [ i ] for i in range ( self . n_images )] self . caxis_info [ key ][ 'format' ] = ' %.0f ' self . caxis_info [ key ][ 'min' ] = np . min ([ im . min () for im in im_data ] + [ - 1e-20 ]) self . caxis_info [ key ][ 'max' ] = np . max ([ im . max () for im in im_data ] + [ 1e-20 ]) self . caxis_info [ key ][ 'max' ] = np . max ([ self . caxis_info [ key ][ 'max' ], - self . caxis_info [ key ][ 'min' ]]) # have equal either side of zero so small negatives don't look large self . caxis_info [ key ][ 'min' ] = - self . caxis_info [ key ][ 'max' ] self . caxis_info [ key ][ 'clims' ] = [ self . caxis_info [ key ][ 'min' ], self . caxis_info [ key ][ 'max' ]] # cmap_norm is so cmap is white at 0. self . caxis_info [ key ][ 'cmap_norm' ] = \\ matplotlib . colors . TwoSlopeNorm ( vmin = self . caxis_info [ key ][ 'min' ], vcenter = 0 , vmax = self . caxis_info [ key ][ 'max' ]) self . fig , self . ax = plt . subplots ( subplot_row_columns [ 0 ], subplot_row_columns [ 1 ], figsize = fig_size , sharex = True , sharey = True ) if self . n_images == 1 : self . ax = [ self . ax ] # need it to be a list elif subplot_row_columns [ 0 ] > 1 and subplot_row_columns [ 1 ] > 1 : self . ax = self . ax . flatten () # only have 1 ax index oob_axes = np . arange ( self . n_images , subplot_row_columns [ 0 ] * subplot_row_columns [ 1 ]) if oob_axes . size > 0 : for i in oob_axes : self . fig . delaxes ( self . ax [ i ]) # delete excess subplots self . ax = self . ax [: self . n_images ] self . fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) self . im = [ None ] * self . n_images if self . im_data [ 0 ] . ndim == 3 : # For 3D data, start by showing just the first plane self . im_data_3d = self . im_data . copy () self . im_data = [ val [:, :, 0 ] for val in self . im_data_3d ] if self . color_norm is not None : self . color_norm_3d = self . color_norm . copy () self . color_norm = [ val [:, :, 0 ] for val in self . color_norm_3d ] else : self . im_data_3d = None self . color_norm_3d = None # initialise plots with a zero array for i in range ( self . n_images ): self . im [ i ] = self . ax [ i ] . imshow ( np . zeros ( self . im_data [ 0 ] . shape [: 2 ]), cmap = \"seismic\" , aspect = 'auto' , norm = self . caxis_info [ self . method ][ 'cmap_norm' ]) cbar_ax = self . fig . add_axes ( cbar_pos ) # left, bottom, width, height self . fig . colorbar ( self . im [ 0 ], cax = cbar_ax ) self . slider_ax = self . fig . add_axes ( self . slider_pos ) self . color_slider = None if self . color_norm is not None : self . norm_button_color = 'white' self . norm_button_color_press = 'red' if self . method == 'raw' : current_color = self . norm_button_color_press else : current_color = self . norm_button_color self . norm_button_ax = self . fig . add_axes ( button_pos ) self . norm_button = Button ( self . norm_button_ax , 'Norm' , hovercolor = '0.275' ) self . norm_button . label . set_color ( current_color ) self . norm_button . on_clicked ( self . change_norm )","title":"ColorPlotBase"},{"location":"code/plot/call_spots/#dot-product","text":"","title":"Dot Product"},{"location":"code/plot/call_spots/#view_score","text":"This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically the normalised inverse variance). The bottom row shows the contribution from background and genes to the variance. The iteration as well as the alpha and beta parameters used to compute the weight can be changed through the text boxes. If the weight plot is clicked on, the view_weight plot will open for the current iteration. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' g Optional [ int ] Gene to view dot product calculation for. If left as None , will show the gene with the largest dot product score. None iter int Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted ( iter=0 will have only background fit, iter=1 will have background + 1 gene etc.). The score saved as nb.ref_spots.score can be viewed with iter=0 . 0 omp_fit_info Optional [ List ] This is a list containing [track_info, bled_codes, dp_thresh] . It is only ever used to call this function from view_omp_fit . None check_weight bool When this is True , we raise an error if weight computed here is different to that computed with get_track_info . False Source code in coppafish/plot/call_spots/dot_product.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , g : Optional [ int ] = None , iter : int = 0 , omp_fit_info : Optional [ List ] = None , check_weight : bool = False ): \"\"\" This produces 4 plots on the first row, showing spot_color, residual, variance and weight squared (basically the normalised inverse variance). The bottom row shows the contribution from background and genes to the variance. The iteration as well as the alpha and beta parameters used to compute the weight can be changed through the text boxes. If the weight plot is clicked on, the `view_weight` plot will open for the current iteration. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. g: Gene to view dot product calculation for. If left as `None`, will show the gene with the largest dot product score. iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (`iter=0` will have only background fit, `iter=1` will have background + 1 gene etc.). The score saved as `nb.ref_spots.score` can be viewed with `iter=0`. omp_fit_info: This is a list containing `[track_info, bled_codes, dp_thresh]`. It is only ever used to call this function from `view_omp_fit`. check_weight: When this is `True`, we raise an error if weight computed here is different to that computed with `get_track_info`. \"\"\" self . spot_no = spot_no if omp_fit_info is None : self . track_info , self . bled_codes , self . dp_thresh = get_track_info ( nb , spot_no , method ) else : self . track_info , self . bled_codes , self . dp_thresh = omp_fit_info self . n_genes , self . n_rounds_use , self . n_channels_use = self . bled_codes . shape # allow to view dot product with background self . bled_codes = np . append ( self . bled_codes , self . track_info [ 'background_codes' ], axis = 0 ) self . n_genes_all = self . bled_codes . shape [ 0 ] self . spot_color = self . track_info [ 'residual' ][ 0 ] # Get saved values if anchor method if method . lower () != 'omp' : self . g_saved = nb . ref_spots . gene_no [ spot_no ] if self . track_info [ 'gene_added' ][ 2 ] < self . n_genes : # Possibility best gene will be background here, but impossible for saved best gene to be background if self . track_info [ 'gene_added' ][ 2 ] != self . g_saved and check_weight : raise ValueError ( f \" \\n Best gene saved was { self . g_saved } but with parameters used here, it \" f \"was { self . track_info [ 'gene_added' ][ 2 ] } . \\n Ensure that alpha and beta in \" f \"config['call_spots'] have not been changed. \\n \" f \"Set check_weight=False to skip this error.\" ) self . dp_val_saved = nb . ref_spots . score [ spot_no ] config_name = 'call_spots' else : self . g_saved = None self . dp_val_saved = None config_name = 'omp' # Get default dot product params config = nb . get_config () self . alpha = config [ config_name ][ 'alpha' ] self . beta = config [ config_name ][ 'beta' ] self . dp_norm_shift = nb . call_spots . dp_norm_shift self . check_weight = check_weight self . n_iter = self . track_info [ 'residual' ] . shape [ 0 ] - 2 # first two indices in track is not added gene if iter >= self . n_iter or iter < 0 : warnings . warn ( f \"Only { self . n_iter } iterations for this pixel but iter= { iter } , \" f \"setting iter = { self . n_iter - 1 } .\" ) iter = self . n_iter - 1 self . iter = iter if g is None : g = self . track_info [ 'gene_added' ][ 2 + iter ] self . g = g self . gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds # Initialize data self . dp_val = None self . dp_weight_val = None self . im_data = None self . update_data () # Initialize plot self . title = None self . vmax = None self . get_cax_lim () self . fig = plt . figure ( figsize = ( 16 , 7 )) ax1 = self . fig . add_subplot ( 2 , 4 , 1 ) ax2 = self . fig . add_subplot ( 2 , 4 , 5 ) ax3 = self . fig . add_subplot ( 2 , 4 , 2 ) ax4 = self . fig . add_subplot ( 2 , 4 , 6 ) ax5 = self . fig . add_subplot ( 1 , 4 , 3 ) ax6 = self . fig . add_subplot ( 2 , 4 , 4 ) ax7 = self . fig . add_subplot ( 2 , 4 , 8 ) self . ax = [ ax1 , ax2 , ax3 , ax4 , ax5 , ax6 , ax7 ] self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . subplot_adjust = [ 0.05 , 0.87 , 0.05 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * len ( self . ax ) self . set_up_plots () self . set_titles () self . add_rectangles () # Text boxes to change parameters text_box_labels = [ 'Gene' , 'Iteration' , r '$\\alpha$' , r '$\\beta$' , r 'dp_shift, $\\lambda_d$' ] text_box_values = [ self . g , self . iter , self . alpha , self . beta , self . dp_norm_shift ] text_box_funcs = [ self . update_g , self . update_iter , self . update_alpha , self . update_beta , self . update_dp_norm_shift ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 3 ] - 0.15 * ( i + 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Make so if click on weight plot, it opens view_weight self . nb = nb self . method = method self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_weight ) plt . show ()","title":"view_score"},{"location":"code/plot/call_spots/#weight","text":"","title":"Weight"},{"location":"code/plot/call_spots/#view_weight","text":"This produces at least 5 plots which show how the weight used in the dot product score was calculated. The iteration as well as the alpha and beta parameters used to compute the weight can be changed with the text boxes. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' iter int Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted ( iter=0 will have only background fit, iter=1 will have background + 1 gene etc.). The score saved as nb.ref_spots.score can be viewed with iter=0 . 0 score_info Optional [ List ] This is a list containing [track_info, bled_codes, weight_vmax] . It is only ever used to call this function from view_score . None check_weight bool When this is True , we raise an error if weight computed here is different to that computed with get_track_info . True Source code in coppafish/plot/call_spots/weight.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , iter : int = 0 , alpha : Optional [ float ] = None , beta : Optional [ float ] = None , score_info : Optional [ List ] = None , check_weight : bool = True ): \"\"\" This produces at least 5 plots which show how the weight used in the dot product score was calculated. The iteration as well as the alpha and beta parameters used to compute the weight can be changed with the text boxes. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. iter: Iteration in OMP to view the dot product calculation for i.e. the number of genes which have already been fitted (`iter=0` will have only background fit, `iter=1` will have background + 1 gene etc.). The score saved as `nb.ref_spots.score` can be viewed with `iter=0`. score_info: This is a list containing `[track_info, bled_codes, weight_vmax]`. It is only ever used to call this function from `view_score`. check_weight: When this is `True`, we raise an error if weight computed here is different to that computed with `get_track_info`. \"\"\" self . spot_no = spot_no if score_info is None : self . track_info , self . bled_codes = get_track_info ( nb , spot_no , method )[: 2 ] self . weight_vmax = None else : self . track_info , self . bled_codes , self . weight_vmax = score_info self . n_genes , self . n_rounds_use , self . n_channels_use = self . bled_codes . shape # allow to view dot product with background self . bled_codes = np . append ( self . bled_codes , self . track_info [ 'background_codes' ], axis = 0 ) self . n_genes_all = self . bled_codes . shape [ 0 ] self . spot_color = self . track_info [ 'residual' ][ 0 ] if method . lower () == 'omp' : config_name = 'omp' else : config_name = 'call_spots' # Get default params config = nb . get_config () if alpha is None : alpha = config [ config_name ][ 'alpha' ] if beta is None : beta = config [ config_name ][ 'beta' ] self . alpha = alpha self . beta = beta self . check_weight = check_weight self . n_iter = self . track_info [ 'residual' ] . shape [ 0 ] - 2 # first two indices in track is not added gene if iter >= self . n_iter or iter < 0 : warnings . warn ( f \"Only { self . n_iter } iterations for this pixel but iter= { iter } , \" f \"setting iter = { self . n_iter - 1 } .\" ) iter = self . n_iter - 1 self . iter = iter self . gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds # Initialize data self . n_plots = self . n_plots_top_row + self . n_iter self . update_data () # Initialize plots self . vmax = None self . get_cax_lim () n_rows = 2 n_cols = int ( np . max ([ self . n_plots_top_row , self . n_iter ])) self . fig = plt . figure ( figsize = ( 16 , 7 )) self . ax = [] for i in range ( self . n_plots ): if i >= self . n_plots_top_row : # So goes to next row self . ax += [ self . fig . add_subplot ( n_rows , n_cols , n_cols + i + 1 - self . n_plots_top_row )] else : self . ax += [ self . fig . add_subplot ( n_rows , n_cols , i + 1 )] # Y and X axis are the same for all plots hence share self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) self . subplot_adjust = [ 0.05 , 0.87 , 0.07 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * self . n_plots self . variance_cbar = None self . set_up_plots () self . set_titles () self . add_rectangles () # Text boxes to change parameters text_box_labels = [ 'Iteration' , r '$\\alpha$' , r '$\\beta$' ] text_box_values = [ self . iter , self . alpha , self . beta ] text_box_funcs = [ self . update_iter , self . update_alpha , self . update_beta ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 3 ] - 0.15 * ( i + 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) self . nb = nb self . method = method self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_background ) plt . show ()","title":"view_weight"},{"location":"code/plot/call_spots/#background","text":"","title":"Background"},{"location":"code/plot/call_spots/#view_background","text":"This shows how the background coefficients were calculated. The weighted dot product is equal to weight multiplied by dot product. Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c. Also shows residual after removing background. Parameters: Name Type Description Default nb Notebook Notebook containing at least the call_spots and ref_spots pages. required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' check bool When this is True , we raise an error if background coefs computed here is different to that computed with get_track_info . True track_info Optional [ List ] To use when calling from view_weight . None Source code in coppafish/plot/call_spots/background.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , check : bool = True , track_info : Optional [ List ] = None ): \"\"\" This shows how the background coefficients were calculated. The weighted dot product is equal to weight multiplied by dot product. Coefficient for background gene c is the sum over all rounds of weighted dot product in channel c. Also shows residual after removing background. Args: nb: *Notebook* containing at least the *call_spots* and *ref_spots* pages. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. check: When this is `True`, we raise an error if background coefs computed here is different to that computed with `get_track_info`. track_info: To use when calling from `view_weight`. \"\"\" self . spot_no = spot_no if track_info is None : self . track_info = get_track_info ( nb , spot_no , method )[ 0 ] self . color_vmax = None else : self . track_info , self . color_vmax = track_info self . n_genes_all = self . track_info [ 'coef' ][ 0 ] . size self . spot_color = self . track_info [ 'residual' ][ 0 ] self . n_rounds_use , self . n_channels_use = self . spot_color . shape self . n_genes = self . n_genes_all - self . n_channels_use self . use_channels = nb . basic_info . use_channels self . use_rounds = nb . basic_info . use_rounds self . background_weight_shift = nb . call_spots . background_weight_shift self . check = check # Initialize data self . im_data = None self . update_data () hi = 5 # Initialize plots self . vmax = None self . weight_dp_max_initial = None self . get_cax_lim () self . fig = plt . figure ( figsize = ( 16 , 7 )) self . ax = [] ax1 = self . fig . add_subplot ( 2 , 5 , 1 ) ax2 = self . fig . add_subplot ( 2 , 5 , 6 ) ax3 = self . fig . add_subplot ( 1 , 5 , 2 ) ax4 = self . fig . add_subplot ( 2 , 5 , 3 ) ax5 = self . fig . add_subplot ( 2 , 5 , 8 ) ax6 = self . fig . add_subplot ( 2 , 5 , 4 ) ax7 = self . fig . add_subplot ( 2 , 5 , 9 ) ax8 = self . fig . add_subplot ( 2 , 5 , 5 ) ax9 = self . fig . add_subplot ( 2 , 5 , 10 ) self . ax = [ ax1 , ax2 , ax3 , ax4 , ax5 , ax6 , ax7 , ax8 , ax9 ] self . ax [ 0 ] . get_shared_y_axes () . join ( self . ax [ 0 ], * self . ax [ 1 :]) # Coef plots have different x-axis self . ax [ self . coef_plot_ind [ 0 ]] . get_shared_x_axes () . join ( self . ax [ self . coef_plot_ind [ 0 ]], self . ax [ self . coef_plot_ind [ 1 ]]) # All other plots have same axis self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ 1 : self . coef_plot_ind [ 0 ]]) self . ax [ 0 ] . get_shared_x_axes () . join ( self . ax [ 0 ], * self . ax [ self . coef_plot_ind [ 1 ] + 1 :]) self . subplot_adjust = [ 0.05 , 0.97 , 0.07 , 0.9 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . im = [ None ] * len ( self . ax ) self . set_up_plots () self . set_titles () weight_plot_pos = self . ax [ self . weight_plot_ind ] . get_position () box_x = np . mean ([ weight_plot_pos . x0 , weight_plot_pos . x1 ]) box_y = weight_plot_pos . y0 text_ax = self . fig . add_axes ([ box_x , box_y - 0.15 , 0.05 , 0.04 ]) self . text_box = TextBox ( text_ax , r 'background_shift, $\\lambda_b$' , self . background_weight_shift , color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 1.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . update_background_weight_shift ) plt . show ()","title":"view_background"},{"location":"code/plot/call_spots/#gene-counts","text":"","title":"Gene Counts"},{"location":"code/plot/call_spots/#gene_counts","text":"This shows the number of reference spots assigned to each gene which pass the quality thresholding based on the parameters score_thresh and intensity_thresh . If nb has the OMP page, then the number of omp spots will also be shown, where the quality thresholding is based on score_omp_thresh , score_omp_multiplier and intensity_thresh . There will also be a second reference spots histogram, the difference with this is that the spots were allowed to be assigned to some fake genes with bled_codes specified through fake_bled_codes . Note fake_bled_codes have dimension n_fake x nbp_basic.n_rounds x nbp_basic.n_channels not n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) . Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required fake_bled_codes Optional [ np . ndarray ] float [n_fake_genes x n_rounds x n_channels] . colors of fake genes to find dot product with Will find new gene assignment of anchor spots to new set of bled_codes which include these in addition to bled_codes_ge . By default, will have a fake gene for each round and channel such that it is \\(1\\) in round r, channel \\(c\\) and 0 everywhere else. None fake_gene_names Optional [ List [ str ]] str [n_fake_genes] . Can give name of each fake gene. If None , fake gene \\(i\\) will be called FAKE: \\(i\\) . None score_thresh Optional [ float ] Threshold for score for ref_spots. Can be changed with text box. None intensity_thresh Optional [ float ] Threshold for intensity. Can be changed with text box. None score_omp_thresh Optional [ float ] Threshold for score for omp_spots. Can be changed with text box. None score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. Can be changed with text box. None Source code in coppafish/plot/call_spots/gene_counts.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def __init__ ( self , nb : Notebook , fake_bled_codes : Optional [ np . ndarray ] = None , fake_gene_names : Optional [ List [ str ]] = None , score_thresh : Optional [ float ] = None , intensity_thresh : Optional [ float ] = None , score_omp_thresh : Optional [ float ] = None , score_omp_multiplier : Optional [ float ] = None ): \"\"\" This shows the number of reference spots assigned to each gene which pass the quality thresholding based on the parameters `score_thresh` and `intensity_thresh`. If `nb` has the *OMP* page, then the number of omp spots will also be shown, where the quality thresholding is based on `score_omp_thresh`, `score_omp_multiplier` and `intensity_thresh`. There will also be a second reference spots histogram, the difference with this is that the spots were allowed to be assigned to some fake genes with `bled_codes` specified through `fake_bled_codes`. !!! note `fake_bled_codes` have dimension `n_fake x nbp_basic.n_rounds x nbp_basic.n_channels` not `n_fake x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)`. Args: nb: *Notebook* containing at least `call_spots` page. fake_bled_codes: `float [n_fake_genes x n_rounds x n_channels]`. colors of fake genes to find dot product with Will find new gene assignment of anchor spots to new set of `bled_codes` which include these in addition to `bled_codes_ge`. By default, will have a fake gene for each round and channel such that it is $1$ in round r, channel $c$ and 0 everywhere else. fake_gene_names: `str [n_fake_genes]`. Can give name of each fake gene. If `None`, fake gene $i$ will be called FAKE:$i$. score_thresh: Threshold for score for ref_spots. Can be changed with text box. intensity_thresh: Threshold for intensity. Can be changed with text box. score_omp_thresh: Threshold for score for omp_spots. Can be changed with text box. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. Can be changed with text box. \"\"\" # Add fake genes if fake_bled_codes is None : # Default have binary fake gene for each used round and channel n_fake = len ( nb . basic_info . use_rounds ) * len ( nb . basic_info . use_channels ) fake_bled_codes = np . zeros (( n_fake , nb . basic_info . n_rounds , nb . basic_info . n_channels )) i = 0 # Cluster fake genes by channel because more likely to be a faulty channel for c in nb . basic_info . use_channels : for r in nb . basic_info . use_rounds : fake_bled_codes [ i , r , c ] = 1 i += 1 fake_gene_names = [ f 'r { r } c { c } ' for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] n_fake = fake_bled_codes . shape [ 0 ] if fake_gene_names is None : fake_gene_names = [ f 'FAKE: { i } ' for i in range ( n_fake )] self . gene_names = nb . call_spots . gene_names . tolist () + fake_gene_names self . n_genes = len ( self . gene_names ) self . n_genes_real = self . n_genes - n_fake # Do new gene assignment for anchor spots when fake genes are included spot_colors_pb , background_var = background_fitting ( nb , 'anchor' )[ 1 :] # Fit background before dot product score grc_ind = np . ix_ ( np . arange ( self . n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels ) # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds bled_codes = np . append ( nb . call_spots . bled_codes_ge , fake_bled_codes , axis = 0 ) bled_codes = bled_codes [ grc_ind ] # Ensure L2 norm of 1 for each gene norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes = bled_codes / norm_factor score , gene_no = get_dot_product_score ( spot_colors_pb , bled_codes , None , nb . call_spots . dp_norm_shift , background_var ) # Add quality thresholding info and gene assigned to for method with no fake genes and method with fake genes self . intensity = [ nb . ref_spots . intensity . astype ( np . float16 )] * 2 self . score = [ nb . ref_spots . score . astype ( np . float16 ), score . astype ( np . float16 )] self . gene_no = [ nb . ref_spots . gene_no , gene_no ] # Add current thresholds config = nb . get_config ()[ 'thresholds' ] if config [ 'intensity' ] is None : config [ 'intensity' ] = nb . call_spots . gene_efficiency_intensity_thresh if score_thresh is None : score_thresh = config [ 'score_ref' ] if intensity_thresh is None : intensity_thresh = config [ 'intensity' ] self . score_thresh = [ score_thresh ] * 2 self . intensity_thresh = intensity_thresh # Add omp gene assignment if have page if nb . has_page ( 'omp' ): if score_omp_multiplier is None : score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_omp_multiplier self . nbp_omp = nb . omp if score_omp_thresh is None : score_omp_thresh = config [ 'score_omp' ] self . score_thresh += [ score_omp_thresh ] self . score += [ omp_spot_score ( self . nbp_omp , self . score_multiplier ) . astype ( np . float16 )] self . intensity += [ self . nbp_omp . intensity . astype ( np . float16 )] self . gene_no += [ self . nbp_omp . gene_no ] self . omp = True else : self . omp = False self . n_plots = len ( self . score ) self . use = None self . update_use () # Initialise plot self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 16 , 7 )) self . subplot_adjust = [ 0.07 , 0.85 , 0.12 , 0.93 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax . set_ylabel ( r \"Number of Spots\" ) self . ax . set_xlabel ( 'Gene' ) self . ax . set_title ( f \"Number of Spots assigned to each Gene\" ) # record min and max for textbox input self . score_min = np . around ( self . score [ 0 ] . min (), 2 ) # Min score of score[1] cannot be less than this self . score_max = np . around ( self . score [ 1 ] . max (), 2 ) # Max score of score[0] cannot be more than this self . intensity_min = np . around ( self . intensity [ 0 ] . min (), 2 ) self . intensity_max = np . around ( self . intensity [ 0 ] . max (), 2 ) self . plots = [ None ] * self . n_plots default_colors = plt . rcParams [ 'axes.prop_cycle' ] . _left default_colors = default_colors [: 2 ] + default_colors [ 3 : 4 ] # default 3 is better than default 2 for omp plot for i in range ( self . n_plots ): self . plots [ i ], = self . ax . plot ( np . arange ( self . n_genes ), np . histogram ( self . gene_no [ i ][ self . use [ i ]], np . arange ( self . n_genes + 1 ) - 0.5 )[ 0 ], color = default_colors [ i ][ 'color' ]) if i == 1 : self . plots [ i ] . set_visible ( False ) self . ax . set_ylim ( 0 , None ) self . ax . set_xticks ( np . arange ( self . n_genes )) self . ax . set_xticklabels ( self . gene_names , rotation = 90 , size = 7 ) self . ax . set_xlim ( - 0.5 , self . n_genes_real - 0.5 ) # Add text box to change score multiplier text_box_labels = [ r 'Score, $\\Delta_s$' + ' \\n Threshold' , r 'Intensity, $\\chi_s$' + ' \\n Threshold' ] text_box_values = [ np . around ( self . score_thresh [ 0 ], 2 ), np . around ( self . intensity_thresh , 2 )] text_box_funcs = [ self . update_score_thresh , self . update_intensity_thresh ] if self . omp : text_box_labels += [ r 'OMP Score, $\\gamma_s$' + ' \\n Threshold' , 'Score \\n ' + r 'Multiplier, $\\rho$' ] text_box_values += [ np . around ( self . score_thresh [ 2 ], 2 ), np . around ( self . score_multiplier , 2 )] text_box_funcs += [ self . update_score_omp_thresh , self . update_score_multiplier ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 2 ] + 0.15 * ( len ( text_box_labels ) - i - 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 2.75 ]) # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Add buttons to add/remove score_dp histograms self . buttons_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.02 , self . subplot_adjust [ 3 ] - 0.25 , 0.15 , 0.3 ]) plt . axis ( 'off' ) self . button_labels = [ \"Ref Spots\" , \"Ref Spots - Fake Genes\" ] label_checked = [ True , False ] if self . omp : self . button_labels += [ \"OMP Spots\" ] label_checked += [ True ] self . buttons = CheckButtons ( self . buttons_ax , self . button_labels , label_checked ) for i in range ( self . n_plots ): self . buttons . labels [ i ] . set_fontsize ( 7 ) self . buttons . labels [ i ] . set_color ( default_colors [ i ][ 'color' ]) self . buttons . rectangles [ i ] . set_color ( 'w' ) self . buttons . on_clicked ( self . choose_plots ) plt . show ()","title":"gene_counts"},{"location":"code/plot/call_spots/#score-calculation","text":"","title":"Score Calculation"},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.score_calc.background_fitting","text":"Computes background using parameters in config file. Then removes this from the spot_colors . Parameters: Name Type Description Default nb Notebook Notebook containing call_spots page required method str 'omp' or 'anchor', indicating which spot_colors to use. required Returns: Type Description np . ndarray spot_colors - float [n_spots x n_rounds_use x n_channels_use] . spot_color after normalised by color_norm_factor but before background fit. np . ndarray spot_colors_pb - float [n_spots x n_rounds_use x n_channels_use] . spot_color after background removed. np . ndarray background_var - float [n_spots x n_rounds_use x n_channels_use] . inverse of the weighting used for dot product score calculation. Source code in coppafish/plot/call_spots/score_calc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def background_fitting ( nb : Notebook , method : str ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray ]: \"\"\" Computes background using parameters in config file. Then removes this from the `spot_colors`. Args: nb: Notebook containing call_spots page method: 'omp' or 'anchor', indicating which `spot_colors` to use. Returns: `spot_colors` - `float [n_spots x n_rounds_use x n_channels_use]`. `spot_color` after normalised by `color_norm_factor` but before background fit. `spot_colors_pb` - `float [n_spots x n_rounds_use x n_channels_use]`. `spot_color` after background removed. `background_var` - `float [n_spots x n_rounds_use x n_channels_use]`. inverse of the weighting used for dot product score calculation. \"\"\" rc_ind = np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels ) if method . lower () == 'omp' : spot_colors = np . moveaxis ( np . moveaxis ( nb . omp . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) config = nb . get_config ()[ 'omp' ] else : spot_colors = np . moveaxis ( np . moveaxis ( nb . ref_spots . colors , 0 , - 1 )[ rc_ind ], - 1 , 0 ) config = nb . get_config ()[ 'call_spots' ] alpha = config [ 'alpha' ] beta = config [ 'beta' ] spot_colors = spot_colors / nb . call_spots . color_norm_factor [ rc_ind ] spot_colors_pb , background_coef , background_codes = \\ fit_background ( spot_colors , nb . call_spots . background_weight_shift ) background_codes = background_codes . reshape ( background_codes . shape [ 0 ], - 1 ) background_var = background_coef ** 2 @ background_codes ** 2 * alpha + beta ** 2 return spot_colors , spot_colors_pb , background_var","title":"background_fitting()"},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.score_calc.get_dot_product_score","text":"Finds dot product score for each spot_color given to the gene indicated by spot_gene_no . Parameters: Name Type Description Default spot_colors np . ndarray float [n_spots x n_rounds_use x n_channels_use] . colors of spots to find score of. required bled_codes np . ndarray float [n_genes x n_rounds_use x n_channels_use] . colors of genes to find dot product with. required spot_gene_no Optional [ np . ndarray ] int [n_spots] . Gene that each spot was assigned to. If None, will set spot_gene_no[s] to gene for which score was largest. required dp_norm_shift float Normalisation constant for single round used for dot product calculation. I.e. nb.call_spots.dp_norm_shift . required background_var Optional [ np . ndarray ] float [n_spots x n_rounds_use x n_channels_use] . inverse of the weighting used for dot product score calculation. required Returns: Type Description np . ndarray spot_score - float [n_spots] . Dot product score for each spot. np . ndarray spot_gene_no - will be same as input if given, otherwise will be the best gene assigned. Source code in coppafish/plot/call_spots/score_calc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_dot_product_score ( spot_colors : np . ndarray , bled_codes : np . ndarray , spot_gene_no : Optional [ np . ndarray ], dp_norm_shift : float , background_var : Optional [ np . ndarray ]) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Finds dot product score for each `spot_color` given to the gene indicated by `spot_gene_no`. Args: spot_colors: `float [n_spots x n_rounds_use x n_channels_use]`. colors of spots to find score of. bled_codes: `float [n_genes x n_rounds_use x n_channels_use]`. colors of genes to find dot product with. spot_gene_no: `int [n_spots]`. Gene that each spot was assigned to. If None, will set `spot_gene_no[s]` to gene for which score was largest. dp_norm_shift: Normalisation constant for single round used for dot product calculation. I.e. `nb.call_spots.dp_norm_shift`. background_var: `float [n_spots x n_rounds_use x n_channels_use]`. inverse of the weighting used for dot product score calculation. Returns: `spot_score` - `float [n_spots]`. Dot product score for each spot. `spot_gene_no` - will be same as input if given, otherwise will be the best gene assigned. \"\"\" n_spots , n_rounds_use = spot_colors . shape [: 2 ] n_genes = bled_codes . shape [ 0 ] dp_norm_shift = dp_norm_shift * np . sqrt ( n_rounds_use ) if background_var is None : weight = None else : weight = 1 / background_var scores = np . asarray ( dot_product_score ( spot_colors . reshape ( n_spots , - 1 ), bled_codes . reshape ( n_genes , - 1 ), dp_norm_shift , weight )) if spot_gene_no is None : spot_gene_no = np . argmax ( scores , 1 ) spot_score = scores [ np . arange ( n_spots ), spot_gene_no ] return spot_score , spot_gene_no","title":"get_dot_product_score()"},{"location":"code/plot/call_spots/#scaled-k-means","text":"","title":"Scaled K Means"},{"location":"code/plot/call_spots/#coppafish.plot.call_spots.scaled_k_means.view_scaled_k_means","text":"Plot to show how scaled_k_means was used to compute the bleed matrix. There will be upto 3 columns, each with 2 plots. The vector for dye \\(d\\) in the bleed_matrix is computed from all the spot round vectors whose dot product to the dye \\(d\\) vector was the highest. The boxplots in the first row show these dot product values for each dye. The second row then shows the bleed matrix at each stage of the computation. The first column shows the initial bleed matrix. The second column shows the bleed matrix after running scaled_k_means once with a score threshold of 0. The third column shows the final bleed_matrix after running scaled_k_means a second time with score_thresh for dye \\(d\\) set to the median of the scores assigned to dye \\(d\\) in the first run. Third column only present if config['call_spots']['bleed_matrix_anneal']==True . Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required r int Round of bleed matrix to view. Only relevant if config['call_spots']['bleed_matrix_method'] = 'separate' . 0 check bool If True, will raise error if bleed_matrix computed here is different to that saved in notebook False Source code in coppafish/plot/call_spots/scaled_k_means.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def view_scaled_k_means ( nb : Notebook , r : int = 0 , check : bool = False ): \"\"\" Plot to show how `scaled_k_means` was used to compute the bleed matrix. There will be upto 3 columns, each with 2 plots. The vector for dye $d$ in the `bleed_matrix` is computed from all the spot round vectors whose dot product to the dye $d$ vector was the highest. The boxplots in the first row show these dot product values for each dye. The second row then shows the bleed matrix at each stage of the computation. The first column shows the initial bleed matrix. The second column shows the bleed matrix after running `scaled_k_means` once with a score threshold of 0. The third column shows the final `bleed_matrix` after running `scaled_k_means` a second time with `score_thresh` for dye $d$ set to the median of the scores assigned to dye $d$ in the first run. Third column only present if `config['call_spots']['bleed_matrix_anneal']==True`. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. r: Round of bleed matrix to view. Only relevant if `config['call_spots']['bleed_matrix_method'] = 'separate'`. check: If True, will raise error if `bleed_matrix` computed here is different to that saved in notebook \"\"\" # Fit background to spot_colors as is done in call_reference_spots before bleed_matrix calc spot_colors = background_fitting ( nb , 'ref' )[ 1 ] # Get bleed matrix and plotting info rcd_ind = np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels , nb . basic_info . use_dyes ) initial_bleed_matrix = nb . call_spots . initial_bleed_matrix [ rcd_ind ] config = nb . get_config ()[ 'call_spots' ] if config [ 'bleed_matrix_method' ] . lower () == 'separate' : r_ind = int ( np . where ( np . asarray ( nb . basic_info . use_rounds ) == r )[ 0 ]) title_start = f \"Bleed matrix for round { r } \" else : r_ind = 0 title_start = \"Bleed matrix \" debug_info = get_bleed_matrix ( spot_colors [ nb . ref_spots . isolated ], initial_bleed_matrix , config [ 'bleed_matrix_method' ], config [ 'bleed_matrix_score_thresh' ], config [ 'bleed_matrix_min_cluster_size' ], config [ 'bleed_matrix_n_iter' ], config [ 'bleed_matrix_anneal' ], r_ind )[ 1 ] if check : if np . abs ( nb . call_spots . bleed_matrix [ rcd_ind ][ r_ind ] - debug_info [ 'bleed_matrix' ][ - 1 ]) . max () > 1e-3 : raise ValueError ( \"Bleed Matrix saved to Notebook is different from that computed here \" \"with get_bleed_matrix. \\n Make sure that the background and bleed_matrix\" \"parameters in the config file have not changed.\" ) # Make it so all bleed matrices have same L2 norm as final one bm_norm = np . linalg . norm ( debug_info [ 'bleed_matrix' ][ - 1 ]) bleed_matrix = [ bm * bm_norm / np . linalg . norm ( bm ) for bm in debug_info [ 'bleed_matrix' ]] vmax = np . max ( bleed_matrix ) # Set up plot n_plots , n_use_channels , n_use_dyes = debug_info [ 'bleed_matrix' ] . shape fig , ax = plt . subplots ( 2 , n_plots , figsize = ( 11 , 7 ), sharex = True ) ax [ 0 , 0 ] . get_shared_y_axes () . join ( ax [ 0 , 0 ], ax [ 0 , 1 ]) subplot_adjust = [ 0.075 , 0.92 , 0.08 , 0.88 ] fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) titles = [ 'Initial Bleed Matrix' , 'Bleed Matrix after Scaled K Means 1' , 'Bleed Matrix after Scaled K Means 2' ] if not config [ 'bleed_matrix_anneal' ]: titles [ 1 ] = 'Bleed Matrix after Scaled K Means' for i in range ( n_plots ): box_data = [ debug_info [ 'cluster_score' ][ i ][ debug_info [ 'cluster_ind' ][ i ] == d ] for d in range ( n_use_dyes )] bp = ax [ 0 , i ] . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) for d in range ( n_use_dyes ): ax [ 0 , i ] . text ( d + 1 , np . percentile ( box_data [ d ], 25 ), \" {:.1e} \" . format ( len ( box_data [ d ])), horizontalalignment = 'center' , color = bp [ 'medians' ][ d ] . get_color (), size = 5 , clip_on = True ) im = ax [ 1 , i ] . imshow ( bleed_matrix [ i ], extent = [ 0.5 , n_use_dyes + 0.5 , n_use_channels - 0.5 , - 0.5 ], aspect = 'auto' , vmin = 0 , vmax = vmax ) if nb . basic_info . dye_names is None : ax [ 1 , i ] . set_xticks ( ticks = np . arange ( 1 , n_use_dyes + 1 ), labels = nb . basic_info . use_dyes ) else : subplot_adjust [ 2 ] = 0.15 fig . subplots_adjust ( bottom = subplot_adjust [ 2 ]) ax [ 1 , i ] . set_xticks ( ticks = np . arange ( 1 , n_use_dyes + 1 ), labels = np . asarray ( nb . basic_info . dye_names )[ nb . basic_info . use_dyes ], rotation = 45 ) ax [ 1 , i ] . set_yticks ( ticks = np . arange ( n_use_channels ), labels = nb . basic_info . use_channels ) ax [ 0 , i ] . set_title ( titles [ i ], fontsize = 10 ) if i == 0 : ax [ 0 , i ] . set_ylabel ( 'Dot Product to Best Dye Vector' ) ax [ 1 , i ] . set_ylabel ( 'Channel' ) fig . supxlabel ( 'Dye' , size = 12 ) mid_point = ( subplot_adjust [ 2 ] + subplot_adjust [ 3 ]) / 2 cbar_ax = fig . add_axes ([ subplot_adjust [ 1 ] + 0.01 , subplot_adjust [ 2 ], 0.005 , mid_point - subplot_adjust [ 2 ] - 0.04 ]) # left, bottom, width, height fig . colorbar ( im , cax = cbar_ax ) plt . suptitle ( f ' { title_start } at { n_plots } different stages \\n Box plots showing the dot product of spot round ' f 'vectors with the dye vector they best matched to in the bleed matrix' , size = 11 ) ax [ 1 , 1 ] . set_title ( 'Bleed Matrix where each dye column was computed from all vectors assigned to that dye ' 'in the boxplot above' , size = 11 ) plt . show ()","title":"view_scaled_k_means()"},{"location":"code/plot/extract/","text":"Viewer view_filter Function to view filtering of raw data in napari. There will be 2 scrollbars in 3D. One to change between raw/difference_of_hanning/difference_of_hanning+smoothed and one to change z-plane. There are also sliders to change the parameters for the filtering/smoothing. When the sliders are changed, the time taken for the new filtering/smoothing will be printed to the console. Note When r_smooth is set to [1, 1, 1] , no smoothing will be performed. When this is the case, changing the filtering radius using the slider will be quicker because it will only do filtering and not any smoothing. If r == anchor_round and c == dapi_channel , the filtering will be tophat filtering and no smoothing will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel. The current difference of hanning kernel can be viewed by pressing the 'h' key. Requires access to nb.file_names.input_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. None t int npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 r int round to view 0 c int Channel to view. 0 use_z Optional [ Union [ int , List [ int ]]] Which z-planes to load in from raw data. If None , will use load all z-planes (except from first one if config['basic_info']['ignore_first_z_plane'] == True ). None config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/extract/viewer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , nb : Optional [ Notebook ] = None , t : int = 0 , r : int = 0 , c : int = 0 , use_z : Optional [ Union [ int , List [ int ]]] = None , config_file : Optional [ str ] = None ): \"\"\" Function to view filtering of raw data in napari. There will be 2 scrollbars in 3D. One to change between *raw/difference_of_hanning/difference_of_hanning+smoothed* and one to change z-plane. There are also sliders to change the parameters for the filtering/smoothing. When the sliders are changed, the time taken for the new filtering/smoothing will be printed to the console. !!! note When `r_smooth` is set to `[1, 1, 1]`, no smoothing will be performed. When this is the case, changing the filtering radius using the slider will be quicker because it will only do filtering and not any smoothing. If `r == anchor_round` and `c == dapi_channel`, the filtering will be tophat filtering and no smoothing will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel. The current difference of hanning kernel can be viewed by pressing the 'h' key. !!! warning \"Requires access to `nb.file_names.input_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. t: npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | r: round to view c: Channel to view. use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first one if `config['basic_info']['ignore_first_z_plane'] == True`). config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if use_z is None : use_z = nb . basic_info . use_z t , r , c , use_z = number_to_list ([ t , r , c , use_z ]) self . image_raw = get_raw_images ( nb , t , r , c , use_z )[ 0 , 0 , 0 ] self . is_3d = nb . basic_info . is_3d if not self . is_3d : self . image_raw = extract . focus_stack ( self . image_raw ) self . image_plot = np . zeros (( 3 , nb . basic_info . tile_sz , nb . basic_info . tile_sz ), dtype = np . int32 ) self . image_plot [ 0 ] = self . image_raw else : self . image_plot = np . zeros (( 3 , len ( use_z ), nb . basic_info . tile_sz , nb . basic_info . tile_sz ), dtype = np . int32 ) self . image_plot [ 0 ] = np . moveaxis ( self . image_raw , 2 , 0 ) # put z axis first for plotting self . raw_max = self . image_raw . max () # find faulty columns and update self.image_raw so don't have to run strip_hack each time we filter self . image_raw , self . bad_columns = extract . strip_hack ( self . image_raw ) # Get default filter info # label for each image in image_plot self . ax0_labels = [ 'Raw' , 'Difference of Hanning' , 'Difference of Hanning and Smoothed' ] if not self . is_3d : self . ax0_labels [ 0 ] = 'Raw (Focus Stacked)' config = nb . get_config ()[ 'extract' ] if r [ 0 ] == nb . basic_info . anchor_round and c [ 0 ] == nb . basic_info . dapi_channel : self . dapi = True if config [ 'r_dapi' ] is None : if config [ 'r_dapi_auto_microns' ] is not None : config [ 'r_dapi' ] = extract . get_pixel_length ( config [ 'r_dapi_auto_microns' ], nb . basic_info . pixel_size_xy ) else : config [ 'r_dapi' ] = 48 # good starting value self . r_filter = config [ 'r_dapi' ] self . image_plot = self . image_plot [: 2 ] # no smoothing if dapi self . ax0_labels = self . ax0_labels [: 2 ] self . update_filter_image () r_filter_lims = [ 10 , 70 ] else : self . dapi = False self . r_filter = config [ 'r1' ] r2 = config [ 'r2' ] if self . r_filter is None : self . r_filter = extract . get_pixel_length ( config [ 'r1_auto_microns' ], nb . basic_info . pixel_size_xy ) if r2 is None : r2 = self . r_filter * 2 r_filter_lims = [ 2 , 10 ] self . update_filter_image ( r2 ) self . r_filter2 = r2 # Get default smoothing info if config [ 'r_smooth' ] is None : # start with no smoothing. Quicker to change filter params as no need to update smoothing too. config [ 'r_smooth' ] = [ 1 , 1 , 1 ] if not nb . basic_info . is_3d : config [ 'r_smooth' ] = config [ 'r_smooth' ][: 2 ] self . r_smooth = config [ 'r_smooth' ] self . update_smooth_image () self . viewer = napari . Viewer () self . viewer . add_image ( self . image_plot , name = f \"Tile { t [ 0 ] } , Round { r [ 0 ] } , Channel { c [ 0 ] } \" ) # Set min image contrast to 0 for better comparison between images self . viewer . layers [ 0 ] . contrast_limits = [ 0 , 0.9 * self . viewer . layers [ 0 ] . contrast_limits [ 1 ]] self . ax0_ind = 0 self . viewer . dims . set_point ( 0 , self . ax0_ind ) # set filter type to raw initially self . viewer . dims . events . current_step . connect ( self . filter_type_status ) self . filter_slider = QSlider ( Qt . Orientation . Horizontal ) self . filter_slider . setRange ( r_filter_lims [ 0 ], r_filter_lims [ 1 ]) self . filter_slider . setValue ( self . r_filter ) # When dragging, status will show r_filter value self . filter_slider . valueChanged . connect ( lambda x : self . show_filter_radius ( x )) # On release of slider, filtered / smoothed images updated self . filter_slider . sliderReleased . connect ( self . filter_slider_func ) if self . dapi : filter_slider_name = 'Tophat kernel radius' else : filter_slider_name = 'Difference of Hanning Radius' self . viewer . window . add_dock_widget ( self . filter_slider , area = \"left\" , name = filter_slider_name ) if not self . dapi : self . smooth_yx_slider = QSlider ( Qt . Orientation . Horizontal ) self . smooth_yx_slider . setRange ( 1 , 5 ) # gets very slow with large values self . smooth_yx_slider . setValue ( self . r_smooth [ 0 ]) # When dragging, status will show r_smooth value self . smooth_yx_slider . valueChanged . connect ( lambda x : self . show_smooth_radius_yx ( x )) # On release of slider, smoothed image updated self . smooth_yx_slider . sliderReleased . connect ( self . smooth_slider_func ) smooth_title = \"Smooth Radius\" if self . is_3d : smooth_title = smooth_title + \" YX\" self . viewer . window . add_dock_widget ( self . smooth_yx_slider , area = \"left\" , name = smooth_title ) if self . is_3d and not self . dapi : self . smooth_z_slider = QSlider ( Qt . Orientation . Horizontal ) self . smooth_z_slider . setRange ( 1 , 5 ) # gets very slow with large values self . smooth_z_slider . setValue ( self . r_smooth [ 2 ]) # When dragging, status will show r_smooth value self . smooth_z_slider . valueChanged . connect ( lambda x : self . show_smooth_radius_z ( x )) # On release of slider, smoothed image updated self . smooth_z_slider . sliderReleased . connect ( self . smooth_slider_func ) self . viewer . window . add_dock_widget ( self . smooth_z_slider , area = \"left\" , name = \"Smooth Radius Z\" ) if self . is_3d : self . viewer . dims . axis_labels = [ 'Filter Method' , 'z' , 'y' , 'x' ] else : self . viewer . dims . axis_labels = [ 'Filter Method' , 'y' , 'x' ] self . key_call_functions () napari . run () Diagnostics thresh_box_plots Function to plot distribution of auto_threshold values amongst tiles for each round and channel. Parameters: Name Type Description Default nb Notebook Notebook containing the extract NotebookPage. required Source code in coppafish/plot/extract/diagnostics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def thresh_box_plots ( nb : Notebook ): \"\"\" Function to plot distribution of auto_threshold values amongst tiles for each round and channel. Args: nb: Notebook containing the extract NotebookPage. \"\"\" box_data = [ nb . extract . auto_thresh [:, r , c ] for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] if nb . basic_info . use_anchor : box_data = box_data + [ nb . extract . auto_thresh [:, nb . basic_info . anchor_round , nb . basic_info . anchor_channel ]] fig , ax1 = plt . subplots ( figsize = ( 10 , 6 )) fig . subplots_adjust ( left = 0.075 , right = 0.95 , bottom = 0.15 ) n_use_channels = len ( nb . basic_info . use_channels ) # different boxplot color for each channel (+ different color for anchor channel) # Must be distinct from black and white channel_colors = distinctipy . get_colors ( n_use_channels + int ( nb . basic_info . use_anchor ), [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) bp = ax1 . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) c = - 1 tick_labels = np . tile ( nb . basic_info . use_rounds , n_use_channels ) . tolist () leg_labels = nb . basic_info . use_channels if nb . basic_info . use_anchor : tick_labels = tick_labels + [ 'Anchor' ] leg_labels = leg_labels + [ f 'Anchor ( { nb . basic_info . anchor_channel } )' ] ax1 . set_xticklabels ( tick_labels ) if nb . basic_info . use_anchor : ticks = ax1 . get_xticklabels () ticks [ - 1 ] . set_rotation ( 90 ) leg_markers = [] for i in range ( len ( box_data )): if i % n_use_channels == 0 : c += 1 leg_markers = leg_markers + [ bp [ 'boxes' ][ i ]] bp [ 'boxes' ][ i ] . set_facecolor ( channel_colors [ c ]) ax1 . legend ( leg_markers , leg_labels , title = 'Channel' ) ax1 . set_xlabel ( 'Round' ) ax1 . set_ylabel ( 'Auto Threshold' ) ax1 . set_title ( 'Boxplots showing distribution of Auto Threshold amongst tiles for each round and channel' ) plt . show () histogram_plots Plots histograms showing distribution of intensity values combined from all tiles for each round and channel. There is also a Norm button which equalises color channels so all color channels should have most intensity values between -1 and 1. In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels and a long tail from around 0.1 to just beyond 1 accounting for spot pixels. Parameters: Name Type Description Default nb Notebook Notebook containing the extract NotebookPage. required Source code in coppafish/plot/extract/diagnostics.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , nb : Notebook ): \"\"\" Plots histograms showing distribution of intensity values combined from all tiles for each round and channel. There is also a Norm button which equalises color channels so all color channels should have most intensity values between -1 and 1. In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels and a long tail from around 0.1 to just beyond 1 accounting for spot pixels. Args: nb: Notebook containing the extract NotebookPage. \"\"\" self . use_rounds = nb . basic_info . use_rounds self . use_channels = nb . basic_info . use_channels self . hist_values = nb . extract . hist_values self . hist_values_norm = np . arange ( - 10 , 10.004 , 0.005 ) n_use_rounds = len ( self . use_rounds ) n_use_channels = len ( self . use_channels ) self . fig , self . ax1 = plt . subplots ( n_use_channels , n_use_rounds , figsize = ( 10 , 6 ), sharey = True , sharex = True ) self . ax1 = self . ax1 . flatten () # Compute color_norm_factor as it will be computed at call_spots step of pipeline config = nb . get_config ()[ 'call_spots' ] rc_ind = np . ix_ ( self . use_rounds , self . use_channels ) hist_counts_use = np . moveaxis ( np . moveaxis ( nb . extract . hist_counts , 0 , - 1 )[ rc_ind ], - 1 , 0 ) self . color_norm_factor = color_normalisation ( self . hist_values , hist_counts_use , config [ 'color_norm_intensities' ], config [ 'color_norm_probs' ], config [ 'bleed_matrix_method' ]) self . color_norm_factor = self . color_norm_factor . T . flatten () i = 0 min_value = 3 # clip hist counts to this so don't get log(0) error self . plot_lines = [] self . hist_counts_norm = [] self . hist_counts = [] for c in self . use_channels : for r in self . use_rounds : # Clip histogram to stop log(0) error. self . hist_counts = self . hist_counts + [ np . clip ( nb . extract . hist_counts [:, r , c ], min_value , np . inf )] self . hist_counts_norm = self . hist_counts_norm + \\ [ resample_histogram ( self . hist_values / self . color_norm_factor [ i ], self . hist_counts [ i ], self . hist_values_norm )] # Normalise histograms to give probabilities self . hist_counts [ i ] = self . hist_counts [ i ] / np . sum ( nb . extract . hist_counts [:, r , c ]) self . hist_counts_norm [ i ] = self . hist_counts_norm [ i ] / np . sum ( nb . extract . hist_counts [:, r , c ]) self . plot_lines = self . plot_lines + self . ax1 [ i ] . plot ( self . hist_values , self . hist_counts [ i ]) if r == nb . basic_info . use_rounds [ 0 ]: self . ax1 [ i ] . set_ylabel ( c ) if c == nb . basic_info . use_channels [ - 1 ]: self . ax1 [ i ] . set_xlabel ( r ) i += 1 self . ax1 [ 0 ] . set_yscale ( 'log' ) self . fig . supylabel ( 'Channel' ) self . fig . supxlabel ( 'Round' ) plt . suptitle ( 'Histograms showing distribution of intensity values combined from ' 'all tiles for each round and channel' ) self . norm = False self . xlims_norm = [ - 1 , 1 ] self . xlims = [ - 300 , 300 ] self . ax1 [ 0 ] . set_xlim ( self . xlims [ 0 ], self . xlims [ 1 ]) self . norm_button_ax = self . fig . add_axes ([ 0.85 , 0.02 , 0.1 , 0.05 ]) self . norm_button = Button ( self . norm_button_ax , 'Norm' , hovercolor = '0.275' ) self . norm_button . on_clicked ( self . change_norm ) plt . show ()","title":"Extract"},{"location":"code/plot/extract/#viewer","text":"","title":"Viewer"},{"location":"code/plot/extract/#view_filter","text":"Function to view filtering of raw data in napari. There will be 2 scrollbars in 3D. One to change between raw/difference_of_hanning/difference_of_hanning+smoothed and one to change z-plane. There are also sliders to change the parameters for the filtering/smoothing. When the sliders are changed, the time taken for the new filtering/smoothing will be printed to the console. Note When r_smooth is set to [1, 1, 1] , no smoothing will be performed. When this is the case, changing the filtering radius using the slider will be quicker because it will only do filtering and not any smoothing. If r == anchor_round and c == dapi_channel , the filtering will be tophat filtering and no smoothing will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel. The current difference of hanning kernel can be viewed by pressing the 'h' key. Requires access to nb.file_names.input_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. None t int npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 r int round to view 0 c int Channel to view. 0 use_z Optional [ Union [ int , List [ int ]]] Which z-planes to load in from raw data. If None , will use load all z-planes (except from first one if config['basic_info']['ignore_first_z_plane'] == True ). None config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/extract/viewer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def __init__ ( self , nb : Optional [ Notebook ] = None , t : int = 0 , r : int = 0 , c : int = 0 , use_z : Optional [ Union [ int , List [ int ]]] = None , config_file : Optional [ str ] = None ): \"\"\" Function to view filtering of raw data in napari. There will be 2 scrollbars in 3D. One to change between *raw/difference_of_hanning/difference_of_hanning+smoothed* and one to change z-plane. There are also sliders to change the parameters for the filtering/smoothing. When the sliders are changed, the time taken for the new filtering/smoothing will be printed to the console. !!! note When `r_smooth` is set to `[1, 1, 1]`, no smoothing will be performed. When this is the case, changing the filtering radius using the slider will be quicker because it will only do filtering and not any smoothing. If `r == anchor_round` and `c == dapi_channel`, the filtering will be tophat filtering and no smoothing will be allowed. Otherwise, the filtering will be convolution with a difference of hanning kernel. The current difference of hanning kernel can be viewed by pressing the 'h' key. !!! warning \"Requires access to `nb.file_names.input_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. t: npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | r: round to view c: Channel to view. use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first one if `config['basic_info']['ignore_first_z_plane'] == True`). config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if use_z is None : use_z = nb . basic_info . use_z t , r , c , use_z = number_to_list ([ t , r , c , use_z ]) self . image_raw = get_raw_images ( nb , t , r , c , use_z )[ 0 , 0 , 0 ] self . is_3d = nb . basic_info . is_3d if not self . is_3d : self . image_raw = extract . focus_stack ( self . image_raw ) self . image_plot = np . zeros (( 3 , nb . basic_info . tile_sz , nb . basic_info . tile_sz ), dtype = np . int32 ) self . image_plot [ 0 ] = self . image_raw else : self . image_plot = np . zeros (( 3 , len ( use_z ), nb . basic_info . tile_sz , nb . basic_info . tile_sz ), dtype = np . int32 ) self . image_plot [ 0 ] = np . moveaxis ( self . image_raw , 2 , 0 ) # put z axis first for plotting self . raw_max = self . image_raw . max () # find faulty columns and update self.image_raw so don't have to run strip_hack each time we filter self . image_raw , self . bad_columns = extract . strip_hack ( self . image_raw ) # Get default filter info # label for each image in image_plot self . ax0_labels = [ 'Raw' , 'Difference of Hanning' , 'Difference of Hanning and Smoothed' ] if not self . is_3d : self . ax0_labels [ 0 ] = 'Raw (Focus Stacked)' config = nb . get_config ()[ 'extract' ] if r [ 0 ] == nb . basic_info . anchor_round and c [ 0 ] == nb . basic_info . dapi_channel : self . dapi = True if config [ 'r_dapi' ] is None : if config [ 'r_dapi_auto_microns' ] is not None : config [ 'r_dapi' ] = extract . get_pixel_length ( config [ 'r_dapi_auto_microns' ], nb . basic_info . pixel_size_xy ) else : config [ 'r_dapi' ] = 48 # good starting value self . r_filter = config [ 'r_dapi' ] self . image_plot = self . image_plot [: 2 ] # no smoothing if dapi self . ax0_labels = self . ax0_labels [: 2 ] self . update_filter_image () r_filter_lims = [ 10 , 70 ] else : self . dapi = False self . r_filter = config [ 'r1' ] r2 = config [ 'r2' ] if self . r_filter is None : self . r_filter = extract . get_pixel_length ( config [ 'r1_auto_microns' ], nb . basic_info . pixel_size_xy ) if r2 is None : r2 = self . r_filter * 2 r_filter_lims = [ 2 , 10 ] self . update_filter_image ( r2 ) self . r_filter2 = r2 # Get default smoothing info if config [ 'r_smooth' ] is None : # start with no smoothing. Quicker to change filter params as no need to update smoothing too. config [ 'r_smooth' ] = [ 1 , 1 , 1 ] if not nb . basic_info . is_3d : config [ 'r_smooth' ] = config [ 'r_smooth' ][: 2 ] self . r_smooth = config [ 'r_smooth' ] self . update_smooth_image () self . viewer = napari . Viewer () self . viewer . add_image ( self . image_plot , name = f \"Tile { t [ 0 ] } , Round { r [ 0 ] } , Channel { c [ 0 ] } \" ) # Set min image contrast to 0 for better comparison between images self . viewer . layers [ 0 ] . contrast_limits = [ 0 , 0.9 * self . viewer . layers [ 0 ] . contrast_limits [ 1 ]] self . ax0_ind = 0 self . viewer . dims . set_point ( 0 , self . ax0_ind ) # set filter type to raw initially self . viewer . dims . events . current_step . connect ( self . filter_type_status ) self . filter_slider = QSlider ( Qt . Orientation . Horizontal ) self . filter_slider . setRange ( r_filter_lims [ 0 ], r_filter_lims [ 1 ]) self . filter_slider . setValue ( self . r_filter ) # When dragging, status will show r_filter value self . filter_slider . valueChanged . connect ( lambda x : self . show_filter_radius ( x )) # On release of slider, filtered / smoothed images updated self . filter_slider . sliderReleased . connect ( self . filter_slider_func ) if self . dapi : filter_slider_name = 'Tophat kernel radius' else : filter_slider_name = 'Difference of Hanning Radius' self . viewer . window . add_dock_widget ( self . filter_slider , area = \"left\" , name = filter_slider_name ) if not self . dapi : self . smooth_yx_slider = QSlider ( Qt . Orientation . Horizontal ) self . smooth_yx_slider . setRange ( 1 , 5 ) # gets very slow with large values self . smooth_yx_slider . setValue ( self . r_smooth [ 0 ]) # When dragging, status will show r_smooth value self . smooth_yx_slider . valueChanged . connect ( lambda x : self . show_smooth_radius_yx ( x )) # On release of slider, smoothed image updated self . smooth_yx_slider . sliderReleased . connect ( self . smooth_slider_func ) smooth_title = \"Smooth Radius\" if self . is_3d : smooth_title = smooth_title + \" YX\" self . viewer . window . add_dock_widget ( self . smooth_yx_slider , area = \"left\" , name = smooth_title ) if self . is_3d and not self . dapi : self . smooth_z_slider = QSlider ( Qt . Orientation . Horizontal ) self . smooth_z_slider . setRange ( 1 , 5 ) # gets very slow with large values self . smooth_z_slider . setValue ( self . r_smooth [ 2 ]) # When dragging, status will show r_smooth value self . smooth_z_slider . valueChanged . connect ( lambda x : self . show_smooth_radius_z ( x )) # On release of slider, smoothed image updated self . smooth_z_slider . sliderReleased . connect ( self . smooth_slider_func ) self . viewer . window . add_dock_widget ( self . smooth_z_slider , area = \"left\" , name = \"Smooth Radius Z\" ) if self . is_3d : self . viewer . dims . axis_labels = [ 'Filter Method' , 'z' , 'y' , 'x' ] else : self . viewer . dims . axis_labels = [ 'Filter Method' , 'y' , 'x' ] self . key_call_functions () napari . run ()","title":"view_filter"},{"location":"code/plot/extract/#diagnostics","text":"","title":"Diagnostics"},{"location":"code/plot/extract/#thresh_box_plots","text":"Function to plot distribution of auto_threshold values amongst tiles for each round and channel. Parameters: Name Type Description Default nb Notebook Notebook containing the extract NotebookPage. required Source code in coppafish/plot/extract/diagnostics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def thresh_box_plots ( nb : Notebook ): \"\"\" Function to plot distribution of auto_threshold values amongst tiles for each round and channel. Args: nb: Notebook containing the extract NotebookPage. \"\"\" box_data = [ nb . extract . auto_thresh [:, r , c ] for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] if nb . basic_info . use_anchor : box_data = box_data + [ nb . extract . auto_thresh [:, nb . basic_info . anchor_round , nb . basic_info . anchor_channel ]] fig , ax1 = plt . subplots ( figsize = ( 10 , 6 )) fig . subplots_adjust ( left = 0.075 , right = 0.95 , bottom = 0.15 ) n_use_channels = len ( nb . basic_info . use_channels ) # different boxplot color for each channel (+ different color for anchor channel) # Must be distinct from black and white channel_colors = distinctipy . get_colors ( n_use_channels + int ( nb . basic_info . use_anchor ), [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) bp = ax1 . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) c = - 1 tick_labels = np . tile ( nb . basic_info . use_rounds , n_use_channels ) . tolist () leg_labels = nb . basic_info . use_channels if nb . basic_info . use_anchor : tick_labels = tick_labels + [ 'Anchor' ] leg_labels = leg_labels + [ f 'Anchor ( { nb . basic_info . anchor_channel } )' ] ax1 . set_xticklabels ( tick_labels ) if nb . basic_info . use_anchor : ticks = ax1 . get_xticklabels () ticks [ - 1 ] . set_rotation ( 90 ) leg_markers = [] for i in range ( len ( box_data )): if i % n_use_channels == 0 : c += 1 leg_markers = leg_markers + [ bp [ 'boxes' ][ i ]] bp [ 'boxes' ][ i ] . set_facecolor ( channel_colors [ c ]) ax1 . legend ( leg_markers , leg_labels , title = 'Channel' ) ax1 . set_xlabel ( 'Round' ) ax1 . set_ylabel ( 'Auto Threshold' ) ax1 . set_title ( 'Boxplots showing distribution of Auto Threshold amongst tiles for each round and channel' ) plt . show ()","title":"thresh_box_plots"},{"location":"code/plot/extract/#histogram_plots","text":"Plots histograms showing distribution of intensity values combined from all tiles for each round and channel. There is also a Norm button which equalises color channels so all color channels should have most intensity values between -1 and 1. In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels and a long tail from around 0.1 to just beyond 1 accounting for spot pixels. Parameters: Name Type Description Default nb Notebook Notebook containing the extract NotebookPage. required Source code in coppafish/plot/extract/diagnostics.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , nb : Notebook ): \"\"\" Plots histograms showing distribution of intensity values combined from all tiles for each round and channel. There is also a Norm button which equalises color channels so all color channels should have most intensity values between -1 and 1. In the normalised histograms, a good channel will have a sharp peak near 0 accounting for non-spot pixels and a long tail from around 0.1 to just beyond 1 accounting for spot pixels. Args: nb: Notebook containing the extract NotebookPage. \"\"\" self . use_rounds = nb . basic_info . use_rounds self . use_channels = nb . basic_info . use_channels self . hist_values = nb . extract . hist_values self . hist_values_norm = np . arange ( - 10 , 10.004 , 0.005 ) n_use_rounds = len ( self . use_rounds ) n_use_channels = len ( self . use_channels ) self . fig , self . ax1 = plt . subplots ( n_use_channels , n_use_rounds , figsize = ( 10 , 6 ), sharey = True , sharex = True ) self . ax1 = self . ax1 . flatten () # Compute color_norm_factor as it will be computed at call_spots step of pipeline config = nb . get_config ()[ 'call_spots' ] rc_ind = np . ix_ ( self . use_rounds , self . use_channels ) hist_counts_use = np . moveaxis ( np . moveaxis ( nb . extract . hist_counts , 0 , - 1 )[ rc_ind ], - 1 , 0 ) self . color_norm_factor = color_normalisation ( self . hist_values , hist_counts_use , config [ 'color_norm_intensities' ], config [ 'color_norm_probs' ], config [ 'bleed_matrix_method' ]) self . color_norm_factor = self . color_norm_factor . T . flatten () i = 0 min_value = 3 # clip hist counts to this so don't get log(0) error self . plot_lines = [] self . hist_counts_norm = [] self . hist_counts = [] for c in self . use_channels : for r in self . use_rounds : # Clip histogram to stop log(0) error. self . hist_counts = self . hist_counts + [ np . clip ( nb . extract . hist_counts [:, r , c ], min_value , np . inf )] self . hist_counts_norm = self . hist_counts_norm + \\ [ resample_histogram ( self . hist_values / self . color_norm_factor [ i ], self . hist_counts [ i ], self . hist_values_norm )] # Normalise histograms to give probabilities self . hist_counts [ i ] = self . hist_counts [ i ] / np . sum ( nb . extract . hist_counts [:, r , c ]) self . hist_counts_norm [ i ] = self . hist_counts_norm [ i ] / np . sum ( nb . extract . hist_counts [:, r , c ]) self . plot_lines = self . plot_lines + self . ax1 [ i ] . plot ( self . hist_values , self . hist_counts [ i ]) if r == nb . basic_info . use_rounds [ 0 ]: self . ax1 [ i ] . set_ylabel ( c ) if c == nb . basic_info . use_channels [ - 1 ]: self . ax1 [ i ] . set_xlabel ( r ) i += 1 self . ax1 [ 0 ] . set_yscale ( 'log' ) self . fig . supylabel ( 'Channel' ) self . fig . supxlabel ( 'Round' ) plt . suptitle ( 'Histograms showing distribution of intensity values combined from ' 'all tiles for each round and channel' ) self . norm = False self . xlims_norm = [ - 1 , 1 ] self . xlims = [ - 300 , 300 ] self . ax1 [ 0 ] . set_xlim ( self . xlims [ 0 ], self . xlims [ 1 ]) self . norm_button_ax = self . fig . add_axes ([ 0.85 , 0.02 , 0.1 , 0.05 ]) self . norm_button = Button ( self . norm_button_ax , 'Norm' , hovercolor = '0.275' ) self . norm_button . on_clicked ( self . change_norm ) plt . show ()","title":"histogram_plots"},{"location":"code/plot/find_spots/","text":"view_find_spots This viewer shows how spots are detected in an image. There are sliders to vary the parameters used for spot detection so the effect of them can be seen. Can also view points from \u00b1z_thick z-planes on current z-plane using the z thickness slider. Initially, z thickness will be 1. Requires access to nb.file_names.input_dir or nb.file_names.tile_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. In this case, the raw images will be loaded and then filtered according to parameters in config['extract'] . None t int npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 r int round to view 0 c int Channel to view. 0 show_isolated bool Spots which are identified as isolated in the anchor round/channel are used to compute the bleed_matrix . Can see which spots are isolated by setting this to True . Note, this is very slow in 3D , around 300s for a 2048 x 2048 x 50 image. False config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/find_spots/viewer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def __init__ ( self , nb : Optional [ Notebook ] = None , t : int = 0 , r : int = 0 , c : int = 0 , show_isolated : bool = False , config_file : Optional [ str ] = None ): \"\"\" This viewer shows how spots are detected in an image. There are sliders to vary the parameters used for spot detection so the effect of them can be seen. Can also view points from `\u00b1z_thick` z-planes on current z-plane using the z thickness slider. Initially, z thickness will be 1. !!! warning \"Requires access to `nb.file_names.input_dir` or `nb.file_names.tile_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. In this case, the raw images will be loaded and then filtered according to parameters in `config['extract']`. t: npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | r: round to view c: Channel to view. show_isolated: Spots which are identified as *isolated* in the anchor round/channel are used to compute the `bleed_matrix`. Can see which spots are isolated by setting this to `True`. Note, this is very slow in *3D*, around 300s for a 2048 x 2048 x 50 image. config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if r == nb . basic_info . anchor_round : if c != nb . basic_info . anchor_channel : raise ValueError ( f 'No spots are found on round { r } , channel { c } in the pipeline. \\n ' f 'Only spots on anchor_channel = { nb . basic_info . anchor_channel } used for the ' f 'anchor round.' ) if r == nb . basic_info . ref_round and c == nb . basic_info . ref_channel : if show_isolated : self . show_isolated = True else : self . show_isolated = False else : if show_isolated : warnings . warn ( f 'Not showing isolated spots as slow and isolated status not used for round { r } ,' f ' channel { c } ' ) self . show_isolated = False self . is_3d = nb . basic_info . is_3d if self . is_3d : tile_file = nb . file_names . tile [ t ][ r ][ c ] else : tile_file = nb . file_names . tile [ t ][ r ] if not os . path . isfile ( tile_file ): warnings . warn ( f \"The file { tile_file } \\n does not exist so loading raw image and filtering it\" ) self . image = get_filtered_image ( nb , t , r , c ) else : self . image = utils . npy . load_tile ( nb . file_names , nb . basic_info , t , r , c ) scale = 1 # Can be any value as not actually used but needed as argument in get_extract_info # Get auto_threshold value used to detect spots if nb . has_page ( 'extract' ): self . auto_thresh = nb . extract . auto_thresh [ t , r , c ] else : config = nb . get_config ()[ 'extract' ] z_info = int ( np . floor ( nb . basic_info . nz / 2 )) hist_values = np . arange ( - nb . basic_info . tile_pixel_value_shift , np . iinfo ( np . uint16 ) . max - nb . basic_info . tile_pixel_value_shift + 2 , 1 ) hist_bin_edges = np . concatenate (( hist_values - 0.5 , hist_values [ - 1 :] + 0.5 )) max_npy_pixel_value = np . iinfo ( np . uint16 ) . max - nb . basic_info . tile_pixel_value_shift self . auto_thresh = extract . get_extract_info ( self . image , config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale , z_info )[ 0 ] config = nb . get_config ()[ 'find_spots' ] self . r_xy = config [ 'radius_xy' ] if self . is_3d : self . r_z = config [ 'radius_z' ] else : self . r_z = None if config [ 'isolation_thresh' ] is None : config [ 'isolation_thresh' ] = self . auto_thresh * config [ 'auto_isolation_thresh_multiplier' ] self . isolation_thresh = config [ 'isolation_thresh' ] self . r_isolation_inner = config [ 'isolation_radius_inner' ] self . r_isolation_xy = config [ 'isolation_radius_xy' ] self . normal_color = np . array ([ 1 , 0 , 0 , 1 ]) # red self . isolation_color = np . array ([ 0 , 1 , 0 , 1 ]) # green self . neg_neighb_color = np . array ([ 0 , 0 , 1 , 1 ]) # blue self . point_size = 9 self . z_thick = 1 # show +/- 1 plane initially self . z_thick_list = np . arange ( 1 , 1 + 15 * 2 , 2 ) # only odd z-thick make any difference if self . is_3d : self . r_isolation_z = config [ 'isolation_radius_z' ] else : self . r_isolation_z = None self . small = 1e-6 # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image). # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = self . small * 2 , high = 0.2 , size = self . image . shape ) self . image = self . image + rand_im_shift self . dilate = None self . spot_zyx = None self . image_isolated = None self . no_negative_neighbour = None self . update_dilate () if self . show_isolated : self . update_isolated_image () self . viewer = napari . Viewer () name = f \"Tile { t } , Round { r } , Channel { c } \" if self . is_3d : self . viewer . add_image ( np . moveaxis ( np . rint ( self . image ) . astype ( np . int32 ), 2 , 0 ), name = name ) else : self . viewer . add_image ( np . rint ( self . image ) . astype ( np . int32 ), name = name ) self . thresh_slider = QSlider ( Qt . Orientation . Horizontal ) self . thresh_slider . setRange ( 0 , 3 * self . auto_thresh ) self . thresh_slider . setValue ( self . auto_thresh ) # When dragging, status will show auto_thresh value self . thresh_slider . valueChanged . connect ( lambda x : self . show_thresh ( x )) # On release of slider, filtered / smoothed images updated self . thresh_slider . sliderReleased . connect ( self . update_spots ) self . viewer . window . add_dock_widget ( self . thresh_slider , area = \"left\" , name = 'Intensity Threshold' ) if self . show_isolated : self . isolation_thresh_slider = QSlider ( Qt . Orientation . Horizontal ) self . isolation_thresh_slider . setRange ( - 2 * np . abs ( self . isolation_thresh ), 0 ) self . isolation_thresh_slider . setValue ( self . isolation_thresh ) # When dragging, status will show auto_thresh value self . isolation_thresh_slider . valueChanged . connect ( lambda x : self . show_isolation_thresh ( x )) # On release of slider, filtered / smoothed images updated self . isolation_thresh_slider . sliderReleased . connect ( self . update_isolated_spots ) self . update_spots () self . r_xy_slider = QSlider ( Qt . Orientation . Horizontal ) self . r_xy_slider . setRange ( 2 , 10 ) self . r_xy_slider . setValue ( self . r_xy ) # When dragging, status will show r_xy value self . r_xy_slider . valueChanged . connect ( lambda x : self . show_radius_xy ( x )) # On release of slider, filtered / smoothed images updated self . r_xy_slider . sliderReleased . connect ( self . radius_slider_func ) self . viewer . window . add_dock_widget ( self . r_xy_slider , area = \"left\" , name = 'Detection Radius YX' ) if self . is_3d : self . r_z_slider = QSlider ( Qt . Orientation . Horizontal ) self . r_z_slider . setRange ( 2 , 12 ) self . r_z_slider . setValue ( self . r_xy ) # When dragging, status will show r_z value self . r_z_slider . valueChanged . connect ( lambda x : self . show_radius_z ( x )) # On release of slider, filtered / smoothed images updated self . r_z_slider . sliderReleased . connect ( self . radius_slider_func ) self . viewer . window . add_dock_widget ( self . r_z_slider , area = \"left\" , name = 'Detection Radius Z' ) self . z_thick_slider = QSlider ( Qt . Orientation . Horizontal ) self . z_thick_slider . setRange ( 0 , int (( self . z_thick_list [ - 1 ] - 1 ) / 2 )) self . z_thick_slider . setValue ( self . z_thick ) # When dragging, status will show r_z value self . z_thick_slider . valueChanged . connect ( lambda x : self . change_z_thick ( x )) self . viewer . window . add_dock_widget ( self . z_thick_slider , area = \"left\" , name = 'Z Thickness' ) if self . show_isolated : self . viewer . window . add_dock_widget ( self . isolation_thresh_slider , area = \"left\" , name = 'Isolation Threshold' ) # set image as selected layer so can see intensity values in status self . viewer . layers . selection . active = self . viewer . layers [ 0 ] napari . run () n_spots_grid ( nb , n_spots_thresh = None ) Plots a grid indicating the number of spots detected on each tile, round and channel. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required n_spots_thresh Optional [ int ] tiles/rounds/channels with fewer spots than this will be highlighted. If None , will use n_spots_warn_fraction from config file. None Source code in coppafish/plot/find_spots/n_spots.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def n_spots_grid ( nb : Notebook , n_spots_thresh : Optional [ int ] = None ): \"\"\" Plots a grid indicating the number of spots detected on each tile, round and channel. Args: nb: *Notebook* containing `find_spots` page. n_spots_thresh: tiles/rounds/channels with fewer spots than this will be highlighted. If `None`, will use `n_spots_warn_fraction` from config file. \"\"\" if n_spots_thresh is None : config = nb . get_config ()[ 'find_spots' ] if nb . basic_info . is_3d : n_spots_thresh = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_3d' ] * nb . basic_info . nz else : n_spots_thresh = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_2d' ] n_spots_thresh = int ( np . ceil ( n_spots_thresh )) use_tiles = np . asarray ( nb . basic_info . use_tiles ) use_rounds = np . asarray ( nb . basic_info . use_rounds ) # don't consider anchor in this analysis if len ( use_rounds ) > 0 : use_channels = np . asarray ( nb . basic_info . use_channels ) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] spot_no = np . moveaxis ( spot_no , 1 , 2 ) # put rounds last spot_no = spot_no . reshape ( len ( use_tiles ), - 1 ) . T # spot_no[n_rounds, t] is now spot_no[t, r=0, c=1] n_round_channels = len ( use_rounds ) * len ( use_channels ) y_labels = np . tile ( use_rounds , len ( use_channels )) vmax = spot_no . max () # clip colorbar at max of imaging rounds/channels because anchor can be a lot higher else : # Deal with case where only anchor round use_channels = np . asarray ([]) spot_no = np . zeros (( 0 , len ( use_tiles )), dtype = np . int32 ) n_round_channels = 0 y_labels = np . zeros ( 0 , dtype = int ) vmax = None if nb . basic_info . use_anchor : anchor_spot_no = nb . find_spots . spot_no [ use_tiles , nb . basic_info . anchor_round , nb . basic_info . anchor_channel ][ np . newaxis ] spot_no = np . append ( spot_no , anchor_spot_no , axis = 0 ) y_labels = y_labels . astype ( str ) . tolist () y_labels += [ 'Anchor' ] n_round_channels += 1 if vmax is None : vmax = spot_no . max () fig , ax = plt . subplots ( 1 , 1 , figsize = ( np . clip ( 5 + len ( use_tiles ) / 2 , 3 , 18 ), 12 )) subplot_adjust = [ 0.12 , 1 , 0.07 , 0.9 ] fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) im = ax . imshow ( spot_no , aspect = 'auto' , vmax = vmax ) fig . colorbar ( im , ax = ax ) ax . set_yticks ( np . arange ( n_round_channels )) ax . set_yticklabels ( y_labels ) ax . set_xticks ( np . arange ( len ( use_tiles ))) ax . set_xticklabels ( use_tiles ) ax . set_xlabel ( 'Tile' ) for c in range ( len ( use_channels )): y_ind = 1 - c * 1 / ( len ( use_channels )) ax . text ( - 0.1 , y_ind , f \"Channel { use_channels [ c ] } \" , va = \"top\" , ha = \"left\" , transform = ax . transAxes , rotation = 'vertical' ) fig . supylabel ( 'Channel/Round' , transform = ax . transAxes , x =- 0.15 ) plt . xticks ( rotation = 90 ) low_spots = np . where ( spot_no < n_spots_thresh ) for j in range ( len ( low_spots [ 0 ])): rectangle = plt . Rectangle (( low_spots [ 1 ][ j ] - 0.5 , low_spots [ 0 ][ j ] - 0.5 ), 1 , 1 , fill = False , ec = \"r\" , linestyle = ':' , lw = 2 ) ax . add_patch ( rectangle ) plt . suptitle ( f \"Number of Spots Found on each Tile, Round and Channel\" ) plt . show ()","title":"Find Spots"},{"location":"code/plot/find_spots/#view_find_spots","text":"This viewer shows how spots are detected in an image. There are sliders to vary the parameters used for spot detection so the effect of them can be seen. Can also view points from \u00b1z_thick z-planes on current z-plane using the z thickness slider. Initially, z thickness will be 1. Requires access to nb.file_names.input_dir or nb.file_names.tile_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. In this case, the raw images will be loaded and then filtered according to parameters in config['extract'] . None t int npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 r int round to view 0 c int Channel to view. 0 show_isolated bool Spots which are identified as isolated in the anchor round/channel are used to compute the bleed_matrix . Can see which spots are isolated by setting this to True . Note, this is very slow in 3D , around 300s for a 2048 x 2048 x 50 image. False config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/find_spots/viewer.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def __init__ ( self , nb : Optional [ Notebook ] = None , t : int = 0 , r : int = 0 , c : int = 0 , show_isolated : bool = False , config_file : Optional [ str ] = None ): \"\"\" This viewer shows how spots are detected in an image. There are sliders to vary the parameters used for spot detection so the effect of them can be seen. Can also view points from `\u00b1z_thick` z-planes on current z-plane using the z thickness slider. Initially, z thickness will be 1. !!! warning \"Requires access to `nb.file_names.input_dir` or `nb.file_names.tile_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. In this case, the raw images will be loaded and then filtered according to parameters in `config['extract']`. t: npy (as opposed to nd2 fov) tile index to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | r: round to view c: Channel to view. show_isolated: Spots which are identified as *isolated* in the anchor round/channel are used to compute the `bleed_matrix`. Can see which spots are isolated by setting this to `True`. Note, this is very slow in *3D*, around 300s for a 2048 x 2048 x 50 image. config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if r == nb . basic_info . anchor_round : if c != nb . basic_info . anchor_channel : raise ValueError ( f 'No spots are found on round { r } , channel { c } in the pipeline. \\n ' f 'Only spots on anchor_channel = { nb . basic_info . anchor_channel } used for the ' f 'anchor round.' ) if r == nb . basic_info . ref_round and c == nb . basic_info . ref_channel : if show_isolated : self . show_isolated = True else : self . show_isolated = False else : if show_isolated : warnings . warn ( f 'Not showing isolated spots as slow and isolated status not used for round { r } ,' f ' channel { c } ' ) self . show_isolated = False self . is_3d = nb . basic_info . is_3d if self . is_3d : tile_file = nb . file_names . tile [ t ][ r ][ c ] else : tile_file = nb . file_names . tile [ t ][ r ] if not os . path . isfile ( tile_file ): warnings . warn ( f \"The file { tile_file } \\n does not exist so loading raw image and filtering it\" ) self . image = get_filtered_image ( nb , t , r , c ) else : self . image = utils . npy . load_tile ( nb . file_names , nb . basic_info , t , r , c ) scale = 1 # Can be any value as not actually used but needed as argument in get_extract_info # Get auto_threshold value used to detect spots if nb . has_page ( 'extract' ): self . auto_thresh = nb . extract . auto_thresh [ t , r , c ] else : config = nb . get_config ()[ 'extract' ] z_info = int ( np . floor ( nb . basic_info . nz / 2 )) hist_values = np . arange ( - nb . basic_info . tile_pixel_value_shift , np . iinfo ( np . uint16 ) . max - nb . basic_info . tile_pixel_value_shift + 2 , 1 ) hist_bin_edges = np . concatenate (( hist_values - 0.5 , hist_values [ - 1 :] + 0.5 )) max_npy_pixel_value = np . iinfo ( np . uint16 ) . max - nb . basic_info . tile_pixel_value_shift self . auto_thresh = extract . get_extract_info ( self . image , config [ 'auto_thresh_multiplier' ], hist_bin_edges , max_npy_pixel_value , scale , z_info )[ 0 ] config = nb . get_config ()[ 'find_spots' ] self . r_xy = config [ 'radius_xy' ] if self . is_3d : self . r_z = config [ 'radius_z' ] else : self . r_z = None if config [ 'isolation_thresh' ] is None : config [ 'isolation_thresh' ] = self . auto_thresh * config [ 'auto_isolation_thresh_multiplier' ] self . isolation_thresh = config [ 'isolation_thresh' ] self . r_isolation_inner = config [ 'isolation_radius_inner' ] self . r_isolation_xy = config [ 'isolation_radius_xy' ] self . normal_color = np . array ([ 1 , 0 , 0 , 1 ]) # red self . isolation_color = np . array ([ 0 , 1 , 0 , 1 ]) # green self . neg_neighb_color = np . array ([ 0 , 0 , 1 , 1 ]) # blue self . point_size = 9 self . z_thick = 1 # show +/- 1 plane initially self . z_thick_list = np . arange ( 1 , 1 + 15 * 2 , 2 ) # only odd z-thick make any difference if self . is_3d : self . r_isolation_z = config [ 'isolation_radius_z' ] else : self . r_isolation_z = None self . small = 1e-6 # for computing local maxima: shouldn't matter what it is (keep below 0.01 for int image). # perturb image by small amount so two neighbouring pixels that did have the same value now differ slightly. # hence when find maxima, will only get one of the pixels not both. rng = np . random . default_rng ( 0 ) # So shift is always the same. # rand_shift must be larger than small to detect a single spot. rand_im_shift = rng . uniform ( low = self . small * 2 , high = 0.2 , size = self . image . shape ) self . image = self . image + rand_im_shift self . dilate = None self . spot_zyx = None self . image_isolated = None self . no_negative_neighbour = None self . update_dilate () if self . show_isolated : self . update_isolated_image () self . viewer = napari . Viewer () name = f \"Tile { t } , Round { r } , Channel { c } \" if self . is_3d : self . viewer . add_image ( np . moveaxis ( np . rint ( self . image ) . astype ( np . int32 ), 2 , 0 ), name = name ) else : self . viewer . add_image ( np . rint ( self . image ) . astype ( np . int32 ), name = name ) self . thresh_slider = QSlider ( Qt . Orientation . Horizontal ) self . thresh_slider . setRange ( 0 , 3 * self . auto_thresh ) self . thresh_slider . setValue ( self . auto_thresh ) # When dragging, status will show auto_thresh value self . thresh_slider . valueChanged . connect ( lambda x : self . show_thresh ( x )) # On release of slider, filtered / smoothed images updated self . thresh_slider . sliderReleased . connect ( self . update_spots ) self . viewer . window . add_dock_widget ( self . thresh_slider , area = \"left\" , name = 'Intensity Threshold' ) if self . show_isolated : self . isolation_thresh_slider = QSlider ( Qt . Orientation . Horizontal ) self . isolation_thresh_slider . setRange ( - 2 * np . abs ( self . isolation_thresh ), 0 ) self . isolation_thresh_slider . setValue ( self . isolation_thresh ) # When dragging, status will show auto_thresh value self . isolation_thresh_slider . valueChanged . connect ( lambda x : self . show_isolation_thresh ( x )) # On release of slider, filtered / smoothed images updated self . isolation_thresh_slider . sliderReleased . connect ( self . update_isolated_spots ) self . update_spots () self . r_xy_slider = QSlider ( Qt . Orientation . Horizontal ) self . r_xy_slider . setRange ( 2 , 10 ) self . r_xy_slider . setValue ( self . r_xy ) # When dragging, status will show r_xy value self . r_xy_slider . valueChanged . connect ( lambda x : self . show_radius_xy ( x )) # On release of slider, filtered / smoothed images updated self . r_xy_slider . sliderReleased . connect ( self . radius_slider_func ) self . viewer . window . add_dock_widget ( self . r_xy_slider , area = \"left\" , name = 'Detection Radius YX' ) if self . is_3d : self . r_z_slider = QSlider ( Qt . Orientation . Horizontal ) self . r_z_slider . setRange ( 2 , 12 ) self . r_z_slider . setValue ( self . r_xy ) # When dragging, status will show r_z value self . r_z_slider . valueChanged . connect ( lambda x : self . show_radius_z ( x )) # On release of slider, filtered / smoothed images updated self . r_z_slider . sliderReleased . connect ( self . radius_slider_func ) self . viewer . window . add_dock_widget ( self . r_z_slider , area = \"left\" , name = 'Detection Radius Z' ) self . z_thick_slider = QSlider ( Qt . Orientation . Horizontal ) self . z_thick_slider . setRange ( 0 , int (( self . z_thick_list [ - 1 ] - 1 ) / 2 )) self . z_thick_slider . setValue ( self . z_thick ) # When dragging, status will show r_z value self . z_thick_slider . valueChanged . connect ( lambda x : self . change_z_thick ( x )) self . viewer . window . add_dock_widget ( self . z_thick_slider , area = \"left\" , name = 'Z Thickness' ) if self . show_isolated : self . viewer . window . add_dock_widget ( self . isolation_thresh_slider , area = \"left\" , name = 'Isolation Threshold' ) # set image as selected layer so can see intensity values in status self . viewer . layers . selection . active = self . viewer . layers [ 0 ] napari . run ()","title":"view_find_spots"},{"location":"code/plot/find_spots/#coppafish.plot.find_spots.n_spots.n_spots_grid","text":"Plots a grid indicating the number of spots detected on each tile, round and channel. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required n_spots_thresh Optional [ int ] tiles/rounds/channels with fewer spots than this will be highlighted. If None , will use n_spots_warn_fraction from config file. None Source code in coppafish/plot/find_spots/n_spots.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def n_spots_grid ( nb : Notebook , n_spots_thresh : Optional [ int ] = None ): \"\"\" Plots a grid indicating the number of spots detected on each tile, round and channel. Args: nb: *Notebook* containing `find_spots` page. n_spots_thresh: tiles/rounds/channels with fewer spots than this will be highlighted. If `None`, will use `n_spots_warn_fraction` from config file. \"\"\" if n_spots_thresh is None : config = nb . get_config ()[ 'find_spots' ] if nb . basic_info . is_3d : n_spots_thresh = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_3d' ] * nb . basic_info . nz else : n_spots_thresh = config [ 'n_spots_warn_fraction' ] * config [ 'max_spots_2d' ] n_spots_thresh = int ( np . ceil ( n_spots_thresh )) use_tiles = np . asarray ( nb . basic_info . use_tiles ) use_rounds = np . asarray ( nb . basic_info . use_rounds ) # don't consider anchor in this analysis if len ( use_rounds ) > 0 : use_channels = np . asarray ( nb . basic_info . use_channels ) spot_no = nb . find_spots . spot_no [ np . ix_ ( use_tiles , use_rounds , use_channels )] spot_no = np . moveaxis ( spot_no , 1 , 2 ) # put rounds last spot_no = spot_no . reshape ( len ( use_tiles ), - 1 ) . T # spot_no[n_rounds, t] is now spot_no[t, r=0, c=1] n_round_channels = len ( use_rounds ) * len ( use_channels ) y_labels = np . tile ( use_rounds , len ( use_channels )) vmax = spot_no . max () # clip colorbar at max of imaging rounds/channels because anchor can be a lot higher else : # Deal with case where only anchor round use_channels = np . asarray ([]) spot_no = np . zeros (( 0 , len ( use_tiles )), dtype = np . int32 ) n_round_channels = 0 y_labels = np . zeros ( 0 , dtype = int ) vmax = None if nb . basic_info . use_anchor : anchor_spot_no = nb . find_spots . spot_no [ use_tiles , nb . basic_info . anchor_round , nb . basic_info . anchor_channel ][ np . newaxis ] spot_no = np . append ( spot_no , anchor_spot_no , axis = 0 ) y_labels = y_labels . astype ( str ) . tolist () y_labels += [ 'Anchor' ] n_round_channels += 1 if vmax is None : vmax = spot_no . max () fig , ax = plt . subplots ( 1 , 1 , figsize = ( np . clip ( 5 + len ( use_tiles ) / 2 , 3 , 18 ), 12 )) subplot_adjust = [ 0.12 , 1 , 0.07 , 0.9 ] fig . subplots_adjust ( left = subplot_adjust [ 0 ], right = subplot_adjust [ 1 ], bottom = subplot_adjust [ 2 ], top = subplot_adjust [ 3 ]) im = ax . imshow ( spot_no , aspect = 'auto' , vmax = vmax ) fig . colorbar ( im , ax = ax ) ax . set_yticks ( np . arange ( n_round_channels )) ax . set_yticklabels ( y_labels ) ax . set_xticks ( np . arange ( len ( use_tiles ))) ax . set_xticklabels ( use_tiles ) ax . set_xlabel ( 'Tile' ) for c in range ( len ( use_channels )): y_ind = 1 - c * 1 / ( len ( use_channels )) ax . text ( - 0.1 , y_ind , f \"Channel { use_channels [ c ] } \" , va = \"top\" , ha = \"left\" , transform = ax . transAxes , rotation = 'vertical' ) fig . supylabel ( 'Channel/Round' , transform = ax . transAxes , x =- 0.15 ) plt . xticks ( rotation = 90 ) low_spots = np . where ( spot_no < n_spots_thresh ) for j in range ( len ( low_spots [ 0 ])): rectangle = plt . Rectangle (( low_spots [ 1 ][ j ] - 0.5 , low_spots [ 0 ][ j ] - 0.5 ), 1 , 1 , fill = False , ec = \"r\" , linestyle = ':' , lw = 2 ) ax . add_patch ( rectangle ) plt . suptitle ( f \"Number of Spots Found on each Tile, Round and Channel\" ) plt . show ()","title":"n_spots_grid()"},{"location":"code/plot/omp/","text":"Coefficients view_omp Diagnostic to show omp coefficients of all genes in neighbourhood of spot. Only genes for which a significant number of pixels are non-zero will be plotted. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' im_size int Radius of image to be plotted for each gene. 8 Source code in coppafish/plot/omp/coefs.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , im_size : int = 8 ): \"\"\" Diagnostic to show omp coefficients of all genes in neighbourhood of spot. Only genes for which a significant number of pixels are non-zero will be plotted. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: Radius of image to be plotted for each gene. \"\"\" coef_images , min_global_yxz , max_global_yxz = get_coef_images ( nb , spot_no , method , [ im_size , im_size , 0 ]) if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] all_gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in range ( nb . basic_info . n_channels )] spot_yxz_global = spot_yxz + nb . stitch . tile_origin [ t ] n_genes = nb . call_spots . bled_codes_ge . shape [ 0 ] n_nonzero_pixels_thresh = np . min ([ im_size , 5 ]) # If 5 pixels non-zero, plot that gene plot_genes = np . where ( np . sum ( coef_images != 0 , axis = ( 1 , 2 , 3 )) > n_nonzero_pixels_thresh )[ 0 ] coef_images = coef_images [ plot_genes , :, :, 0 ] n_plot = len ( plot_genes ) # at most n_max_rows rows if n_plot <= 16 : n_max_rows = 4 else : n_max_rows = int ( np . ceil ( np . sqrt ( n_plot ))) n_cols = int ( np . ceil ( n_plot / n_max_rows )) subplot_row_columns = [ int ( np . ceil ( n_plot / n_cols )), n_cols ] fig_size = np . clip ([ n_cols + 5 , subplot_row_columns [ 0 ] + 4 ], 3 , 12 ) subplot_adjust = [ 0.05 , 0.775 , 0.05 , 0.91 ] super () . __init__ ( coef_images , None , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size , cbar_pos = [ 0.9 , 0.05 , 0.03 , 0.86 ], slider_pos = [ 0.85 , 0.05 , 0.01 , 0.86 ]) # set x, y coordinates to be those of the global coordinate system plot_extent = [ min_global_yxz [ 1 ] - 0.5 , max_global_yxz [ 1 ] + 0.5 , min_global_yxz [ 0 ] - 0.5 , max_global_yxz [ 0 ] + 0.5 ] for i in range ( self . n_images ): # Add cross-hair self . ax [ i ] . axes . plot ([ spot_yxz_global [ 1 ], spot_yxz_global [ 1 ]], [ plot_extent [ 2 ], plot_extent [ 3 ]], 'k' , linestyle = \":\" , lw = 1 ) self . ax [ i ] . axes . plot ([ plot_extent [ 0 ], plot_extent [ 1 ]], [ spot_yxz_global [ 0 ], spot_yxz_global [ 0 ]], 'k' , linestyle = \":\" , lw = 1 ) self . im [ i ] . set_extent ( plot_extent ) self . ax [ i ] . tick_params ( labelbottom = False , labelleft = False ) # Add title title_text = f ' { plot_genes [ i ] } : { all_gene_names [ plot_genes [ i ]] } ' if plot_genes [ i ] >= n_genes : text_color = ( 0.7 , 0.7 , 0.7 ) # If background, make grey title_text = all_gene_names [ plot_genes [ i ]] elif plot_genes [ i ] == gene_no : text_color = 'g' else : text_color = 'w' # TODO: maybe make color same as used in plot for each gene self . ax [ i ] . set_title ( title_text , color = text_color ) plt . subplots_adjust ( hspace = 0.32 ) plt . suptitle ( f 'OMP gene coefficients for spot { spot_no } (match' f ' { str ( np . around ( spot_score , 2 )) } to { gene_name } )' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 , size = 13 ) self . change_norm () plt . show () view_omp_fit Diagnostic to run omp on a single pixel and see which genes fitted at which iteration. Right-clicking on a particular bled code will cause coppafish.plot.call_spots.dot_product.view_score to run, indicating how the dot product calculation for that iteration was performed. Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run, indicating how the dot product calculation for performed. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' dp_thresh Optional [ float ] If None, will use value in omp section of config file. None max_genes Optional [ int ] If None, will use value in omp section of config file. None Source code in coppafish/plot/omp/coefs.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , dp_thresh : Optional [ float ] = None , max_genes : Optional [ int ] = None ): \"\"\" Diagnostic to run omp on a single pixel and see which genes fitted at which iteration. Right-clicking on a particular bled code will cause *coppafish.plot.call_spots.dot_product.view_score* to run, indicating how the dot product calculation for that iteration was performed. Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run, indicating how the dot product calculation for performed. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. dp_thresh: If None, will use value in omp section of config file. max_genes: If None, will use value in omp section of config file. \"\"\" track_info , bled_codes , dp_thresh = get_track_info ( nb , spot_no , method , dp_thresh , max_genes ) # Add info so can call view_dot_product self . nb = nb self . track_info = track_info self . bled_codes = bled_codes self . dp_thresh = dp_thresh self . spot_no = spot_no self . fitting_method = method n_genes , n_use_rounds , n_use_channels = bled_codes . shape n_residual_images = track_info [ 'residual' ] . shape [ 0 ] residual_images = [ track_info [ 'residual' ][ i ] . transpose () for i in range ( n_residual_images )] background_image = np . zeros (( n_use_rounds , n_use_channels )) for c in range ( n_use_channels ): background_image += track_info [ 'background_codes' ][ c ] * track_info [ 'background_coefs' ][ c ] background_image = background_image . transpose () # allow for possibly adding background vector # TODO: Think may get index error if best gene ever was background_vector. bled_codes = np . append ( bled_codes , track_info [ 'background_codes' ], axis = 0 ) all_gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] gene_images = [ bled_codes [ track_info [ 'gene_added' ][ i ]] . transpose () * track_info [ 'coef' ][ i ][ track_info [ 'gene_added' ][ i ]] for i in range ( 2 , n_residual_images )] all_images = residual_images + [ background_image ] + gene_images # Plot all images subplot_adjust = [ 0.06 , 0.82 , 0.075 , 0.9 ] super () . __init__ ( all_images , None , [ 2 , n_residual_images ], subplot_adjust = subplot_adjust , fig_size = ( 15 , 7 )) # label axis self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 0 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ]), labels = nb . basic_info . use_rounds ) self . fig . supxlabel ( 'Round' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) plt . suptitle ( f 'Residual at each iteration of OMP for Spot { spot_no } . ' + r '$\\Delta_ {thresh} $ = ' + f ' { dp_thresh } ' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) # Add titles for each subplot titles = [ 'Initial' , 'Post Background' ] for g in track_info [ 'gene_added' ][ 2 :]: titles = titles + [ 'Post ' + all_gene_names [ g ]] for i in range ( n_residual_images ): titles [ i ] = titles [ i ] + ' \\n Res = {:.2f} ' . format ( np . linalg . norm ( residual_images [ i ])) titles = titles + [ 'Background' ] for i in range ( 2 , n_residual_images ): g = track_info [ 'gene_added' ][ i ] titles = titles + [ f ' { g } : { all_gene_names [ g ] } ' ] titles [ - 1 ] = titles [ - 1 ] + ' \\n ' + r '$\\Delta_{s' + f ' { i - 2 } ' + '}$ = ' + ' {:.2f} ' . format ( track_info [ 'dot_product' ][ i ]) # Make title red if dot product fell below dp_thresh or if best gene background is_fail_thresh = False for i in range ( self . n_images ): if np . isin ( i , np . arange ( 2 , n_residual_images )): # failed if bad dot product, gene added is background or gene added has previously been added is_fail_thresh = np . abs ( track_info [ 'dot_product' ][ i ]) < dp_thresh or \\ track_info [ 'gene_added' ][ i ] >= n_genes or \\ np . isin ( track_info [ 'gene_added' ][ i ], track_info [ 'gene_added' ][ 2 : i ]) if is_fail_thresh : text_color = 'r' else : text_color = 'w' elif i == self . n_images - 1 and is_fail_thresh : text_color = 'r' else : text_color = 'w' self . ax [ i ] . set_title ( titles [ i ], size = 8 , color = text_color ) # Add rectangles where added gene is intense for i in range ( len ( gene_images )): gene_coef = track_info [ 'coef' ][ i + 2 ][ track_info [ 'gene_added' ][ i + 2 ]] intense_gene_cr = np . where ( np . abs ( gene_images [ i ] / gene_coef ) > self . intense_gene_thresh ) for j in range ( len ( intense_gene_cr [ 0 ])): for k in [ i + 1 , i + 1 + n_residual_images ]: # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( intense_gene_cr [ 1 ][ j ] - 0.5 , intense_gene_cr [ 0 ][ j ] - 0.5 ), 1 , 1 , fill = False , ec = \"g\" , linestyle = ':' , lw = 2 ) self . ax [ k ] . add_patch ( rectangle ) self . change_norm () self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_calc ) self . track_info = track_info plt . show () get_coef_images Gets image of \\(yxz\\) dimension (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1) of the coefficients fitted by omp for each gene. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to get gene coefficient images for. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. required im_size List [ int ] \\(yxz\\) radius of image to get for each gene. required Returns: Type Description np . ndarray coef_images - float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)] . Image for each gene, axis order is \\(gyxz\\) . coef_images[g, 0, 0, 0] refers to coefficient of gene g at global_yxz = min_global_yxz . coef_images[g, -1, -1, -1] refers to coefficient of gene g at global_yxz = max_global_yxz . List [ float ] min_global_yxz - float [3] . Min \\(yxz\\) coordinates of image in global coordinates. List [ float ] max_global_yxz - float [3] . Max \\(yxz\\) coordinates of image in global coordinates. Source code in coppafish/plot/omp/coefs.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_coef_images ( nb : Notebook , spot_no : int , method : str , im_size : List [ int ]) -> Tuple [ np . ndarray , List [ float ], List [ float ]]: \"\"\" Gets image of $yxz$ dimension `(2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)` of the coefficients fitted by omp for each gene. Args: nb: *Notebook* containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to get gene coefficient images for. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: $yxz$ radius of image to get for each gene. Returns: `coef_images` - `float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)]`. Image for each gene, axis order is $gyxz$. `coef_images[g, 0, 0, 0]` refers to coefficient of gene g at `global_yxz = min_global_yxz`. `coef_images[g, -1, -1, -1]` refers to coefficient of gene g at `global_yxz = max_global_yxz`. `min_global_yxz` - `float [3]`. Min $yxz$ coordinates of image in global coordinates. `max_global_yxz` - `float [3]`. Max $yxz$ coordinates of image in global coordinates. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] if method . lower () == 'omp' : page_name = 'omp' else : page_name = 'ref_spots' t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] # Subtlety here, may have y-axis flipped, but I think it is correct: # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct. # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5]) # will show the value 1 at max_y, min_x+1. im_yxz = np . array ( np . meshgrid ( np . arange ( spot_yxz [ 0 ] - im_size [ 0 ], spot_yxz [ 0 ] + im_size [ 0 ] + 1 )[:: - 1 ], np . arange ( spot_yxz [ 1 ] - im_size [ 1 ], spot_yxz [ 1 ] + im_size [ 1 ] + 1 ), spot_yxz [ 2 ]), dtype = np . int16 ) . T . reshape ( - 1 , 3 ) z = np . arange ( - im_size [ 2 ], im_size [ 2 ] + 1 ) im_yxz = np . vstack ([ im_yxz + [ 0 , 0 , val ] for val in z ]) im_diameter_yx = [ 2 * im_size [ 0 ] + 1 , 2 * im_size [ 1 ] + 1 ] spot_colors = get_spot_colors ( im_yxz , t , nb . register . transform , nb . file_names , nb . basic_info ) / color_norm # Only look at pixels with high enough intensity - same as in full pipeline spot_intensity = get_spot_intensity ( np . abs ( spot_colors )) config = nb . get_config ()[ 'omp' ] if nb . has_page ( 'omp' ): initial_intensity_thresh = nb . omp . initial_intensity_thresh else : initial_intensity_thresh = get_initial_intensity_thresh ( config , nb . call_spots ) keep = spot_intensity > initial_intensity_thresh bled_codes = nb . call_spots . bled_codes_ge n_genes = bled_codes . shape [ 0 ] bled_codes = np . asarray ( bled_codes [ np . ix_ ( np . arange ( n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels )]) n_use_rounds = len ( nb . basic_info . use_rounds ) dp_norm_shift = nb . call_spots . dp_norm_shift * np . sqrt ( n_use_rounds ) dp_thresh = config [ 'dp_thresh' ] if method . lower () == 'omp' : alpha = config [ 'alpha' ] beta = config [ 'beta' ] else : config_call_spots = nb . get_config ()[ 'call_spots' ] alpha = config_call_spots [ 'alpha' ] beta = config_call_spots [ 'beta' ] max_genes = config [ 'max_genes' ] weight_coef_fit = config [ 'weight_coef_fit' ] all_coefs = np . zeros (( spot_colors . shape [ 0 ], n_genes + nb . basic_info . n_channels )) all_coefs [ np . ix_ ( keep , np . arange ( n_genes ))], \\ all_coefs [ np . ix_ ( keep , np . array ( nb . basic_info . use_channels ) + n_genes )] = \\ get_all_coefs ( spot_colors [ keep ], bled_codes , nb . call_spots . background_weight_shift , dp_norm_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit ) n_genes = all_coefs . shape [ 1 ] nz = len ( z ) coef_images = np . zeros (( n_genes , len ( z ), im_diameter_yx [ 0 ], im_diameter_yx [ 1 ])) for g in range ( n_genes ): ind = 0 for z in range ( nz ): coef_images [ g , z ] = all_coefs [ ind : ind + np . prod ( im_diameter_yx ), g ] . reshape ( im_diameter_yx [ 0 ], im_diameter_yx [ 1 ]) ind += np . prod ( im_diameter_yx ) coef_images = np . moveaxis ( coef_images , 1 , - 1 ) # move z index to end min_global_yxz = im_yxz . min ( axis = 0 ) + nb . stitch . tile_origin [ t ] max_global_yxz = im_yxz . max ( axis = 0 ) + nb . stitch . tile_origin [ t ] return coef_images . astype ( np . float16 ), min_global_yxz , max_global_yxz Score Shape view_omp_score Diagnostic to show how score is computed in the omp method Hatched region in top plot shows pixels which contribute to the final score. Score is actually equal to the absolute sum of the top plots in the hatched regions. Can also see how score_omp_multiplier affects the final score. The larger this is, the more the positive pixels contribute compared to the negative. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' score_multiplier Optional [ float ] Initial value of score_omp_multiplier . None check bool If True , will compare score found to that saved in Notebook and raise error if they differ. Will also check that absolute sum of the top plots in the hatched regions is equal to score calculated from counting the number of pixels with the correct sign. False Source code in coppafish/plot/omp/score_shape.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , score_multiplier : Optional [ float ] = None , check : bool = False ): \"\"\" Diagnostic to show how score is computed in the omp method Hatched region in top plot shows pixels which contribute to the final score. Score is actually equal to the absolute sum of the top plots in the hatched regions. Can also see how `score_omp_multiplier` affects the final score. The larger this is, the more the positive pixels contribute compared to the negative. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. score_multiplier: Initial value of `score_omp_multiplier`. check: If `True`, will compare score found to that saved in *Notebook* and raise error if they differ. Will also check that absolute sum of the top plots in the hatched regions is equal to score calculated from counting the number of pixels with the correct sign. \"\"\" # TODO: The textbox for this plot seems to be much less responsive than in the other diagnostics # for some reason. if method . lower () == 'omp' : page_name = 'omp' else : page_name = 'ref_spots' check = False self . check = check self . nbp_omp = nb . omp self . spot_no = spot_no self . gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] self . gene_names = nb . call_spots . gene_names self . expected_sign_image = nb . omp . spot_shape self . nz = self . expected_sign_image . shape [ 2 ] im_radius = (( np . asarray ( self . expected_sign_image . shape ) - 1 ) / 2 ) . astype ( int ) self . coef_images , min_global_yxz , max_global_yxz = get_coef_images ( nb , spot_no , method , im_radius ) # Find where sign of coef image matches that of expected sign image self . both_positive = np . asarray ([ self . coef_images [ self . gene_no ] > 0 , self . expected_sign_image > 0 ]) . all ( axis = 0 ) self . both_negative = np . asarray ([ self . coef_images [ self . gene_no ] < 0 , self . expected_sign_image < 0 ]) . all ( axis = 0 ) self . check_neighbours () # Start with default multiplier config = nb . get_config ()[ 'thresholds' ] if score_multiplier is None : score_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_multiplier self . score_thresh = config [ 'score_omp' ] # maximum possible value of any one pixel in expected_shape for any score_multiplier self . vmax_expected = 1 / np . min ([ np . sum ( self . expected_sign_image > 0 ), np . sum ( self . expected_sign_image < 0 )]) self . vmax_coef = np . abs ( self . coef_images [ self . gene_no ]) . max () # Initialize plots self . plot_extent = [ min_global_yxz [ 1 ] - 0.5 , max_global_yxz [ 1 ] + 0.5 , min_global_yxz [ 0 ] - 0.5 , max_global_yxz [ 0 ] + 0.5 ] self . fig , self . ax = plt . subplots ( 2 , self . nz , figsize = ( 14 , 5 ), sharex = True , sharey = True ) self . subplot_adjust = [ 0.05 , 0.88 , 0.09 , 0.85 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax = self . ax . flatten () self . im = [ None ] * self . nz * 2 expected_image_plot = self . expected_image () self . score = self . get_score ( expected_image_plot ) for i in range ( self . nz ): self . im [ i ] = self . ax [ i ] . imshow ( expected_image_plot [:, :, i ], vmin =- self . vmax_expected , vmax = self . vmax_expected , cmap = 'bwr' , extent = self . plot_extent ) self . im [ i + self . nz ] = self . ax [ i + self . nz ] . imshow ( self . coef_images [ self . gene_no , :, :, i ], vmin =- self . vmax_coef , vmax = self . vmax_coef , cmap = 'bwr' , extent = self . plot_extent ) if i == ( self . nz - 1 ) / 2 : self . ax [ i ] . set_title ( f \"Expected Coefficient Sign \\n Z= { int ( np . rint ( min_global_yxz [ 2 ] + i )) } \" ) else : self . ax [ i ] . set_title ( f \"Z= { int ( np . rint ( min_global_yxz [ 2 ] + i )) } \" ) self . set_coef_plot_title () self . add_hatching () # Set up colorbars for each plot mid_point = ( self . subplot_adjust [ 2 ] + self . subplot_adjust [ 3 ]) / 2 gap_size = 0.08 cbar_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.01 , mid_point + gap_size / 2 , 0.005 , self . subplot_adjust [ 3 ] - mid_point - gap_size / 2 ]) # left, bottom, width, height self . fig . colorbar ( self . im [ 0 ], cax = cbar_ax ) cbar_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.01 , self . subplot_adjust [ 2 ] + gap_size / 5 , 0.005 , mid_point - self . subplot_adjust [ 2 ] - gap_size / 2 ]) # left, bottom, width, height self . fig . colorbar ( self . im [ self . nz ], cax = cbar_ax ) # Add titles self . fig . supylabel ( 'Y' ) self . fig . supxlabel ( 'X' , size = 12 , x = ( self . subplot_adjust [ 0 ] + self . subplot_adjust [ 1 ]) / 2 ) plt . suptitle ( f \"OMP Score Calculation for Spot { spot_no } , Gene { self . gene_no } : { self . gene_names [ self . gene_no ] } \" , x = ( self . subplot_adjust [ 0 ] + self . subplot_adjust [ 1 ]) / 2 ) # Add text box to change score multiplier text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.062 , self . subplot_adjust [ 2 ] + gap_size / 5 , 0.05 , 0.04 ]) self . text_box = TextBox ( text_ax , 'Score \\n ' + r 'Multiplier, $\\rho$' , str ( np . around ( self . score_multiplier , 2 )), color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 2.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . update ) plt . show () Score Histogram histogram_score If method is anchor, this will show the histogram of nb.ref_spots.score with the option to view the histogram of the score computed using various other configurations of background fitting and gene_efficiency . This allows one to see how the these affect the score. If method is omp, this will show the histogram of omp score, computed with coppafish.call_spots.omp_spot_score . There will also be the option to view the histograms shown for the anchor method. I.e. we compute the dot product score for the omp spots. Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. None check bool If True , and method='anchor' , will check that scores computed here match those saved to Notebook. False hist_spacing float Initial width of bin in histogram. 0.001 show_plot bool Whether to run plt.show() or not. True Source code in coppafish/plot/omp/score_hist.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def __init__ ( self , nb : Notebook , method : str = 'omp' , score_omp_multiplier : Optional [ float ] = None , check : bool = False , hist_spacing : float = 0.001 , show_plot : bool = True ): \"\"\" If method is anchor, this will show the histogram of `nb.ref_spots.score` with the option to view the histogram of the score computed using various other configurations of `background` fitting and `gene_efficiency`. This allows one to see how the these affect the score. If `method` is omp, this will show the histogram of omp score, computed with `coppafish.call_spots.omp_spot_score`. There will also be the option to view the histograms shown for the anchor method. I.e. we compute the dot product score for the omp spots. Args: nb: *Notebook* containing at least `call_spots` page. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. check: If `True`, and `method='anchor'`, will check that scores computed here match those saved to Notebook. hist_spacing: Initial width of bin in histogram. show_plot: Whether to run `plt.show()` or not. \"\"\" # Add data if score_omp_multiplier is None : config = nb . get_config ()[ 'thresholds' ] score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_omp_multiplier self . gene_names = nb . call_spots . gene_names self . n_genes = self . gene_names . size # Use all genes by default self . genes_use = np . arange ( self . n_genes ) # For computing score_dp spot_colors , spot_colors_pb , background_var = background_fitting ( nb , method ) grc_ind = np . ix_ ( np . arange ( self . n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels ) # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds bled_codes = nb . call_spots . bled_codes [ grc_ind ] bled_codes_ge = nb . call_spots . bled_codes_ge [ grc_ind ] # Save score_dp for each permutation of with/without background/gene_efficiency self . n_plots = 5 if method . lower () == 'omp' : self . n_plots += 1 self . nbp_omp = nb . omp self . gene_no = self . nbp_omp . gene_no self . score = np . zeros (( self . gene_no . size , self . n_plots ), dtype = np . float32 ) self . score [:, - 1 ] = omp_spot_score ( self . nbp_omp , self . score_multiplier ) self . method = 'OMP' else : self . gene_no = nb . ref_spots . gene_no self . score = np . zeros (( self . gene_no . size , self . n_plots ), dtype = np . float32 ) self . method = 'Anchor' self . use = np . isin ( self . gene_no , self . genes_use ) # which spots to plot # DP score self . score [:, 0 ] = get_dot_product_score ( spot_colors_pb , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , background_var )[ 0 ] if method . lower () != 'omp' and check : if np . max ( np . abs ( self . score [:, 0 ] - nb . ref_spots . score )) > self . check_tol : raise ValueError ( f \"nb.ref_spots.score differs to that computed here \\n \" f \"Set check=False to get past this error\" ) # DP score no weight self . score [:, 1 ] = get_dot_product_score ( spot_colors_pb , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # DP score no background self . score [:, 2 ] = get_dot_product_score ( spot_colors , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # DP score no gene efficiency self . score [:, 3 ] = get_dot_product_score ( spot_colors_pb , bled_codes , self . gene_no , nb . call_spots . dp_norm_shift , background_var )[ 0 ] # DP score no background or gene efficiency self . score [:, 4 ] = get_dot_product_score ( spot_colors , bled_codes , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # Initialise plot self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 11 , 5 )) self . subplot_adjust = [ 0.07 , 0.85 , 0.1 , 0.93 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax . set_ylabel ( r \"Number of Spots\" ) if method . lower () == 'omp' : self . ax . set_xlabel ( r \"Score, $\\gamma_s$ or $\\Delta_s$\" ) else : self . ax . set_xlabel ( r \"Score, $\\Delta_s$\" ) self . ax . set_title ( f \"Distribution of Scores for all { self . method } spots\" ) # Set lower bound based on dot product score with no GE/no background as likely to be lowest self . hist_min = np . percentile ( self . score [:, 4 ], 0.1 ) # Set upper bound based on dot product score with GE and background because this is likely to be highest self . hist_max = np . clip ( np . percentile ( self . score [:, 0 ], 99.9 ), 1 , 2 ) self . hist_spacing = hist_spacing hist_bins = np . arange ( self . hist_min , self . hist_max + self . hist_spacing / 2 , self . hist_spacing ) self . plots = [ None ] * self . n_plots default_colors = plt . rcParams [ 'axes.prop_cycle' ] . _left for i in range ( self . n_plots ): y , x = np . histogram ( self . score [ self . use , i ], hist_bins ) x = x [: - 1 ] + self . hist_spacing / 2 # so same length as x self . plots [ i ], = self . ax . plot ( x , y , color = default_colors [ i ][ 'color' ]) if method . lower () == 'omp' and i < self . n_plots - 1 : self . plots [ i ] . set_visible ( False ) elif i > 0 and method . lower () != 'omp' : self . plots [ i ] . set_visible ( False ) self . ax . set_xlim ( self . hist_min , self . hist_max ) self . ax . set_ylim ( 0 , None ) # Add text box to change score multiplier text_box_labels = [ 'Gene' , 'Histogram \\n Spacing' , 'Score \\n ' + r 'Multiplier, $\\rho$' ] text_box_values = [ 'all' , self . hist_spacing , np . around ( self . score_multiplier , 2 )] text_box_funcs = [ self . update_genes , self . update_hist_spacing , self . update_score_multiplier ] if method . lower () != 'omp' : text_box_labels = text_box_labels [: 2 ] text_box_values = text_box_values [: 2 ] text_box_funcs = text_box_funcs [: 2 ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 2 ] + 0.15 * ( len ( text_box_labels ) - i - 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis if i == 0 : label . set_position ([ 0.5 , 1.77 ]) # [x,y] - change here to set the position else : label . set_position ([ 0.5 , 2.75 ]) # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Add buttons to add/remove score_dp histograms self . buttons_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.02 , self . subplot_adjust [ 3 ] - 0.45 , 0.15 , 0.5 ]) plt . axis ( 'off' ) self . button_labels = [ r \"$\\Delta_s$\" + \" \\n Dot Product Score\" , r \"$\\Delta_s$\" + \" \\n No Weighting\" , r \"$\\Delta_s$\" + \" \\n No Background\" , r \"$\\Delta_s$\" + \" \\n No Gene Efficiency\" , r \"$\\Delta_s$\" + \" \\n No Background \\n No Gene Efficiency\" ] label_checked = [ True , False , False , False , False ] if method . lower () == 'omp' : self . button_labels += [ r \"$\\gamma_s$\" + \" \\n OMP Score\" ] label_checked += [ True ] label_checked [ 0 ] = False self . buttons = CheckButtons ( self . buttons_ax , self . button_labels , label_checked ) for i in range ( self . n_plots ): self . buttons . labels [ i ] . set_fontsize ( 7 ) self . buttons . labels [ i ] . set_color ( default_colors [ i ][ 'color' ]) self . buttons . rectangles [ i ] . set_color ( 'w' ) self . buttons . on_clicked ( self . choose_plots ) if show_plot : plt . show () histogram_2d_score This plots the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\) . Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. None Source code in coppafish/plot/omp/score_hist.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __init__ ( self , nb : Notebook , score_omp_multiplier : Optional [ float ] = None ): \"\"\" This plots the bivariate histogram to see the correlation between the omp spot score, $\\gamma_s$ and the dot product score $\\Delta_s$. Args: nb: *Notebook* containing at least `call_spots` page. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. \"\"\" # large hist_spacing so quick as we change it anway super () . __init__ ( nb , 'omp' , score_omp_multiplier , False , 0.5 , False ) self . ax . clear () # Get rid of buttons - only use actual dot product score self . buttons_ax . clear () plt . axis ( 'off' ) self . score = self . score [:, [ 0 , self . n_plots - 1 ]] self . n_plots = 2 del self . plots hist_bins = np . arange ( self . hist_min , self . hist_max + self . hist_spacing / 2 , self . hist_spacing ) self . x_score_ind = 0 self . hist_spacing = 0.01 self . plot = self . ax . hist2d ( self . score [:, self . x_score_ind ], self . score [:, - 1 ], hist_bins )[ 3 ] self . cbar = self . fig . colorbar ( self . plot , ax = self . ax ) self . ax . set_xlim ( self . hist_min , self . hist_max ) self . ax . set_ylim ( self . hist_min , self . hist_max ) self . text_boxes [ 1 ] . set_val ( self . hist_spacing ) self . ax . set_xlabel ( self . button_labels [ 0 ] . replace ( ' \\n ' , ', ' )) self . ax . set_ylabel ( self . button_labels [ - 1 ] . replace ( ' \\n ' , ', ' )) plt . show () Track Fit get_track_info ( nb , spot_no , method , dp_thresh = None , max_genes = None ) This runs omp while tracking the residual at each stage. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to get track_info for. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. required dp_thresh Optional [ float ] If None, will use value in omp section of config file. None max_genes Optional [ int ] If None, will use value in omp section of config file. None Returns: Type Description dict track_info - dictionary containing info about genes added at each step returned: background_codes - float [n_channels x n_rounds x n_channels] . background_codes[c] is the background vector for channel c with L2 norm of 1. background_coefs - float [n_channels] . background_coefs[c] is the coefficient value for background_codes[c] . gene_added - int [n_genes_added + 2] . gene_added[0] and gene_added[1] are -1. gene_added[2+i] is the ith gene that was added. residual - float [(n_genes_added + 2) x n_rounds x n_channels] . residual[0] is the initial pixel_color . residual[1] is the post background pixel_color . residual[2+i] is the pixel_color after removing gene gene_added[2+i] . coef - float [(n_genes_added + 2) x n_genes] . coef[0] and coef[1] are all 0. coef[2+i] are the coefficients for all genes after the ith gene has been added. dot_product - float [n_genes_added + 2] . dot_product[0] and dot_product[1] are 0. dot_product[2+i] is the dot product for the gene gene_added[2+i] . inverse_var - float [(n_genes_added + 2) x n_rounds x n_channels] . inverse_var[0] and inverse_var[1] are all 0. inverse_var[2+i] is the weighting used to compute dot_product[2+i] , which down-weights rounds/channels for which a gene has already been fitted. np . ndarray bled_codes - float [n_genes x n_use_rounds x n_use_channels] . gene bled_codes used in omp with L2 norm = 1. float dp_thresh - threshold dot product score, above which gene is fitted. Source code in coppafish/plot/omp/track_fit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_track_info ( nb : Notebook , spot_no : int , method : str , dp_thresh : Optional [ float ] = None , max_genes : Optional [ int ] = None ) -> Tuple [ dict , np . ndarray , float ]: \"\"\" This runs omp while tracking the residual at each stage. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to get track_info for. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. dp_thresh: If None, will use value in omp section of config file. max_genes: If None, will use value in omp section of config file. Returns: `track_info` - dictionary containing info about genes added at each step returned: - `background_codes` - `float [n_channels x n_rounds x n_channels]`. `background_codes[c]` is the background vector for channel `c` with L2 norm of 1. - `background_coefs` - `float [n_channels]`. `background_coefs[c]` is the coefficient value for `background_codes[c]`. - `gene_added` - `int [n_genes_added + 2]`. `gene_added[0]` and `gene_added[1]` are -1. `gene_added[2+i]` is the `ith` gene that was added. - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `residual[0]` is the initial `pixel_color`. `residual[1]` is the post background `pixel_color`. `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`. - `coef` - `float [(n_genes_added + 2) x n_genes]`. `coef[0]` and `coef[1]` are all 0. `coef[2+i]` are the coefficients for all genes after the ith gene has been added. - `dot_product` - `float [n_genes_added + 2]`. `dot_product[0]` and `dot_product[1]` are 0. `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`. - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `inverse_var[0]` and `inverse_var[1]` are all 0. `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`, which down-weights rounds/channels for which a gene has already been fitted. `bled_codes` - `float [n_genes x n_use_rounds x n_use_channels]`. gene `bled_codes` used in omp with L2 norm = 1. `dp_thresh` - threshold dot product score, above which gene is fitted. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape if method . lower () == 'omp' : page_name = 'omp' config_name = 'omp' else : page_name = 'ref_spots' config_name = 'call_spots' spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] / color_norm n_genes = nb . call_spots . bled_codes_ge . shape [ 0 ] bled_codes = np . asarray ( nb . call_spots . bled_codes_ge [ np . ix_ ( np . arange ( n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels )]) # ensure L2 norm is 1 for bled codes norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes = bled_codes / norm_factor # Get info to run omp dp_norm_shift = nb . call_spots . dp_norm_shift * np . sqrt ( n_use_rounds ) config = nb . get_config () if dp_thresh is None : dp_thresh = config [ 'omp' ][ 'dp_thresh' ] alpha = config [ config_name ][ 'alpha' ] beta = config [ config_name ][ 'beta' ] if max_genes is None : max_genes = config [ 'omp' ][ 'max_genes' ] weight_coef_fit = config [ 'omp' ][ 'weight_coef_fit' ] # Run omp with track to get residual at each stage track_info = get_all_coefs ( spot_color [ np . newaxis ], bled_codes , nb . call_spots . background_weight_shift , dp_norm_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit , True )[ 2 ] return track_info , bled_codes , dp_thresh","title":"OMP"},{"location":"code/plot/omp/#coefficients","text":"","title":"Coefficients"},{"location":"code/plot/omp/#view_omp","text":"Diagnostic to show omp coefficients of all genes in neighbourhood of spot. Only genes for which a significant number of pixels are non-zero will be plotted. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' im_size int Radius of image to be plotted for each gene. 8 Source code in coppafish/plot/omp/coefs.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , im_size : int = 8 ): \"\"\" Diagnostic to show omp coefficients of all genes in neighbourhood of spot. Only genes for which a significant number of pixels are non-zero will be plotted. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: Radius of image to be plotted for each gene. \"\"\" coef_images , min_global_yxz , max_global_yxz = get_coef_images ( nb , spot_no , method , [ im_size , im_size , 0 ]) if method . lower () == 'omp' : page_name = 'omp' config = nb . get_config ()[ 'thresholds' ] spot_score = omp_spot_score ( nb . omp , config [ 'score_omp_multiplier' ], spot_no ) else : page_name = 'ref_spots' spot_score = nb . ref_spots . score [ spot_no ] gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] gene_name = nb . call_spots . gene_names [ gene_no ] all_gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in range ( nb . basic_info . n_channels )] spot_yxz_global = spot_yxz + nb . stitch . tile_origin [ t ] n_genes = nb . call_spots . bled_codes_ge . shape [ 0 ] n_nonzero_pixels_thresh = np . min ([ im_size , 5 ]) # If 5 pixels non-zero, plot that gene plot_genes = np . where ( np . sum ( coef_images != 0 , axis = ( 1 , 2 , 3 )) > n_nonzero_pixels_thresh )[ 0 ] coef_images = coef_images [ plot_genes , :, :, 0 ] n_plot = len ( plot_genes ) # at most n_max_rows rows if n_plot <= 16 : n_max_rows = 4 else : n_max_rows = int ( np . ceil ( np . sqrt ( n_plot ))) n_cols = int ( np . ceil ( n_plot / n_max_rows )) subplot_row_columns = [ int ( np . ceil ( n_plot / n_cols )), n_cols ] fig_size = np . clip ([ n_cols + 5 , subplot_row_columns [ 0 ] + 4 ], 3 , 12 ) subplot_adjust = [ 0.05 , 0.775 , 0.05 , 0.91 ] super () . __init__ ( coef_images , None , subplot_row_columns , subplot_adjust = subplot_adjust , fig_size = fig_size , cbar_pos = [ 0.9 , 0.05 , 0.03 , 0.86 ], slider_pos = [ 0.85 , 0.05 , 0.01 , 0.86 ]) # set x, y coordinates to be those of the global coordinate system plot_extent = [ min_global_yxz [ 1 ] - 0.5 , max_global_yxz [ 1 ] + 0.5 , min_global_yxz [ 0 ] - 0.5 , max_global_yxz [ 0 ] + 0.5 ] for i in range ( self . n_images ): # Add cross-hair self . ax [ i ] . axes . plot ([ spot_yxz_global [ 1 ], spot_yxz_global [ 1 ]], [ plot_extent [ 2 ], plot_extent [ 3 ]], 'k' , linestyle = \":\" , lw = 1 ) self . ax [ i ] . axes . plot ([ plot_extent [ 0 ], plot_extent [ 1 ]], [ spot_yxz_global [ 0 ], spot_yxz_global [ 0 ]], 'k' , linestyle = \":\" , lw = 1 ) self . im [ i ] . set_extent ( plot_extent ) self . ax [ i ] . tick_params ( labelbottom = False , labelleft = False ) # Add title title_text = f ' { plot_genes [ i ] } : { all_gene_names [ plot_genes [ i ]] } ' if plot_genes [ i ] >= n_genes : text_color = ( 0.7 , 0.7 , 0.7 ) # If background, make grey title_text = all_gene_names [ plot_genes [ i ]] elif plot_genes [ i ] == gene_no : text_color = 'g' else : text_color = 'w' # TODO: maybe make color same as used in plot for each gene self . ax [ i ] . set_title ( title_text , color = text_color ) plt . subplots_adjust ( hspace = 0.32 ) plt . suptitle ( f 'OMP gene coefficients for spot { spot_no } (match' f ' { str ( np . around ( spot_score , 2 )) } to { gene_name } )' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 , size = 13 ) self . change_norm () plt . show ()","title":"view_omp"},{"location":"code/plot/omp/#view_omp_fit","text":"Diagnostic to run omp on a single pixel and see which genes fitted at which iteration. Right-clicking on a particular bled code will cause coppafish.plot.call_spots.dot_product.view_score to run, indicating how the dot product calculation for that iteration was performed. Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run, indicating how the dot product calculation for performed. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' dp_thresh Optional [ float ] If None, will use value in omp section of config file. None max_genes Optional [ int ] If None, will use value in omp section of config file. None Source code in coppafish/plot/omp/coefs.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , dp_thresh : Optional [ float ] = None , max_genes : Optional [ int ] = None ): \"\"\" Diagnostic to run omp on a single pixel and see which genes fitted at which iteration. Right-clicking on a particular bled code will cause *coppafish.plot.call_spots.dot_product.view_score* to run, indicating how the dot product calculation for that iteration was performed. Left-clicking on background image will cause coppafish.plot.call_spots.background.view_background to run, indicating how the dot product calculation for performed. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. dp_thresh: If None, will use value in omp section of config file. max_genes: If None, will use value in omp section of config file. \"\"\" track_info , bled_codes , dp_thresh = get_track_info ( nb , spot_no , method , dp_thresh , max_genes ) # Add info so can call view_dot_product self . nb = nb self . track_info = track_info self . bled_codes = bled_codes self . dp_thresh = dp_thresh self . spot_no = spot_no self . fitting_method = method n_genes , n_use_rounds , n_use_channels = bled_codes . shape n_residual_images = track_info [ 'residual' ] . shape [ 0 ] residual_images = [ track_info [ 'residual' ][ i ] . transpose () for i in range ( n_residual_images )] background_image = np . zeros (( n_use_rounds , n_use_channels )) for c in range ( n_use_channels ): background_image += track_info [ 'background_codes' ][ c ] * track_info [ 'background_coefs' ][ c ] background_image = background_image . transpose () # allow for possibly adding background vector # TODO: Think may get index error if best gene ever was background_vector. bled_codes = np . append ( bled_codes , track_info [ 'background_codes' ], axis = 0 ) all_gene_names = list ( nb . call_spots . gene_names ) + [ f 'BG { i } ' for i in nb . basic_info . use_channels ] gene_images = [ bled_codes [ track_info [ 'gene_added' ][ i ]] . transpose () * track_info [ 'coef' ][ i ][ track_info [ 'gene_added' ][ i ]] for i in range ( 2 , n_residual_images )] all_images = residual_images + [ background_image ] + gene_images # Plot all images subplot_adjust = [ 0.06 , 0.82 , 0.075 , 0.9 ] super () . __init__ ( all_images , None , [ 2 , n_residual_images ], subplot_adjust = subplot_adjust , fig_size = ( 15 , 7 )) # label axis self . ax [ 0 ] . set_yticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 0 ]), labels = nb . basic_info . use_channels ) self . ax [ 0 ] . set_xticks ( ticks = np . arange ( self . im_data [ 0 ] . shape [ 1 ]), labels = nb . basic_info . use_rounds ) self . fig . supxlabel ( 'Round' , size = 12 , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) self . fig . supylabel ( 'Color Channel' , size = 12 ) plt . suptitle ( f 'Residual at each iteration of OMP for Spot { spot_no } . ' + r '$\\Delta_ {thresh} $ = ' + f ' { dp_thresh } ' , x = ( subplot_adjust [ 0 ] + subplot_adjust [ 1 ]) / 2 ) # Add titles for each subplot titles = [ 'Initial' , 'Post Background' ] for g in track_info [ 'gene_added' ][ 2 :]: titles = titles + [ 'Post ' + all_gene_names [ g ]] for i in range ( n_residual_images ): titles [ i ] = titles [ i ] + ' \\n Res = {:.2f} ' . format ( np . linalg . norm ( residual_images [ i ])) titles = titles + [ 'Background' ] for i in range ( 2 , n_residual_images ): g = track_info [ 'gene_added' ][ i ] titles = titles + [ f ' { g } : { all_gene_names [ g ] } ' ] titles [ - 1 ] = titles [ - 1 ] + ' \\n ' + r '$\\Delta_{s' + f ' { i - 2 } ' + '}$ = ' + ' {:.2f} ' . format ( track_info [ 'dot_product' ][ i ]) # Make title red if dot product fell below dp_thresh or if best gene background is_fail_thresh = False for i in range ( self . n_images ): if np . isin ( i , np . arange ( 2 , n_residual_images )): # failed if bad dot product, gene added is background or gene added has previously been added is_fail_thresh = np . abs ( track_info [ 'dot_product' ][ i ]) < dp_thresh or \\ track_info [ 'gene_added' ][ i ] >= n_genes or \\ np . isin ( track_info [ 'gene_added' ][ i ], track_info [ 'gene_added' ][ 2 : i ]) if is_fail_thresh : text_color = 'r' else : text_color = 'w' elif i == self . n_images - 1 and is_fail_thresh : text_color = 'r' else : text_color = 'w' self . ax [ i ] . set_title ( titles [ i ], size = 8 , color = text_color ) # Add rectangles where added gene is intense for i in range ( len ( gene_images )): gene_coef = track_info [ 'coef' ][ i + 2 ][ track_info [ 'gene_added' ][ i + 2 ]] intense_gene_cr = np . where ( np . abs ( gene_images [ i ] / gene_coef ) > self . intense_gene_thresh ) for j in range ( len ( intense_gene_cr [ 0 ])): for k in [ i + 1 , i + 1 + n_residual_images ]: # can't add rectangle to multiple axes hence second for loop rectangle = plt . Rectangle (( intense_gene_cr [ 1 ][ j ] - 0.5 , intense_gene_cr [ 0 ][ j ] - 0.5 ), 1 , 1 , fill = False , ec = \"g\" , linestyle = ':' , lw = 2 ) self . ax [ k ] . add_patch ( rectangle ) self . change_norm () self . fig . canvas . mpl_connect ( 'button_press_event' , self . show_calc ) self . track_info = track_info plt . show ()","title":"view_omp_fit"},{"location":"code/plot/omp/#get_coef_images","text":"Gets image of \\(yxz\\) dimension (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1) of the coefficients fitted by omp for each gene. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to get gene coefficient images for. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. required im_size List [ int ] \\(yxz\\) radius of image to get for each gene. required Returns: Type Description np . ndarray coef_images - float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)] . Image for each gene, axis order is \\(gyxz\\) . coef_images[g, 0, 0, 0] refers to coefficient of gene g at global_yxz = min_global_yxz . coef_images[g, -1, -1, -1] refers to coefficient of gene g at global_yxz = max_global_yxz . List [ float ] min_global_yxz - float [3] . Min \\(yxz\\) coordinates of image in global coordinates. List [ float ] max_global_yxz - float [3] . Max \\(yxz\\) coordinates of image in global coordinates. Source code in coppafish/plot/omp/coefs.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_coef_images ( nb : Notebook , spot_no : int , method : str , im_size : List [ int ]) -> Tuple [ np . ndarray , List [ float ], List [ float ]]: \"\"\" Gets image of $yxz$ dimension `(2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)` of the coefficients fitted by omp for each gene. Args: nb: *Notebook* containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to get gene coefficient images for. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. im_size: $yxz$ radius of image to get for each gene. Returns: `coef_images` - `float16 [n_genes x (2*im_size[0]+1) x (2*im_size[1]+1) x (2*im_size[2]+1)]`. Image for each gene, axis order is $gyxz$. `coef_images[g, 0, 0, 0]` refers to coefficient of gene g at `global_yxz = min_global_yxz`. `coef_images[g, -1, -1, -1]` refers to coefficient of gene g at `global_yxz = max_global_yxz`. `min_global_yxz` - `float [3]`. Min $yxz$ coordinates of image in global coordinates. `max_global_yxz` - `float [3]`. Max $yxz$ coordinates of image in global coordinates. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] if method . lower () == 'omp' : page_name = 'omp' else : page_name = 'ref_spots' t = nb . __getattribute__ ( page_name ) . tile [ spot_no ] spot_yxz = nb . __getattribute__ ( page_name ) . local_yxz [ spot_no ] # Subtlety here, may have y-axis flipped, but I think it is correct: # note im_yxz[1] refers to point at max_y, min_x+1, z. So when reshape and set plot_extent, should be correct. # I.e. im = np.zeros(49); im[1] = 1; im = im.reshape(7,7); plt.imshow(im, extent=[-0.5, 6.5, -0.5, 6.5]) # will show the value 1 at max_y, min_x+1. im_yxz = np . array ( np . meshgrid ( np . arange ( spot_yxz [ 0 ] - im_size [ 0 ], spot_yxz [ 0 ] + im_size [ 0 ] + 1 )[:: - 1 ], np . arange ( spot_yxz [ 1 ] - im_size [ 1 ], spot_yxz [ 1 ] + im_size [ 1 ] + 1 ), spot_yxz [ 2 ]), dtype = np . int16 ) . T . reshape ( - 1 , 3 ) z = np . arange ( - im_size [ 2 ], im_size [ 2 ] + 1 ) im_yxz = np . vstack ([ im_yxz + [ 0 , 0 , val ] for val in z ]) im_diameter_yx = [ 2 * im_size [ 0 ] + 1 , 2 * im_size [ 1 ] + 1 ] spot_colors = get_spot_colors ( im_yxz , t , nb . register . transform , nb . file_names , nb . basic_info ) / color_norm # Only look at pixels with high enough intensity - same as in full pipeline spot_intensity = get_spot_intensity ( np . abs ( spot_colors )) config = nb . get_config ()[ 'omp' ] if nb . has_page ( 'omp' ): initial_intensity_thresh = nb . omp . initial_intensity_thresh else : initial_intensity_thresh = get_initial_intensity_thresh ( config , nb . call_spots ) keep = spot_intensity > initial_intensity_thresh bled_codes = nb . call_spots . bled_codes_ge n_genes = bled_codes . shape [ 0 ] bled_codes = np . asarray ( bled_codes [ np . ix_ ( np . arange ( n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels )]) n_use_rounds = len ( nb . basic_info . use_rounds ) dp_norm_shift = nb . call_spots . dp_norm_shift * np . sqrt ( n_use_rounds ) dp_thresh = config [ 'dp_thresh' ] if method . lower () == 'omp' : alpha = config [ 'alpha' ] beta = config [ 'beta' ] else : config_call_spots = nb . get_config ()[ 'call_spots' ] alpha = config_call_spots [ 'alpha' ] beta = config_call_spots [ 'beta' ] max_genes = config [ 'max_genes' ] weight_coef_fit = config [ 'weight_coef_fit' ] all_coefs = np . zeros (( spot_colors . shape [ 0 ], n_genes + nb . basic_info . n_channels )) all_coefs [ np . ix_ ( keep , np . arange ( n_genes ))], \\ all_coefs [ np . ix_ ( keep , np . array ( nb . basic_info . use_channels ) + n_genes )] = \\ get_all_coefs ( spot_colors [ keep ], bled_codes , nb . call_spots . background_weight_shift , dp_norm_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit ) n_genes = all_coefs . shape [ 1 ] nz = len ( z ) coef_images = np . zeros (( n_genes , len ( z ), im_diameter_yx [ 0 ], im_diameter_yx [ 1 ])) for g in range ( n_genes ): ind = 0 for z in range ( nz ): coef_images [ g , z ] = all_coefs [ ind : ind + np . prod ( im_diameter_yx ), g ] . reshape ( im_diameter_yx [ 0 ], im_diameter_yx [ 1 ]) ind += np . prod ( im_diameter_yx ) coef_images = np . moveaxis ( coef_images , 1 , - 1 ) # move z index to end min_global_yxz = im_yxz . min ( axis = 0 ) + nb . stitch . tile_origin [ t ] max_global_yxz = im_yxz . max ( axis = 0 ) + nb . stitch . tile_origin [ t ] return coef_images . astype ( np . float16 ), min_global_yxz , max_global_yxz","title":"get_coef_images"},{"location":"code/plot/omp/#score-shape","text":"","title":"Score Shape"},{"location":"code/plot/omp/#view_omp_score","text":"Diagnostic to show how score is computed in the omp method Hatched region in top plot shows pixels which contribute to the final score. Score is actually equal to the absolute sum of the top plots in the hatched regions. Can also see how score_omp_multiplier affects the final score. The larger this is, the more the positive pixels contribute compared to the negative. Requires access to nb.file_names.tile_dir Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to be plotted. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' score_multiplier Optional [ float ] Initial value of score_omp_multiplier . None check bool If True , will compare score found to that saved in Notebook and raise error if they differ. Will also check that absolute sum of the top plots in the hatched regions is equal to score calculated from counting the number of pixels with the correct sign. False Source code in coppafish/plot/omp/score_shape.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def __init__ ( self , nb : Notebook , spot_no : int , method : str = 'omp' , score_multiplier : Optional [ float ] = None , check : bool = False ): \"\"\" Diagnostic to show how score is computed in the omp method Hatched region in top plot shows pixels which contribute to the final score. Score is actually equal to the absolute sum of the top plots in the hatched regions. Can also see how `score_omp_multiplier` affects the final score. The larger this is, the more the positive pixels contribute compared to the negative. !!! warning \"Requires access to `nb.file_names.tile_dir`\" Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to be plotted. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. score_multiplier: Initial value of `score_omp_multiplier`. check: If `True`, will compare score found to that saved in *Notebook* and raise error if they differ. Will also check that absolute sum of the top plots in the hatched regions is equal to score calculated from counting the number of pixels with the correct sign. \"\"\" # TODO: The textbox for this plot seems to be much less responsive than in the other diagnostics # for some reason. if method . lower () == 'omp' : page_name = 'omp' else : page_name = 'ref_spots' check = False self . check = check self . nbp_omp = nb . omp self . spot_no = spot_no self . gene_no = nb . __getattribute__ ( page_name ) . gene_no [ spot_no ] self . gene_names = nb . call_spots . gene_names self . expected_sign_image = nb . omp . spot_shape self . nz = self . expected_sign_image . shape [ 2 ] im_radius = (( np . asarray ( self . expected_sign_image . shape ) - 1 ) / 2 ) . astype ( int ) self . coef_images , min_global_yxz , max_global_yxz = get_coef_images ( nb , spot_no , method , im_radius ) # Find where sign of coef image matches that of expected sign image self . both_positive = np . asarray ([ self . coef_images [ self . gene_no ] > 0 , self . expected_sign_image > 0 ]) . all ( axis = 0 ) self . both_negative = np . asarray ([ self . coef_images [ self . gene_no ] < 0 , self . expected_sign_image < 0 ]) . all ( axis = 0 ) self . check_neighbours () # Start with default multiplier config = nb . get_config ()[ 'thresholds' ] if score_multiplier is None : score_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_multiplier self . score_thresh = config [ 'score_omp' ] # maximum possible value of any one pixel in expected_shape for any score_multiplier self . vmax_expected = 1 / np . min ([ np . sum ( self . expected_sign_image > 0 ), np . sum ( self . expected_sign_image < 0 )]) self . vmax_coef = np . abs ( self . coef_images [ self . gene_no ]) . max () # Initialize plots self . plot_extent = [ min_global_yxz [ 1 ] - 0.5 , max_global_yxz [ 1 ] + 0.5 , min_global_yxz [ 0 ] - 0.5 , max_global_yxz [ 0 ] + 0.5 ] self . fig , self . ax = plt . subplots ( 2 , self . nz , figsize = ( 14 , 5 ), sharex = True , sharey = True ) self . subplot_adjust = [ 0.05 , 0.88 , 0.09 , 0.85 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax = self . ax . flatten () self . im = [ None ] * self . nz * 2 expected_image_plot = self . expected_image () self . score = self . get_score ( expected_image_plot ) for i in range ( self . nz ): self . im [ i ] = self . ax [ i ] . imshow ( expected_image_plot [:, :, i ], vmin =- self . vmax_expected , vmax = self . vmax_expected , cmap = 'bwr' , extent = self . plot_extent ) self . im [ i + self . nz ] = self . ax [ i + self . nz ] . imshow ( self . coef_images [ self . gene_no , :, :, i ], vmin =- self . vmax_coef , vmax = self . vmax_coef , cmap = 'bwr' , extent = self . plot_extent ) if i == ( self . nz - 1 ) / 2 : self . ax [ i ] . set_title ( f \"Expected Coefficient Sign \\n Z= { int ( np . rint ( min_global_yxz [ 2 ] + i )) } \" ) else : self . ax [ i ] . set_title ( f \"Z= { int ( np . rint ( min_global_yxz [ 2 ] + i )) } \" ) self . set_coef_plot_title () self . add_hatching () # Set up colorbars for each plot mid_point = ( self . subplot_adjust [ 2 ] + self . subplot_adjust [ 3 ]) / 2 gap_size = 0.08 cbar_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.01 , mid_point + gap_size / 2 , 0.005 , self . subplot_adjust [ 3 ] - mid_point - gap_size / 2 ]) # left, bottom, width, height self . fig . colorbar ( self . im [ 0 ], cax = cbar_ax ) cbar_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.01 , self . subplot_adjust [ 2 ] + gap_size / 5 , 0.005 , mid_point - self . subplot_adjust [ 2 ] - gap_size / 2 ]) # left, bottom, width, height self . fig . colorbar ( self . im [ self . nz ], cax = cbar_ax ) # Add titles self . fig . supylabel ( 'Y' ) self . fig . supxlabel ( 'X' , size = 12 , x = ( self . subplot_adjust [ 0 ] + self . subplot_adjust [ 1 ]) / 2 ) plt . suptitle ( f \"OMP Score Calculation for Spot { spot_no } , Gene { self . gene_no } : { self . gene_names [ self . gene_no ] } \" , x = ( self . subplot_adjust [ 0 ] + self . subplot_adjust [ 1 ]) / 2 ) # Add text box to change score multiplier text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.062 , self . subplot_adjust [ 2 ] + gap_size / 5 , 0.05 , 0.04 ]) self . text_box = TextBox ( text_ax , 'Score \\n ' + r 'Multiplier, $\\rho$' , str ( np . around ( self . score_multiplier , 2 )), color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis label . set_position ([ 0.5 , 2.75 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . update ) plt . show ()","title":"view_omp_score"},{"location":"code/plot/omp/#score-histogram","text":"","title":"Score Histogram"},{"location":"code/plot/omp/#histogram_score","text":"If method is anchor, this will show the histogram of nb.ref_spots.score with the option to view the histogram of the score computed using various other configurations of background fitting and gene_efficiency . This allows one to see how the these affect the score. If method is omp, this will show the histogram of omp score, computed with coppafish.call_spots.omp_spot_score . There will also be the option to view the histograms shown for the anchor method. I.e. we compute the dot product score for the omp spots. Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. 'omp' score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. None check bool If True , and method='anchor' , will check that scores computed here match those saved to Notebook. False hist_spacing float Initial width of bin in histogram. 0.001 show_plot bool Whether to run plt.show() or not. True Source code in coppafish/plot/omp/score_hist.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def __init__ ( self , nb : Notebook , method : str = 'omp' , score_omp_multiplier : Optional [ float ] = None , check : bool = False , hist_spacing : float = 0.001 , show_plot : bool = True ): \"\"\" If method is anchor, this will show the histogram of `nb.ref_spots.score` with the option to view the histogram of the score computed using various other configurations of `background` fitting and `gene_efficiency`. This allows one to see how the these affect the score. If `method` is omp, this will show the histogram of omp score, computed with `coppafish.call_spots.omp_spot_score`. There will also be the option to view the histograms shown for the anchor method. I.e. we compute the dot product score for the omp spots. Args: nb: *Notebook* containing at least `call_spots` page. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. check: If `True`, and `method='anchor'`, will check that scores computed here match those saved to Notebook. hist_spacing: Initial width of bin in histogram. show_plot: Whether to run `plt.show()` or not. \"\"\" # Add data if score_omp_multiplier is None : config = nb . get_config ()[ 'thresholds' ] score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_multiplier = score_omp_multiplier self . gene_names = nb . call_spots . gene_names self . n_genes = self . gene_names . size # Use all genes by default self . genes_use = np . arange ( self . n_genes ) # For computing score_dp spot_colors , spot_colors_pb , background_var = background_fitting ( nb , method ) grc_ind = np . ix_ ( np . arange ( self . n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels ) # Bled codes saved to Notebook should already have L2 norm = 1 over used_channels and rounds bled_codes = nb . call_spots . bled_codes [ grc_ind ] bled_codes_ge = nb . call_spots . bled_codes_ge [ grc_ind ] # Save score_dp for each permutation of with/without background/gene_efficiency self . n_plots = 5 if method . lower () == 'omp' : self . n_plots += 1 self . nbp_omp = nb . omp self . gene_no = self . nbp_omp . gene_no self . score = np . zeros (( self . gene_no . size , self . n_plots ), dtype = np . float32 ) self . score [:, - 1 ] = omp_spot_score ( self . nbp_omp , self . score_multiplier ) self . method = 'OMP' else : self . gene_no = nb . ref_spots . gene_no self . score = np . zeros (( self . gene_no . size , self . n_plots ), dtype = np . float32 ) self . method = 'Anchor' self . use = np . isin ( self . gene_no , self . genes_use ) # which spots to plot # DP score self . score [:, 0 ] = get_dot_product_score ( spot_colors_pb , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , background_var )[ 0 ] if method . lower () != 'omp' and check : if np . max ( np . abs ( self . score [:, 0 ] - nb . ref_spots . score )) > self . check_tol : raise ValueError ( f \"nb.ref_spots.score differs to that computed here \\n \" f \"Set check=False to get past this error\" ) # DP score no weight self . score [:, 1 ] = get_dot_product_score ( spot_colors_pb , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # DP score no background self . score [:, 2 ] = get_dot_product_score ( spot_colors , bled_codes_ge , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # DP score no gene efficiency self . score [:, 3 ] = get_dot_product_score ( spot_colors_pb , bled_codes , self . gene_no , nb . call_spots . dp_norm_shift , background_var )[ 0 ] # DP score no background or gene efficiency self . score [:, 4 ] = get_dot_product_score ( spot_colors , bled_codes , self . gene_no , nb . call_spots . dp_norm_shift , None )[ 0 ] # Initialise plot self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 11 , 5 )) self . subplot_adjust = [ 0.07 , 0.85 , 0.1 , 0.93 ] self . fig . subplots_adjust ( left = self . subplot_adjust [ 0 ], right = self . subplot_adjust [ 1 ], bottom = self . subplot_adjust [ 2 ], top = self . subplot_adjust [ 3 ]) self . ax . set_ylabel ( r \"Number of Spots\" ) if method . lower () == 'omp' : self . ax . set_xlabel ( r \"Score, $\\gamma_s$ or $\\Delta_s$\" ) else : self . ax . set_xlabel ( r \"Score, $\\Delta_s$\" ) self . ax . set_title ( f \"Distribution of Scores for all { self . method } spots\" ) # Set lower bound based on dot product score with no GE/no background as likely to be lowest self . hist_min = np . percentile ( self . score [:, 4 ], 0.1 ) # Set upper bound based on dot product score with GE and background because this is likely to be highest self . hist_max = np . clip ( np . percentile ( self . score [:, 0 ], 99.9 ), 1 , 2 ) self . hist_spacing = hist_spacing hist_bins = np . arange ( self . hist_min , self . hist_max + self . hist_spacing / 2 , self . hist_spacing ) self . plots = [ None ] * self . n_plots default_colors = plt . rcParams [ 'axes.prop_cycle' ] . _left for i in range ( self . n_plots ): y , x = np . histogram ( self . score [ self . use , i ], hist_bins ) x = x [: - 1 ] + self . hist_spacing / 2 # so same length as x self . plots [ i ], = self . ax . plot ( x , y , color = default_colors [ i ][ 'color' ]) if method . lower () == 'omp' and i < self . n_plots - 1 : self . plots [ i ] . set_visible ( False ) elif i > 0 and method . lower () != 'omp' : self . plots [ i ] . set_visible ( False ) self . ax . set_xlim ( self . hist_min , self . hist_max ) self . ax . set_ylim ( 0 , None ) # Add text box to change score multiplier text_box_labels = [ 'Gene' , 'Histogram \\n Spacing' , 'Score \\n ' + r 'Multiplier, $\\rho$' ] text_box_values = [ 'all' , self . hist_spacing , np . around ( self . score_multiplier , 2 )] text_box_funcs = [ self . update_genes , self . update_hist_spacing , self . update_score_multiplier ] if method . lower () != 'omp' : text_box_labels = text_box_labels [: 2 ] text_box_values = text_box_values [: 2 ] text_box_funcs = text_box_funcs [: 2 ] self . text_boxes = [ None ] * len ( text_box_labels ) for i in range ( len ( text_box_labels )): text_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.05 , self . subplot_adjust [ 2 ] + 0.15 * ( len ( text_box_labels ) - i - 1 ), 0.05 , 0.04 ]) self . text_boxes [ i ] = TextBox ( text_ax , text_box_labels [ i ], text_box_values [ i ], color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_boxes [ i ] . cursor . set_color ( 'r' ) label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis if i == 0 : label . set_position ([ 0.5 , 1.77 ]) # [x,y] - change here to set the position else : label . set_position ([ 0.5 , 2.75 ]) # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_boxes [ i ] . on_submit ( text_box_funcs [ i ]) # Add buttons to add/remove score_dp histograms self . buttons_ax = self . fig . add_axes ([ self . subplot_adjust [ 1 ] + 0.02 , self . subplot_adjust [ 3 ] - 0.45 , 0.15 , 0.5 ]) plt . axis ( 'off' ) self . button_labels = [ r \"$\\Delta_s$\" + \" \\n Dot Product Score\" , r \"$\\Delta_s$\" + \" \\n No Weighting\" , r \"$\\Delta_s$\" + \" \\n No Background\" , r \"$\\Delta_s$\" + \" \\n No Gene Efficiency\" , r \"$\\Delta_s$\" + \" \\n No Background \\n No Gene Efficiency\" ] label_checked = [ True , False , False , False , False ] if method . lower () == 'omp' : self . button_labels += [ r \"$\\gamma_s$\" + \" \\n OMP Score\" ] label_checked += [ True ] label_checked [ 0 ] = False self . buttons = CheckButtons ( self . buttons_ax , self . button_labels , label_checked ) for i in range ( self . n_plots ): self . buttons . labels [ i ] . set_fontsize ( 7 ) self . buttons . labels [ i ] . set_color ( default_colors [ i ][ 'color' ]) self . buttons . rectangles [ i ] . set_color ( 'w' ) self . buttons . on_clicked ( self . choose_plots ) if show_plot : plt . show ()","title":"histogram_score"},{"location":"code/plot/omp/#histogram_2d_score","text":"This plots the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score \\(\\Delta_s\\) . Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required score_omp_multiplier Optional [ float ] Can specify the value of score_omp_multiplier to use to compute omp score. If None , will use value in config file. None Source code in coppafish/plot/omp/score_hist.py 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def __init__ ( self , nb : Notebook , score_omp_multiplier : Optional [ float ] = None ): \"\"\" This plots the bivariate histogram to see the correlation between the omp spot score, $\\gamma_s$ and the dot product score $\\Delta_s$. Args: nb: *Notebook* containing at least `call_spots` page. score_omp_multiplier: Can specify the value of score_omp_multiplier to use to compute omp score. If `None`, will use value in config file. \"\"\" # large hist_spacing so quick as we change it anway super () . __init__ ( nb , 'omp' , score_omp_multiplier , False , 0.5 , False ) self . ax . clear () # Get rid of buttons - only use actual dot product score self . buttons_ax . clear () plt . axis ( 'off' ) self . score = self . score [:, [ 0 , self . n_plots - 1 ]] self . n_plots = 2 del self . plots hist_bins = np . arange ( self . hist_min , self . hist_max + self . hist_spacing / 2 , self . hist_spacing ) self . x_score_ind = 0 self . hist_spacing = 0.01 self . plot = self . ax . hist2d ( self . score [:, self . x_score_ind ], self . score [:, - 1 ], hist_bins )[ 3 ] self . cbar = self . fig . colorbar ( self . plot , ax = self . ax ) self . ax . set_xlim ( self . hist_min , self . hist_max ) self . ax . set_ylim ( self . hist_min , self . hist_max ) self . text_boxes [ 1 ] . set_val ( self . hist_spacing ) self . ax . set_xlabel ( self . button_labels [ 0 ] . replace ( ' \\n ' , ', ' )) self . ax . set_ylabel ( self . button_labels [ - 1 ] . replace ( ' \\n ' , ', ' )) plt . show ()","title":"histogram_2d_score"},{"location":"code/plot/omp/#track-fit","text":"","title":"Track Fit"},{"location":"code/plot/omp/#coppafish.plot.omp.track_fit.get_track_info","text":"This runs omp while tracking the residual at each stage. Parameters: Name Type Description Default nb Notebook Notebook containing experiment details. Must have run at least as far as call_reference_spots . required spot_no int Spot of interest to get track_info for. required method str 'anchor' or 'omp' . Which method of gene assignment used i.e. spot_no belongs to ref_spots or omp page of Notebook. required dp_thresh Optional [ float ] If None, will use value in omp section of config file. None max_genes Optional [ int ] If None, will use value in omp section of config file. None Returns: Type Description dict track_info - dictionary containing info about genes added at each step returned: background_codes - float [n_channels x n_rounds x n_channels] . background_codes[c] is the background vector for channel c with L2 norm of 1. background_coefs - float [n_channels] . background_coefs[c] is the coefficient value for background_codes[c] . gene_added - int [n_genes_added + 2] . gene_added[0] and gene_added[1] are -1. gene_added[2+i] is the ith gene that was added. residual - float [(n_genes_added + 2) x n_rounds x n_channels] . residual[0] is the initial pixel_color . residual[1] is the post background pixel_color . residual[2+i] is the pixel_color after removing gene gene_added[2+i] . coef - float [(n_genes_added + 2) x n_genes] . coef[0] and coef[1] are all 0. coef[2+i] are the coefficients for all genes after the ith gene has been added. dot_product - float [n_genes_added + 2] . dot_product[0] and dot_product[1] are 0. dot_product[2+i] is the dot product for the gene gene_added[2+i] . inverse_var - float [(n_genes_added + 2) x n_rounds x n_channels] . inverse_var[0] and inverse_var[1] are all 0. inverse_var[2+i] is the weighting used to compute dot_product[2+i] , which down-weights rounds/channels for which a gene has already been fitted. np . ndarray bled_codes - float [n_genes x n_use_rounds x n_use_channels] . gene bled_codes used in omp with L2 norm = 1. float dp_thresh - threshold dot product score, above which gene is fitted. Source code in coppafish/plot/omp/track_fit.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_track_info ( nb : Notebook , spot_no : int , method : str , dp_thresh : Optional [ float ] = None , max_genes : Optional [ int ] = None ) -> Tuple [ dict , np . ndarray , float ]: \"\"\" This runs omp while tracking the residual at each stage. Args: nb: Notebook containing experiment details. Must have run at least as far as `call_reference_spots`. spot_no: Spot of interest to get track_info for. method: `'anchor'` or `'omp'`. Which method of gene assignment used i.e. `spot_no` belongs to `ref_spots` or `omp` page of Notebook. dp_thresh: If None, will use value in omp section of config file. max_genes: If None, will use value in omp section of config file. Returns: `track_info` - dictionary containing info about genes added at each step returned: - `background_codes` - `float [n_channels x n_rounds x n_channels]`. `background_codes[c]` is the background vector for channel `c` with L2 norm of 1. - `background_coefs` - `float [n_channels]`. `background_coefs[c]` is the coefficient value for `background_codes[c]`. - `gene_added` - `int [n_genes_added + 2]`. `gene_added[0]` and `gene_added[1]` are -1. `gene_added[2+i]` is the `ith` gene that was added. - `residual` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `residual[0]` is the initial `pixel_color`. `residual[1]` is the post background `pixel_color`. `residual[2+i]` is the `pixel_color` after removing gene `gene_added[2+i]`. - `coef` - `float [(n_genes_added + 2) x n_genes]`. `coef[0]` and `coef[1]` are all 0. `coef[2+i]` are the coefficients for all genes after the ith gene has been added. - `dot_product` - `float [n_genes_added + 2]`. `dot_product[0]` and `dot_product[1]` are 0. `dot_product[2+i]` is the dot product for the gene `gene_added[2+i]`. - `inverse_var` - `float [(n_genes_added + 2) x n_rounds x n_channels]`. `inverse_var[0]` and `inverse_var[1]` are all 0. `inverse_var[2+i]` is the weighting used to compute `dot_product[2+i]`, which down-weights rounds/channels for which a gene has already been fitted. `bled_codes` - `float [n_genes x n_use_rounds x n_use_channels]`. gene `bled_codes` used in omp with L2 norm = 1. `dp_thresh` - threshold dot product score, above which gene is fitted. \"\"\" color_norm = nb . call_spots . color_norm_factor [ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] n_use_rounds , n_use_channels = color_norm . shape if method . lower () == 'omp' : page_name = 'omp' config_name = 'omp' else : page_name = 'ref_spots' config_name = 'call_spots' spot_color = nb . __getattribute__ ( page_name ) . colors [ spot_no ][ np . ix_ ( nb . basic_info . use_rounds , nb . basic_info . use_channels )] / color_norm n_genes = nb . call_spots . bled_codes_ge . shape [ 0 ] bled_codes = np . asarray ( nb . call_spots . bled_codes_ge [ np . ix_ ( np . arange ( n_genes ), nb . basic_info . use_rounds , nb . basic_info . use_channels )]) # ensure L2 norm is 1 for bled codes norm_factor = np . expand_dims ( np . linalg . norm ( bled_codes , axis = ( 1 , 2 )), ( 1 , 2 )) norm_factor [ norm_factor == 0 ] = 1 # For genes with no dye in use_dye, this avoids blow up on next line bled_codes = bled_codes / norm_factor # Get info to run omp dp_norm_shift = nb . call_spots . dp_norm_shift * np . sqrt ( n_use_rounds ) config = nb . get_config () if dp_thresh is None : dp_thresh = config [ 'omp' ][ 'dp_thresh' ] alpha = config [ config_name ][ 'alpha' ] beta = config [ config_name ][ 'beta' ] if max_genes is None : max_genes = config [ 'omp' ][ 'max_genes' ] weight_coef_fit = config [ 'omp' ][ 'weight_coef_fit' ] # Run omp with track to get residual at each stage track_info = get_all_coefs ( spot_color [ np . newaxis ], bled_codes , nb . call_spots . background_weight_shift , dp_norm_shift , dp_thresh , alpha , beta , max_genes , weight_coef_fit , True )[ 2 ] return track_info , bled_codes , dp_thresh","title":"get_track_info()"},{"location":"code/plot/raw/","text":"add_basic_info_no_save ( nb ) This adds the basic_info page to the notebook without saving the notebook. Parameters: Name Type Description Default nb Notebook Notebook with no basic_info page. required Source code in coppafish/plot/raw.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def add_basic_info_no_save ( nb : Notebook ): \"\"\" This adds the `basic_info` page to the notebook without saving the notebook. Args: nb: Notebook with no `basic_info` page. \"\"\" if not nb . has_page ( \"basic_info\" ): nb . _no_save_pages [ 'basic_info' ] = {} # don't save if add basic_info page config = nb . get_config () nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb += nbp_basic get_raw_images ( nb , tiles , rounds , channels , use_z ) This loads in raw images for the experiment corresponding to the Notebook . Parameters: Name Type Description Default nb Notebook Notebook for experiment required tiles List [ int ] npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | required rounds List [ int ] Rounds to view. required channels List [ int ] Channels to view. required use_z List [ int ] Which z-planes to load in from raw data. required Returns: Type Description np . ndarray raw_images - [len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)] uint16 array. np . ndarray raw_images[t, r, c] is the [n_y x n_x x len(use_z)] image for tile tiles[t] , round rounds[r] and channel np . ndarray channels[c] . Source code in coppafish/plot/raw.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_raw_images ( nb : Notebook , tiles : List [ int ], rounds : List [ int ], channels : List [ int ], use_z : List [ int ]) -> np . ndarray : \"\"\" This loads in raw images for the experiment corresponding to the *Notebook*. Args: nb: Notebook for experiment tiles: npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | rounds: Rounds to view. channels: Channels to view. use_z: Which z-planes to load in from raw data. Returns: `raw_images` - `[len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)]` uint16 array. `raw_images[t, r, c]` is the `[n_y x n_x x len(use_z)]` image for tile `tiles[t]`, round `rounds[r]` and channel `channels[c]`. \"\"\" n_tiles = len ( tiles ) n_rounds = len ( rounds ) n_channels = len ( channels ) n_images = n_rounds * n_tiles * n_channels ny = nb . basic_info . tile_sz nx = ny nz = len ( use_z ) raw_images = np . zeros (( n_tiles , n_rounds , n_channels , ny , nx , nz ), dtype = np . uint16 ) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f 'Loading in raw data' ) for r in range ( n_rounds ): round_dask_array = raw . load ( nb . file_names , nb . basic_info , r = rounds [ r ]) # TODO: Can get rid of these two for loops, when round_dask_array is always a dask array. # At the moment though, is not dask array when using nd2_reader (On Mac M1). for t in range ( n_tiles ): for c in range ( n_channels ): pbar . set_postfix ({ 'round' : rounds [ r ], 'tile' : tiles [ t ], 'channel' : channels [ c ]}) raw_images [ t , r , c ] = raw . load ( nb . file_names , nb . basic_info , round_dask_array , rounds [ r ], tiles [ t ], channels [ c ], use_z ) pbar . update ( 1 ) return raw_images view_raw ( nb = None , tiles = 0 , rounds = 0 , channels = None , use_z = None , config_file = None ) Function to view raw data in napari. There will upto 4 scrollbars for each image to change tile, round, channel and z-plane. Requires access to nb.file_names.input_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. None tiles Union [ int , List [ int ]] npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 rounds Union [ int , List [ int ]] rounds to view ( anchor will be nb.basic_info.n_rounds i.e. the last round.) 0 channels Optional [ Union [ int , List [ int ]]] Channels to view. If None , will load all channels. None use_z Optional [ Union [ int , List [ int ]]] Which z-planes to load in from raw data. If None , will use load all z-planes (except from first one if config['basic_info']['ignore_first_z_plane'] == True ). None config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/raw.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def view_raw ( nb : Optional [ Notebook ] = None , tiles : Union [ int , List [ int ]] = 0 , rounds : Union [ int , List [ int ]] = 0 , channels : Optional [ Union [ int , List [ int ]]] = None , use_z : Optional [ Union [ int , List [ int ]]] = None , config_file : Optional [ str ] = None ): \"\"\" Function to view raw data in napari. There will upto 4 scrollbars for each image to change tile, round, channel and z-plane. !!! warning \"Requires access to `nb.file_names.input_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. tiles: npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | rounds: rounds to view (`anchor` will be `nb.basic_info.n_rounds` i.e. the last round.) channels: Channels to view. If `None`, will load all channels. use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first one if `config['basic_info']['ignore_first_z_plane'] == True`). config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if channels is None : channels = np . arange ( nb . basic_info . n_channels ) if use_z is None : use_z = nb . basic_info . use_z tiles , rounds , channels , use_z = number_to_list ([ tiles , rounds , channels , use_z ]) raw_images = get_raw_images ( nb , tiles , rounds , channels , use_z ) viewer = napari . Viewer () viewer . add_image ( np . moveaxis ( raw_images , - 1 , 3 ), name = 'Raw Images' ) @viewer . dims . events . current_step . connect def update_slider ( event ): viewer . status = f 'Tile: { tiles [ event . value [ 0 ]] } , Round: { rounds [ event . value [ 1 ]] } , ' \\ f 'Channel: { channels [ event . value [ 2 ]] } , Z: { use_z [ event . value [ 3 ]] } ' viewer . dims . axis_labels = [ 'Tile' , 'Round' , 'Channel' , 'z' , 'y' , 'x' ] viewer . dims . set_point ([ 0 , 1 , 2 ], [ 0 , 0 , 0 ]) # set to first tile, round and channel initially napari . run ()","title":"Raw"},{"location":"code/plot/raw/#coppafish.plot.raw.add_basic_info_no_save","text":"This adds the basic_info page to the notebook without saving the notebook. Parameters: Name Type Description Default nb Notebook Notebook with no basic_info page. required Source code in coppafish/plot/raw.py 11 12 13 14 15 16 17 18 19 20 21 22 23 def add_basic_info_no_save ( nb : Notebook ): \"\"\" This adds the `basic_info` page to the notebook without saving the notebook. Args: nb: Notebook with no `basic_info` page. \"\"\" if not nb . has_page ( \"basic_info\" ): nb . _no_save_pages [ 'basic_info' ] = {} # don't save if add basic_info page config = nb . get_config () nbp_basic = set_basic_info ( config [ 'file_names' ], config [ 'basic_info' ]) nb += nbp_basic","title":"add_basic_info_no_save()"},{"location":"code/plot/raw/#coppafish.plot.raw.get_raw_images","text":"This loads in raw images for the experiment corresponding to the Notebook . Parameters: Name Type Description Default nb Notebook Notebook for experiment required tiles List [ int ] npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | required rounds List [ int ] Rounds to view. required channels List [ int ] Channels to view. required use_z List [ int ] Which z-planes to load in from raw data. required Returns: Type Description np . ndarray raw_images - [len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)] uint16 array. np . ndarray raw_images[t, r, c] is the [n_y x n_x x len(use_z)] image for tile tiles[t] , round rounds[r] and channel np . ndarray channels[c] . Source code in coppafish/plot/raw.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_raw_images ( nb : Notebook , tiles : List [ int ], rounds : List [ int ], channels : List [ int ], use_z : List [ int ]) -> np . ndarray : \"\"\" This loads in raw images for the experiment corresponding to the *Notebook*. Args: nb: Notebook for experiment tiles: npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | rounds: Rounds to view. channels: Channels to view. use_z: Which z-planes to load in from raw data. Returns: `raw_images` - `[len(tiles) x len(rounds) x len(channels) x n_y x n_x x len(use_z)]` uint16 array. `raw_images[t, r, c]` is the `[n_y x n_x x len(use_z)]` image for tile `tiles[t]`, round `rounds[r]` and channel `channels[c]`. \"\"\" n_tiles = len ( tiles ) n_rounds = len ( rounds ) n_channels = len ( channels ) n_images = n_rounds * n_tiles * n_channels ny = nb . basic_info . tile_sz nx = ny nz = len ( use_z ) raw_images = np . zeros (( n_tiles , n_rounds , n_channels , ny , nx , nz ), dtype = np . uint16 ) with tqdm ( total = n_images ) as pbar : pbar . set_description ( f 'Loading in raw data' ) for r in range ( n_rounds ): round_dask_array = raw . load ( nb . file_names , nb . basic_info , r = rounds [ r ]) # TODO: Can get rid of these two for loops, when round_dask_array is always a dask array. # At the moment though, is not dask array when using nd2_reader (On Mac M1). for t in range ( n_tiles ): for c in range ( n_channels ): pbar . set_postfix ({ 'round' : rounds [ r ], 'tile' : tiles [ t ], 'channel' : channels [ c ]}) raw_images [ t , r , c ] = raw . load ( nb . file_names , nb . basic_info , round_dask_array , rounds [ r ], tiles [ t ], channels [ c ], use_z ) pbar . update ( 1 ) return raw_images","title":"get_raw_images()"},{"location":"code/plot/raw/#coppafish.plot.raw.view_raw","text":"Function to view raw data in napari. There will upto 4 scrollbars for each image to change tile, round, channel and z-plane. Requires access to nb.file_names.input_dir Parameters: Name Type Description Default nb Optional [ Notebook ] Notebook for experiment. If no Notebook exists, pass config_file instead. None tiles Union [ int , List [ int ]] npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | 0 rounds Union [ int , List [ int ]] rounds to view ( anchor will be nb.basic_info.n_rounds i.e. the last round.) 0 channels Optional [ Union [ int , List [ int ]]] Channels to view. If None , will load all channels. None use_z Optional [ Union [ int , List [ int ]]] Which z-planes to load in from raw data. If None , will use load all z-planes (except from first one if config['basic_info']['ignore_first_z_plane'] == True ). None config_file Optional [ str ] path to config file for experiment. None Source code in coppafish/plot/raw.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def view_raw ( nb : Optional [ Notebook ] = None , tiles : Union [ int , List [ int ]] = 0 , rounds : Union [ int , List [ int ]] = 0 , channels : Optional [ Union [ int , List [ int ]]] = None , use_z : Optional [ Union [ int , List [ int ]]] = None , config_file : Optional [ str ] = None ): \"\"\" Function to view raw data in napari. There will upto 4 scrollbars for each image to change tile, round, channel and z-plane. !!! warning \"Requires access to `nb.file_names.input_dir`\" Args: nb: *Notebook* for experiment. If no *Notebook* exists, pass `config_file` instead. tiles: npy (as opposed to nd2 fov) tile indices to view. For an experiment where the tiles are arranged in a 4 x 3 (ny x nx) grid, tile indices are indicated as below: | 2 | 1 | 0 | | 5 | 4 | 3 | | 8 | 7 | 6 | | 11 | 10 | 9 | rounds: rounds to view (`anchor` will be `nb.basic_info.n_rounds` i.e. the last round.) channels: Channels to view. If `None`, will load all channels. use_z: Which z-planes to load in from raw data. If `None`, will use load all z-planes (except from first one if `config['basic_info']['ignore_first_z_plane'] == True`). config_file: path to config file for experiment. \"\"\" if nb is None : nb = Notebook ( config_file = config_file ) add_basic_info_no_save ( nb ) # deal with case where there is no notebook yet if channels is None : channels = np . arange ( nb . basic_info . n_channels ) if use_z is None : use_z = nb . basic_info . use_z tiles , rounds , channels , use_z = number_to_list ([ tiles , rounds , channels , use_z ]) raw_images = get_raw_images ( nb , tiles , rounds , channels , use_z ) viewer = napari . Viewer () viewer . add_image ( np . moveaxis ( raw_images , - 1 , 3 ), name = 'Raw Images' ) @viewer . dims . events . current_step . connect def update_slider ( event ): viewer . status = f 'Tile: { tiles [ event . value [ 0 ]] } , Round: { rounds [ event . value [ 1 ]] } , ' \\ f 'Channel: { channels [ event . value [ 2 ]] } , Z: { use_z [ event . value [ 3 ]] } ' viewer . dims . axis_labels = [ 'Tile' , 'Round' , 'Channel' , 'z' , 'y' , 'x' ] viewer . dims . set_point ([ 0 , 1 , 2 ], [ 0 , 0 , 0 ]) # set to first tile, round and channel initially napari . run ()","title":"view_raw()"},{"location":"code/plot/register/","text":"Register Initial view_register_shift_info For all shifts to imaging rounds from the reference round computed in the register_initial section of the pipeline, this plots the values of the shifts found and the score compared to the score_thresh . For each round, there will be 3 plots: y shift vs x shift for all tiles z shift vs x shift for all tiles score vs score_thresh for all tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the register_initial page. required outlier bool If True , will plot nb.register_initial.shift_outlier instead of nb.register_initial.shift . In this case, only tiles for which the two are different are plotted for each round. False Source code in coppafish/plot/register/shift.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def view_register_shift_info ( nb : Notebook , outlier : bool = False ): \"\"\" For all shifts to imaging rounds from the reference round computed in the `register_initial` section of the pipeline, this plots the values of the shifts found and the `score` compared to the `score_thresh`. For each round, there will be 3 plots: * y shift vs x shift for all tiles * z shift vs x shift for all tiles * `score` vs `score_thresh` for all tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if `score > score_thresh` and red otherwise. Args: nb: Notebook containing at least the `register_initial` page. outlier: If `True`, will plot `nb.register_initial.shift_outlier` instead of `nb.register_initial.shift`. In this case, only tiles for which the two are different are plotted for each round. \"\"\" shift_info = {} if nb . basic_info . is_3d : ndim = 3 else : ndim = 2 for r in nb . basic_info . use_rounds : name = f 'Round { r } ' shift_info [ name ] = {} shift_info [ name ][ 'tile' ] = nb . basic_info . use_tiles if outlier : shift_info [ name ][ 'shift' ] = nb . register_initial . shift_outlier [ nb . basic_info . use_tiles , r , : ndim ] shift_info [ name ][ 'score' ] = nb . register_initial . shift_score_outlier [ nb . basic_info . use_tiles , r ] else : shift_info [ name ][ 'shift' ] = nb . register_initial . shift [ nb . basic_info . use_tiles , r , : ndim ] shift_info [ name ][ 'score' ] = nb . register_initial . shift_score [ nb . basic_info . use_tiles , r ] shift_info [ name ][ 'score_thresh' ] = nb . register_initial . shift_score_thresh [ nb . basic_info . use_tiles , r ] if outlier : title_start = \"Outlier \" else : title_start = \"\" shift_info_plot ( shift_info , f \" { title_start } Shifts found in register_initial part of pipeline \" f \"from round { nb . basic_info . ref_round } , channel \" f \" { nb . basic_info . ref_channel } to channel \" f \" { nb . register_initial . shift_channel } for each round and tile\" ) view_register_search Function to plot results of exhaustive search to find shift between ref_round/ref_channel and round r , channel c for tile t . This shift will then be used as the starting point when running point cloud registration to find affine transform. Useful for debugging the register_initial section of the pipeline. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. required t int tile interested in. required r int Want to find the shift between the reference round and this round. required c Optional [ int ] Want to find the shift between the reference channel and this channel. If None , config['shift_channel'] will be used, as it is in the pipeline. None return_shift bool If True, will return shift found and will not call plt.show() otherwise will return None. False Returns: Type Description Optional [ np . ndarray ] best_shift - float [shift_y, shift_x, shift_z] . Best shift found. shift_z is in units of z-pixels. Source code in coppafish/plot/register/shift.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def view_register_search ( nb : Notebook , t : int , r : int , c : Optional [ int ] = None , return_shift : bool = False ) -> Optional [ np . ndarray ]: \"\"\" Function to plot results of exhaustive search to find shift between `ref_round/ref_channel` and round `r`, channel `c` for tile `t`. This shift will then be used as the starting point when running point cloud registration to find affine transform. Useful for debugging the `register_initial` section of the pipeline. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. t: tile interested in. r: Want to find the shift between the reference round and this round. c: Want to find the shift between the reference channel and this channel. If `None`, `config['shift_channel']` will be used, as it is in the pipeline. return_shift: If True, will return shift found and will not call plt.show() otherwise will return None. Returns: `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. `shift_z` is in units of z-pixels. \"\"\" config = nb . get_config ()[ 'register_initial' ] if c is None : c = config [ 'shift_channel' ] if c is None : c = nb . basic_info . ref_channel if not np . isin ( c , nb . basic_info . use_channels ): raise ValueError ( f \"c should be in nb.basic_info.use_channels, but value given is \\n \" f \" { c } which is not in use_channels = { nb . basic_info . use_channels } .\" ) coords = [ 'y' , 'x' , 'z' ] shifts = [{}] for i in range ( len ( coords )): shifts [ 0 ][ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if not nb . basic_info . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 0 ][ 'z' ] = np . array ([ 0 ], dtype = int ) shifts = shifts * nb . basic_info . n_rounds # get one set of shifts for each round c_ref = nb . basic_info . ref_channel r_ref = nb . basic_info . ref_round # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy print ( f 'Finding shift between round { r_ref } , channel { c_ref } to round { r } , channel { c } for tile { t } ' ) shift , shift_score , shift_score_thresh , debug_info = \\ compute_shift ( spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref ), spot_yxz ( nb . find_spots . spot_details , t , r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) title = f 'Shift between r= { r_ref } , c= { c_ref } and r= { r } , c= { c } for tile { t } . YXZ Shift = { shift } .' if return_shift : show = False else : show = True view_shifts ( debug_info [ 'shifts_2d' ], debug_info [ 'scores_2d' ], debug_info [ 'shifts_3d' ], debug_info [ 'scores_3d' ], shift , debug_info [ 'min_score_2d' ], debug_info [ 'shift_2d_initial' ], shift_score_thresh , debug_info [ 'shift_thresh' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], title , show ) if return_shift : return shift Register scale_box_plots Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel. Want very similar values for a given channel across all tiles and rounds for each dimension. Also expect \\(y\\) and \\(x\\) scaling to be very similar. \\(z\\) scaling different due to unit conversion. Parameters: Name Type Description Default nb Notebook Notebook containing the register and register_debug NotebookPages . required Source code in coppafish/plot/register/diagnostics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def scale_box_plots ( nb : Notebook ): \"\"\" Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel. Want very similar values for a given channel across all tiles and rounds for each dimension. Also expect $y$ and $x$ scaling to be very similar. $z$ scaling different due to unit conversion. Args: nb: *Notebook* containing the `register` and `register_debug` *NotebookPages*. \"\"\" if nb . basic_info . is_3d : ndim = 3 if np . ptp ( nb . register . transform [ np . ix_ ( nb . basic_info . use_tiles , nb . basic_info . use_rounds , nb . basic_info . use_channels )][:, :, :, 2 , 2 ]) < 1e-5 : ndim = 2 warnings . warn ( \"Not showing z-scaling as all are the same\" ) else : ndim = 2 fig , ax = plt . subplots ( ndim , figsize = ( 10 , 6 ), sharex = True ) ax [ 0 ] . get_shared_y_axes () . join ( ax [ 0 ], ax [ 1 ]) fig . subplots_adjust ( left = 0.075 , right = 0.95 , bottom = 0.15 ) y_titles = [ \"Scaling - Y\" , \"Scaling - X\" , \"Scaling - Z\" ] n_use_channels = len ( nb . basic_info . use_channels ) # different boxplot color for each channel # Must be distinct from black and white channel_colors = distinctipy . get_colors ( n_use_channels , [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) for i in range ( ndim ): box_data = [ nb . register . transform [ nb . basic_info . use_tiles , r , c , i , i ] for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] bp = ax [ i ] . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) leg_markers = [] c = - 1 for j in range ( len ( box_data )): if j % n_use_channels == 0 : c += 1 leg_markers = leg_markers + [ bp [ 'boxes' ][ j ]] bp [ 'boxes' ][ j ] . set_facecolor ( channel_colors [ c ]) ax [ i ] . set_ylabel ( y_titles [ i ]) if i == ndim - 1 : tick_labels = np . tile ( nb . basic_info . use_rounds , n_use_channels ) . tolist () leg_labels = nb . basic_info . use_channels ax [ i ] . set_xticks ( np . arange ( len ( tick_labels ))) ax [ i ] . set_xticklabels ( tick_labels ) ax [ i ] . legend ( leg_markers , leg_labels , title = 'Channel' ) ax [ i ] . set_xlabel ( 'Round' ) ax [ 0 ] . set_title ( 'Boxplots showing distribution of scalings due to \\n chromatic aberration amongst tiles for each ' 'round and channel' ) plt . show () view_affine_shift_info For all affine transforms to imaging rounds/channels from the reference round computed in the register section of the pipeline, this plots the values of the shifts, n_matches (number of neighbours found) and error (average distance between neighbours). For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots: y shift vs x shift for all tiles z shift vs x shift for all tiles n_matches vs error for all tiles In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if nb.register_debug.n_matches > nb.register_debug.n_matches_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the register page. required c Optional [ int ] If None, will give option to scroll with mouse to change channel. If specify c, will show just one channel with no scrolling. None outlier bool If True , will plot shifts from nb.register_debug.transform_outlier instead of nb.register.transform . In this case, only tiles for which nb.register_debug.failed == True are plotted for each round/channel. False Source code in coppafish/plot/register/diagnostics.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , nb : Notebook , c : Optional [ int ] = None , outlier : bool = False ): \"\"\" For all affine transforms to imaging rounds/channels from the reference round computed in the `register` section of the pipeline, this plots the values of the shifts, `n_matches` (number of neighbours found) and `error` (average distance between neighbours). For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots: * y shift vs x shift for all tiles * z shift vs x shift for all tiles * `n_matches` vs `error` for all tiles In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if `nb.register_debug.n_matches > nb.register_debug.n_matches_thresh` and red otherwise. Args: nb: Notebook containing at least the `register` page. c: If None, will give option to scroll with mouse to change channel. If specify c, will show just one channel with no scrolling. outlier: If `True`, will plot shifts from `nb.register_debug.transform_outlier` instead of `nb.register.transform`. In this case, only tiles for which `nb.register_debug.failed == True` are plotted for each round/channel. \"\"\" self . outlier = outlier self . nb = nb if c is None : if self . outlier : # only show channels for which there is an outlier shift self . channels = np . sort ( np . unique ( np . where ( nb . register_debug . failed )[ 2 ])) if len ( self . channels ) == 0 : raise ValueError ( f \"No outlier transforms were computed\" ) else : self . channels = np . asarray ( nb . basic_info . use_channels ) else : self . channels = [ c ] self . n_channels = len ( self . channels ) self . c_ind = 0 self . c = self . channels [ self . c_ind ] n_cols = len ( nb . basic_info . use_rounds ) if nb . basic_info . is_3d : n_rows = 3 else : n_rows = 2 self . fig , self . ax = plt . subplots ( n_rows , n_cols , figsize = ( 15 , 7 )) self . fig . subplots_adjust ( hspace = 0.4 , bottom = 0.08 , left = 0.06 , right = 0.97 , top = 0.9 ) self . shift_info = self . get_ax_lims ( self . nb , self . channels , self . outlier ) self . update () if self . n_channels > 1 : self . fig . canvas . mpl_connect ( 'scroll_event' , self . z_scroll ) plt . show () ICP view_icp Function to plot results of iterative closest point to find affine transform between ref_round/ref_channel and round r , channel c for tile t . Useful for debugging the register section of the pipeline. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. If contains register_initial and/or register pages, then transform from these will be used. required t int tile interested in. required r int Want to find the transform between the reference round and this round. required c int Want to find the transform between the reference channel and this channel. required Source code in coppafish/plot/register/icp.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def view_icp ( nb : Notebook , t : int , r : int , c : int ): \"\"\" Function to plot results of iterative closest point to find affine transform between `ref_round/ref_channel` and round `r`, channel `c` for tile `t`. Useful for debugging the `register` section of the pipeline. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. If contains `register_initial` and/or `register` pages, then transform from these will be used. t: tile interested in. r: Want to find the transform between the reference round and this round. c: Want to find the transform between the reference channel and this channel. \"\"\" config = nb . get_config ()[ 'register' ] if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] point_clouds = [] # 1st point cloud is imaging one as does not change point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r , c )] # only keep isolated spots, those whose second neighbour is far away # Do this only for imaging point cloud as that is what is done in pipeline/register isolated = get_isolated_points ( point_clouds [ 0 ] * z_scale , 2 * neighb_dist_thresh ) point_clouds [ 0 ] = point_clouds [ 0 ][ isolated ] # 2nd is untransformed reference point cloud r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref )] z_scale = z_scale [ 2 ] # Add shifted reference point cloud if nb . has_page ( 'register_initial' ): shift = nb . register_initial . shift [ t , r ] else : shift = view_register_search ( nb , t , r , return_shift = True ) point_clouds = point_clouds + [ point_clouds [ 1 ] + shift ] # Add reference point cloud transformed by an affine transform transform_outlier = None if nb . has_page ( 'register' ): transform = nb . register . transform [ t , r , c ] if nb . has_page ( 'register_debug' ): # If particular tile/round/channel was found by regularised least squares transform_outlier = nb . register_debug . transform_outlier [ t , r , c ] if np . abs ( transform_outlier ) . max () == 0 : transform_outlier = None else : start_transform = np . eye ( 4 , 3 ) # no scaling just shift to start off icp start_transform [ 3 ] = shift * [ 1 , 1 , z_scale ] transform = get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , config [ 'n_iter' ])[ 0 ] if not nb . basic_info . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , nb . basic_info . nz ], dtype = np . int16 ) if transform_outlier is not None : point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform_outlier , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] pc_labels = [ f 'Imaging: r { r } , c { c } ' , f 'Reference: r { r_ref } , c { c_ref } ' , f 'Reference: r { r_ref } , c { c_ref } - Shift' , f 'Reference: r { r_ref } , c { c_ref } - Affine' ] if transform_outlier is not None : pc_labels = pc_labels + [ f 'Reference: r { r_ref } , c { c_ref } - Regularized' ] view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Transform of tile { t } to round { r } , channel { c } ' ) plt . show () view_icp_reg Function to plot how regularisation changes the affine transform found through iterative closest point between ref_round/ref_channel and round \\(r\\) , channel \\(c\\) for tile \\(t\\) . Useful for finding suitable values for config['register']['regularize_constant'] and config['register']['regularize_factor'] . Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. Must contain register_initial / register_debug pages if start_transform / reg_transform not specified. required t int tile interested in. required r int Want to find the transform between the reference round and this round. required c int Want to find the transform between the reference channel and this channel. required reg_constant Optional [ List ] int [n_reg] Constant used when doing regularized least squares. Will be a point cloud produced for affine transformed produced with each of these parameters. Value will be indicated by \\(\\lambda\\) in legend/buttons. If None , will use config['register']['regularize_constant'] . None reg_factor Optional [ List ] float [n_reg] Factor to boost rotation/scaling term in regularized least squares. Must be same length as reg_constant . Value will be indicated by \\(\\mu\\) in legend/buttons. If None , will use config['register']['regularize_factor'] . The regularized term in the loss function for finding the transform is: \\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and reg_transform[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and reg_transform[3] . I.e. the squared difference of the shift part of the transform from the target. \\(\\lambda\\) is reg_constant - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\) . \\(\\mu\\) is reg_factor - the larger it is, the smaller \\(D_{scale}^2\\) . None reg_transform Optional [ np . ndarray ] Transform to regularize towards i.e. the expected transform. If not specified, will use average transformation based on nb.register_debug.av_scaling[c] and nb.register_debug.av_shifts[t, r] with no rotation. This is the same as what is used in coppafish.register.icp . None start_transform Optional [ np . ndarray ] Initial transform to use as starting point for ICP. If None , will use nb.register_initial.shift[t, r] for the non-regularized case and reg_transform for the regularized case to match method used in coppafish.register.icp . None plot_residual bool If True , will run plot_reg_residual as well. False Source code in coppafish/plot/register/icp.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def view_icp_reg ( nb : Notebook , t : int , r : int , c : int , reg_constant : Optional [ List ] = None , reg_factor : Optional [ List ] = None , reg_transform : Optional [ np . ndarray ] = None , start_transform : Optional [ np . ndarray ] = None , plot_residual : bool = False ): \"\"\" Function to plot how regularisation changes the affine transform found through iterative closest point between `ref_round/ref_channel` and round $r$, channel $c$ for tile $t$. Useful for finding suitable values for `config['register']['regularize_constant']` and `config['register']['regularize_factor']`. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. Must contain `register_initial`/`register_debug` pages if `start_transform`/`reg_transform` not specified. t: tile interested in. r: Want to find the transform between the reference round and this round. c: Want to find the transform between the reference channel and this channel. reg_constant: `int [n_reg]` Constant used when doing regularized least squares. Will be a point cloud produced for affine transformed produced with each of these parameters. Value will be indicated by $\\lambda$ in legend/buttons. If `None`, will use `config['register']['regularize_constant']`. reg_factor: `float [n_reg]` Factor to boost rotation/scaling term in regularized least squares. Must be same length as `reg_constant`. Value will be indicated by $\\mu$ in legend/buttons. If `None`, will use `config['register']['regularize_factor']`. The regularized term in the loss function for finding the transform is: $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$ Where: * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`. I.e. the squared difference of the scaling/rotation part of the transform from the target. * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`. I.e. the squared difference of the shift part of the transform from the target. * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$. * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$. reg_transform: Transform to regularize towards i.e. the expected transform. If not specified, will use average transformation based on `nb.register_debug.av_scaling[c]` and `nb.register_debug.av_shifts[t, r]` with no rotation. This is the same as what is used in `coppafish.register.icp`. start_transform: Initial transform to use as starting point for ICP. If `None`, will use `nb.register_initial.shift[t, r]` for the non-regularized case and `reg_transform` for the regularized case to match method used in `coppafish.register.icp`. plot_residual: If `True`, will run plot_reg_residual as well. \"\"\" config = nb . get_config ()[ 'register' ] n_iter = config [ 'n_iter' ] if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] if not nb . basic_info . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , nb . basic_info . nz ], dtype = np . int16 ) point_clouds = [] # 1st point cloud is imaging one as does not change point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r , c )] # only keep isolated spots, those whose second neighbour is far away # Do this only for imaging point cloud as that is what is done in pipeline/register isolated = get_isolated_points ( point_clouds [ 0 ] * z_scale , 2 * neighb_dist_thresh ) point_clouds [ 0 ] = point_clouds [ 0 ][ isolated ] # 2nd is untransformed reference point cloud r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref )] z_scale = z_scale [ 2 ] # 3rd is ref point cloud transformed according to affine transform with no regularisation if start_transform is None : # no scaling just shift to start off icp as used in pipeline if no start_transform given shift = nb . register_initial . shift [ t , r ] start_transform = np . eye ( 4 , 3 ) start_transform [ 3 ] = shift * [ 1 , 1 , z_scale ] transform_no_reg = get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , n_iter )[ 0 ] point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform_no_reg , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] # 4th is ref point cloud transformed according to target reg transform if reg_transform is None : # If reg transform not specified, use av scaling for c and av shift for t, r. # Also set start_transform to reg_transform if not specified # This is same as what is done in `coppafish.register.base.icp` for t/r/c where regularisation required. reg_transform = np . eye ( 4 , 3 ) * nb . register_debug . av_scaling [ c ] reg_transform [ 3 ] = nb . register_debug . av_shifts [ t , r ] start_transform = reg_transform . copy () point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], reg_transform , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] pc_labels = [ f 'Imaging: r { r } , c { c } ' , f 'Reference: r { r_ref } , c { c_ref } ' , r '$\\lambda=0$' , r '$\\lambda=\\infty$' ] # Now add ref point cloud transformed according to reg transform found with all reg params given. # If None given, use default params. if reg_constant is None : reg_constant = [ config [ 'regularize_constant' ]] if reg_factor is None : reg_factor = [ config [ 'regularize_factor' ]] n_reg = len ( reg_constant ) if n_reg != len ( reg_factor ): raise ValueError ( f \"reg_constant and reg_factor need to be same size but they \" f \"have { n_reg } and { len ( reg_factor ) } values respectively\" ) transforms = np . zeros (( n_reg , 4 , 3 )) for i in range ( n_reg ): # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant. reg_constant_scale = np . sqrt ( 0.5 * reg_constant [ i ] * reg_factor [ i ]) reg_constant_shift = np . sqrt ( 0.5 * reg_constant [ i ]) transforms [ i ] = \\ get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , n_iter , reg_constant_scale , reg_constant_shift , reg_transform )[ 0 ] point_clouds += [ apply_transform ( point_clouds [ 1 ], transforms [ i ], nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] if reg_constant [ i ] > 1000 : pc_labels += [ r '$\\lambda= {:.0E} ,\\mu= {:.0E} $' . format ( int ( reg_constant [ i ]), int ( reg_factor [ i ]))] else : pc_labels += [ r '$\\lambda= {:.0f} ,\\mu= {:.0E} $' . format ( int ( reg_constant [ i ]), int ( reg_factor [ i ]))] vpc = view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Regularized transform of tile { t } to round { r } , channel { c } ' ) n_matches_reg_target = np . sum ( vpc . neighb [ 3 ] >= 0 ) if plot_residual and n_reg > 1 : plot_reg_residual ( reg_transform , transforms , reg_constant , reg_factor , transform_no_reg , n_matches_reg_target ) else : plt . show () plot_reg_residual This shows how changing the regularization parameters affect how close the affine transform is to that which it was being regularized towards. E.g. it should show that the larger reg_constant , the smaller the difference (y-axis values in the plots). There will be up to 4 plots, in each, the different colors refer to the different reg_constant / reg_factor combinations and the smaller the y-axis value, the closer the transform is to reg_transform . The different axis variables in the plot are explained in the reg_factor variable description below. Parameters: Name Type Description Default reg_transform np . ndarray float [4 x 3] Transform which was regularized towards i.e. the expected transform. required transforms_plot List [ np . ndarray ] [n_reg] . transforms_plot[i] is the [4 x 3] transform found with regularization parameters reg_constant[i] and reg_factor[i] . required reg_constant List int [n_reg] Constant used when doing regularized least squares. Value will be indicated by \\(\\lambda\\) on x-axis. required reg_factor List float [n_reg] Factor to boost rotation/scaling term in regularized least squares. Must be same length as reg_constant . Value will be indicated by \\(\\mu\\) on x-axis. The regularized term in the loss function for finding the transform is: \\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and reg_transform[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and reg_transform[3] . I.e. the squared difference of the shift part of the transform from the target. \\(\\lambda\\) is reg_constant - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\) . \\(\\mu\\) is reg_factor - the larger it is, the smaller \\(D_{scale}^2\\) . required transform_no_reg Optional [ np . ndarray ] float [4 x 3] . Transform found with no regularization. If given, will show white line labelled by \\(\\lambda=0\\) with y-axis value indicating value with no regularization. None n_matches Optional [ int ] Number of nearest neighbours found for reg_transform . If given, will show white line labelled by \\(n_{matches}\\) with x-axis value equal to this on the plots where \\(\\lambda\\) is the x-axis variable. This is because we expect the regularization should have more of an effect if reg_constant > n_matches , i.e. the y-axis variable should go towards zero at x-axis values beyond this line. None Source code in coppafish/plot/register/icp.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def plot_reg_residual ( reg_transform : np . ndarray , transforms_plot : List [ np . ndarray ], reg_constant : List , reg_factor : List , transform_no_reg : Optional [ np . ndarray ] = None , n_matches : Optional [ int ] = None ): \"\"\" This shows how changing the regularization parameters affect how close the affine transform is to that which it was being regularized towards. E.g. it should show that the larger `reg_constant`, the smaller the difference (y-axis values in the plots). There will be up to 4 plots, in each, the different colors refer to the different `reg_constant`/`reg_factor` combinations and the smaller the y-axis value, the closer the transform is to `reg_transform`. The different axis variables in the plot are explained in the `reg_factor` variable description below. Args: reg_transform: `float [4 x 3]` Transform which was regularized towards i.e. the expected transform. transforms_plot: `[n_reg]`. `transforms_plot[i]` is the `[4 x 3]` transform found with regularization parameters `reg_constant[i]` and `reg_factor[i]`. reg_constant: `int [n_reg]` Constant used when doing regularized least squares. Value will be indicated by $\\lambda$ on x-axis. reg_factor: `float [n_reg]` Factor to boost rotation/scaling term in regularized least squares. Must be same length as `reg_constant`. Value will be indicated by $\\mu$ on x-axis. The regularized term in the loss function for finding the transform is: $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$ Where: * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`. I.e. the squared difference of the scaling/rotation part of the transform from the target. * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`. I.e. the squared difference of the shift part of the transform from the target. * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$. * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$. transform_no_reg: `float [4 x 3]`. Transform found with no regularization. If given, will show white line labelled by $\\lambda=0$ with y-axis value indicating value with no regularization. n_matches: Number of nearest neighbours found for `reg_transform`. If given, will show white line labelled by $n_{matches}$ with x-axis value equal to this on the plots where $\\lambda$ is the x-axis variable. This is because we expect the regularization should have more of an effect if `reg_constant > n_matches`, i.e. the y-axis variable should go towards zero at x-axis values beyond this line. \"\"\" n_reg = len ( reg_constant ) col_info = {} col_ind = 0 if len ( np . unique ( reg_constant )) > 1 : col_info [ col_ind ] = {} if len ( np . unique ( reg_factor )) == 1 : col_info [ col_ind ][ 'title' ] = r \"Varying $\\lambda$ ($\\mu = {:.0E} $)\" . format ( int ( reg_factor [ 0 ])) else : col_info [ col_ind ][ 'title' ] = f \"Varying $\\lambda$\" col_info [ col_ind ][ 'x_label' ] = r \"$\\lambda$\" col_info [ col_ind ][ 'x' ] = reg_constant col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( reg_constant )), int ( 1.1 * np . max ( reg_constant ))] if n_matches is not None : col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( list ( reg_constant ) + [ n_matches ])), int ( 1.1 * np . max ( list ( reg_constant ) + [ n_matches ]))] col_info [ col_ind ][ 'log_x' ] = np . ptp ( col_info [ col_ind ][ 'x_lims' ]) > 100 col_info [ col_ind ][ 'n_matches' ] = n_matches col_ind += 1 if len ( np . unique ( reg_factor )) > 1 : col_info [ col_ind ] = {} if len ( np . unique ( reg_constant )) == 1 : col_info [ col_ind ][ 'title' ] = rf \"Varying $\\mu$ ($\\lambda = { int ( reg_constant [ 0 ]) } $)\" else : col_info [ col_ind ][ 'title' ] = f \"Varying $\\mu$\" col_info [ col_ind ][ 'x_label' ] = r \"$\\mu$\" col_info [ col_ind ][ 'x' ] = reg_factor col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( reg_factor )), int ( 1.1 * np . max ( reg_factor ))] col_info [ col_ind ][ 'log_x' ] = np . ptp ( col_info [ col_ind ][ 'x_lims' ]) > 100 col_info [ col_ind ][ 'n_matches' ] = None if n_reg != len ( reg_factor ): raise ValueError ( f \"reg_constant and reg_factor need to be same size but they \" f \"have { n_reg } and { len ( reg_factor ) } values respectively\" ) if n_reg != len ( transforms_plot ): raise ValueError ( f \"reg_constant and transforms_plot need to be same size but they \" f \"have { n_reg } and { len ( transforms_plot ) } values respectively\" ) if len ( col_info ) == 0 : raise ValueError ( \"Not enough data to plot\" ) y0 = [ np . linalg . norm ( transforms_plot [ i ][: 3 , :] - reg_transform [: 3 , :]) for i in range ( n_reg )] y1 = [ np . linalg . norm ( transforms_plot [ i ][ 3 ] - reg_transform [ 3 ]) for i in range ( n_reg )] if transform_no_reg is not None : y0_no_reg = np . linalg . norm ( transform_no_reg [: 3 , :] - reg_transform [: 3 ,:]) y1_no_reg = np . linalg . norm ( transform_no_reg [ 3 ] - reg_transform [ 3 ]) y0_lim = [ np . min ( y0 + [ y0_no_reg ]) - 0.0002 , np . max ( y0 + [ y0_no_reg ]) + 0.0002 ] y1_lim = [ np . min ( y1 + [ y1_no_reg ]) - 0.2 , np . max ( y1 + [ y1_no_reg ]) + 0.2 ] else : y0_lim = [ np . min ( y0 ) - 0.0002 , np . max ( y0 ) + 0.0002 ] y1_lim = [ np . min ( y1 ) - 0.2 , np . max ( y1 ) + 0.2 ] y0_lim = np . clip ( y0_lim , 0 , np . inf ) y1_lim = np . clip ( y1_lim , 0 , np . inf ) colors = distinctipy . get_colors ( n_reg , [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) fig , ax = plt . subplots ( 2 , len ( col_info ), figsize = ( 15 , 7 )) if len ( col_info ) == 1 : ax = ax [:, np . newaxis ] for i in range ( len ( col_info )): if transform_no_reg is not None : ax [ 0 , i ] . plot ( col_info [ i ][ 'x_lims' ], [ y0_no_reg , y0_no_reg ], 'w:' ) ax [ 0 , i ] . text ( np . mean ( col_info [ i ][ 'x_lims' ]), y0_no_reg , r \"$\\lambda = 0$\" , va = 'bottom' , ha = \"center\" , color = 'w' ) ax [ 1 , i ] . plot ( col_info [ i ][ 'x_lims' ], [ y1_no_reg , y1_no_reg ], 'w:' ) ax [ 1 , i ] . text ( np . mean ( col_info [ i ][ 'x_lims' ]), y1_no_reg , r \"$\\lambda = 0$\" , va = 'bottom' , ha = \"center\" , color = 'w' ) if col_info [ i ][ 'n_matches' ] is not None : ax [ 0 , i ] . plot ([ n_matches , n_matches ], y0_lim , 'w:' ) ax [ 0 , i ] . text ( n_matches , np . percentile ( y0_lim , 10 ), r \"$n_ {matches} $\" , va = 'bottom' , ha = \"right\" , color = 'w' , rotation = 90 ) ax [ 1 , i ] . plot ([ n_matches , n_matches ], y1_lim , 'w:' ) ax [ 1 , i ] . text ( n_matches , np . percentile ( y1_lim , 10 ), r \"$n_ {matches} $\" , va = 'bottom' , ha = \"right\" , color = 'w' , rotation = 90 ) ax [ 0 , i ] . scatter ( col_info [ i ][ 'x' ], y0 , color = colors ) ax [ 1 , i ] . scatter ( col_info [ i ][ 'x' ], y1 , color = colors ) ax [ 0 , i ] . get_shared_x_axes () . join ( ax [ 0 , i ], ax [ 1 , i ]) ax [ 1 , i ] . set_xlabel ( col_info [ i ][ 'x_label' ]) ax [ 0 , i ] . set_title ( col_info [ i ][ 'title' ]) if col_info [ i ][ 'log_x' ]: ax [ 1 , i ] . set_xscale ( 'log' ) ax [ 1 , i ] . set_xlim ( col_info [ i ][ 'x_lims' ]) if i == 1 : ax [ 0 , 0 ] . get_shared_y_axes () . join ( ax [ 0 , 0 ], ax [ 0 , 1 ]) ax [ 1 , 0 ] . get_shared_y_axes () . join ( ax [ 1 , 0 ], ax [ 1 , 1 ]) ax [ 0 , 0 ] . set_ylim ( y0_lim ) ax [ 0 , 0 ] . set_ylabel ( \"$D_ {scale} $\" ) ax [ 1 , 0 ] . set_ylim ( y1_lim ) ax [ 1 , 0 ] . set_ylabel ( \"$D_ {shift} $\" ) fig . suptitle ( \"How varying regularization parameters affects how similar transform found is to target transform\" ) plt . show ()","title":"Register"},{"location":"code/plot/register/#register-initial","text":"","title":"Register Initial"},{"location":"code/plot/register/#view_register_shift_info","text":"For all shifts to imaging rounds from the reference round computed in the register_initial section of the pipeline, this plots the values of the shifts found and the score compared to the score_thresh . For each round, there will be 3 plots: y shift vs x shift for all tiles z shift vs x shift for all tiles score vs score_thresh for all tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the register_initial page. required outlier bool If True , will plot nb.register_initial.shift_outlier instead of nb.register_initial.shift . In this case, only tiles for which the two are different are plotted for each round. False Source code in coppafish/plot/register/shift.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def view_register_shift_info ( nb : Notebook , outlier : bool = False ): \"\"\" For all shifts to imaging rounds from the reference round computed in the `register_initial` section of the pipeline, this plots the values of the shifts found and the `score` compared to the `score_thresh`. For each round, there will be 3 plots: * y shift vs x shift for all tiles * z shift vs x shift for all tiles * `score` vs `score_thresh` for all tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if `score > score_thresh` and red otherwise. Args: nb: Notebook containing at least the `register_initial` page. outlier: If `True`, will plot `nb.register_initial.shift_outlier` instead of `nb.register_initial.shift`. In this case, only tiles for which the two are different are plotted for each round. \"\"\" shift_info = {} if nb . basic_info . is_3d : ndim = 3 else : ndim = 2 for r in nb . basic_info . use_rounds : name = f 'Round { r } ' shift_info [ name ] = {} shift_info [ name ][ 'tile' ] = nb . basic_info . use_tiles if outlier : shift_info [ name ][ 'shift' ] = nb . register_initial . shift_outlier [ nb . basic_info . use_tiles , r , : ndim ] shift_info [ name ][ 'score' ] = nb . register_initial . shift_score_outlier [ nb . basic_info . use_tiles , r ] else : shift_info [ name ][ 'shift' ] = nb . register_initial . shift [ nb . basic_info . use_tiles , r , : ndim ] shift_info [ name ][ 'score' ] = nb . register_initial . shift_score [ nb . basic_info . use_tiles , r ] shift_info [ name ][ 'score_thresh' ] = nb . register_initial . shift_score_thresh [ nb . basic_info . use_tiles , r ] if outlier : title_start = \"Outlier \" else : title_start = \"\" shift_info_plot ( shift_info , f \" { title_start } Shifts found in register_initial part of pipeline \" f \"from round { nb . basic_info . ref_round } , channel \" f \" { nb . basic_info . ref_channel } to channel \" f \" { nb . register_initial . shift_channel } for each round and tile\" )","title":"view_register_shift_info"},{"location":"code/plot/register/#view_register_search","text":"Function to plot results of exhaustive search to find shift between ref_round/ref_channel and round r , channel c for tile t . This shift will then be used as the starting point when running point cloud registration to find affine transform. Useful for debugging the register_initial section of the pipeline. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. required t int tile interested in. required r int Want to find the shift between the reference round and this round. required c Optional [ int ] Want to find the shift between the reference channel and this channel. If None , config['shift_channel'] will be used, as it is in the pipeline. None return_shift bool If True, will return shift found and will not call plt.show() otherwise will return None. False Returns: Type Description Optional [ np . ndarray ] best_shift - float [shift_y, shift_x, shift_z] . Best shift found. shift_z is in units of z-pixels. Source code in coppafish/plot/register/shift.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def view_register_search ( nb : Notebook , t : int , r : int , c : Optional [ int ] = None , return_shift : bool = False ) -> Optional [ np . ndarray ]: \"\"\" Function to plot results of exhaustive search to find shift between `ref_round/ref_channel` and round `r`, channel `c` for tile `t`. This shift will then be used as the starting point when running point cloud registration to find affine transform. Useful for debugging the `register_initial` section of the pipeline. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. t: tile interested in. r: Want to find the shift between the reference round and this round. c: Want to find the shift between the reference channel and this channel. If `None`, `config['shift_channel']` will be used, as it is in the pipeline. return_shift: If True, will return shift found and will not call plt.show() otherwise will return None. Returns: `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. `shift_z` is in units of z-pixels. \"\"\" config = nb . get_config ()[ 'register_initial' ] if c is None : c = config [ 'shift_channel' ] if c is None : c = nb . basic_info . ref_channel if not np . isin ( c , nb . basic_info . use_channels ): raise ValueError ( f \"c should be in nb.basic_info.use_channels, but value given is \\n \" f \" { c } which is not in use_channels = { nb . basic_info . use_channels } .\" ) coords = [ 'y' , 'x' , 'z' ] shifts = [{}] for i in range ( len ( coords )): shifts [ 0 ][ coords [ i ]] = np . arange ( config [ 'shift_min' ][ i ], config [ 'shift_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if not nb . basic_info . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 shifts [ 0 ][ 'z' ] = np . array ([ 0 ], dtype = int ) shifts = shifts * nb . basic_info . n_rounds # get one set of shifts for each round c_ref = nb . basic_info . ref_channel r_ref = nb . basic_info . ref_round # to convert z coordinate units to xy pixels when calculating distance to nearest neighbours z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy print ( f 'Finding shift between round { r_ref } , channel { c_ref } to round { r } , channel { c } for tile { t } ' ) shift , shift_score , shift_score_thresh , debug_info = \\ compute_shift ( spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref ), spot_yxz ( nb . find_spots . spot_details , t , r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ r ][ 'y' ], shifts [ r ][ 'x' ], shifts [ r ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) title = f 'Shift between r= { r_ref } , c= { c_ref } and r= { r } , c= { c } for tile { t } . YXZ Shift = { shift } .' if return_shift : show = False else : show = True view_shifts ( debug_info [ 'shifts_2d' ], debug_info [ 'scores_2d' ], debug_info [ 'shifts_3d' ], debug_info [ 'scores_3d' ], shift , debug_info [ 'min_score_2d' ], debug_info [ 'shift_2d_initial' ], shift_score_thresh , debug_info [ 'shift_thresh' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], title , show ) if return_shift : return shift","title":"view_register_search"},{"location":"code/plot/register/#register","text":"","title":"Register"},{"location":"code/plot/register/#scale_box_plots","text":"Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel. Want very similar values for a given channel across all tiles and rounds for each dimension. Also expect \\(y\\) and \\(x\\) scaling to be very similar. \\(z\\) scaling different due to unit conversion. Parameters: Name Type Description Default nb Notebook Notebook containing the register and register_debug NotebookPages . required Source code in coppafish/plot/register/diagnostics.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def scale_box_plots ( nb : Notebook ): \"\"\" Function to plot distribution of chromatic aberration scaling amongst tiles for each round and channel. Want very similar values for a given channel across all tiles and rounds for each dimension. Also expect $y$ and $x$ scaling to be very similar. $z$ scaling different due to unit conversion. Args: nb: *Notebook* containing the `register` and `register_debug` *NotebookPages*. \"\"\" if nb . basic_info . is_3d : ndim = 3 if np . ptp ( nb . register . transform [ np . ix_ ( nb . basic_info . use_tiles , nb . basic_info . use_rounds , nb . basic_info . use_channels )][:, :, :, 2 , 2 ]) < 1e-5 : ndim = 2 warnings . warn ( \"Not showing z-scaling as all are the same\" ) else : ndim = 2 fig , ax = plt . subplots ( ndim , figsize = ( 10 , 6 ), sharex = True ) ax [ 0 ] . get_shared_y_axes () . join ( ax [ 0 ], ax [ 1 ]) fig . subplots_adjust ( left = 0.075 , right = 0.95 , bottom = 0.15 ) y_titles = [ \"Scaling - Y\" , \"Scaling - X\" , \"Scaling - Z\" ] n_use_channels = len ( nb . basic_info . use_channels ) # different boxplot color for each channel # Must be distinct from black and white channel_colors = distinctipy . get_colors ( n_use_channels , [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) for i in range ( ndim ): box_data = [ nb . register . transform [ nb . basic_info . use_tiles , r , c , i , i ] for c in nb . basic_info . use_channels for r in nb . basic_info . use_rounds ] bp = ax [ i ] . boxplot ( box_data , notch = 0 , sym = '+' , patch_artist = True ) leg_markers = [] c = - 1 for j in range ( len ( box_data )): if j % n_use_channels == 0 : c += 1 leg_markers = leg_markers + [ bp [ 'boxes' ][ j ]] bp [ 'boxes' ][ j ] . set_facecolor ( channel_colors [ c ]) ax [ i ] . set_ylabel ( y_titles [ i ]) if i == ndim - 1 : tick_labels = np . tile ( nb . basic_info . use_rounds , n_use_channels ) . tolist () leg_labels = nb . basic_info . use_channels ax [ i ] . set_xticks ( np . arange ( len ( tick_labels ))) ax [ i ] . set_xticklabels ( tick_labels ) ax [ i ] . legend ( leg_markers , leg_labels , title = 'Channel' ) ax [ i ] . set_xlabel ( 'Round' ) ax [ 0 ] . set_title ( 'Boxplots showing distribution of scalings due to \\n chromatic aberration amongst tiles for each ' 'round and channel' ) plt . show ()","title":"scale_box_plots"},{"location":"code/plot/register/#view_affine_shift_info","text":"For all affine transforms to imaging rounds/channels from the reference round computed in the register section of the pipeline, this plots the values of the shifts, n_matches (number of neighbours found) and error (average distance between neighbours). For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots: y shift vs x shift for all tiles z shift vs x shift for all tiles n_matches vs error for all tiles In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if nb.register_debug.n_matches > nb.register_debug.n_matches_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the register page. required c Optional [ int ] If None, will give option to scroll with mouse to change channel. If specify c, will show just one channel with no scrolling. None outlier bool If True , will plot shifts from nb.register_debug.transform_outlier instead of nb.register.transform . In this case, only tiles for which nb.register_debug.failed == True are plotted for each round/channel. False Source code in coppafish/plot/register/diagnostics.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def __init__ ( self , nb : Notebook , c : Optional [ int ] = None , outlier : bool = False ): \"\"\" For all affine transforms to imaging rounds/channels from the reference round computed in the `register` section of the pipeline, this plots the values of the shifts, `n_matches` (number of neighbours found) and `error` (average distance between neighbours). For each round and channel (channel is changed by scrolling with the mouse), there will be 3 plots: * y shift vs x shift for all tiles * z shift vs x shift for all tiles * `n_matches` vs `error` for all tiles In each case, the markers in the plots are numbers. These numbers indicate the tile the shift was found for. The number will be blue if `nb.register_debug.n_matches > nb.register_debug.n_matches_thresh` and red otherwise. Args: nb: Notebook containing at least the `register` page. c: If None, will give option to scroll with mouse to change channel. If specify c, will show just one channel with no scrolling. outlier: If `True`, will plot shifts from `nb.register_debug.transform_outlier` instead of `nb.register.transform`. In this case, only tiles for which `nb.register_debug.failed == True` are plotted for each round/channel. \"\"\" self . outlier = outlier self . nb = nb if c is None : if self . outlier : # only show channels for which there is an outlier shift self . channels = np . sort ( np . unique ( np . where ( nb . register_debug . failed )[ 2 ])) if len ( self . channels ) == 0 : raise ValueError ( f \"No outlier transforms were computed\" ) else : self . channels = np . asarray ( nb . basic_info . use_channels ) else : self . channels = [ c ] self . n_channels = len ( self . channels ) self . c_ind = 0 self . c = self . channels [ self . c_ind ] n_cols = len ( nb . basic_info . use_rounds ) if nb . basic_info . is_3d : n_rows = 3 else : n_rows = 2 self . fig , self . ax = plt . subplots ( n_rows , n_cols , figsize = ( 15 , 7 )) self . fig . subplots_adjust ( hspace = 0.4 , bottom = 0.08 , left = 0.06 , right = 0.97 , top = 0.9 ) self . shift_info = self . get_ax_lims ( self . nb , self . channels , self . outlier ) self . update () if self . n_channels > 1 : self . fig . canvas . mpl_connect ( 'scroll_event' , self . z_scroll ) plt . show ()","title":"view_affine_shift_info"},{"location":"code/plot/register/#icp","text":"","title":"ICP"},{"location":"code/plot/register/#view_icp","text":"Function to plot results of iterative closest point to find affine transform between ref_round/ref_channel and round r , channel c for tile t . Useful for debugging the register section of the pipeline. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. If contains register_initial and/or register pages, then transform from these will be used. required t int tile interested in. required r int Want to find the transform between the reference round and this round. required c int Want to find the transform between the reference channel and this channel. required Source code in coppafish/plot/register/icp.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def view_icp ( nb : Notebook , t : int , r : int , c : int ): \"\"\" Function to plot results of iterative closest point to find affine transform between `ref_round/ref_channel` and round `r`, channel `c` for tile `t`. Useful for debugging the `register` section of the pipeline. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. If contains `register_initial` and/or `register` pages, then transform from these will be used. t: tile interested in. r: Want to find the transform between the reference round and this round. c: Want to find the transform between the reference channel and this channel. \"\"\" config = nb . get_config ()[ 'register' ] if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] point_clouds = [] # 1st point cloud is imaging one as does not change point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r , c )] # only keep isolated spots, those whose second neighbour is far away # Do this only for imaging point cloud as that is what is done in pipeline/register isolated = get_isolated_points ( point_clouds [ 0 ] * z_scale , 2 * neighb_dist_thresh ) point_clouds [ 0 ] = point_clouds [ 0 ][ isolated ] # 2nd is untransformed reference point cloud r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref )] z_scale = z_scale [ 2 ] # Add shifted reference point cloud if nb . has_page ( 'register_initial' ): shift = nb . register_initial . shift [ t , r ] else : shift = view_register_search ( nb , t , r , return_shift = True ) point_clouds = point_clouds + [ point_clouds [ 1 ] + shift ] # Add reference point cloud transformed by an affine transform transform_outlier = None if nb . has_page ( 'register' ): transform = nb . register . transform [ t , r , c ] if nb . has_page ( 'register_debug' ): # If particular tile/round/channel was found by regularised least squares transform_outlier = nb . register_debug . transform_outlier [ t , r , c ] if np . abs ( transform_outlier ) . max () == 0 : transform_outlier = None else : start_transform = np . eye ( 4 , 3 ) # no scaling just shift to start off icp start_transform [ 3 ] = shift * [ 1 , 1 , z_scale ] transform = get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , config [ 'n_iter' ])[ 0 ] if not nb . basic_info . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , nb . basic_info . nz ], dtype = np . int16 ) if transform_outlier is not None : point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform_outlier , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] pc_labels = [ f 'Imaging: r { r } , c { c } ' , f 'Reference: r { r_ref } , c { c_ref } ' , f 'Reference: r { r_ref } , c { c_ref } - Shift' , f 'Reference: r { r_ref } , c { c_ref } - Affine' ] if transform_outlier is not None : pc_labels = pc_labels + [ f 'Reference: r { r_ref } , c { c_ref } - Regularized' ] view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Transform of tile { t } to round { r } , channel { c } ' ) plt . show ()","title":"view_icp"},{"location":"code/plot/register/#view_icp_reg","text":"Function to plot how regularisation changes the affine transform found through iterative closest point between ref_round/ref_channel and round \\(r\\) , channel \\(c\\) for tile \\(t\\) . Useful for finding suitable values for config['register']['regularize_constant'] and config['register']['regularize_factor'] . Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. Must contain register_initial / register_debug pages if start_transform / reg_transform not specified. required t int tile interested in. required r int Want to find the transform between the reference round and this round. required c int Want to find the transform between the reference channel and this channel. required reg_constant Optional [ List ] int [n_reg] Constant used when doing regularized least squares. Will be a point cloud produced for affine transformed produced with each of these parameters. Value will be indicated by \\(\\lambda\\) in legend/buttons. If None , will use config['register']['regularize_constant'] . None reg_factor Optional [ List ] float [n_reg] Factor to boost rotation/scaling term in regularized least squares. Must be same length as reg_constant . Value will be indicated by \\(\\mu\\) in legend/buttons. If None , will use config['register']['regularize_factor'] . The regularized term in the loss function for finding the transform is: \\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and reg_transform[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and reg_transform[3] . I.e. the squared difference of the shift part of the transform from the target. \\(\\lambda\\) is reg_constant - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\) . \\(\\mu\\) is reg_factor - the larger it is, the smaller \\(D_{scale}^2\\) . None reg_transform Optional [ np . ndarray ] Transform to regularize towards i.e. the expected transform. If not specified, will use average transformation based on nb.register_debug.av_scaling[c] and nb.register_debug.av_shifts[t, r] with no rotation. This is the same as what is used in coppafish.register.icp . None start_transform Optional [ np . ndarray ] Initial transform to use as starting point for ICP. If None , will use nb.register_initial.shift[t, r] for the non-regularized case and reg_transform for the regularized case to match method used in coppafish.register.icp . None plot_residual bool If True , will run plot_reg_residual as well. False Source code in coppafish/plot/register/icp.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def view_icp_reg ( nb : Notebook , t : int , r : int , c : int , reg_constant : Optional [ List ] = None , reg_factor : Optional [ List ] = None , reg_transform : Optional [ np . ndarray ] = None , start_transform : Optional [ np . ndarray ] = None , plot_residual : bool = False ): \"\"\" Function to plot how regularisation changes the affine transform found through iterative closest point between `ref_round/ref_channel` and round $r$, channel $c$ for tile $t$. Useful for finding suitable values for `config['register']['regularize_constant']` and `config['register']['regularize_factor']`. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. Must contain `register_initial`/`register_debug` pages if `start_transform`/`reg_transform` not specified. t: tile interested in. r: Want to find the transform between the reference round and this round. c: Want to find the transform between the reference channel and this channel. reg_constant: `int [n_reg]` Constant used when doing regularized least squares. Will be a point cloud produced for affine transformed produced with each of these parameters. Value will be indicated by $\\lambda$ in legend/buttons. If `None`, will use `config['register']['regularize_constant']`. reg_factor: `float [n_reg]` Factor to boost rotation/scaling term in regularized least squares. Must be same length as `reg_constant`. Value will be indicated by $\\mu$ in legend/buttons. If `None`, will use `config['register']['regularize_factor']`. The regularized term in the loss function for finding the transform is: $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$ Where: * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`. I.e. the squared difference of the scaling/rotation part of the transform from the target. * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`. I.e. the squared difference of the shift part of the transform from the target. * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$. * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$. reg_transform: Transform to regularize towards i.e. the expected transform. If not specified, will use average transformation based on `nb.register_debug.av_scaling[c]` and `nb.register_debug.av_shifts[t, r]` with no rotation. This is the same as what is used in `coppafish.register.icp`. start_transform: Initial transform to use as starting point for ICP. If `None`, will use `nb.register_initial.shift[t, r]` for the non-regularized case and `reg_transform` for the regularized case to match method used in `coppafish.register.icp`. plot_residual: If `True`, will run plot_reg_residual as well. \"\"\" config = nb . get_config ()[ 'register' ] n_iter = config [ 'n_iter' ] if nb . basic_info . is_3d : neighb_dist_thresh = config [ 'neighb_dist_thresh_3d' ] else : neighb_dist_thresh = config [ 'neighb_dist_thresh_2d' ] z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] if not nb . basic_info . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nb . basic_info . tile_sz , nb . basic_info . tile_sz , nb . basic_info . nz ], dtype = np . int16 ) point_clouds = [] # 1st point cloud is imaging one as does not change point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r , c )] # only keep isolated spots, those whose second neighbour is far away # Do this only for imaging point cloud as that is what is done in pipeline/register isolated = get_isolated_points ( point_clouds [ 0 ] * z_scale , 2 * neighb_dist_thresh ) point_clouds [ 0 ] = point_clouds [ 0 ][ isolated ] # 2nd is untransformed reference point cloud r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t , r_ref , c_ref )] z_scale = z_scale [ 2 ] # 3rd is ref point cloud transformed according to affine transform with no regularisation if start_transform is None : # no scaling just shift to start off icp as used in pipeline if no start_transform given shift = nb . register_initial . shift [ t , r ] start_transform = np . eye ( 4 , 3 ) start_transform [ 3 ] = shift * [ 1 , 1 , z_scale ] transform_no_reg = get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , n_iter )[ 0 ] point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], transform_no_reg , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] # 4th is ref point cloud transformed according to target reg transform if reg_transform is None : # If reg transform not specified, use av scaling for c and av shift for t, r. # Also set start_transform to reg_transform if not specified # This is same as what is done in `coppafish.register.base.icp` for t/r/c where regularisation required. reg_transform = np . eye ( 4 , 3 ) * nb . register_debug . av_scaling [ c ] reg_transform [ 3 ] = nb . register_debug . av_shifts [ t , r ] start_transform = reg_transform . copy () point_clouds = point_clouds + [ apply_transform ( point_clouds [ 1 ], reg_transform , nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] pc_labels = [ f 'Imaging: r { r } , c { c } ' , f 'Reference: r { r_ref } , c { c_ref } ' , r '$\\lambda=0$' , r '$\\lambda=\\infty$' ] # Now add ref point cloud transformed according to reg transform found with all reg params given. # If None given, use default params. if reg_constant is None : reg_constant = [ config [ 'regularize_constant' ]] if reg_factor is None : reg_factor = [ config [ 'regularize_factor' ]] n_reg = len ( reg_constant ) if n_reg != len ( reg_factor ): raise ValueError ( f \"reg_constant and reg_factor need to be same size but they \" f \"have { n_reg } and { len ( reg_factor ) } values respectively\" ) transforms = np . zeros (( n_reg , 4 , 3 )) for i in range ( n_reg ): # Deviation in scale/rotation is much less than permitted deviation in shift so boost scale reg constant. reg_constant_scale = np . sqrt ( 0.5 * reg_constant [ i ] * reg_factor [ i ]) reg_constant_shift = np . sqrt ( 0.5 * reg_constant [ i ]) transforms [ i ] = \\ get_single_affine_transform ( point_clouds [ 1 ], point_clouds [ 0 ], z_scale , z_scale , start_transform , neighb_dist_thresh , nb . basic_info . tile_centre , n_iter , reg_constant_scale , reg_constant_shift , reg_transform )[ 0 ] point_clouds += [ apply_transform ( point_clouds [ 1 ], transforms [ i ], nb . basic_info . tile_centre , z_scale , tile_sz )[ 0 ]] if reg_constant [ i ] > 1000 : pc_labels += [ r '$\\lambda= {:.0E} ,\\mu= {:.0E} $' . format ( int ( reg_constant [ i ]), int ( reg_factor [ i ]))] else : pc_labels += [ r '$\\lambda= {:.0f} ,\\mu= {:.0E} $' . format ( int ( reg_constant [ i ]), int ( reg_factor [ i ]))] vpc = view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Regularized transform of tile { t } to round { r } , channel { c } ' ) n_matches_reg_target = np . sum ( vpc . neighb [ 3 ] >= 0 ) if plot_residual and n_reg > 1 : plot_reg_residual ( reg_transform , transforms , reg_constant , reg_factor , transform_no_reg , n_matches_reg_target ) else : plt . show ()","title":"view_icp_reg"},{"location":"code/plot/register/#plot_reg_residual","text":"This shows how changing the regularization parameters affect how close the affine transform is to that which it was being regularized towards. E.g. it should show that the larger reg_constant , the smaller the difference (y-axis values in the plots). There will be up to 4 plots, in each, the different colors refer to the different reg_constant / reg_factor combinations and the smaller the y-axis value, the closer the transform is to reg_transform . The different axis variables in the plot are explained in the reg_factor variable description below. Parameters: Name Type Description Default reg_transform np . ndarray float [4 x 3] Transform which was regularized towards i.e. the expected transform. required transforms_plot List [ np . ndarray ] [n_reg] . transforms_plot[i] is the [4 x 3] transform found with regularization parameters reg_constant[i] and reg_factor[i] . required reg_constant List int [n_reg] Constant used when doing regularized least squares. Value will be indicated by \\(\\lambda\\) on x-axis. required reg_factor List float [n_reg] Factor to boost rotation/scaling term in regularized least squares. Must be same length as reg_constant . Value will be indicated by \\(\\mu\\) on x-axis. The regularized term in the loss function for finding the transform is: \\(0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)\\) Where: \\(D_{scale}^2\\) is the squared distance between transform[:3, :] and reg_transform[:3, :] . I.e. the squared difference of the scaling/rotation part of the transform from the target. \\(D_{shift}^2\\) is the squared distance between transform[3] and reg_transform[3] . I.e. the squared difference of the shift part of the transform from the target. \\(\\lambda\\) is reg_constant - the larger it is, the smaller \\(D_{scale}^2\\) and \\(D_{shift}^2\\) . \\(\\mu\\) is reg_factor - the larger it is, the smaller \\(D_{scale}^2\\) . required transform_no_reg Optional [ np . ndarray ] float [4 x 3] . Transform found with no regularization. If given, will show white line labelled by \\(\\lambda=0\\) with y-axis value indicating value with no regularization. None n_matches Optional [ int ] Number of nearest neighbours found for reg_transform . If given, will show white line labelled by \\(n_{matches}\\) with x-axis value equal to this on the plots where \\(\\lambda\\) is the x-axis variable. This is because we expect the regularization should have more of an effect if reg_constant > n_matches , i.e. the y-axis variable should go towards zero at x-axis values beyond this line. None Source code in coppafish/plot/register/icp.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def plot_reg_residual ( reg_transform : np . ndarray , transforms_plot : List [ np . ndarray ], reg_constant : List , reg_factor : List , transform_no_reg : Optional [ np . ndarray ] = None , n_matches : Optional [ int ] = None ): \"\"\" This shows how changing the regularization parameters affect how close the affine transform is to that which it was being regularized towards. E.g. it should show that the larger `reg_constant`, the smaller the difference (y-axis values in the plots). There will be up to 4 plots, in each, the different colors refer to the different `reg_constant`/`reg_factor` combinations and the smaller the y-axis value, the closer the transform is to `reg_transform`. The different axis variables in the plot are explained in the `reg_factor` variable description below. Args: reg_transform: `float [4 x 3]` Transform which was regularized towards i.e. the expected transform. transforms_plot: `[n_reg]`. `transforms_plot[i]` is the `[4 x 3]` transform found with regularization parameters `reg_constant[i]` and `reg_factor[i]`. reg_constant: `int [n_reg]` Constant used when doing regularized least squares. Value will be indicated by $\\lambda$ on x-axis. reg_factor: `float [n_reg]` Factor to boost rotation/scaling term in regularized least squares. Must be same length as `reg_constant`. Value will be indicated by $\\mu$ on x-axis. The regularized term in the loss function for finding the transform is: $0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2)$ Where: * $D_{scale}^2$ is the squared distance between `transform[:3, :]` and `reg_transform[:3, :]`. I.e. the squared difference of the scaling/rotation part of the transform from the target. * $D_{shift}^2$ is the squared distance between `transform[3]` and `reg_transform[3]`. I.e. the squared difference of the shift part of the transform from the target. * $\\lambda$ is `reg_constant` - the larger it is, the smaller $D_{scale}^2$ and $D_{shift}^2$. * $\\mu$ is `reg_factor` - the larger it is, the smaller $D_{scale}^2$. transform_no_reg: `float [4 x 3]`. Transform found with no regularization. If given, will show white line labelled by $\\lambda=0$ with y-axis value indicating value with no regularization. n_matches: Number of nearest neighbours found for `reg_transform`. If given, will show white line labelled by $n_{matches}$ with x-axis value equal to this on the plots where $\\lambda$ is the x-axis variable. This is because we expect the regularization should have more of an effect if `reg_constant > n_matches`, i.e. the y-axis variable should go towards zero at x-axis values beyond this line. \"\"\" n_reg = len ( reg_constant ) col_info = {} col_ind = 0 if len ( np . unique ( reg_constant )) > 1 : col_info [ col_ind ] = {} if len ( np . unique ( reg_factor )) == 1 : col_info [ col_ind ][ 'title' ] = r \"Varying $\\lambda$ ($\\mu = {:.0E} $)\" . format ( int ( reg_factor [ 0 ])) else : col_info [ col_ind ][ 'title' ] = f \"Varying $\\lambda$\" col_info [ col_ind ][ 'x_label' ] = r \"$\\lambda$\" col_info [ col_ind ][ 'x' ] = reg_constant col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( reg_constant )), int ( 1.1 * np . max ( reg_constant ))] if n_matches is not None : col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( list ( reg_constant ) + [ n_matches ])), int ( 1.1 * np . max ( list ( reg_constant ) + [ n_matches ]))] col_info [ col_ind ][ 'log_x' ] = np . ptp ( col_info [ col_ind ][ 'x_lims' ]) > 100 col_info [ col_ind ][ 'n_matches' ] = n_matches col_ind += 1 if len ( np . unique ( reg_factor )) > 1 : col_info [ col_ind ] = {} if len ( np . unique ( reg_constant )) == 1 : col_info [ col_ind ][ 'title' ] = rf \"Varying $\\mu$ ($\\lambda = { int ( reg_constant [ 0 ]) } $)\" else : col_info [ col_ind ][ 'title' ] = f \"Varying $\\mu$\" col_info [ col_ind ][ 'x_label' ] = r \"$\\mu$\" col_info [ col_ind ][ 'x' ] = reg_factor col_info [ col_ind ][ 'x_lims' ] = [ int ( 0.9 * np . min ( reg_factor )), int ( 1.1 * np . max ( reg_factor ))] col_info [ col_ind ][ 'log_x' ] = np . ptp ( col_info [ col_ind ][ 'x_lims' ]) > 100 col_info [ col_ind ][ 'n_matches' ] = None if n_reg != len ( reg_factor ): raise ValueError ( f \"reg_constant and reg_factor need to be same size but they \" f \"have { n_reg } and { len ( reg_factor ) } values respectively\" ) if n_reg != len ( transforms_plot ): raise ValueError ( f \"reg_constant and transforms_plot need to be same size but they \" f \"have { n_reg } and { len ( transforms_plot ) } values respectively\" ) if len ( col_info ) == 0 : raise ValueError ( \"Not enough data to plot\" ) y0 = [ np . linalg . norm ( transforms_plot [ i ][: 3 , :] - reg_transform [: 3 , :]) for i in range ( n_reg )] y1 = [ np . linalg . norm ( transforms_plot [ i ][ 3 ] - reg_transform [ 3 ]) for i in range ( n_reg )] if transform_no_reg is not None : y0_no_reg = np . linalg . norm ( transform_no_reg [: 3 , :] - reg_transform [: 3 ,:]) y1_no_reg = np . linalg . norm ( transform_no_reg [ 3 ] - reg_transform [ 3 ]) y0_lim = [ np . min ( y0 + [ y0_no_reg ]) - 0.0002 , np . max ( y0 + [ y0_no_reg ]) + 0.0002 ] y1_lim = [ np . min ( y1 + [ y1_no_reg ]) - 0.2 , np . max ( y1 + [ y1_no_reg ]) + 0.2 ] else : y0_lim = [ np . min ( y0 ) - 0.0002 , np . max ( y0 ) + 0.0002 ] y1_lim = [ np . min ( y1 ) - 0.2 , np . max ( y1 ) + 0.2 ] y0_lim = np . clip ( y0_lim , 0 , np . inf ) y1_lim = np . clip ( y1_lim , 0 , np . inf ) colors = distinctipy . get_colors ( n_reg , [( 0 , 0 , 0 ), ( 1 , 1 , 1 )]) fig , ax = plt . subplots ( 2 , len ( col_info ), figsize = ( 15 , 7 )) if len ( col_info ) == 1 : ax = ax [:, np . newaxis ] for i in range ( len ( col_info )): if transform_no_reg is not None : ax [ 0 , i ] . plot ( col_info [ i ][ 'x_lims' ], [ y0_no_reg , y0_no_reg ], 'w:' ) ax [ 0 , i ] . text ( np . mean ( col_info [ i ][ 'x_lims' ]), y0_no_reg , r \"$\\lambda = 0$\" , va = 'bottom' , ha = \"center\" , color = 'w' ) ax [ 1 , i ] . plot ( col_info [ i ][ 'x_lims' ], [ y1_no_reg , y1_no_reg ], 'w:' ) ax [ 1 , i ] . text ( np . mean ( col_info [ i ][ 'x_lims' ]), y1_no_reg , r \"$\\lambda = 0$\" , va = 'bottom' , ha = \"center\" , color = 'w' ) if col_info [ i ][ 'n_matches' ] is not None : ax [ 0 , i ] . plot ([ n_matches , n_matches ], y0_lim , 'w:' ) ax [ 0 , i ] . text ( n_matches , np . percentile ( y0_lim , 10 ), r \"$n_ {matches} $\" , va = 'bottom' , ha = \"right\" , color = 'w' , rotation = 90 ) ax [ 1 , i ] . plot ([ n_matches , n_matches ], y1_lim , 'w:' ) ax [ 1 , i ] . text ( n_matches , np . percentile ( y1_lim , 10 ), r \"$n_ {matches} $\" , va = 'bottom' , ha = \"right\" , color = 'w' , rotation = 90 ) ax [ 0 , i ] . scatter ( col_info [ i ][ 'x' ], y0 , color = colors ) ax [ 1 , i ] . scatter ( col_info [ i ][ 'x' ], y1 , color = colors ) ax [ 0 , i ] . get_shared_x_axes () . join ( ax [ 0 , i ], ax [ 1 , i ]) ax [ 1 , i ] . set_xlabel ( col_info [ i ][ 'x_label' ]) ax [ 0 , i ] . set_title ( col_info [ i ][ 'title' ]) if col_info [ i ][ 'log_x' ]: ax [ 1 , i ] . set_xscale ( 'log' ) ax [ 1 , i ] . set_xlim ( col_info [ i ][ 'x_lims' ]) if i == 1 : ax [ 0 , 0 ] . get_shared_y_axes () . join ( ax [ 0 , 0 ], ax [ 0 , 1 ]) ax [ 1 , 0 ] . get_shared_y_axes () . join ( ax [ 1 , 0 ], ax [ 1 , 1 ]) ax [ 0 , 0 ] . set_ylim ( y0_lim ) ax [ 0 , 0 ] . set_ylabel ( \"$D_ {scale} $\" ) ax [ 1 , 0 ] . set_ylim ( y1_lim ) ax [ 1 , 0 ] . set_ylabel ( \"$D_ {shift} $\" ) fig . suptitle ( \"How varying regularization parameters affects how similar transform found is to target transform\" ) plt . show ()","title":"plot_reg_residual"},{"location":"code/plot/stitch/","text":"Shift view_stitch_search Function to plot results of exhaustive search to find overlap between tile t and its neighbours. Useful for debugging the stitch section of the pipeline. White in the color plot refers to the value of score_thresh for this search. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. required t int Want to look at overlap between tile t and its north/east neighbour. required direction Optional [ str ] Direction of overlap interested in - either 'south' / 'north' or 'west' / 'east' . If None , then will look at both directions. None Source code in coppafish/plot/stitch/shift.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def view_stitch_search ( nb : Notebook , t : int , direction : Optional [ str ] = None ): \"\"\" Function to plot results of exhaustive search to find overlap between tile `t` and its neighbours. Useful for debugging the `stitch` section of the pipeline. White in the color plot refers to the value of `score_thresh` for this search. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. t: Want to look at overlap between tile `t` and its north/east neighbour. direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`. If `None`, then will look at both directions. \"\"\" # NOTE that directions should actually be 'north' and 'east' if direction is None : directions = [ 'south' , 'west' ] elif direction . lower () == 'south' or direction . lower () == 'west' : directions = [ direction . lower ()] elif direction . lower () == 'north' : directions = [ 'south' ] elif direction . lower () == 'east' : directions = [ 'west' ] else : raise ValueError ( f \"direction must be either 'south' or 'west' but { direction } given.\" ) direction_label = { 'south' : 'north' , 'west' : 'east' } # label refers to actual direction config = nb . get_config ()[ 'stitch' ] # determine shifts to search over shifts = get_shifts_to_search ( config , nb . basic_info ) if not nb . basic_info . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 # find shifts between overlapping tiles c = nb . basic_info . ref_channel r = nb . basic_info . ref_round t_neighb = { 'south' : [], 'west' : []} z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy # align to south neighbour followed by west neighbour t_neighb [ 'south' ] = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] t_neighb [ 'west' ] = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] fig = [] for j in directions : if t_neighb [ j ] in nb . basic_info . use_tiles : print ( f 'Finding shift between tiles { t } and { t_neighb [ j ][ 0 ] } ( { direction_label [ j ] } overlap)' ) shift , score , score_thresh , debug_info = \\ compute_shift ( spot_yxz ( nb . find_spots . spot_details , t , r , c ), spot_yxz ( nb . find_spots . spot_details , t_neighb [ j ][ 0 ], r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) title = f 'Overlap between t= { t } and neighbor in { direction_label [ j ] } (t= { t_neighb [ j ][ 0 ] } ). ' \\ f 'YXZ Shift = { shift } .' fig = fig + [ view_shifts ( debug_info [ 'shifts_2d' ], debug_info [ 'scores_2d' ], debug_info [ 'shifts_3d' ], debug_info [ 'scores_3d' ], shift , debug_info [ 'min_score_2d' ], debug_info [ 'shift_2d_initial' ], score_thresh , debug_info [ 'shift_thresh' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], title , False )] if len ( fig ) > 0 : plt . show () else : warnings . warn ( f \"Tile { t } has no overlapping tiles in nb.basic_info.use_tiles.\" ) view_shifts Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts applied to one of them. I.e. you can use this to view the output from coppafish/stitch/shift/compute_shift function. Parameters: Name Type Description Default shifts_2d np . ndarray int [n_shifts_2d x 2] . shifts_2d[i] is the yx shift which achieved scores_2d[i] when considering just yx shift between point clouds. I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then applying a yx shift to these planes. required scores_2d np . ndarray float [n_shifts_2d] . scores_2d[i] is the score corresponding to shifts_2d[i] . It is approximately the number of neighbours between the two point clouds after the shift was applied. required shifts_3d Optional [ np . ndarray ] int [n_shifts_3d x 3] . shifts_3d[i] is the yxz shift which achieved scores_3d[i] when considering the yxz shift between point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels. If None, only 2D image plotted. None scores_3d Optional [ np . ndarray ] float [n_shifts_3d] . scores_3d[i] is the score corresponding to shifts_3d[i] . It is approximately the number of neighbours between the two point clouds after the shift was applied. None best_shift Optional [ np . ndarray ] int [y_shift, x_shift, z_shift] . Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels. Will be plotted as black cross on image if provided. None score_thresh_2d Optional [ float ] Threshold returned by compute_shift function for 2d calculation, if score is above this, it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding to score_thresh_2d in the 2D plot None best_shift_initial Optional [ np . ndarray ] int [y_shift, x_shift] . Best yx shift found by in first search of algorithm. I.e. score_thresh computation based on this. Will show as green x if given. None score_thresh_3d Optional [ float ] Threshold returned by compute_shift function for 3d calculation. If given, a red-white-blue colorbar will be used with white corresponding to score_thresh_3d in the 3D plots. None thresh_shift Optional [ np . ndarray ] int [y_shift, x_shift, z_shift] . yx shift corresponding to score_thresh . Will show as green + in both 2D and 3D plots if given. None thresh_min_dist Optional [ int ] shift_thresh is the shift with the max score in an annulus a distance between thresh_min_dist and thresh_max_dist away from best_shift_initial . Annulus will be shown in green if given. None thresh_max_dist Optional [ int ] shift_thresh is the shift with the max score in an annulus a distance between thresh_min_dist and thresh_max_dist away from best_shift_initial . Annulus will be shown in green if given. None title Optional [ str ] Title to show. None show bool If True , will call plt.show() , else will return fig . True Source code in coppafish/plot/stitch/shift.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def view_shifts ( shifts_2d : np . ndarray , scores_2d : np . ndarray , shifts_3d : Optional [ np . ndarray ] = None , scores_3d : Optional [ np . ndarray ] = None , best_shift : Optional [ np . ndarray ] = None , score_thresh_2d : Optional [ float ] = None , best_shift_initial : Optional [ np . ndarray ] = None , score_thresh_3d : Optional [ float ] = None , thresh_shift : Optional [ np . ndarray ] = None , thresh_min_dist : Optional [ int ] = None , thresh_max_dist : Optional [ int ] = None , title : Optional [ str ] = None , show : bool = True ): \"\"\" Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts applied to one of them. I.e. you can use this to view the output from `coppafish/stitch/shift/compute_shift` function. Args: shifts_2d: `int [n_shifts_2d x 2]`. `shifts_2d[i]` is the yx shift which achieved `scores_2d[i]` when considering just yx shift between point clouds. I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then applying a yx shift to these planes. scores_2d: `float [n_shifts_2d]`. `scores_2d[i]` is the score corresponding to `shifts_2d[i]`. It is approximately the number of neighbours between the two point clouds after the shift was applied. shifts_3d: `int [n_shifts_3d x 3]`. `shifts_3d[i]` is the yxz shift which achieved `scores_3d[i]` when considering the yxz shift between point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels. If None, only 2D image plotted. scores_3d: `float [n_shifts_3d]`. `scores_3d[i]` is the score corresponding to `shifts_3d[i]`. It is approximately the number of neighbours between the two point clouds after the shift was applied. best_shift: `int [y_shift, x_shift, z_shift]`. Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels. Will be plotted as black cross on image if provided. score_thresh_2d: Threshold returned by `compute_shift` function for 2d calculation, if `score` is above this, it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding to `score_thresh_2d` in the 2D plot best_shift_initial: `int [y_shift, x_shift]`. Best yx shift found by in first search of algorithm. I.e. `score_thresh` computation based on this. Will show as green x if given. score_thresh_3d: Threshold returned by `compute_shift` function for 3d calculation. If given, a red-white-blue colorbar will be used with white corresponding to `score_thresh_3d` in the 3D plots. thresh_shift: `int [y_shift, x_shift, z_shift]`. yx shift corresponding to `score_thresh`. Will show as green + in both 2D and 3D plots if given. thresh_min_dist: `shift_thresh` is the shift with the max score in an annulus a distance between `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`. Annulus will be shown in green if given. thresh_max_dist: `shift_thresh` is the shift with the max score in an annulus a distance between `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`. Annulus will be shown in green if given. title: Title to show. show: If `True`, will call `plt.show()`, else will `return fig`. \"\"\" image_2d , extent_2d = get_plot_images_from_shifts ( np . rint ( shifts_2d ) . astype ( int ), scores_2d ) image_2d = interpolate_array ( image_2d , 0 ) # replace 0 with nearest neighbor value fig = plt . figure ( figsize = ( 12 , 8 )) if score_thresh_2d is None : score_thresh_2d = ( image_2d . min () + image_2d . max ()) / 2 cmap_2d = 'virids' else : cmap_2d = 'bwr' v_max = np . max ([ image_2d . max (), 1.2 * score_thresh_2d ]) v_min = image_2d . min () if cmap_2d == 'bwr' : cmap_extent = np . max ([ v_max - score_thresh_2d , score_thresh_2d - v_min ]) # Have equal range above and below score_thresh to not skew colormap v_min = score_thresh_2d - cmap_extent v_max = score_thresh_2d + cmap_extent cmap_norm = matplotlib . colors . TwoSlopeNorm ( vmin = v_min , vcenter = score_thresh_2d , vmax = v_max ) if shifts_3d is not None : images_3d , extent_3d = get_plot_images_from_shifts ( np . rint ( shifts_3d ) . astype ( int ), scores_3d ) images_3d = interpolate_array ( images_3d , 0 ) # replace 0 with nearest neighbor value if score_thresh_3d is None : score_thresh_3d = ( images_3d . min () + images_3d . max ()) / 2 cmap_3d = 'virids' else : cmap_3d = 'bwr' v_max_3d = np . max ([ images_3d . max (), 1.2 * score_thresh_3d ]) v_min_3d = images_3d . min () if cmap_3d == 'bwr' : cmap_extent = np . max ([ v_max_3d - score_thresh_3d , score_thresh_3d - v_min_3d ]) # Have equal range above and below score_thresh to not skew colormap v_min_3d = score_thresh_3d - cmap_extent v_max_3d = score_thresh_3d + cmap_extent cmap_norm_3d = matplotlib . colors . TwoSlopeNorm ( vmin = v_min_3d , vcenter = score_thresh_3d , vmax = v_max_3d ) n_cols = images_3d . shape [ 2 ] if n_cols > 13 : # If loads of z-planes, just show the 13 with the largest score n_cols = 13 max_score_z = images_3d . max ( axis = ( 0 , 1 )) use_z = np . sort ( np . argsort ( max_score_z )[:: - 1 ][: n_cols ]) else : use_z = np . arange ( n_cols ) plot_3d_height = int ( np . ceil ( n_cols / 4 )) plot_2d_height = n_cols - plot_3d_height ax_2d = plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( 0 , 0 ), colspan = n_cols , rowspan = plot_2d_height ) ax_3d = [ plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( plot_2d_height + 1 , i ), rowspan = plot_3d_height ) for i in range ( n_cols )] for i in range ( n_cols ): # share axes for 3D plots ax_3d [ i ] . get_shared_y_axes () . join ( ax_3d [ i ], * ax_3d ) ax_3d [ i ] . get_shared_x_axes () . join ( ax_3d [ i ], * ax_3d ) im_3d = ax_3d [ i ] . imshow ( images_3d [:, :, use_z [ i ]], extent = extent_3d [: 4 ], aspect = 'auto' , cmap = cmap_3d , norm = cmap_norm_3d ) z_plane = int ( np . rint ( extent_3d [ 4 ] + use_z [ i ] + 0.5 )) ax_3d [ i ] . set_title ( f 'Z = { z_plane } ' ) if thresh_shift is not None and z_plane == thresh_shift [ 2 ]: # Indicate threshold shift on correct 3d plot ax_3d [ i ] . plot ( thresh_shift [ 1 ], thresh_shift [ 0 ], '+' , color = 'lime' , label = 'Thresh shift' ) if i > 0 : ax_3d [ i ] . tick_params ( labelbottom = False , labelleft = False ) if best_shift is not None : if z_plane == best_shift [ 2 ]: ax_3d [ i ] . plot ( best_shift [ 1 ], best_shift [ 0 ], 'kx' ) fig . supxlabel ( 'X' ) fig . supylabel ( 'Y' ) ax_3d [ 0 ] . invert_yaxis () cbar_gap = 0.05 cbar_3d_height = plot_3d_height / ( plot_3d_height + plot_2d_height ) cbar_2d_height = plot_2d_height / ( plot_3d_height + plot_2d_height ) cbar_ax = fig . add_axes ([ 0.9 , 0.07 , 0.03 , cbar_3d_height - 2 * cbar_gap ]) # left, bottom, width, height fig . colorbar ( im_3d , cax = cbar_ax ) cbar_ax . set_ylim ( np . clip ( v_min_3d , 0 , v_max_3d ), v_max_3d ) else : n_cols = 1 ax_2d = plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( 0 , 0 )) ax_2d . set_xlabel ( 'X' ) ax_2d . set_ylabel ( 'Y' ) im_2d = ax_2d . imshow ( image_2d , extent = extent_2d , aspect = 'auto' , cmap = cmap_2d , norm = cmap_norm ) if best_shift is not None : ax_2d . plot ( best_shift [ 1 ], best_shift [ 0 ], 'kx' , label = 'Best shift' ) if best_shift_initial is not None and best_shift_initial != best_shift : ax_2d . plot ( best_shift_initial [ 1 ], best_shift_initial [ 0 ], 'x' , color = 'lime' , label = 'Best shift initial' ) if thresh_shift is not None : ax_2d . plot ( thresh_shift [ 1 ], thresh_shift [ 0 ], '+' , color = 'lime' , label = 'Thresh shift' ) if thresh_min_dist is not None and best_shift_initial is not None : ax_2d . add_patch ( plt . Circle (( best_shift_initial [ 1 ], best_shift_initial [ 0 ]), thresh_min_dist , color = 'lime' , fill = False )) if thresh_max_dist is not None and best_shift_initial is not None : ax_2d . add_patch ( plt . Circle (( best_shift_initial [ 1 ], best_shift_initial [ 0 ]), thresh_max_dist , color = 'lime' , fill = False )) ax_2d . invert_yaxis () if title is None : title = 'Approx number of neighbours found for all shifts' ax_2d . set_title ( title ) ax_2d . legend ( facecolor = 'b' ) fig . subplots_adjust ( left = 0.07 , right = 0.85 , bottom = 0.07 , top = 0.95 ) if shifts_3d is None : cbar_ax = fig . add_axes ([ 0.9 , 0.07 , 0.03 , 0.9 ]) # left, bottom, width, height else : cbar_ax = fig . add_axes ([ 0.9 , 0.07 + cbar_3d_height , 0.03 , cbar_2d_height - 2.5 * cbar_gap ]) fig . colorbar ( im_2d , cax = cbar_ax ) cbar_ax . set_ylim ( np . clip ( v_min , 0 , v_max ), v_max ) if show : plt . show () else : return fig Point Clouds view_stitch_overlap This plots point clouds of neighbouring tiles with: No overlap Initial guess at shift using config['stitch']['expected_overlap'] Overlap determined in stitch stage of the pipeline (using nb.stitch.south_shifts or nb.stitch.west_shifts ) Their final global coordinate system positions (using nb.stitch.tile_origin ) Parameters: Name Type Description Default nb Notebook Notebook containing at least stitch page. required t int Want to look at overlap between tile t and its north or east neighbour. required direction str Direction of overlap interested in - either 'south' / 'north' or 'west' / 'east' . 'south' Source code in coppafish/plot/stitch/point_clouds.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def view_stitch_overlap ( nb : Notebook , t : int , direction : str = 'south' ): \"\"\" This plots point clouds of neighbouring tiles with: * No overlap * Initial guess at shift using `config['stitch']['expected_overlap']` * Overlap determined in stitch stage of the pipeline (using `nb.stitch.south_shifts` or `nb.stitch.west_shifts`) * Their final global coordinate system positions (using `nb.stitch.tile_origin`) Args: nb: *Notebook* containing at least `stitch` page. t: Want to look at overlap between tile `t` and its north or east neighbour. direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`. \"\"\" # NOTE that directions should actually be 'north' and 'east' if direction . lower () == 'south' or direction . lower () == 'west' : direction = direction . lower () elif direction . lower () == 'north' : direction = 'south' elif direction . lower () == 'east' : direction = 'west' else : raise ValueError ( f \"direction must be either 'south' or 'west' but { direction } given.\" ) direction_label = { 'south' : 'north' , 'west' : 'east' } # label refers to actual direction if direction == 'south' : t_neighb = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] if t_neighb not in nb . basic_info . use_tiles : warnings . warn ( f \"Tile { t } has no overlapping tiles in the south direction so changing to west.\" ) direction = 'west' else : no_overlap_shift = np . array ([ - nb . basic_info . tile_sz , 0 , 0 ]) # assuming no overlap between tiles found_shift = nb . stitch . south_shifts [ np . where ( nb . stitch . south_pairs [:, 0 ] == t )[ 0 ]][ 0 ] if direction == 'west' : t_neighb = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] if t_neighb not in nb . basic_info . use_tiles : raise ValueError ( f \"Tile { t } has no overlapping tiles in the west direction.\" ) no_overlap_shift = np . array ([ 0 , - nb . basic_info . tile_sz , 0 ]) # assuming no overlap between tiles found_shift = nb . stitch . west_shifts [ np . where ( nb . stitch . west_pairs [:, 0 ] == t )[ 0 ]][ 0 ] config = nb . get_config ()[ 'stitch' ] t_neighb = t_neighb [ 0 ] r = nb . basic_info . ref_round c = nb . basic_info . ref_channel point_clouds = [] # add global coordinates of neighbour tile as point cloud that is always present. point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t_neighb , r , c ) + nb . stitch . tile_origin [ t_neighb ]] local_yxz_t = spot_yxz ( nb . find_spots . spot_details , t , r , c ) # Add point cloud for tile t assuming no overlap point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + no_overlap_shift ] # Add point cloud assuming expected overlap initial_shift = ( 1 - config [ 'expected_overlap' ]) * no_overlap_shift point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + initial_shift ] # Add point cloud for tile t with found shift point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + found_shift ] # Add point cloud for tile t in global coordinate system point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t ]] neighb_dist_thresh = config [ 'neighb_dist_thresh' ] z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy pc_labels = [ f 'Tile { t_neighb } ' , f 'Tile { t } - No overlap' , f \"Tile { t } - { int ( config [ 'expected_overlap' ] * 100 ) } % overlap\" , f 'Tile { t } - Shift' , f 'Tile { t } - Final' ] view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Overlap between tile { t } and tile { t_neighb } in the { direction_label [ direction ] } ' ) plt . show () view_stitch This plots all the reference spots found ( ref_round / ref_channel ) in the global coordinate system created in the stitch stage of the pipeline. It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre they are closest to). These will be removed in the get_reference_spots step of the pipeline so we don't double count the same spot. Parameters: Name Type Description Default nb Notebook Notebook containing at least stitch page. required Source code in coppafish/plot/stitch/point_clouds.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def view_stitch ( nb : Notebook ): \"\"\" This plots all the reference spots found (`ref_round`/`ref_channel`) in the global coordinate system created in the `stitch` stage of the pipeline. It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre they are closest to). These will be removed in the `get_reference_spots` step of the pipeline so we don't double count the same spot. Args: nb: *Notebook* containing at least `stitch` page. \"\"\" is_ref = np . all (( nb . find_spots . spot_details [:, 1 ] == nb . basic_info . ref_round , nb . find_spots . spot_details [:, 2 ] == nb . basic_info . ref_channel ), axis = 0 ) local_yxz = nb . find_spots . spot_details [ is_ref , - 3 :] tile = nb . find_spots . spot_details [ is_ref , 0 ] local_yxz = local_yxz [ np . isin ( tile , nb . basic_info . use_tiles )] tile = tile [ np . isin ( tile , nb . basic_info . use_tiles )] # find duplicate spots as those detected on a tile which is not tile centre they are closest to tile_origin = nb . stitch . tile_origin not_duplicate = get_non_duplicate ( tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , local_yxz , tile ) global_yxz = local_yxz + nb . stitch . tile_origin [ tile ] global_yxz [:, 2 ] = np . rint ( global_yxz [:, 2 ]) # make z coordinate an integer config = nb . get_config ()[ 'stitch' ] neighb_dist_thresh = config [ 'neighb_dist_thresh' ] z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy point_clouds = [ global_yxz [ not_duplicate ], global_yxz [ np . invert ( not_duplicate )]] pc_labels = [ 'Not Duplicate' , 'Duplicate' ] if nb . has_page ( 'ref_spots' ): # Add point cloud indicating those spots that were not saved in ref_spots page # because they were shifted outside the tile bounds on at least one round/channel # and thus spot_color could not be found. local_yxz = local_yxz [ not_duplicate ] # Want to find those which were not duplicates but still removed tile = tile [ not_duplicate ] local_yxz_saved = nb . ref_spots . local_yxz tile_saved = nb . ref_spots . tile local_yxz_saved = local_yxz_saved [ np . isin ( tile_saved , nb . basic_info . use_tiles )] tile_saved = tile_saved [ np . isin ( tile_saved , nb . basic_info . use_tiles )] global_yxz_ns = np . zeros (( 0 , 3 )) # not saved in ref_spots spots for t in nb . basic_info . use_tiles : missing_ind = - 100 local_yxz_t = local_yxz [ tile == t ] removed_ind = np . where ( numpy_indexed . indices ( local_yxz_saved [ tile_saved == t ], local_yxz_t , missing = missing_ind ) == missing_ind )[ 0 ] global_yxz_ns = np . append ( global_yxz_ns , local_yxz_t [ removed_ind ] + nb . stitch . tile_origin [ t ], axis = 0 ) global_yxz_ns [:, 2 ] = np . rint ( global_yxz_ns [:, 2 ]) point_clouds += [ global_yxz_ns ] pc_labels += [ \"No Spot Color\" ] # Sometimes can be empty point cloud, so remove these use_pc = [ len ( pc ) > 0 for pc in point_clouds ] pc_labels = [ pc_labels [ i ] for i in range ( len ( use_pc )) if use_pc [ i ]] point_clouds = [ point_clouds [ i ] for i in range ( len ( use_pc )) if use_pc [ i ]] vpc = view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , \"Reference Spots in the Global Coordinate System\" ) tile_sz = nb . basic_info . tile_sz for t in nb . basic_info . use_tiles : rect = matplotlib . patches . Rectangle (( tile_origin [ t , 1 ], tile_origin [ t , 0 ]), tile_sz , tile_sz , linewidth = 1 , edgecolor = 'w' , facecolor = 'none' , linestyle = ':' ) vpc . ax . add_patch ( rect ) vpc . ax . text ( tile_origin [ t , 1 ] + 20 , tile_origin [ t , 0 ] + 20 , f \"Tile { t } \" , size = 6 , color = 'w' , ha = 'left' , weight = 'light' ) plt . show () view_point_clouds Plots two point clouds. point_clouds[0] always plotted but you can change the second point cloud using radio buttons. Parameters: Name Type Description Default point_clouds List List of point clouds, each of which is yxz coordinates float [n_points x 3] . point_clouds[0] is always plotted. YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels. Radio buttons used to select other point_cloud plotted. required pc_labels List List of labels to appear in legend/radio-buttons for each point cloud. Must provide one for each point_cloud . required neighb_dist_thresh float If distance between neighbours is less than this, a white line will connect them. 5 z_scale float pixel_size_z / pixel_size_y i.e. used to convert z coordinates from z-pixels to yx pixels. 1 super_title Optional [ str ] Optional title for plot None Source code in coppafish/plot/stitch/point_clouds.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , point_clouds : List , pc_labels : List , neighb_dist_thresh : float = 5 , z_scale : float = 1 , super_title : Optional [ str ] = None ): \"\"\" Plots two point clouds. `point_clouds[0]` always plotted but you can change the second point cloud using radio buttons. Args: point_clouds: List of point clouds, each of which is yxz coordinates `float [n_points x 3]`. `point_clouds[0]` is always plotted. YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels. Radio buttons used to select other point_cloud plotted. pc_labels: List of labels to appear in legend/radio-buttons for each point cloud. Must provide one for each `point_cloud`. neighb_dist_thresh: If distance between neighbours is less than this, a white line will connect them. z_scale: `pixel_size_z / pixel_size_y` i.e. used to convert z coordinates from z-pixels to yx pixels. super_title: Optional title for plot \"\"\" n_point_clouds = len ( point_clouds ) if len ( point_clouds ) != len ( pc_labels ): raise ValueError ( f 'There are { n_point_clouds } point clouds but { len ( pc_labels ) } labels' ) self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 8 )) subplots_adjust = [ 0.07 , 0.775 , 0.095 , 0.89 ] self . fig . subplots_adjust ( left = subplots_adjust [ 0 ], right = subplots_adjust [ 1 ], bottom = subplots_adjust [ 2 ], top = subplots_adjust [ 3 ]) # Find neighbours between all point clouds and the first. self . neighb = [ None ] n_matches_str = 'Number Of Matches' for i in range ( 1 , n_point_clouds ): tree = KDTree ( point_clouds [ i ] * [ 1 , 1 , z_scale ]) dist , neighb = tree . query ( point_clouds [ 0 ] * [ 1 , 1 , z_scale ], distance_upper_bound = neighb_dist_thresh * 3 ) neighb [ dist > neighb_dist_thresh ] = - 1 # set too large distance neighb_ind to -1. n_matches_str = n_matches_str + f ' \\n - { pc_labels [ i ] } : { np . sum ( neighb >= 0 ) } ' self . neighb = self . neighb + [ neighb ] # Add text box to indicate number of matches between first point cloud and each of the others. n_matches_ax = self . fig . add_axes ([ 0.8 , subplots_adjust [ 3 ] - 0.75 , 0.15 , 0.2 ]) plt . axis ( 'off' ) n_matches_ax . text ( 0.05 , 0.95 , n_matches_str ) # round z to closest z-plane for plotting for i in range ( n_point_clouds ): point_clouds [ i ][:, 2 ] = np . rint ( point_clouds [ i ][:, 2 ]) # get yxz axis limits taking account all point clouds pc_min_lims = np . zeros (( n_point_clouds , 3 )) pc_max_lims = np . zeros_like ( pc_min_lims ) for i in range ( n_point_clouds ): pc_min_lims [ i ] = np . min ( point_clouds [ i ], axis = 0 ) pc_max_lims [ i ] = np . max ( point_clouds [ i ], axis = 0 ) self . z_planes = np . arange ( int ( np . min ( pc_min_lims [:, 2 ])), int ( np . max ( pc_max_lims [:, 2 ]) + 1 )) self . nz = len ( self . z_planes ) self . z_ind = 0 self . z = self . z_planes [ self . z_ind ] self . z_thick = 0 self . active_pc = [ 0 , 1 ] self . in_z = [ np . array ([ val [:, 2 ] >= self . z - self . z_thick , val [:, 2 ] <= self . z + self . z_thick ]) . all ( axis = 0 ) for val in point_clouds ] self . point_clouds = point_clouds self . pc_labels = np . array ( pc_labels ) self . pc_shapes = [ 'rx' , 'bo' ] alpha = [ 1 , 0.7 ] self . pc_plots = [ self . ax . plot ( point_clouds [ self . active_pc [ i ]][ self . in_z [ i ], 1 ], point_clouds [ self . active_pc [ i ]][ self . in_z [ i ], 0 ], self . pc_shapes [ i ], label = self . pc_labels [ i ], alpha = alpha [ i ])[ 0 ] for i in range ( 2 )] self . neighb_yx = None self . neighb_plot = None self . update_neighb_lines () self . ax . legend ( loc = 'upper right' ) self . ax . set_ylabel ( 'Y' ) self . ax . set_xlabel ( 'X' ) self . ax . set_ylim ( np . min ( pc_min_lims [:, 0 ]), np . max ( pc_max_lims [:, 0 ])) self . ax . set_xlim ( np . min ( pc_min_lims [:, 1 ]), np . max ( pc_max_lims [:, 1 ])) if self . nz > 1 : # If 3D, add text box to change number of z-planes collapsed onto single plane # and add scrolling to change z-plane self . ax . set_title ( f 'Z = { int ( self . z ) } ' , size = 10 ) self . fig . canvas . mpl_connect ( 'scroll_event' , self . z_scroll ) text_ax = self . fig . add_axes ([ 0.8 , 0.095 , 0.15 , 0.04 ]) else : # For some reason in 2D, still need the text box otherwise buttons don't do work # But shift it off-screen and make small text_ax = self . fig . add_axes ([ 40 , 40 , 0.00001 , 0.00001 ]) self . text_box = TextBox ( text_ax , 'Z-Thick' , self . z_thick , color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis if self . nz == 1 : label . set_position ([ 40 , 40 ]) # shift label off-screen in 2D else : label . set_position ([ 0.5 , 2 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . text_update ) if n_point_clouds >= 3 : # If 3 or more point clouds, add radio button to change the second point cloud shown. buttons_ax = self . fig . add_axes ([ 0.8 , subplots_adjust [ 3 ] - 0.2 , 0.15 , 0.2 ]) plt . axis ( 'off' ) self . buttons = RadioButtons ( buttons_ax , self . pc_labels [ 1 :], 0 , activecolor = 'w' ) for i in range ( n_point_clouds - 1 ): self . buttons . circles [ i ] . set_color ( 'w' ) self . buttons . circles [ i ] . set_color ( 'w' ) self . buttons . set_active ( 0 ) self . buttons . on_clicked ( self . button_update ) if super_title is not None : plt . suptitle ( super_title , x = ( 0.07 + 0.775 ) / 2 ) Diagnostics view_stitch_shift_info For all north/south and east/west shifts computed in the stitch section of the pipeline, this plots the values of the shifts found and the score compared to the score_thresh . For each direction, there will be 3 plots: y shift vs x shift for all pairs of neighbouring tiles z shift vs x shift for all pairs of neighbouring tiles score vs score_thresh for all pairs of neighbouring tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile, the shift was applied to, to take it to its north or east neighbour i.e. nb.stitch.south_pairs[:, 0] or nb.stitch.west_pairs[:, 0] . The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the stitch page. required outlier bool If True , will plot nb.stitch.south_shift_outlier instead of nb.stitch.south_shift . In this case, only tiles for which the two are different are plotted for each round. False Source code in coppafish/plot/stitch/diagnostics.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def view_stitch_shift_info ( nb : Notebook , outlier : bool = False ): \"\"\" For all north/south and east/west shifts computed in the `stitch` section of the pipeline, this plots the values of the shifts found and the `score` compared to the `score_thresh`. For each direction, there will be 3 plots: * y shift vs x shift for all pairs of neighbouring tiles * z shift vs x shift for all pairs of neighbouring tiles * `score` vs `score_thresh` for all pairs of neighbouring tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile, the shift was applied to, to take it to its north or east neighbour i.e. `nb.stitch.south_pairs[:, 0]` or `nb.stitch.west_pairs[:, 0]`. The number will be blue if `score > score_thresh` and red otherwise. Args: nb: Notebook containing at least the `stitch` page. outlier: If `True`, will plot `nb.stitch.south_shift_outlier` instead of `nb.stitch.south_shift`. In this case, only tiles for which the two are different are plotted for each round. \"\"\" if nb . basic_info . is_3d : ndim = 3 else : ndim = 2 shift_info = {} if len ( nb . stitch . south_shifts ) > 0 : shift_info [ 'South' ] = {} shift_info [ 'South' ][ 'tile' ] = nb . stitch . south_pairs [:, 0 ] shift_info [ 'South' ][ 'score_thresh' ] = nb . stitch . south_score_thresh if outlier : shift_info [ 'South' ][ 'shift' ] = nb . stitch . south_outlier_shifts [:, : ndim ] shift_info [ 'South' ][ 'score' ] = nb . stitch . south_outlier_score else : shift_info [ 'South' ][ 'shift' ] = nb . stitch . south_shifts [:, : ndim ] shift_info [ 'South' ][ 'score' ] = nb . stitch . south_score if len ( nb . stitch . west_shifts ) > 0 : shift_info [ 'West' ] = {} shift_info [ 'West' ][ 'tile' ] = nb . stitch . west_pairs [:, 0 ] shift_info [ 'West' ][ 'score_thresh' ] = nb . stitch . west_score_thresh if outlier : shift_info [ 'West' ][ 'shift' ] = nb . stitch . west_outlier_shifts [:, : ndim ] shift_info [ 'West' ][ 'score' ] = nb . stitch . west_outlier_score else : shift_info [ 'West' ][ 'shift' ] = nb . stitch . west_shifts [:, : ndim ] shift_info [ 'West' ][ 'score' ] = nb . stitch . west_score if outlier : title_start = \"Outlier \" else : title_start = \"\" shift_info_plot ( shift_info , f \" { title_start } Shifts found in stitch part of pipeline between each tile and the \" f \"neighbouring tile in the direction specified\" ) shift_info_plot If shift_info contains \\(n\\) keys, this will produce an \\(n\\) column x 3 row grid of subplots. For each key in shift_info dictionary, there are 3 plots: y shift vs x shift z shift vs x shift score vs score_thresh or n_matches vs error In each case, the markers in the plots are numbers. These numbers are given by shift_info[key][tile] . The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default shift_info dict Dictionary containing \\(n\\) dictionaries. Each of these dictionaries contains (either ( score and score_thresh ) or ( n_matches , n_matches_thresh and error )): shift - float [n_tiles x 3] . \\(yxz\\) shifts for each tile. tile - int [n_tiles] . Indicates tile each shift was found for. score - float [n_tiles] . Indicates score found for each shift (approx number of matches between point clouds). score_thresh - float [n_tiles] . If score<score_thresh , it will be shown in red. n_matches - int [n_tiles] . Indicates score found for each shift (approx number of matches between point clouds). n_matches_thresh - int [n_tiles] . If n_matches<n_matches_thresh , it will be shown in red. error - float [n_tiles] . Average distance between neighbours. x_lim - float [n_plots x 2] . Can optionally specify number axis limits for each plot. y_lim - float [n_plots x 2] . Can optionally specify number axis limits for each plot. required title Optional [ str ] Overall title for the plot. None score_plot_thresh int Only shifts with score (or n_matches ) > score_plot_thresh are shown. 0 fig Optional [ plt . Figure ] Can provide previous figure to plot on. None ax Optional [ np . ndarray ] Can provide array of plt.Axes to plot on. None return_ax bool If True , ax will be returned and plt.show() will not be run. False Source code in coppafish/plot/stitch/diagnostics.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def shift_info_plot ( shift_info : dict , title : Optional [ str ] = None , score_plot_thresh : int = 0 , fig : Optional [ plt . Figure ] = None , ax : Optional [ np . ndarray ] = None , return_ax : bool = False ): \"\"\" If `shift_info` contains $n$ keys, this will produce an $n$ column x 3 row grid of subplots. For each key in `shift_info` dictionary, there are 3 plots: * y shift vs x shift * z shift vs x shift * `score` vs `score_thresh` or `n_matches` vs `error` In each case, the markers in the plots are numbers. These numbers are given by `shift_info[key][tile]`. The number will be blue if `score > score_thresh` and red otherwise. Args: shift_info: Dictionary containing $n$ dictionaries. Each of these dictionaries contains (either (`score` and `score_thresh`) or (`n_matches`, `n_matches_thresh` and `error`)): * shift - `float [n_tiles x 3]`. $yxz$ shifts for each tile. * tile - `int [n_tiles]`. Indicates tile each shift was found for. * score - `float [n_tiles]`. Indicates `score` found for each shift (approx number of matches between point clouds). * score_thresh - `float [n_tiles]`. If `score<score_thresh`, it will be shown in red. * n_matches - `int [n_tiles]`. Indicates `score` found for each shift (approx number of matches between point clouds). * n_matches_thresh - `int [n_tiles]`. If `n_matches<n_matches_thresh`, it will be shown in red. * error - `float [n_tiles]`. Average distance between neighbours. * x_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot. * y_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot. title: Overall title for the plot. score_plot_thresh: Only shifts with `score` (or `n_matches`) > `score_plot_thresh` are shown. fig: Can provide previous figure to plot on. ax: Can provide array of plt.Axes to plot on. return_ax: If `True`, ax will be returned and `plt.show()` will not be run. \"\"\" n_cols = len ( shift_info ) col_titles = list ( shift_info . keys ()) n_rows = len ( shift_info [ col_titles [ 0 ]][ 'shift' ][ 0 ]) # 2 if 2D shift, 3 if 3D. if fig is None : fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 15 , 7 )) fig . subplots_adjust ( hspace = 0.4 , bottom = 0.08 , left = 0.06 , right = 0.97 , top = 0.9 ) if n_cols == 1 and len ( ax . shape ) == 1 : ax = ax [:, np . newaxis ] for i in range ( n_cols ): shift_info_i = shift_info [ col_titles [ i ]] # good tiles are blue, bad tiles are red n_tiles = len ( shift_info [ col_titles [ i ]][ 'tile' ]) tile_color = np . full ( n_tiles , 'b' ) if 'score' in shift_info_i : tile_color [( shift_info_i [ 'score' ] < shift_info_i [ 'score_thresh' ]) . flatten ()] = 'r' skip_tile = shift_info_i [ 'score' ] <= score_plot_thresh # don't plot if score = 0 elif 'n_matches' in shift_info_i : tile_color [( shift_info_i [ 'n_matches' ] < shift_info_i [ 'n_matches_thresh' ]) . flatten ()] = 'r' skip_tile = shift_info_i [ 'n_matches' ] <= score_plot_thresh # don't plot if n_matches = 0 else : raise ValueError ( f \"shift_info must contain either a 'score' or 'n_matches' key\" ) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ 0 , i ] . text ( shift_info_i [ 'shift' ][ t , 1 ], shift_info_i [ 'shift' ][ t , 0 ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if 'x_lim' in shift_info_i : ax [ 0 , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ 0 ]) else : ax [ 0 , i ] . set_xlim ([ np . min ( shift_info_i [ 'shift' ][:, 1 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 1 ]) + 3 ]) if 'y_lim' in shift_info_i : ax [ 0 , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ 0 ]) else : ax [ 0 , i ] . set_ylim ([ np . min ( shift_info_i [ 'shift' ][:, 0 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 0 ]) + 3 ]) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ 0 , i ] . set_xlabel ( 'X Shift' ) if i == 0 : ax [ 0 , i ] . set_ylabel ( 'Y Shift' ) ax [ 0 , i ] . set_title ( col_titles [ i ]) row_ind = 1 if n_rows == 3 : for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'shift' ][ t , 1 ], shift_info_i [ 'shift' ][ t , 2 ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if 'x_lim' in shift_info_i : ax [ row_ind , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_xlim ([ np . min ( shift_info_i [ 'shift' ][:, 1 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 1 ]) + 3 ]) if 'y_lim' in shift_info_i : ax [ row_ind , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_ylim ([ np . min ( shift_info_i [ 'shift' ][:, 2 ]) - 1 , np . max ( shift_info_i [ 'shift' ][:, 2 ]) + 1 ]) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'X Shift' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( 'Z Shift' ) row_ind += 1 if 'score' in shift_info_i : # Plot line so that all with score > score_thresh are above it if 'x_lim' in shift_info_i and 'y_lim' in shift_info_i : score_min = np . min ( shift_info_i [ 'y_lim' ][ row_ind , 0 ], shift_info_i [ 'x_lim' ][ row_ind , 0 ]) - 5 score_max = np . max ( shift_info_i [ 'y_lim' ][ row_ind , 1 ], shift_info_i [ 'x_lim' ][ row_ind , 1 ]) + 5 else : score_min = np . min ( np . vstack ([ shift_info_i [ 'score_thresh' ], shift_info_i [ 'score' ]])) - 5 score_max = np . max ( np . vstack ([ shift_info_i [ 'score_thresh' ], shift_info_i [ 'score' ]])) + 5 ax [ row_ind , i ] . set_xlim ([ score_min , score_max ]) ax [ row_ind , i ] . set_ylim ([ score_min , score_max ]) ax [ row_ind , i ] . plot ([ score_min , score_max ], [ score_min , score_max ], 'lime' , linestyle = ':' , linewidth = 2 , alpha = 0.5 ) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'score_thresh' ][ t ], shift_info_i [ 'score' ][ t ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'Score Threshold' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( 'Score' ) elif 'n_matches' in shift_info_i : if 'x_lim' in shift_info_i : ax [ row_ind , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_xlim ([ np . min ( shift_info_i [ 'error' ]) - 0.1 , np . max ( shift_info_i [ 'error' ]) + 0.1 ]) if 'y_lim' in shift_info_i : ax [ row_ind , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_ylim ([ np . min ( shift_info_i [ 'n_matches' ]) - 100 , np . max ( shift_info_i [ 'n_matches' ]) + 100 ]) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'error' ][ t ], shift_info_i [ 'n_matches' ][ t ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'Error' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( r '$n_ {matches} $' ) if title is not None : plt . suptitle ( title ) if return_ax : return ax else : plt . show ()","title":"Stitch"},{"location":"code/plot/stitch/#shift","text":"","title":"Shift"},{"location":"code/plot/stitch/#view_stitch_search","text":"Function to plot results of exhaustive search to find overlap between tile t and its neighbours. Useful for debugging the stitch section of the pipeline. White in the color plot refers to the value of score_thresh for this search. Parameters: Name Type Description Default nb Notebook Notebook containing results of the experiment. Must contain find_spots page. required t int Want to look at overlap between tile t and its north/east neighbour. required direction Optional [ str ] Direction of overlap interested in - either 'south' / 'north' or 'west' / 'east' . If None , then will look at both directions. None Source code in coppafish/plot/stitch/shift.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def view_stitch_search ( nb : Notebook , t : int , direction : Optional [ str ] = None ): \"\"\" Function to plot results of exhaustive search to find overlap between tile `t` and its neighbours. Useful for debugging the `stitch` section of the pipeline. White in the color plot refers to the value of `score_thresh` for this search. Args: nb: Notebook containing results of the experiment. Must contain `find_spots` page. t: Want to look at overlap between tile `t` and its north/east neighbour. direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`. If `None`, then will look at both directions. \"\"\" # NOTE that directions should actually be 'north' and 'east' if direction is None : directions = [ 'south' , 'west' ] elif direction . lower () == 'south' or direction . lower () == 'west' : directions = [ direction . lower ()] elif direction . lower () == 'north' : directions = [ 'south' ] elif direction . lower () == 'east' : directions = [ 'west' ] else : raise ValueError ( f \"direction must be either 'south' or 'west' but { direction } given.\" ) direction_label = { 'south' : 'north' , 'west' : 'east' } # label refers to actual direction config = nb . get_config ()[ 'stitch' ] # determine shifts to search over shifts = get_shifts_to_search ( config , nb . basic_info ) if not nb . basic_info . is_3d : config [ 'nz_collapse' ] = None config [ 'shift_widen' ][ 2 ] = 0 # so don't look for shifts in z direction config [ 'shift_max_range' ][ 2 ] = 0 # find shifts between overlapping tiles c = nb . basic_info . ref_channel r = nb . basic_info . ref_round t_neighb = { 'south' : [], 'west' : []} z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy # align to south neighbour followed by west neighbour t_neighb [ 'south' ] = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] t_neighb [ 'west' ] = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] fig = [] for j in directions : if t_neighb [ j ] in nb . basic_info . use_tiles : print ( f 'Finding shift between tiles { t } and { t_neighb [ j ][ 0 ] } ( { direction_label [ j ] } overlap)' ) shift , score , score_thresh , debug_info = \\ compute_shift ( spot_yxz ( nb . find_spots . spot_details , t , r , c ), spot_yxz ( nb . find_spots . spot_details , t_neighb [ j ][ 0 ], r , c ), config [ 'shift_score_thresh' ], config [ 'shift_score_thresh_multiplier' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], config [ 'neighb_dist_thresh' ], shifts [ j ][ 'y' ], shifts [ j ][ 'x' ], shifts [ j ][ 'z' ], config [ 'shift_widen' ], config [ 'shift_max_range' ], z_scale , config [ 'nz_collapse' ], config [ 'shift_step' ][ 2 ]) title = f 'Overlap between t= { t } and neighbor in { direction_label [ j ] } (t= { t_neighb [ j ][ 0 ] } ). ' \\ f 'YXZ Shift = { shift } .' fig = fig + [ view_shifts ( debug_info [ 'shifts_2d' ], debug_info [ 'scores_2d' ], debug_info [ 'shifts_3d' ], debug_info [ 'scores_3d' ], shift , debug_info [ 'min_score_2d' ], debug_info [ 'shift_2d_initial' ], score_thresh , debug_info [ 'shift_thresh' ], config [ 'shift_score_thresh_min_dist' ], config [ 'shift_score_thresh_max_dist' ], title , False )] if len ( fig ) > 0 : plt . show () else : warnings . warn ( f \"Tile { t } has no overlapping tiles in nb.basic_info.use_tiles.\" )","title":"view_stitch_search"},{"location":"code/plot/stitch/#view_shifts","text":"Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts applied to one of them. I.e. you can use this to view the output from coppafish/stitch/shift/compute_shift function. Parameters: Name Type Description Default shifts_2d np . ndarray int [n_shifts_2d x 2] . shifts_2d[i] is the yx shift which achieved scores_2d[i] when considering just yx shift between point clouds. I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then applying a yx shift to these planes. required scores_2d np . ndarray float [n_shifts_2d] . scores_2d[i] is the score corresponding to shifts_2d[i] . It is approximately the number of neighbours between the two point clouds after the shift was applied. required shifts_3d Optional [ np . ndarray ] int [n_shifts_3d x 3] . shifts_3d[i] is the yxz shift which achieved scores_3d[i] when considering the yxz shift between point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels. If None, only 2D image plotted. None scores_3d Optional [ np . ndarray ] float [n_shifts_3d] . scores_3d[i] is the score corresponding to shifts_3d[i] . It is approximately the number of neighbours between the two point clouds after the shift was applied. None best_shift Optional [ np . ndarray ] int [y_shift, x_shift, z_shift] . Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels. Will be plotted as black cross on image if provided. None score_thresh_2d Optional [ float ] Threshold returned by compute_shift function for 2d calculation, if score is above this, it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding to score_thresh_2d in the 2D plot None best_shift_initial Optional [ np . ndarray ] int [y_shift, x_shift] . Best yx shift found by in first search of algorithm. I.e. score_thresh computation based on this. Will show as green x if given. None score_thresh_3d Optional [ float ] Threshold returned by compute_shift function for 3d calculation. If given, a red-white-blue colorbar will be used with white corresponding to score_thresh_3d in the 3D plots. None thresh_shift Optional [ np . ndarray ] int [y_shift, x_shift, z_shift] . yx shift corresponding to score_thresh . Will show as green + in both 2D and 3D plots if given. None thresh_min_dist Optional [ int ] shift_thresh is the shift with the max score in an annulus a distance between thresh_min_dist and thresh_max_dist away from best_shift_initial . Annulus will be shown in green if given. None thresh_max_dist Optional [ int ] shift_thresh is the shift with the max score in an annulus a distance between thresh_min_dist and thresh_max_dist away from best_shift_initial . Annulus will be shown in green if given. None title Optional [ str ] Title to show. None show bool If True , will call plt.show() , else will return fig . True Source code in coppafish/plot/stitch/shift.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def view_shifts ( shifts_2d : np . ndarray , scores_2d : np . ndarray , shifts_3d : Optional [ np . ndarray ] = None , scores_3d : Optional [ np . ndarray ] = None , best_shift : Optional [ np . ndarray ] = None , score_thresh_2d : Optional [ float ] = None , best_shift_initial : Optional [ np . ndarray ] = None , score_thresh_3d : Optional [ float ] = None , thresh_shift : Optional [ np . ndarray ] = None , thresh_min_dist : Optional [ int ] = None , thresh_max_dist : Optional [ int ] = None , title : Optional [ str ] = None , show : bool = True ): \"\"\" Function to plot scores indicating number of neighbours between 2 point clouds corresponding to particular shifts applied to one of them. I.e. you can use this to view the output from `coppafish/stitch/shift/compute_shift` function. Args: shifts_2d: `int [n_shifts_2d x 2]`. `shifts_2d[i]` is the yx shift which achieved `scores_2d[i]` when considering just yx shift between point clouds. I.e. first step of finding optimal shift is collapsing 3D point cloud to just a few planes and then applying a yx shift to these planes. scores_2d: `float [n_shifts_2d]`. `scores_2d[i]` is the score corresponding to `shifts_2d[i]`. It is approximately the number of neighbours between the two point clouds after the shift was applied. shifts_3d: `int [n_shifts_3d x 3]`. `shifts_3d[i]` is the yxz shift which achieved `scores_3d[i]` when considering the yxz shift between point clouds. YX shift is in units of YX pixels. Z shift is in units of z-pixels. If None, only 2D image plotted. scores_3d: `float [n_shifts_3d]`. `scores_3d[i]` is the score corresponding to `shifts_3d[i]`. It is approximately the number of neighbours between the two point clouds after the shift was applied. best_shift: `int [y_shift, x_shift, z_shift]`. Best shift found by algorithm. YX shift is in units of YX pixels. Z shift is in units of z-pixels. Will be plotted as black cross on image if provided. score_thresh_2d: Threshold returned by `compute_shift` function for 2d calculation, if `score` is above this, it indicates an accepted 2D shift. If given, a red-white-blue colorbar will be used with white corresponding to `score_thresh_2d` in the 2D plot best_shift_initial: `int [y_shift, x_shift]`. Best yx shift found by in first search of algorithm. I.e. `score_thresh` computation based on this. Will show as green x if given. score_thresh_3d: Threshold returned by `compute_shift` function for 3d calculation. If given, a red-white-blue colorbar will be used with white corresponding to `score_thresh_3d` in the 3D plots. thresh_shift: `int [y_shift, x_shift, z_shift]`. yx shift corresponding to `score_thresh`. Will show as green + in both 2D and 3D plots if given. thresh_min_dist: `shift_thresh` is the shift with the max score in an annulus a distance between `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`. Annulus will be shown in green if given. thresh_max_dist: `shift_thresh` is the shift with the max score in an annulus a distance between `thresh_min_dist` and `thresh_max_dist` away from `best_shift_initial`. Annulus will be shown in green if given. title: Title to show. show: If `True`, will call `plt.show()`, else will `return fig`. \"\"\" image_2d , extent_2d = get_plot_images_from_shifts ( np . rint ( shifts_2d ) . astype ( int ), scores_2d ) image_2d = interpolate_array ( image_2d , 0 ) # replace 0 with nearest neighbor value fig = plt . figure ( figsize = ( 12 , 8 )) if score_thresh_2d is None : score_thresh_2d = ( image_2d . min () + image_2d . max ()) / 2 cmap_2d = 'virids' else : cmap_2d = 'bwr' v_max = np . max ([ image_2d . max (), 1.2 * score_thresh_2d ]) v_min = image_2d . min () if cmap_2d == 'bwr' : cmap_extent = np . max ([ v_max - score_thresh_2d , score_thresh_2d - v_min ]) # Have equal range above and below score_thresh to not skew colormap v_min = score_thresh_2d - cmap_extent v_max = score_thresh_2d + cmap_extent cmap_norm = matplotlib . colors . TwoSlopeNorm ( vmin = v_min , vcenter = score_thresh_2d , vmax = v_max ) if shifts_3d is not None : images_3d , extent_3d = get_plot_images_from_shifts ( np . rint ( shifts_3d ) . astype ( int ), scores_3d ) images_3d = interpolate_array ( images_3d , 0 ) # replace 0 with nearest neighbor value if score_thresh_3d is None : score_thresh_3d = ( images_3d . min () + images_3d . max ()) / 2 cmap_3d = 'virids' else : cmap_3d = 'bwr' v_max_3d = np . max ([ images_3d . max (), 1.2 * score_thresh_3d ]) v_min_3d = images_3d . min () if cmap_3d == 'bwr' : cmap_extent = np . max ([ v_max_3d - score_thresh_3d , score_thresh_3d - v_min_3d ]) # Have equal range above and below score_thresh to not skew colormap v_min_3d = score_thresh_3d - cmap_extent v_max_3d = score_thresh_3d + cmap_extent cmap_norm_3d = matplotlib . colors . TwoSlopeNorm ( vmin = v_min_3d , vcenter = score_thresh_3d , vmax = v_max_3d ) n_cols = images_3d . shape [ 2 ] if n_cols > 13 : # If loads of z-planes, just show the 13 with the largest score n_cols = 13 max_score_z = images_3d . max ( axis = ( 0 , 1 )) use_z = np . sort ( np . argsort ( max_score_z )[:: - 1 ][: n_cols ]) else : use_z = np . arange ( n_cols ) plot_3d_height = int ( np . ceil ( n_cols / 4 )) plot_2d_height = n_cols - plot_3d_height ax_2d = plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( 0 , 0 ), colspan = n_cols , rowspan = plot_2d_height ) ax_3d = [ plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( plot_2d_height + 1 , i ), rowspan = plot_3d_height ) for i in range ( n_cols )] for i in range ( n_cols ): # share axes for 3D plots ax_3d [ i ] . get_shared_y_axes () . join ( ax_3d [ i ], * ax_3d ) ax_3d [ i ] . get_shared_x_axes () . join ( ax_3d [ i ], * ax_3d ) im_3d = ax_3d [ i ] . imshow ( images_3d [:, :, use_z [ i ]], extent = extent_3d [: 4 ], aspect = 'auto' , cmap = cmap_3d , norm = cmap_norm_3d ) z_plane = int ( np . rint ( extent_3d [ 4 ] + use_z [ i ] + 0.5 )) ax_3d [ i ] . set_title ( f 'Z = { z_plane } ' ) if thresh_shift is not None and z_plane == thresh_shift [ 2 ]: # Indicate threshold shift on correct 3d plot ax_3d [ i ] . plot ( thresh_shift [ 1 ], thresh_shift [ 0 ], '+' , color = 'lime' , label = 'Thresh shift' ) if i > 0 : ax_3d [ i ] . tick_params ( labelbottom = False , labelleft = False ) if best_shift is not None : if z_plane == best_shift [ 2 ]: ax_3d [ i ] . plot ( best_shift [ 1 ], best_shift [ 0 ], 'kx' ) fig . supxlabel ( 'X' ) fig . supylabel ( 'Y' ) ax_3d [ 0 ] . invert_yaxis () cbar_gap = 0.05 cbar_3d_height = plot_3d_height / ( plot_3d_height + plot_2d_height ) cbar_2d_height = plot_2d_height / ( plot_3d_height + plot_2d_height ) cbar_ax = fig . add_axes ([ 0.9 , 0.07 , 0.03 , cbar_3d_height - 2 * cbar_gap ]) # left, bottom, width, height fig . colorbar ( im_3d , cax = cbar_ax ) cbar_ax . set_ylim ( np . clip ( v_min_3d , 0 , v_max_3d ), v_max_3d ) else : n_cols = 1 ax_2d = plt . subplot2grid ( shape = ( n_cols , n_cols ), loc = ( 0 , 0 )) ax_2d . set_xlabel ( 'X' ) ax_2d . set_ylabel ( 'Y' ) im_2d = ax_2d . imshow ( image_2d , extent = extent_2d , aspect = 'auto' , cmap = cmap_2d , norm = cmap_norm ) if best_shift is not None : ax_2d . plot ( best_shift [ 1 ], best_shift [ 0 ], 'kx' , label = 'Best shift' ) if best_shift_initial is not None and best_shift_initial != best_shift : ax_2d . plot ( best_shift_initial [ 1 ], best_shift_initial [ 0 ], 'x' , color = 'lime' , label = 'Best shift initial' ) if thresh_shift is not None : ax_2d . plot ( thresh_shift [ 1 ], thresh_shift [ 0 ], '+' , color = 'lime' , label = 'Thresh shift' ) if thresh_min_dist is not None and best_shift_initial is not None : ax_2d . add_patch ( plt . Circle (( best_shift_initial [ 1 ], best_shift_initial [ 0 ]), thresh_min_dist , color = 'lime' , fill = False )) if thresh_max_dist is not None and best_shift_initial is not None : ax_2d . add_patch ( plt . Circle (( best_shift_initial [ 1 ], best_shift_initial [ 0 ]), thresh_max_dist , color = 'lime' , fill = False )) ax_2d . invert_yaxis () if title is None : title = 'Approx number of neighbours found for all shifts' ax_2d . set_title ( title ) ax_2d . legend ( facecolor = 'b' ) fig . subplots_adjust ( left = 0.07 , right = 0.85 , bottom = 0.07 , top = 0.95 ) if shifts_3d is None : cbar_ax = fig . add_axes ([ 0.9 , 0.07 , 0.03 , 0.9 ]) # left, bottom, width, height else : cbar_ax = fig . add_axes ([ 0.9 , 0.07 + cbar_3d_height , 0.03 , cbar_2d_height - 2.5 * cbar_gap ]) fig . colorbar ( im_2d , cax = cbar_ax ) cbar_ax . set_ylim ( np . clip ( v_min , 0 , v_max ), v_max ) if show : plt . show () else : return fig","title":"view_shifts"},{"location":"code/plot/stitch/#point-clouds","text":"","title":"Point Clouds"},{"location":"code/plot/stitch/#view_stitch_overlap","text":"This plots point clouds of neighbouring tiles with: No overlap Initial guess at shift using config['stitch']['expected_overlap'] Overlap determined in stitch stage of the pipeline (using nb.stitch.south_shifts or nb.stitch.west_shifts ) Their final global coordinate system positions (using nb.stitch.tile_origin ) Parameters: Name Type Description Default nb Notebook Notebook containing at least stitch page. required t int Want to look at overlap between tile t and its north or east neighbour. required direction str Direction of overlap interested in - either 'south' / 'north' or 'west' / 'east' . 'south' Source code in coppafish/plot/stitch/point_clouds.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def view_stitch_overlap ( nb : Notebook , t : int , direction : str = 'south' ): \"\"\" This plots point clouds of neighbouring tiles with: * No overlap * Initial guess at shift using `config['stitch']['expected_overlap']` * Overlap determined in stitch stage of the pipeline (using `nb.stitch.south_shifts` or `nb.stitch.west_shifts`) * Their final global coordinate system positions (using `nb.stitch.tile_origin`) Args: nb: *Notebook* containing at least `stitch` page. t: Want to look at overlap between tile `t` and its north or east neighbour. direction: Direction of overlap interested in - either `'south'`/`'north'` or `'west'`/`'east'`. \"\"\" # NOTE that directions should actually be 'north' and 'east' if direction . lower () == 'south' or direction . lower () == 'west' : direction = direction . lower () elif direction . lower () == 'north' : direction = 'south' elif direction . lower () == 'east' : direction = 'west' else : raise ValueError ( f \"direction must be either 'south' or 'west' but { direction } given.\" ) direction_label = { 'south' : 'north' , 'west' : 'east' } # label refers to actual direction if direction == 'south' : t_neighb = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 1 , 0 ], axis = 1 ) == 2 )[ 0 ] if t_neighb not in nb . basic_info . use_tiles : warnings . warn ( f \"Tile { t } has no overlapping tiles in the south direction so changing to west.\" ) direction = 'west' else : no_overlap_shift = np . array ([ - nb . basic_info . tile_sz , 0 , 0 ]) # assuming no overlap between tiles found_shift = nb . stitch . south_shifts [ np . where ( nb . stitch . south_pairs [:, 0 ] == t )[ 0 ]][ 0 ] if direction == 'west' : t_neighb = np . where ( np . sum ( nb . basic_info . tilepos_yx == nb . basic_info . tilepos_yx [ t , :] + [ 0 , 1 ], axis = 1 ) == 2 )[ 0 ] if t_neighb not in nb . basic_info . use_tiles : raise ValueError ( f \"Tile { t } has no overlapping tiles in the west direction.\" ) no_overlap_shift = np . array ([ 0 , - nb . basic_info . tile_sz , 0 ]) # assuming no overlap between tiles found_shift = nb . stitch . west_shifts [ np . where ( nb . stitch . west_pairs [:, 0 ] == t )[ 0 ]][ 0 ] config = nb . get_config ()[ 'stitch' ] t_neighb = t_neighb [ 0 ] r = nb . basic_info . ref_round c = nb . basic_info . ref_channel point_clouds = [] # add global coordinates of neighbour tile as point cloud that is always present. point_clouds = point_clouds + [ spot_yxz ( nb . find_spots . spot_details , t_neighb , r , c ) + nb . stitch . tile_origin [ t_neighb ]] local_yxz_t = spot_yxz ( nb . find_spots . spot_details , t , r , c ) # Add point cloud for tile t assuming no overlap point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + no_overlap_shift ] # Add point cloud assuming expected overlap initial_shift = ( 1 - config [ 'expected_overlap' ]) * no_overlap_shift point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + initial_shift ] # Add point cloud for tile t with found shift point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t_neighb ] + found_shift ] # Add point cloud for tile t in global coordinate system point_clouds = point_clouds + [ local_yxz_t + nb . stitch . tile_origin [ t ]] neighb_dist_thresh = config [ 'neighb_dist_thresh' ] z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy pc_labels = [ f 'Tile { t_neighb } ' , f 'Tile { t } - No overlap' , f \"Tile { t } - { int ( config [ 'expected_overlap' ] * 100 ) } % overlap\" , f 'Tile { t } - Shift' , f 'Tile { t } - Final' ] view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , f 'Overlap between tile { t } and tile { t_neighb } in the { direction_label [ direction ] } ' ) plt . show ()","title":"view_stitch_overlap"},{"location":"code/plot/stitch/#view_stitch","text":"This plots all the reference spots found ( ref_round / ref_channel ) in the global coordinate system created in the stitch stage of the pipeline. It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre they are closest to). These will be removed in the get_reference_spots step of the pipeline so we don't double count the same spot. Parameters: Name Type Description Default nb Notebook Notebook containing at least stitch page. required Source code in coppafish/plot/stitch/point_clouds.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def view_stitch ( nb : Notebook ): \"\"\" This plots all the reference spots found (`ref_round`/`ref_channel`) in the global coordinate system created in the `stitch` stage of the pipeline. It also indicates which of these spots are duplicates (detected on a tile which is not the tile whose centre they are closest to). These will be removed in the `get_reference_spots` step of the pipeline so we don't double count the same spot. Args: nb: *Notebook* containing at least `stitch` page. \"\"\" is_ref = np . all (( nb . find_spots . spot_details [:, 1 ] == nb . basic_info . ref_round , nb . find_spots . spot_details [:, 2 ] == nb . basic_info . ref_channel ), axis = 0 ) local_yxz = nb . find_spots . spot_details [ is_ref , - 3 :] tile = nb . find_spots . spot_details [ is_ref , 0 ] local_yxz = local_yxz [ np . isin ( tile , nb . basic_info . use_tiles )] tile = tile [ np . isin ( tile , nb . basic_info . use_tiles )] # find duplicate spots as those detected on a tile which is not tile centre they are closest to tile_origin = nb . stitch . tile_origin not_duplicate = get_non_duplicate ( tile_origin , nb . basic_info . use_tiles , nb . basic_info . tile_centre , local_yxz , tile ) global_yxz = local_yxz + nb . stitch . tile_origin [ tile ] global_yxz [:, 2 ] = np . rint ( global_yxz [:, 2 ]) # make z coordinate an integer config = nb . get_config ()[ 'stitch' ] neighb_dist_thresh = config [ 'neighb_dist_thresh' ] z_scale = nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy point_clouds = [ global_yxz [ not_duplicate ], global_yxz [ np . invert ( not_duplicate )]] pc_labels = [ 'Not Duplicate' , 'Duplicate' ] if nb . has_page ( 'ref_spots' ): # Add point cloud indicating those spots that were not saved in ref_spots page # because they were shifted outside the tile bounds on at least one round/channel # and thus spot_color could not be found. local_yxz = local_yxz [ not_duplicate ] # Want to find those which were not duplicates but still removed tile = tile [ not_duplicate ] local_yxz_saved = nb . ref_spots . local_yxz tile_saved = nb . ref_spots . tile local_yxz_saved = local_yxz_saved [ np . isin ( tile_saved , nb . basic_info . use_tiles )] tile_saved = tile_saved [ np . isin ( tile_saved , nb . basic_info . use_tiles )] global_yxz_ns = np . zeros (( 0 , 3 )) # not saved in ref_spots spots for t in nb . basic_info . use_tiles : missing_ind = - 100 local_yxz_t = local_yxz [ tile == t ] removed_ind = np . where ( numpy_indexed . indices ( local_yxz_saved [ tile_saved == t ], local_yxz_t , missing = missing_ind ) == missing_ind )[ 0 ] global_yxz_ns = np . append ( global_yxz_ns , local_yxz_t [ removed_ind ] + nb . stitch . tile_origin [ t ], axis = 0 ) global_yxz_ns [:, 2 ] = np . rint ( global_yxz_ns [:, 2 ]) point_clouds += [ global_yxz_ns ] pc_labels += [ \"No Spot Color\" ] # Sometimes can be empty point cloud, so remove these use_pc = [ len ( pc ) > 0 for pc in point_clouds ] pc_labels = [ pc_labels [ i ] for i in range ( len ( use_pc )) if use_pc [ i ]] point_clouds = [ point_clouds [ i ] for i in range ( len ( use_pc )) if use_pc [ i ]] vpc = view_point_clouds ( point_clouds , pc_labels , neighb_dist_thresh , z_scale , \"Reference Spots in the Global Coordinate System\" ) tile_sz = nb . basic_info . tile_sz for t in nb . basic_info . use_tiles : rect = matplotlib . patches . Rectangle (( tile_origin [ t , 1 ], tile_origin [ t , 0 ]), tile_sz , tile_sz , linewidth = 1 , edgecolor = 'w' , facecolor = 'none' , linestyle = ':' ) vpc . ax . add_patch ( rect ) vpc . ax . text ( tile_origin [ t , 1 ] + 20 , tile_origin [ t , 0 ] + 20 , f \"Tile { t } \" , size = 6 , color = 'w' , ha = 'left' , weight = 'light' ) plt . show ()","title":"view_stitch"},{"location":"code/plot/stitch/#view_point_clouds","text":"Plots two point clouds. point_clouds[0] always plotted but you can change the second point cloud using radio buttons. Parameters: Name Type Description Default point_clouds List List of point clouds, each of which is yxz coordinates float [n_points x 3] . point_clouds[0] is always plotted. YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels. Radio buttons used to select other point_cloud plotted. required pc_labels List List of labels to appear in legend/radio-buttons for each point cloud. Must provide one for each point_cloud . required neighb_dist_thresh float If distance between neighbours is less than this, a white line will connect them. 5 z_scale float pixel_size_z / pixel_size_y i.e. used to convert z coordinates from z-pixels to yx pixels. 1 super_title Optional [ str ] Optional title for plot None Source code in coppafish/plot/stitch/point_clouds.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __init__ ( self , point_clouds : List , pc_labels : List , neighb_dist_thresh : float = 5 , z_scale : float = 1 , super_title : Optional [ str ] = None ): \"\"\" Plots two point clouds. `point_clouds[0]` always plotted but you can change the second point cloud using radio buttons. Args: point_clouds: List of point clouds, each of which is yxz coordinates `float [n_points x 3]`. `point_clouds[0]` is always plotted. YX coordinates are in units of yx pixels. Z coordinates are in units of z pixels. Radio buttons used to select other point_cloud plotted. pc_labels: List of labels to appear in legend/radio-buttons for each point cloud. Must provide one for each `point_cloud`. neighb_dist_thresh: If distance between neighbours is less than this, a white line will connect them. z_scale: `pixel_size_z / pixel_size_y` i.e. used to convert z coordinates from z-pixels to yx pixels. super_title: Optional title for plot \"\"\" n_point_clouds = len ( point_clouds ) if len ( point_clouds ) != len ( pc_labels ): raise ValueError ( f 'There are { n_point_clouds } point clouds but { len ( pc_labels ) } labels' ) self . fig , self . ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 8 )) subplots_adjust = [ 0.07 , 0.775 , 0.095 , 0.89 ] self . fig . subplots_adjust ( left = subplots_adjust [ 0 ], right = subplots_adjust [ 1 ], bottom = subplots_adjust [ 2 ], top = subplots_adjust [ 3 ]) # Find neighbours between all point clouds and the first. self . neighb = [ None ] n_matches_str = 'Number Of Matches' for i in range ( 1 , n_point_clouds ): tree = KDTree ( point_clouds [ i ] * [ 1 , 1 , z_scale ]) dist , neighb = tree . query ( point_clouds [ 0 ] * [ 1 , 1 , z_scale ], distance_upper_bound = neighb_dist_thresh * 3 ) neighb [ dist > neighb_dist_thresh ] = - 1 # set too large distance neighb_ind to -1. n_matches_str = n_matches_str + f ' \\n - { pc_labels [ i ] } : { np . sum ( neighb >= 0 ) } ' self . neighb = self . neighb + [ neighb ] # Add text box to indicate number of matches between first point cloud and each of the others. n_matches_ax = self . fig . add_axes ([ 0.8 , subplots_adjust [ 3 ] - 0.75 , 0.15 , 0.2 ]) plt . axis ( 'off' ) n_matches_ax . text ( 0.05 , 0.95 , n_matches_str ) # round z to closest z-plane for plotting for i in range ( n_point_clouds ): point_clouds [ i ][:, 2 ] = np . rint ( point_clouds [ i ][:, 2 ]) # get yxz axis limits taking account all point clouds pc_min_lims = np . zeros (( n_point_clouds , 3 )) pc_max_lims = np . zeros_like ( pc_min_lims ) for i in range ( n_point_clouds ): pc_min_lims [ i ] = np . min ( point_clouds [ i ], axis = 0 ) pc_max_lims [ i ] = np . max ( point_clouds [ i ], axis = 0 ) self . z_planes = np . arange ( int ( np . min ( pc_min_lims [:, 2 ])), int ( np . max ( pc_max_lims [:, 2 ]) + 1 )) self . nz = len ( self . z_planes ) self . z_ind = 0 self . z = self . z_planes [ self . z_ind ] self . z_thick = 0 self . active_pc = [ 0 , 1 ] self . in_z = [ np . array ([ val [:, 2 ] >= self . z - self . z_thick , val [:, 2 ] <= self . z + self . z_thick ]) . all ( axis = 0 ) for val in point_clouds ] self . point_clouds = point_clouds self . pc_labels = np . array ( pc_labels ) self . pc_shapes = [ 'rx' , 'bo' ] alpha = [ 1 , 0.7 ] self . pc_plots = [ self . ax . plot ( point_clouds [ self . active_pc [ i ]][ self . in_z [ i ], 1 ], point_clouds [ self . active_pc [ i ]][ self . in_z [ i ], 0 ], self . pc_shapes [ i ], label = self . pc_labels [ i ], alpha = alpha [ i ])[ 0 ] for i in range ( 2 )] self . neighb_yx = None self . neighb_plot = None self . update_neighb_lines () self . ax . legend ( loc = 'upper right' ) self . ax . set_ylabel ( 'Y' ) self . ax . set_xlabel ( 'X' ) self . ax . set_ylim ( np . min ( pc_min_lims [:, 0 ]), np . max ( pc_max_lims [:, 0 ])) self . ax . set_xlim ( np . min ( pc_min_lims [:, 1 ]), np . max ( pc_max_lims [:, 1 ])) if self . nz > 1 : # If 3D, add text box to change number of z-planes collapsed onto single plane # and add scrolling to change z-plane self . ax . set_title ( f 'Z = { int ( self . z ) } ' , size = 10 ) self . fig . canvas . mpl_connect ( 'scroll_event' , self . z_scroll ) text_ax = self . fig . add_axes ([ 0.8 , 0.095 , 0.15 , 0.04 ]) else : # For some reason in 2D, still need the text box otherwise buttons don't do work # But shift it off-screen and make small text_ax = self . fig . add_axes ([ 40 , 40 , 0.00001 , 0.00001 ]) self . text_box = TextBox ( text_ax , 'Z-Thick' , self . z_thick , color = 'k' , hovercolor = [ 0.2 , 0.2 , 0.2 ]) self . text_box . cursor . set_color ( 'r' ) # change text box title to be above not to the left of box label = text_ax . get_children ()[ 0 ] # label is a child of the TextBox axis if self . nz == 1 : label . set_position ([ 40 , 40 ]) # shift label off-screen in 2D else : label . set_position ([ 0.5 , 2 ]) # [x,y] - change here to set the position # centering the text label . set_verticalalignment ( 'top' ) label . set_horizontalalignment ( 'center' ) self . text_box . on_submit ( self . text_update ) if n_point_clouds >= 3 : # If 3 or more point clouds, add radio button to change the second point cloud shown. buttons_ax = self . fig . add_axes ([ 0.8 , subplots_adjust [ 3 ] - 0.2 , 0.15 , 0.2 ]) plt . axis ( 'off' ) self . buttons = RadioButtons ( buttons_ax , self . pc_labels [ 1 :], 0 , activecolor = 'w' ) for i in range ( n_point_clouds - 1 ): self . buttons . circles [ i ] . set_color ( 'w' ) self . buttons . circles [ i ] . set_color ( 'w' ) self . buttons . set_active ( 0 ) self . buttons . on_clicked ( self . button_update ) if super_title is not None : plt . suptitle ( super_title , x = ( 0.07 + 0.775 ) / 2 )","title":"view_point_clouds"},{"location":"code/plot/stitch/#diagnostics","text":"","title":"Diagnostics"},{"location":"code/plot/stitch/#view_stitch_shift_info","text":"For all north/south and east/west shifts computed in the stitch section of the pipeline, this plots the values of the shifts found and the score compared to the score_thresh . For each direction, there will be 3 plots: y shift vs x shift for all pairs of neighbouring tiles z shift vs x shift for all pairs of neighbouring tiles score vs score_thresh for all pairs of neighbouring tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile, the shift was applied to, to take it to its north or east neighbour i.e. nb.stitch.south_pairs[:, 0] or nb.stitch.west_pairs[:, 0] . The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default nb Notebook Notebook containing at least the stitch page. required outlier bool If True , will plot nb.stitch.south_shift_outlier instead of nb.stitch.south_shift . In this case, only tiles for which the two are different are plotted for each round. False Source code in coppafish/plot/stitch/diagnostics.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def view_stitch_shift_info ( nb : Notebook , outlier : bool = False ): \"\"\" For all north/south and east/west shifts computed in the `stitch` section of the pipeline, this plots the values of the shifts found and the `score` compared to the `score_thresh`. For each direction, there will be 3 plots: * y shift vs x shift for all pairs of neighbouring tiles * z shift vs x shift for all pairs of neighbouring tiles * `score` vs `score_thresh` for all pairs of neighbouring tiles (a green score = score_thresh line is plotted in this). In each case, the markers in the plots are numbers. These numbers indicate the tile, the shift was applied to, to take it to its north or east neighbour i.e. `nb.stitch.south_pairs[:, 0]` or `nb.stitch.west_pairs[:, 0]`. The number will be blue if `score > score_thresh` and red otherwise. Args: nb: Notebook containing at least the `stitch` page. outlier: If `True`, will plot `nb.stitch.south_shift_outlier` instead of `nb.stitch.south_shift`. In this case, only tiles for which the two are different are plotted for each round. \"\"\" if nb . basic_info . is_3d : ndim = 3 else : ndim = 2 shift_info = {} if len ( nb . stitch . south_shifts ) > 0 : shift_info [ 'South' ] = {} shift_info [ 'South' ][ 'tile' ] = nb . stitch . south_pairs [:, 0 ] shift_info [ 'South' ][ 'score_thresh' ] = nb . stitch . south_score_thresh if outlier : shift_info [ 'South' ][ 'shift' ] = nb . stitch . south_outlier_shifts [:, : ndim ] shift_info [ 'South' ][ 'score' ] = nb . stitch . south_outlier_score else : shift_info [ 'South' ][ 'shift' ] = nb . stitch . south_shifts [:, : ndim ] shift_info [ 'South' ][ 'score' ] = nb . stitch . south_score if len ( nb . stitch . west_shifts ) > 0 : shift_info [ 'West' ] = {} shift_info [ 'West' ][ 'tile' ] = nb . stitch . west_pairs [:, 0 ] shift_info [ 'West' ][ 'score_thresh' ] = nb . stitch . west_score_thresh if outlier : shift_info [ 'West' ][ 'shift' ] = nb . stitch . west_outlier_shifts [:, : ndim ] shift_info [ 'West' ][ 'score' ] = nb . stitch . west_outlier_score else : shift_info [ 'West' ][ 'shift' ] = nb . stitch . west_shifts [:, : ndim ] shift_info [ 'West' ][ 'score' ] = nb . stitch . west_score if outlier : title_start = \"Outlier \" else : title_start = \"\" shift_info_plot ( shift_info , f \" { title_start } Shifts found in stitch part of pipeline between each tile and the \" f \"neighbouring tile in the direction specified\" )","title":"view_stitch_shift_info"},{"location":"code/plot/stitch/#shift_info_plot","text":"If shift_info contains \\(n\\) keys, this will produce an \\(n\\) column x 3 row grid of subplots. For each key in shift_info dictionary, there are 3 plots: y shift vs x shift z shift vs x shift score vs score_thresh or n_matches vs error In each case, the markers in the plots are numbers. These numbers are given by shift_info[key][tile] . The number will be blue if score > score_thresh and red otherwise. Parameters: Name Type Description Default shift_info dict Dictionary containing \\(n\\) dictionaries. Each of these dictionaries contains (either ( score and score_thresh ) or ( n_matches , n_matches_thresh and error )): shift - float [n_tiles x 3] . \\(yxz\\) shifts for each tile. tile - int [n_tiles] . Indicates tile each shift was found for. score - float [n_tiles] . Indicates score found for each shift (approx number of matches between point clouds). score_thresh - float [n_tiles] . If score<score_thresh , it will be shown in red. n_matches - int [n_tiles] . Indicates score found for each shift (approx number of matches between point clouds). n_matches_thresh - int [n_tiles] . If n_matches<n_matches_thresh , it will be shown in red. error - float [n_tiles] . Average distance between neighbours. x_lim - float [n_plots x 2] . Can optionally specify number axis limits for each plot. y_lim - float [n_plots x 2] . Can optionally specify number axis limits for each plot. required title Optional [ str ] Overall title for the plot. None score_plot_thresh int Only shifts with score (or n_matches ) > score_plot_thresh are shown. 0 fig Optional [ plt . Figure ] Can provide previous figure to plot on. None ax Optional [ np . ndarray ] Can provide array of plt.Axes to plot on. None return_ax bool If True , ax will be returned and plt.show() will not be run. False Source code in coppafish/plot/stitch/diagnostics.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def shift_info_plot ( shift_info : dict , title : Optional [ str ] = None , score_plot_thresh : int = 0 , fig : Optional [ plt . Figure ] = None , ax : Optional [ np . ndarray ] = None , return_ax : bool = False ): \"\"\" If `shift_info` contains $n$ keys, this will produce an $n$ column x 3 row grid of subplots. For each key in `shift_info` dictionary, there are 3 plots: * y shift vs x shift * z shift vs x shift * `score` vs `score_thresh` or `n_matches` vs `error` In each case, the markers in the plots are numbers. These numbers are given by `shift_info[key][tile]`. The number will be blue if `score > score_thresh` and red otherwise. Args: shift_info: Dictionary containing $n$ dictionaries. Each of these dictionaries contains (either (`score` and `score_thresh`) or (`n_matches`, `n_matches_thresh` and `error`)): * shift - `float [n_tiles x 3]`. $yxz$ shifts for each tile. * tile - `int [n_tiles]`. Indicates tile each shift was found for. * score - `float [n_tiles]`. Indicates `score` found for each shift (approx number of matches between point clouds). * score_thresh - `float [n_tiles]`. If `score<score_thresh`, it will be shown in red. * n_matches - `int [n_tiles]`. Indicates `score` found for each shift (approx number of matches between point clouds). * n_matches_thresh - `int [n_tiles]`. If `n_matches<n_matches_thresh`, it will be shown in red. * error - `float [n_tiles]`. Average distance between neighbours. * x_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot. * y_lim - `float [n_plots x 2]`. Can optionally specify number axis limits for each plot. title: Overall title for the plot. score_plot_thresh: Only shifts with `score` (or `n_matches`) > `score_plot_thresh` are shown. fig: Can provide previous figure to plot on. ax: Can provide array of plt.Axes to plot on. return_ax: If `True`, ax will be returned and `plt.show()` will not be run. \"\"\" n_cols = len ( shift_info ) col_titles = list ( shift_info . keys ()) n_rows = len ( shift_info [ col_titles [ 0 ]][ 'shift' ][ 0 ]) # 2 if 2D shift, 3 if 3D. if fig is None : fig , ax = plt . subplots ( n_rows , n_cols , figsize = ( 15 , 7 )) fig . subplots_adjust ( hspace = 0.4 , bottom = 0.08 , left = 0.06 , right = 0.97 , top = 0.9 ) if n_cols == 1 and len ( ax . shape ) == 1 : ax = ax [:, np . newaxis ] for i in range ( n_cols ): shift_info_i = shift_info [ col_titles [ i ]] # good tiles are blue, bad tiles are red n_tiles = len ( shift_info [ col_titles [ i ]][ 'tile' ]) tile_color = np . full ( n_tiles , 'b' ) if 'score' in shift_info_i : tile_color [( shift_info_i [ 'score' ] < shift_info_i [ 'score_thresh' ]) . flatten ()] = 'r' skip_tile = shift_info_i [ 'score' ] <= score_plot_thresh # don't plot if score = 0 elif 'n_matches' in shift_info_i : tile_color [( shift_info_i [ 'n_matches' ] < shift_info_i [ 'n_matches_thresh' ]) . flatten ()] = 'r' skip_tile = shift_info_i [ 'n_matches' ] <= score_plot_thresh # don't plot if n_matches = 0 else : raise ValueError ( f \"shift_info must contain either a 'score' or 'n_matches' key\" ) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ 0 , i ] . text ( shift_info_i [ 'shift' ][ t , 1 ], shift_info_i [ 'shift' ][ t , 0 ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if 'x_lim' in shift_info_i : ax [ 0 , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ 0 ]) else : ax [ 0 , i ] . set_xlim ([ np . min ( shift_info_i [ 'shift' ][:, 1 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 1 ]) + 3 ]) if 'y_lim' in shift_info_i : ax [ 0 , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ 0 ]) else : ax [ 0 , i ] . set_ylim ([ np . min ( shift_info_i [ 'shift' ][:, 0 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 0 ]) + 3 ]) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ 0 , i ] . set_xlabel ( 'X Shift' ) if i == 0 : ax [ 0 , i ] . set_ylabel ( 'Y Shift' ) ax [ 0 , i ] . set_title ( col_titles [ i ]) row_ind = 1 if n_rows == 3 : for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'shift' ][ t , 1 ], shift_info_i [ 'shift' ][ t , 2 ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if 'x_lim' in shift_info_i : ax [ row_ind , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_xlim ([ np . min ( shift_info_i [ 'shift' ][:, 1 ]) - 3 , np . max ( shift_info_i [ 'shift' ][:, 1 ]) + 3 ]) if 'y_lim' in shift_info_i : ax [ row_ind , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_ylim ([ np . min ( shift_info_i [ 'shift' ][:, 2 ]) - 1 , np . max ( shift_info_i [ 'shift' ][:, 2 ]) + 1 ]) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'X Shift' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( 'Z Shift' ) row_ind += 1 if 'score' in shift_info_i : # Plot line so that all with score > score_thresh are above it if 'x_lim' in shift_info_i and 'y_lim' in shift_info_i : score_min = np . min ( shift_info_i [ 'y_lim' ][ row_ind , 0 ], shift_info_i [ 'x_lim' ][ row_ind , 0 ]) - 5 score_max = np . max ( shift_info_i [ 'y_lim' ][ row_ind , 1 ], shift_info_i [ 'x_lim' ][ row_ind , 1 ]) + 5 else : score_min = np . min ( np . vstack ([ shift_info_i [ 'score_thresh' ], shift_info_i [ 'score' ]])) - 5 score_max = np . max ( np . vstack ([ shift_info_i [ 'score_thresh' ], shift_info_i [ 'score' ]])) + 5 ax [ row_ind , i ] . set_xlim ([ score_min , score_max ]) ax [ row_ind , i ] . set_ylim ([ score_min , score_max ]) ax [ row_ind , i ] . plot ([ score_min , score_max ], [ score_min , score_max ], 'lime' , linestyle = ':' , linewidth = 2 , alpha = 0.5 ) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'score_thresh' ][ t ], shift_info_i [ 'score' ][ t ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'Score Threshold' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( 'Score' ) elif 'n_matches' in shift_info_i : if 'x_lim' in shift_info_i : ax [ row_ind , i ] . set_xlim ( shift_info_i [ 'x_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_xlim ([ np . min ( shift_info_i [ 'error' ]) - 0.1 , np . max ( shift_info_i [ 'error' ]) + 0.1 ]) if 'y_lim' in shift_info_i : ax [ row_ind , i ] . set_ylim ( shift_info_i [ 'y_lim' ][ row_ind ]) else : ax [ row_ind , i ] . set_ylim ([ np . min ( shift_info_i [ 'n_matches' ]) - 100 , np . max ( shift_info_i [ 'n_matches' ]) + 100 ]) for t in range ( n_tiles ): if skip_tile [ t ]: continue ax [ row_ind , i ] . text ( shift_info_i [ 'error' ][ t ], shift_info_i [ 'n_matches' ][ t ], str ( shift_info_i [ 'tile' ][ t ]), color = tile_color [ t ], fontsize = 12 , ha = 'center' , va = 'center' ) if i == int ( np . ceil ( n_cols / 2 ) - 1 ): ax [ row_ind , i ] . set_xlabel ( 'Error' ) if i == 0 : ax [ row_ind , i ] . set_ylabel ( r '$n_ {matches} $' ) if title is not None : plt . suptitle ( title ) if return_ax : return ax else : plt . show ()","title":"shift_info_plot"},{"location":"code/plot/viewer/","text":"Viewer This is the function to view the results of the pipeline i.e. the spots found and which genes they were assigned to. Parameters: Name Type Description Default nb Notebook Notebook containing at least the ref_spots page. required background_image Optional [ Union [ str , np . ndarray ]] Optional file_name or image that will be plotted as the background image. If image, z dimension needs to be first i.e. n_z x n_y x n_x if 3D or n_y x n_x if 2D. If pass 2D image for 3D data, will show same image as background on each z-plane. 'dapi' gene_marker_file Optional [ str ] Path to csv file containing marker and color for each gene. There must be 6 columns in the csv file with the following headers: GeneNames - str, name of gene with first letter capital ColorR - float, Rgb color for plotting ColorG - float, rGb color for plotting ColorB - float, rgB color for plotting napari_symbol - str, symbol used to plot in napari mpl_symbol - str, equivalent of napari symbol in matplotlib. If it is not provided, then the default file coppafish/plot/results_viewer/legend.gene_color.csv will be used. None Source code in coppafish/plot/results_viewer/base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def __init__ ( self , nb : Notebook , background_image : Optional [ Union [ str , np . ndarray ]] = 'dapi' , gene_marker_file : Optional [ str ] = None ): \"\"\" This is the function to view the results of the pipeline i.e. the spots found and which genes they were assigned to. Args: nb: Notebook containing at least the `ref_spots` page. background_image: Optional file_name or image that will be plotted as the background image. If image, z dimension needs to be first i.e. `n_z x n_y x n_x` if 3D or `n_y x n_x` if 2D. If pass *2D* image for *3D* data, will show same image as background on each z-plane. gene_marker_file: Path to csv file containing marker and color for each gene. There must be 6 columns in the csv file with the following headers: * GeneNames - str, name of gene with first letter capital * ColorR - float, Rgb color for plotting * ColorG - float, rGb color for plotting * ColorB - float, rgB color for plotting * napari_symbol - str, symbol used to plot in napari * mpl_symbol - str, equivalent of napari symbol in matplotlib. If it is not provided, then the default file *coppafish/plot/results_viewer/legend.gene_color.csv* will be used. \"\"\" # TODO: flip y axis so origin bottom left self . nb = nb if gene_marker_file is None : gene_marker_file = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), 'gene_color.csv' ) gene_legend_info = pd . read_csv ( gene_marker_file ) # indices of genes in notebook to gene_color data - quicker to look up integers than names # in change_threshold n_legend_genes = len ( gene_legend_info [ 'GeneNames' ]) self . legend_gene_symbol = np . asarray ( gene_legend_info [ 'mpl_symbol' ]) self . legend_gene_no = np . ones ( n_legend_genes , dtype = int ) * - 1 for i in range ( n_legend_genes ): # TODO: maybe guard against different cases in this comparison gene_ind = np . where ( self . nb . call_spots . gene_names == gene_legend_info [ 'GeneNames' ][ i ])[ 0 ] if len ( gene_ind ) > 0 : self . legend_gene_no [ i ] = gene_ind [ 0 ] # concatenate anchor and omp spots so can use button to switch between them. self . omp_0_ind = self . nb . ref_spots . tile . size # number of anchor spots if self . nb . has_page ( 'omp' ): self . n_spots = self . omp_0_ind + self . nb . omp . tile . size # number of anchor + number of omp spots else : self . n_spots = self . omp_0_ind spot_zyx = np . zeros (( self . n_spots , 3 )) spot_zyx [: self . omp_0_ind ] = ( self . nb . ref_spots . local_yxz + self . nb . stitch . tile_origin [ self . nb . ref_spots . tile ] )[:, [ 2 , 0 , 1 ]] if self . nb . has_page ( 'omp' ): spot_zyx [ self . omp_0_ind :] = ( self . nb . omp . local_yxz + self . nb . stitch . tile_origin [ self . nb . omp . tile ] )[:, [ 2 , 0 , 1 ]] if not self . nb . basic_info . is_3d : spot_zyx = spot_zyx [:, 1 :] # indicate spots shown when plot first opened - omp if exists, else anchor if self . nb . has_page ( 'omp' ): show_spots = np . zeros ( self . n_spots , dtype = bool ) show_spots [ self . omp_0_ind :] = quality_threshold ( self . nb , 'omp' ) else : show_spots = quality_threshold ( self . nb , 'anchor' ) # color to plot for all genes in the notebook gene_color = np . ones (( len ( self . nb . call_spots . gene_names ), 3 )) for i in range ( n_legend_genes ): if self . legend_gene_no [ i ] != - 1 : gene_color [ self . legend_gene_no [ i ]] = [ gene_legend_info . loc [ i , 'ColorR' ], gene_legend_info . loc [ i , 'ColorG' ], gene_legend_info . loc [ i , 'ColorB' ]] self . viewer = napari . Viewer () self . viewer . window . qt_viewer . dockLayerList . setVisible ( False ) self . viewer . window . qt_viewer . dockLayerControls . setVisible ( False ) # Add background image if given self . diagnostic_layer_ind = 0 self . image_layer_ind = None if background_image is not None : if isinstance ( background_image , str ): if background_image . lower () == 'dapi' : file_name = nb . file_names . big_dapi_image elif background_image . lower () == 'anchor' : file_name = nb . file_names . big_anchor_image else : file_name = background_image if file_name is not None and os . path . isfile ( file_name ): background_image = np . load ( file_name ) if file_name . endswith ( '.npz' ): # Assume image is first array if .npz file background_image = background_image [ background_image . _files [ 0 ]] else : background_image = None warnings . warn ( f 'No file exists with address = \\n { file_name } \\n so plotting with no background.' ) if background_image is not None : self . viewer . add_image ( background_image ) self . diagnostic_layer_ind = 1 self . image_layer_ind = 0 self . viewer . layers [ self . image_layer_ind ] . contrast_limits_range = [ background_image . min (), background_image . max ()] self . image_contrast_slider = QRangeSlider ( Qt . Orientation . Horizontal ) # Slider to change score_thresh self . image_contrast_slider . setRange ( background_image . min (), background_image . max ()) # Make starting lower bound contrast the 95th percentile value so most appears black # Use mid_z to quicken up calculation mid_z = int ( background_image . shape [ 0 ] / 2 ) start_contrast = np . percentile ( background_image [ mid_z ], [ 95 , 99.99 ]) . astype ( int ) . tolist () self . image_contrast_slider . setValue ( start_contrast ) self . change_image_contrast () # When dragging, status will show contrast values. self . image_contrast_slider . valueChanged . connect ( lambda x : self . show_image_contrast ( x [ 0 ], x [ 1 ])) # On release of slider, genes shown will change self . image_contrast_slider . sliderReleased . connect ( self . change_image_contrast ) # Add legend indicating genes plotted self . legend = { 'fig' : None , 'ax' : None } self . legend [ 'fig' ], self . legend [ 'ax' ], n_gene_label_letters = \\ add_legend ( gene_legend_info = gene_legend_info , genes = nb . call_spots . gene_names ) # xy is position of each symbol in legend, need to see which gene clicked on. self . legend [ 'xy' ] = np . zeros (( len ( self . legend [ 'ax' ] . collections ), 2 ), dtype = float ) self . legend [ 'gene_no' ] = np . zeros ( len ( self . legend [ 'ax' ] . collections ), dtype = int ) # In legend, each gene name label has at most n_gene_label_letters letters so need to crop # gene_names in notebook when looking for corresponding gene in legend. gene_names_crop = np . asarray ([ gene_name [: n_gene_label_letters ] for gene_name in nb . call_spots . gene_names ]) for i in range ( self . legend [ 'xy' ] . shape [ 0 ]): # Position of label for each gene in legend window self . legend [ 'xy' ][ i ] = np . asarray ( self . legend [ 'ax' ] . collections [ i ] . get_offsets ()) # gene no in notebook that each position in the legend corresponds to self . legend [ 'gene_no' ][ i ] = \\ np . where ( gene_names_crop == self . legend [ 'ax' ] . texts [ i ] . get_text ())[ 0 ][ 0 ] self . legend [ 'fig' ] . mpl_connect ( 'button_press_event' , self . update_genes ) self . viewer . window . add_dock_widget ( self . legend [ 'fig' ], area = 'left' , name = 'Genes' ) self . active_genes = np . arange ( len ( nb . call_spots . gene_names )) # start with all genes shown if background_image is not None : # Slider to change background image contrast self . viewer . window . add_dock_widget ( self . image_contrast_slider , area = \"left\" , name = 'Image Contrast' ) # Add all spots in layer as transparent white spots. point_size = 10 # with size=4, spots are too small to see self . viewer . add_points ( spot_zyx , name = 'Diagnostic' , face_color = 'w' , size = point_size + 2 , opacity = 0 , shown = show_spots ) # Add gene spots with coppafish color code - different layer for each symbol if self . nb . has_page ( 'omp' ): self . spot_gene_no = np . hstack (( self . nb . ref_spots . gene_no , self . nb . omp . gene_no )) else : self . spot_gene_no = self . nb . ref_spots . gene_no self . label_prefix = 'Gene Symbol:' # prefix of label for layers showing spots for s in np . unique ( self . legend_gene_symbol ): spots_correct_gene = np . isin ( self . spot_gene_no , self . legend_gene_no [ self . legend_gene_symbol == s ]) if spots_correct_gene . any (): coords_to_plot = spot_zyx [ spots_correct_gene ] spotcolor_to_plot = gene_color [ self . spot_gene_no [ spots_correct_gene ]] symb_to_plot = np . unique ( gene_legend_info [ self . legend_gene_symbol == s ][ 'napari_symbol' ])[ 0 ] self . viewer . add_points ( coords_to_plot , face_color = spotcolor_to_plot , symbol = symb_to_plot , name = f ' { self . label_prefix }{ s } ' , size = point_size , shown = show_spots [ spots_correct_gene ]) # TODO: showing multiple z-planes at once is possible using out_of_slice_display=True, # but at the moment cannot use at same time as show. # When this works, can change n_z shown by changing z-dimension of point_size i.e. # point_size = [z_size, 10, 10] and z_size changes. # On next napari, release should be able to change thickness of spots in z-direction i.e. control what # number of z-planes can be seen at any one time: # https://github.com/napari/napari/issues/4816#issuecomment-1186600574. # Then see find_spots viewer for how I added a z-thick slider self . viewer . layers . selection . active = self . viewer . layers [ self . diagnostic_layer_ind ] # so indicates when a spot is selected in viewer status # It is needed because layer is transparent so can't see when select spot. self . viewer_status_on_select () config = self . nb . get_config ()[ 'thresholds' ] self . score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_thresh_slider = QDoubleRangeSlider ( Qt . Orientation . Horizontal ) # Slider to change score_thresh # Scores for anchor/omp are different so reset score range when change method # Max possible score is that found for ref_spots, as this can be more than 1. # Max possible omp score is 1. max_score = np . around ( round_any ( nb . ref_spots . score . max (), 0.1 , 'ceil' ), 2 ) max_score = float ( np . clip ( max_score , 1 , np . inf )) self . score_range = { 'anchor' : [ config [ 'score_ref' ], max_score ]} if self . nb . has_page ( 'omp' ): self . score_range [ 'omp' ] = [ config [ 'score_omp' ], max_score ] self . score_thresh_slider . setValue ( self . score_range [ 'omp' ]) else : self . score_thresh_slider . setValue ( self . score_range [ 'anchor' ]) self . score_thresh_slider . setRange ( 0 , max_score ) # When dragging, status will show thresh. self . score_thresh_slider . valueChanged . connect ( lambda x : self . show_score_thresh ( x [ 0 ], x [ 1 ])) # On release of slider, genes shown will change self . score_thresh_slider . sliderReleased . connect ( self . update_plot ) self . viewer . window . add_dock_widget ( self . score_thresh_slider , area = \"left\" , name = 'Score Range' ) # OMP Score Multiplier Slider self . omp_score_multiplier_slider = QDoubleSlider ( Qt . Orientation . Horizontal ) self . omp_score_multiplier_slider . setValue ( self . score_omp_multiplier ) self . omp_score_multiplier_slider . setRange ( 0 , 50 ) self . omp_score_multiplier_slider . valueChanged . connect ( lambda x : self . show_omp_score_multiplier ( x )) self . omp_score_multiplier_slider . sliderReleased . connect ( self . update_plot ) # intensity is calculated same way for anchor / omp method so do not reset intensity threshold # when change method. self . intensity_thresh_slider = QDoubleSlider ( Qt . Orientation . Horizontal ) self . intensity_thresh_slider . setRange ( 0 , 1 ) intensity_thresh = get_intensity_thresh ( nb ) self . intensity_thresh_slider . setValue ( intensity_thresh ) # When dragging, status will show thresh. self . intensity_thresh_slider . valueChanged . connect ( lambda x : self . show_intensity_thresh ( x )) # On release of slider, genes shown will change self . intensity_thresh_slider . sliderReleased . connect ( self . update_plot ) self . viewer . window . add_dock_widget ( self . intensity_thresh_slider , area = \"left\" , name = 'Intensity Threshold' ) if self . nb . has_page ( 'omp' ): self . method_buttons = ButtonMethodWindow ( 'OMP' ) # Buttons to change between Anchor and OMP spots showing. else : self . method_buttons = ButtonMethodWindow ( 'Anchor' ) self . method_buttons . button_anchor . clicked . connect ( self . button_anchor_clicked ) self . method_buttons . button_omp . clicked . connect ( self . button_omp_clicked ) if self . nb . has_page ( 'omp' ): self . viewer . window . add_dock_widget ( self . omp_score_multiplier_slider , area = \"left\" , name = 'OMP Score Multiplier' ) # Only have button to change method if have omp page too. self . viewer . window . add_dock_widget ( self . method_buttons , area = \"left\" , name = 'Method' ) self . key_call_functions () if self . nb . basic_info . is_3d : self . viewer . dims . axis_labels = [ 'z' , 'y' , 'x' ] else : self . viewer . dims . axis_labels = [ 'y' , 'x' ] napari . run ()","title":"Viewer"},{"location":"code/plot/viewer/#viewer","text":"This is the function to view the results of the pipeline i.e. the spots found and which genes they were assigned to. Parameters: Name Type Description Default nb Notebook Notebook containing at least the ref_spots page. required background_image Optional [ Union [ str , np . ndarray ]] Optional file_name or image that will be plotted as the background image. If image, z dimension needs to be first i.e. n_z x n_y x n_x if 3D or n_y x n_x if 2D. If pass 2D image for 3D data, will show same image as background on each z-plane. 'dapi' gene_marker_file Optional [ str ] Path to csv file containing marker and color for each gene. There must be 6 columns in the csv file with the following headers: GeneNames - str, name of gene with first letter capital ColorR - float, Rgb color for plotting ColorG - float, rGb color for plotting ColorB - float, rgB color for plotting napari_symbol - str, symbol used to plot in napari mpl_symbol - str, equivalent of napari symbol in matplotlib. If it is not provided, then the default file coppafish/plot/results_viewer/legend.gene_color.csv will be used. None Source code in coppafish/plot/results_viewer/base.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def __init__ ( self , nb : Notebook , background_image : Optional [ Union [ str , np . ndarray ]] = 'dapi' , gene_marker_file : Optional [ str ] = None ): \"\"\" This is the function to view the results of the pipeline i.e. the spots found and which genes they were assigned to. Args: nb: Notebook containing at least the `ref_spots` page. background_image: Optional file_name or image that will be plotted as the background image. If image, z dimension needs to be first i.e. `n_z x n_y x n_x` if 3D or `n_y x n_x` if 2D. If pass *2D* image for *3D* data, will show same image as background on each z-plane. gene_marker_file: Path to csv file containing marker and color for each gene. There must be 6 columns in the csv file with the following headers: * GeneNames - str, name of gene with first letter capital * ColorR - float, Rgb color for plotting * ColorG - float, rGb color for plotting * ColorB - float, rgB color for plotting * napari_symbol - str, symbol used to plot in napari * mpl_symbol - str, equivalent of napari symbol in matplotlib. If it is not provided, then the default file *coppafish/plot/results_viewer/legend.gene_color.csv* will be used. \"\"\" # TODO: flip y axis so origin bottom left self . nb = nb if gene_marker_file is None : gene_marker_file = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), 'gene_color.csv' ) gene_legend_info = pd . read_csv ( gene_marker_file ) # indices of genes in notebook to gene_color data - quicker to look up integers than names # in change_threshold n_legend_genes = len ( gene_legend_info [ 'GeneNames' ]) self . legend_gene_symbol = np . asarray ( gene_legend_info [ 'mpl_symbol' ]) self . legend_gene_no = np . ones ( n_legend_genes , dtype = int ) * - 1 for i in range ( n_legend_genes ): # TODO: maybe guard against different cases in this comparison gene_ind = np . where ( self . nb . call_spots . gene_names == gene_legend_info [ 'GeneNames' ][ i ])[ 0 ] if len ( gene_ind ) > 0 : self . legend_gene_no [ i ] = gene_ind [ 0 ] # concatenate anchor and omp spots so can use button to switch between them. self . omp_0_ind = self . nb . ref_spots . tile . size # number of anchor spots if self . nb . has_page ( 'omp' ): self . n_spots = self . omp_0_ind + self . nb . omp . tile . size # number of anchor + number of omp spots else : self . n_spots = self . omp_0_ind spot_zyx = np . zeros (( self . n_spots , 3 )) spot_zyx [: self . omp_0_ind ] = ( self . nb . ref_spots . local_yxz + self . nb . stitch . tile_origin [ self . nb . ref_spots . tile ] )[:, [ 2 , 0 , 1 ]] if self . nb . has_page ( 'omp' ): spot_zyx [ self . omp_0_ind :] = ( self . nb . omp . local_yxz + self . nb . stitch . tile_origin [ self . nb . omp . tile ] )[:, [ 2 , 0 , 1 ]] if not self . nb . basic_info . is_3d : spot_zyx = spot_zyx [:, 1 :] # indicate spots shown when plot first opened - omp if exists, else anchor if self . nb . has_page ( 'omp' ): show_spots = np . zeros ( self . n_spots , dtype = bool ) show_spots [ self . omp_0_ind :] = quality_threshold ( self . nb , 'omp' ) else : show_spots = quality_threshold ( self . nb , 'anchor' ) # color to plot for all genes in the notebook gene_color = np . ones (( len ( self . nb . call_spots . gene_names ), 3 )) for i in range ( n_legend_genes ): if self . legend_gene_no [ i ] != - 1 : gene_color [ self . legend_gene_no [ i ]] = [ gene_legend_info . loc [ i , 'ColorR' ], gene_legend_info . loc [ i , 'ColorG' ], gene_legend_info . loc [ i , 'ColorB' ]] self . viewer = napari . Viewer () self . viewer . window . qt_viewer . dockLayerList . setVisible ( False ) self . viewer . window . qt_viewer . dockLayerControls . setVisible ( False ) # Add background image if given self . diagnostic_layer_ind = 0 self . image_layer_ind = None if background_image is not None : if isinstance ( background_image , str ): if background_image . lower () == 'dapi' : file_name = nb . file_names . big_dapi_image elif background_image . lower () == 'anchor' : file_name = nb . file_names . big_anchor_image else : file_name = background_image if file_name is not None and os . path . isfile ( file_name ): background_image = np . load ( file_name ) if file_name . endswith ( '.npz' ): # Assume image is first array if .npz file background_image = background_image [ background_image . _files [ 0 ]] else : background_image = None warnings . warn ( f 'No file exists with address = \\n { file_name } \\n so plotting with no background.' ) if background_image is not None : self . viewer . add_image ( background_image ) self . diagnostic_layer_ind = 1 self . image_layer_ind = 0 self . viewer . layers [ self . image_layer_ind ] . contrast_limits_range = [ background_image . min (), background_image . max ()] self . image_contrast_slider = QRangeSlider ( Qt . Orientation . Horizontal ) # Slider to change score_thresh self . image_contrast_slider . setRange ( background_image . min (), background_image . max ()) # Make starting lower bound contrast the 95th percentile value so most appears black # Use mid_z to quicken up calculation mid_z = int ( background_image . shape [ 0 ] / 2 ) start_contrast = np . percentile ( background_image [ mid_z ], [ 95 , 99.99 ]) . astype ( int ) . tolist () self . image_contrast_slider . setValue ( start_contrast ) self . change_image_contrast () # When dragging, status will show contrast values. self . image_contrast_slider . valueChanged . connect ( lambda x : self . show_image_contrast ( x [ 0 ], x [ 1 ])) # On release of slider, genes shown will change self . image_contrast_slider . sliderReleased . connect ( self . change_image_contrast ) # Add legend indicating genes plotted self . legend = { 'fig' : None , 'ax' : None } self . legend [ 'fig' ], self . legend [ 'ax' ], n_gene_label_letters = \\ add_legend ( gene_legend_info = gene_legend_info , genes = nb . call_spots . gene_names ) # xy is position of each symbol in legend, need to see which gene clicked on. self . legend [ 'xy' ] = np . zeros (( len ( self . legend [ 'ax' ] . collections ), 2 ), dtype = float ) self . legend [ 'gene_no' ] = np . zeros ( len ( self . legend [ 'ax' ] . collections ), dtype = int ) # In legend, each gene name label has at most n_gene_label_letters letters so need to crop # gene_names in notebook when looking for corresponding gene in legend. gene_names_crop = np . asarray ([ gene_name [: n_gene_label_letters ] for gene_name in nb . call_spots . gene_names ]) for i in range ( self . legend [ 'xy' ] . shape [ 0 ]): # Position of label for each gene in legend window self . legend [ 'xy' ][ i ] = np . asarray ( self . legend [ 'ax' ] . collections [ i ] . get_offsets ()) # gene no in notebook that each position in the legend corresponds to self . legend [ 'gene_no' ][ i ] = \\ np . where ( gene_names_crop == self . legend [ 'ax' ] . texts [ i ] . get_text ())[ 0 ][ 0 ] self . legend [ 'fig' ] . mpl_connect ( 'button_press_event' , self . update_genes ) self . viewer . window . add_dock_widget ( self . legend [ 'fig' ], area = 'left' , name = 'Genes' ) self . active_genes = np . arange ( len ( nb . call_spots . gene_names )) # start with all genes shown if background_image is not None : # Slider to change background image contrast self . viewer . window . add_dock_widget ( self . image_contrast_slider , area = \"left\" , name = 'Image Contrast' ) # Add all spots in layer as transparent white spots. point_size = 10 # with size=4, spots are too small to see self . viewer . add_points ( spot_zyx , name = 'Diagnostic' , face_color = 'w' , size = point_size + 2 , opacity = 0 , shown = show_spots ) # Add gene spots with coppafish color code - different layer for each symbol if self . nb . has_page ( 'omp' ): self . spot_gene_no = np . hstack (( self . nb . ref_spots . gene_no , self . nb . omp . gene_no )) else : self . spot_gene_no = self . nb . ref_spots . gene_no self . label_prefix = 'Gene Symbol:' # prefix of label for layers showing spots for s in np . unique ( self . legend_gene_symbol ): spots_correct_gene = np . isin ( self . spot_gene_no , self . legend_gene_no [ self . legend_gene_symbol == s ]) if spots_correct_gene . any (): coords_to_plot = spot_zyx [ spots_correct_gene ] spotcolor_to_plot = gene_color [ self . spot_gene_no [ spots_correct_gene ]] symb_to_plot = np . unique ( gene_legend_info [ self . legend_gene_symbol == s ][ 'napari_symbol' ])[ 0 ] self . viewer . add_points ( coords_to_plot , face_color = spotcolor_to_plot , symbol = symb_to_plot , name = f ' { self . label_prefix }{ s } ' , size = point_size , shown = show_spots [ spots_correct_gene ]) # TODO: showing multiple z-planes at once is possible using out_of_slice_display=True, # but at the moment cannot use at same time as show. # When this works, can change n_z shown by changing z-dimension of point_size i.e. # point_size = [z_size, 10, 10] and z_size changes. # On next napari, release should be able to change thickness of spots in z-direction i.e. control what # number of z-planes can be seen at any one time: # https://github.com/napari/napari/issues/4816#issuecomment-1186600574. # Then see find_spots viewer for how I added a z-thick slider self . viewer . layers . selection . active = self . viewer . layers [ self . diagnostic_layer_ind ] # so indicates when a spot is selected in viewer status # It is needed because layer is transparent so can't see when select spot. self . viewer_status_on_select () config = self . nb . get_config ()[ 'thresholds' ] self . score_omp_multiplier = config [ 'score_omp_multiplier' ] self . score_thresh_slider = QDoubleRangeSlider ( Qt . Orientation . Horizontal ) # Slider to change score_thresh # Scores for anchor/omp are different so reset score range when change method # Max possible score is that found for ref_spots, as this can be more than 1. # Max possible omp score is 1. max_score = np . around ( round_any ( nb . ref_spots . score . max (), 0.1 , 'ceil' ), 2 ) max_score = float ( np . clip ( max_score , 1 , np . inf )) self . score_range = { 'anchor' : [ config [ 'score_ref' ], max_score ]} if self . nb . has_page ( 'omp' ): self . score_range [ 'omp' ] = [ config [ 'score_omp' ], max_score ] self . score_thresh_slider . setValue ( self . score_range [ 'omp' ]) else : self . score_thresh_slider . setValue ( self . score_range [ 'anchor' ]) self . score_thresh_slider . setRange ( 0 , max_score ) # When dragging, status will show thresh. self . score_thresh_slider . valueChanged . connect ( lambda x : self . show_score_thresh ( x [ 0 ], x [ 1 ])) # On release of slider, genes shown will change self . score_thresh_slider . sliderReleased . connect ( self . update_plot ) self . viewer . window . add_dock_widget ( self . score_thresh_slider , area = \"left\" , name = 'Score Range' ) # OMP Score Multiplier Slider self . omp_score_multiplier_slider = QDoubleSlider ( Qt . Orientation . Horizontal ) self . omp_score_multiplier_slider . setValue ( self . score_omp_multiplier ) self . omp_score_multiplier_slider . setRange ( 0 , 50 ) self . omp_score_multiplier_slider . valueChanged . connect ( lambda x : self . show_omp_score_multiplier ( x )) self . omp_score_multiplier_slider . sliderReleased . connect ( self . update_plot ) # intensity is calculated same way for anchor / omp method so do not reset intensity threshold # when change method. self . intensity_thresh_slider = QDoubleSlider ( Qt . Orientation . Horizontal ) self . intensity_thresh_slider . setRange ( 0 , 1 ) intensity_thresh = get_intensity_thresh ( nb ) self . intensity_thresh_slider . setValue ( intensity_thresh ) # When dragging, status will show thresh. self . intensity_thresh_slider . valueChanged . connect ( lambda x : self . show_intensity_thresh ( x )) # On release of slider, genes shown will change self . intensity_thresh_slider . sliderReleased . connect ( self . update_plot ) self . viewer . window . add_dock_widget ( self . intensity_thresh_slider , area = \"left\" , name = 'Intensity Threshold' ) if self . nb . has_page ( 'omp' ): self . method_buttons = ButtonMethodWindow ( 'OMP' ) # Buttons to change between Anchor and OMP spots showing. else : self . method_buttons = ButtonMethodWindow ( 'Anchor' ) self . method_buttons . button_anchor . clicked . connect ( self . button_anchor_clicked ) self . method_buttons . button_omp . clicked . connect ( self . button_omp_clicked ) if self . nb . has_page ( 'omp' ): self . viewer . window . add_dock_widget ( self . omp_score_multiplier_slider , area = \"left\" , name = 'OMP Score Multiplier' ) # Only have button to change method if have omp page too. self . viewer . window . add_dock_widget ( self . method_buttons , area = \"left\" , name = 'Method' ) self . key_call_functions () if self . nb . basic_info . is_3d : self . viewer . dims . axis_labels = [ 'z' , 'y' , 'x' ] else : self . viewer . dims . axis_labels = [ 'y' , 'x' ] napari . run ()","title":"Viewer"},{"location":"code/register/base/","text":"get_average_transform ( transforms , n_matches , matches_thresh , scale_thresh , shift_thresh ) This finds all transforms which pass some thresholds and computes the average transform using them. av_transforms[t, r, c] is the average transform for tile t , round r , channel c and has: Zero rotation. Scaling given by median for channel c over all tiles and rounds. I.e. median(av_transforms[:, :, c, 0, 0]) for y scaling. shift given by median for tile t , round r over all channels. I.e. median(av_transforms[t, r, _, 4, 0]) for y shift if dim=3 . Parameters: Name Type Description Default transforms np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms[t, r, c] is the affine transform for tile t from the reference image to round r , channel c . required n_matches np . ndarray int [n_tiles x n_rounds x n_channels] . Number of matches found by point cloud registration. required matches_thresh Union [ int , np . ndarray ] int [n_tiles x n_rounds x n_channels] or single int . n_matches for a particular transform must exceed this to be used when computing av_transforms . Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: 200 . required scale_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: 0.01 . required shift_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: 10 xy pixels in xy direction, 2 z pixels in z direction (normalised to have same units as yx_pixels ). required Returns: Type Description np . ndarray av_transforms - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . av_transforms[t, r, c] is the average transform for tile t , round r , channel c . np . ndarray av_scaling - float [n_channels x dim] . av_scaling[c, d] is the median scaling for channel c , dimension d , over all tiles and rounds. np . ndarray av_shifts - float [n_tiles x n_rounds x dim] . av_shifts[t, r, d] is the median scaling for tile t , round r , dimension d , over all channels. np . ndarray failed - bool [n_tiles x n_rounds x n_channels] . Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating av_transforms . np . ndarray failed_non_matches - bool [n_tiles x n_rounds x n_channels] . Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift or scaling in one or more directions. Source code in coppafish/register/base.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def get_average_transform ( transforms : np . ndarray , n_matches : np . ndarray , matches_thresh : Union [ int , np . ndarray ], scale_thresh : np . ndarray , shift_thresh : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" This finds all transforms which pass some thresholds and computes the average transform using them. `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c` and has: - Zero rotation. - Scaling given by median for channel `c` over all tiles and rounds. I.e. `median(av_transforms[:, :, c, 0, 0])` for y scaling. - shift given by median for tile `t`, round `r` over all channels. I.e. `median(av_transforms[t, r, _, 4, 0])` for y shift if `dim=3`. Args: transforms: ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```. ```transforms[t, r, c]``` is the affine transform for tile ```t``` from the reference image to round ```r```, channel ```c```. n_matches: ```int [n_tiles x n_rounds x n_channels]```. Number of matches found by point cloud registration. matches_thresh: ```int [n_tiles x n_rounds x n_channels]``` or single ```int```. ```n_matches``` for a particular transform must exceed this to be used when computing ```av_transforms```. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: ```200```. scale_thresh: ```float [n_dim]```. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: `0.01`. shift_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: `10` xy pixels in xy direction, `2` z pixels in z direction (normalised to have same units as `yx_pixels`). Returns: - `av_transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c`. - `av_scaling` - `float [n_channels x dim]`. `av_scaling[c, d]` is the median scaling for channel `c`, dimension `d`, over all tiles and rounds. - `av_shifts` - `float [n_tiles x n_rounds x dim]`. `av_shifts[t, r, d]` is the median scaling for tile `t`, round `r`, dimension `d`, over all channels. - `failed` - `bool [n_tiles x n_rounds x n_channels]`. Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating `av_transforms`. - `failed_non_matches` - `bool [n_tiles x n_rounds x n_channels]`. Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift or scaling in one or more directions. \"\"\" dim = transforms . shape [ - 1 ] failed_matches = n_matches < matches_thresh failed = failed_matches . copy () # Assume scaling is the same for particular channel across all tile and rounds scaling = transforms [:, :, :, np . arange ( dim ), np . arange ( dim )] scaling = np . moveaxis ( scaling , - 1 , 0 ) av_scaling = mod_median ( scaling , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = [ 1 , 2 ]) diff_to_av_scaling = np . abs ( scaling - np . expand_dims ( av_scaling , [ 1 , 2 ])) failed_scale = np . max ( diff_to_av_scaling - np . array ( scale_thresh ) . reshape ( dim , 1 , 1 , 1 ) > 0 , axis = 0 ) failed = np . logical_or ( failed , failed_scale ) # Assume shift the same for particular tile and round across all channels shifts = np . moveaxis ( transforms [:, :, :, 3 ], - 1 , 0 ) av_shifts = mod_median ( shifts , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = 3 ) diff_to_av_shift = np . abs ( shifts - np . expand_dims ( av_shifts , 3 )) failed_shift = np . max ( diff_to_av_shift - np . array ( shift_thresh ) . reshape ( dim , 1 , 1 , 1 ), axis = 0 ) > 0 failed = np . logical_or ( failed , failed_shift ) # find average shifts and scaling again using final failed array av_scaling = mod_median ( scaling , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = [ 1 , 2 ]) av_shifts = mod_median ( shifts , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = 3 ) all_failed_scale_c = np . unique ( np . argwhere ( np . isnan ( av_scaling ))[:, 1 :], axis = 0 ) n_failed = len ( all_failed_scale_c ) if n_failed > 0 : # to compute median scale to particular channel, at least one good tile/round. raise ValueError ( f \" \\n No suitable scales found for the following channels across all tiles/rounds \\n \" f \" { [ all_failed_scale_c [ i ][ 0 ] for i in range ( n_failed )] } \\n \" f \"Consider removing these from use_channels.\" ) all_failed_shifts_tr = np . unique ( np . argwhere ( np . isnan ( av_shifts ))[:, 1 :], axis = 0 ) n_failed = len ( all_failed_shifts_tr [:, 0 ]) if n_failed > 0 : # to compute median shift to particular tile/round, at least one good channel is required. raise ValueError ( f \" \\n No suitable shifts found for the following tile/round combinations\" f \" across all colour channels \\n \" f \"t: { [ all_failed_shifts_tr [ i , 0 ] for i in range ( n_failed )] } \\n \" f \"r: { [ all_failed_shifts_tr [ i , 1 ] for i in range ( n_failed )] } \\n \" f \"Look at the following diagnostics to see why registration has few matches for these: \\n \" f \"coppafish.plot.view_register_shift_info \\n coppafish.plot.view_register_search \\n \" f \"coppafish.plot.view_icp \\n \" f \"If it seems to be a single tile/round that is the problem, maybe remove from \" f \"use_tiles/use_rounds and re-run.\" ) av_scaling = np . moveaxis ( av_scaling , 0 , - 1 ) # so get in order channel,dim av_shifts = np . moveaxis ( av_shifts , 0 , - 1 ) # so get in order tile,round,dim av_transforms = transform_from_scale_shift ( av_scaling , av_shifts ) # indicates tiles/rounds/channels which have anomalous transform compared to median independent of number of matches failed_non_matches = np . logical_or ( failed_scale , failed_shift ) return av_transforms , av_scaling , av_shifts , failed , failed_non_matches get_single_affine_transform ( spot_yxz_base , spot_yxz_transform , z_scale_base , z_scale_transform , start_transform , neighb_dist_thresh , tile_centre , n_iter = 100 , reg_constant_scale = None , reg_constant_shift = None , reg_transform = None ) Finds the affine transform taking spot_yxz_base to spot_yxz_transform . Parameters: Name Type Description Default spot_yxz_base np . ndarray Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. required spot_yxz_transform np . ndarray Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. required z_scale_base float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required z_scale_transform float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required start_transform np . ndarray float [4 x 3] . Start affine transform for iterative closest point. Typically, start_transform[:3, :] is identity matrix and start_transform[3] is approx yxz shift (z shift in units of xy pixels). required neighb_dist_thresh float Distance between 2 points must be less than this to be constituted a match. required tile_centre np . ndarray int [3]. yxz coordinates of centre of image where spot_yxz found on. required n_iter int Max number of iterations to perform of ICP. 100 reg_constant_scale Optional [ float ] Constant used for scaling and rotation when doing regularized least squares. None means no regularized least squares performed. Typical = 5e8 . None reg_constant_shift Optional [ float ] Constant used for shift when doing regularized least squares. None means no regularized least squares performed. Typical = 500 None reg_transform Optional [ np . ndarray ] float [4 x 3] . Transform to regularize to when doing regularized least squares. None means no regularized least squares performed. None Returns: Type Description np . ndarray transform - float [4 x 3] . transform is the final affine transform found. int n_matches - Number of matches found for each transform. float error - Average distance between neighbours below neighb_dist_thresh . bool is_converged - False if max iterations reached before transform converged. Source code in coppafish/register/base.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def get_single_affine_transform ( spot_yxz_base : np . ndarray , spot_yxz_transform : np . ndarray , z_scale_base : float , z_scale_transform : float , start_transform : np . ndarray , neighb_dist_thresh : float , tile_centre : np . ndarray , n_iter : int = 100 , reg_constant_scale : Optional [ float ] = None , reg_constant_shift : Optional [ float ] = None , reg_transform : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , int , float , bool ]: \"\"\" Finds the affine transform taking `spot_yxz_base` to `spot_yxz_transform`. Args: spot_yxz_base: Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. spot_yxz_transform: Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. start_transform: `float [4 x 3]`. Start affine transform for iterative closest point. Typically, `start_transform[:3, :]` is identity matrix and `start_transform[3]` is approx yxz shift (z shift in units of xy pixels). neighb_dist_thresh: Distance between 2 points must be less than this to be constituted a match. tile_centre: int [3]. yxz coordinates of centre of image where spot_yxz found on. n_iter: Max number of iterations to perform of ICP. reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. `None` means no regularized least squares performed. Typical = `5e8`. reg_constant_shift: Constant used for shift when doing regularized least squares. `None` means no regularized least squares performed. Typical = `500` reg_transform: `float [4 x 3]`. Transform to regularize to when doing regularized least squares. `None` means no regularized least squares performed. Returns: - `transform` - `float [4 x 3]`. `transform` is the final affine transform found. - `n_matches` - Number of matches found for each transform. - `error` - Average distance between neighbours below `neighb_dist_thresh`. - `is_converged` - `False` if max iterations reached before transform converged. \"\"\" spot_yxz_base = ( spot_yxz_base - tile_centre ) * [ 1 , 1 , z_scale_base ] spot_yxz_transform = ( spot_yxz_transform - tile_centre ) * [ 1 , 1 , z_scale_transform ] tree_transform = KDTree ( spot_yxz_transform ) neighbour = np . zeros ( spot_yxz_base . shape [ 0 ], dtype = int ) transform = start_transform . copy () for i in range ( n_iter ): neighbour_last = neighbour . copy () transform , neighbour , n_matches , error = \\ get_transform ( spot_yxz_base , transform , spot_yxz_transform , neighb_dist_thresh , tree_transform , reg_constant_scale , reg_constant_shift , reg_transform ) is_converged = bool ( np . abs ( neighbour - neighbour_last ) . max () == 0 ) if is_converged : break return transform , n_matches , error , is_converged get_transform ( yxz_base , transform_old , yxz_target , dist_thresh , yxz_target_tree = None , reg_constant_scale = 30000 , reg_constant_shift = 9 , reg_transform = None ) This finds the affine transform that transforms yxz_base such that the distances between the neighbours with yxz_target are minimised. Parameters: Name Type Description Default yxz_base np . ndarray float [n_base_spots x 3] . Coordinates of spots you want to transform. required transform_old np . ndarray float [4 x 3] . Affine transform found for previous iteration of PCR algorithm. required yxz_target np . ndarray float [n_target_spots x 3] . Coordinates of spots in image that you want to transform yxz_base to. required dist_thresh float If neighbours closer than this, they are used to compute the new transform. Typical: 3 . required yxz_target_tree Optional [ KDTree ] KDTree produced from yxz_target . If None , it will be computed. None reg_constant_scale float Constant used for scaling and rotation when doing regularized least squares. 30000 reg_constant_shift float Constant used for shift when doing regularized least squares. 9 reg_transform Optional [ np . ndarray ] float [4 x 3] . Affine transform which we want final transform to be near when doing regularized least squares. If None , then no regularization is performed. None Returns: Type Description np . ndarray transform - float [4 x 3] . Updated affine transform. np . ndarray neighbour - int [n_base_spots] . neighbour[i] is index of coordinate in yxz_target to which transformation of yxz_base[i] is closest. int n_matches - int . Number of neighbours which have distance less than dist_thresh . float error - float . Average distance between neighbours below dist_thresh . Source code in coppafish/register/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_transform ( yxz_base : np . ndarray , transform_old : np . ndarray , yxz_target : np . ndarray , dist_thresh : float , yxz_target_tree : Optional [ KDTree ] = None , reg_constant_scale : float = 30000 , reg_constant_shift : float = 9 , reg_transform : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray , int , float ]: \"\"\" This finds the affine transform that transforms ```yxz_base``` such that the distances between the neighbours with ```yxz_target``` are minimised. Args: yxz_base: ```float [n_base_spots x 3]```. Coordinates of spots you want to transform. transform_old: ```float [4 x 3]```. Affine transform found for previous iteration of PCR algorithm. yxz_target: ```float [n_target_spots x 3]```. Coordinates of spots in image that you want to transform ```yxz_base``` to. dist_thresh: If neighbours closer than this, they are used to compute the new transform. Typical: ```3```. yxz_target_tree: KDTree produced from ```yxz_target```. If ```None```, it will be computed. reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. reg_constant_shift: Constant used for shift when doing regularized least squares. reg_transform: ```float [4 x 3]```. Affine transform which we want final transform to be near when doing regularized least squares. If ```None```, then no regularization is performed. Returns: - ```transform``` - ```float [4 x 3]```. Updated affine transform. - ```neighbour``` - ```int [n_base_spots]```. ```neighbour[i]``` is index of coordinate in ```yxz_target``` to which transformation of ```yxz_base[i]``` is closest. - ```n_matches``` - ```int```. Number of neighbours which have distance less than ```dist_thresh```. - ```error``` - ```float```. Average distance between ```neighbours``` below ```dist_thresh```. \"\"\" if yxz_target_tree is None : yxz_target_tree = KDTree ( yxz_target ) yxz_base_pad = np . pad ( yxz_base , [( 0 , 0 ), ( 0 , 1 )], constant_values = 1 ) yxz_transform = yxz_base_pad @ transform_old distances , neighbour = yxz_target_tree . query ( yxz_transform , distance_upper_bound = dist_thresh ) neighbour = neighbour . flatten () distances = distances . flatten () use = distances < dist_thresh n_matches = int ( np . sum ( use )) error = float ( np . sqrt ( np . mean ( distances [ use ] ** 2 ))) if reg_transform is None : transform = np . linalg . lstsq ( yxz_base_pad [ use , :], yxz_target [ neighbour [ use ], :], rcond = None )[ 0 ] else : scale = np . array ([ reg_constant_scale , reg_constant_scale , reg_constant_scale , reg_constant_shift ]) . reshape ( 4 , 1 ) yxz_base_regularised = np . concatenate (( yxz_base_pad [ use , :], np . eye ( 4 ) * scale ), axis = 0 ) yxz_target_regularised = np . concatenate (( yxz_target [ neighbour [ use ], :], reg_transform * scale ), axis = 0 ) transform = np . linalg . lstsq ( yxz_base_regularised , yxz_target_regularised , rcond = None )[ 0 ] if np . sum ( transform [ 2 , :] == 0 ) == 3 : transform [ 2 , 2 ] = 1 # if 2d transform, set scaling of z to 1 still return transform , neighbour , n_matches , error icp ( yxz_base , yxz_target , transforms_initial , n_iter , dist_thresh , matches_thresh , scale_dev_thresh , shift_dev_thresh , reg_constant_scale = None , reg_constant_shift = None ) This gets the affine transforms from yxz_base to yxz_target using iterative closest point until all iterations used or convergence. For transforms that have matches below matches_thresh or are anomalous compared to av_transform , the transforms are recomputed using regularized least squares to ensure they are close to the av_transform . If either reg_constant_rot = None or reg_constant_shift = None then this is not done. Parameters: Name Type Description Default yxz_base np . ndarray object [n_tiles] . yxz_base[t] is a numpy float array [n_base_spots x dim] . coordinates of spots on reference round of tile t . required yxz_target np . ndarray object [n_tiles x n_rounds x n_channels] . yxz_target[t, r, c] is a numpy float array [n_target_spots x 3] . coordinates of spots in tile t , round r , channel c . required transforms_initial np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms_initial[t, r, c] is the starting affine transform for tile t from the reference image to round r , channel c . transforms_initial[t, r, c, :dim, :dim] is probably going to be the identity matrix. transforms_initial[t, r, c, dim, :] is the shift which needs to be pre-computed somehow to get a good result. required n_iter int Max number of iterations to perform of ICP. required dist_thresh float If neighbours closer than this, they are used to compute the new transform. Typical: 3 . required matches_thresh Union [ int , np . ndarray ] int [n_tiles x n_rounds x n_channels] or single int . n_matches for a particular transform must exceed this to be used when computing av_transform . Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: 200 . required scale_dev_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: 0.01 . required shift_dev_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: 10 xy pixels in xy direction, 2 z pixels in z direction (normalised to have same units as yx_pixels ). required reg_constant_scale Optional [ float ] Constant used for scaling and rotation when doing regularized least squares. None means no regularized least squares performed. Typical = 5e8 . None reg_constant_shift Optional [ float ] Constant used for shift when doing regularized least squares. None means no regularized least squares performed. Typical = 500 None Returns: Type Description np . ndarray transforms - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms[t, r, c] is the final affine transform found for tile t , round r , channel c . dict debug_info - dict containing 7 np.ndarray - n_matches - int [n_tiles x n_rounds x n_channels] . Number of matches found for each transform. error - float [n_tiles x n_rounds x n_channels] . Average distance between neighbours below dist_thresh . failed - bool [n_tiles x n_rounds x n_channels] . Indicates tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating av_scalings or av_shifts . is_converged - bool [n_tiles x n_rounds x n_channels] . False if max iterations reached before transform converged. av_scaling - float [n_channels x n_dim] . Chromatic aberration scaling factor to each channel from reference channel. Made using all rounds and tiles. av_shift - float [n_tiles x n_rounds x dim] . av_shift[t, r] is the average shift from reference round to round r for tile t across all colour channels. transforms_outlier - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms_outlier[t, r, c] is the final affine transform found for tile t , round r , channel c without regularization for t , r , c indicated by failed otherwise it is 0 . Source code in coppafish/register/base.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def icp ( yxz_base : np . ndarray , yxz_target : np . ndarray , transforms_initial : np . ndarray , n_iter : int , dist_thresh : float , matches_thresh : Union [ int , np . ndarray ], scale_dev_thresh : np . ndarray , shift_dev_thresh : np . ndarray , reg_constant_scale : Optional [ float ] = None , reg_constant_shift : Optional [ float ] = None ) -> Tuple [ np . ndarray , dict ]: \"\"\" This gets the affine `transforms` from `yxz_base` to `yxz_target` using iterative closest point until all iterations used or convergence. For `transforms` that have matches below `matches_thresh` or are anomalous compared to `av_transform`, the `transforms` are recomputed using regularized least squares to ensure they are close to the `av_transform`. If either `reg_constant_rot = None` or `reg_constant_shift = None` then this is not done. Args: yxz_base: `object [n_tiles]`. `yxz_base[t]` is a numpy `float` array `[n_base_spots x dim]`. coordinates of spots on reference round of tile `t`. yxz_target: `object [n_tiles x n_rounds x n_channels]`. `yxz_target[t, r, c]` is a numpy `float` array `[n_target_spots x 3]`. coordinates of spots in tile `t`, round `r`, channel `c`. transforms_initial: `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms_initial[t, r, c]` is the starting affine transform for tile `t` from the reference image to round `r`, channel `c`. `transforms_initial[t, r, c, :dim, :dim]` is probably going to be the identity matrix. `transforms_initial[t, r, c, dim, :]` is the shift which needs to be pre-computed somehow to get a good result. n_iter: Max number of iterations to perform of ICP. dist_thresh: If neighbours closer than this, they are used to compute the new transform. Typical: `3`. matches_thresh: `int [n_tiles x n_rounds x n_channels]` or single `int`. `n_matches` for a particular transform must exceed this to be used when computing `av_transform`. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: `200`. scale_dev_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: `0.01`. shift_dev_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: `10` xy pixels in xy direction, `2` z pixels in z direction (normalised to have same units as `yx_pixels`). reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. `None` means no regularized least squares performed. Typical = `5e8`. reg_constant_shift: Constant used for shift when doing regularized least squares. `None` means no regularized least squares performed. Typical = `500` Returns: - `transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c`. - `debug_info` - `dict` containing 7 `np.ndarray` - - `n_matches` - `int [n_tiles x n_rounds x n_channels]`. Number of matches found for each transform. - `error` - `float [n_tiles x n_rounds x n_channels]`. Average distance between neighbours below `dist_thresh`. - `failed` - `bool [n_tiles x n_rounds x n_channels]`. Indicates tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating `av_scalings` or `av_shifts`. - `is_converged` - `bool [n_tiles x n_rounds x n_channels]`. `False` if max iterations reached before transform converged. - `av_scaling` - `float [n_channels x n_dim]`. Chromatic aberration scaling factor to each channel from reference channel. Made using all rounds and tiles. - `av_shift` - `float [n_tiles x n_rounds x dim]`. `av_shift[t, r]` is the average shift from reference round to round `r` for tile `t` across all colour channels. - `transforms_outlier` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms_outlier[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c` without regularization for `t`, `r`, `c` indicated by failed otherwise it is `0`. \"\"\" n_tiles , n_rounds , n_channels = yxz_target . shape if not utils . errors . check_shape ( yxz_base , [ n_tiles ]): raise utils . errors . ShapeError ( \"yxz_base\" , yxz_base . shape , ( n_tiles ,)) tree_target = np . zeros_like ( yxz_target ) for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): tree_target [ t , r , c ] = KDTree ( yxz_target [ t , r , c ]) n_matches = np . zeros_like ( yxz_target , dtype = int ) error = np . zeros_like ( yxz_target , dtype = float ) neighbour = np . zeros_like ( yxz_target ) is_converged = np . zeros_like ( yxz_target , dtype = bool ) transforms = transforms_initial . copy () . astype ( float ) transforms_outlier = np . zeros_like ( transforms ) finished_good_images = False av_transforms = None i_finished_good = 0 i = 0 with tqdm ( total = n_tiles * n_rounds * n_channels ) as pbar : pbar . set_description ( f \"Iterative Closest Point to find affine transforms\" ) while i < n_iter : pbar . set_postfix ({ 'iter' : f ' { i + 1 } / { n_iter } ' , 'regularized' : str ( finished_good_images )}) neighbour_last = neighbour . copy () for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): if is_converged [ t , r , c ]: continue if finished_good_images : reg_transform = av_transforms [ t , r , c ] else : reg_transform = None transforms [ t , r , c ], neighbour [ t , r , c ], n_matches [ t , r , c ], error [ t , r , c ] = \\ get_transform ( yxz_base [ t ], transforms [ t , r , c ], yxz_target [ t , r , c ], dist_thresh , tree_target [ t , r , c ], reg_constant_scale , reg_constant_shift , reg_transform ) if i > i_finished_good : is_converged [ t , r , c ] = np . abs ( neighbour [ t , r , c ] - neighbour_last [ t , r , c ]) . max () == 0 if is_converged [ t , r , c ]: pbar . update ( 1 ) if ( is_converged . all () and not finished_good_images ) or ( i == n_iter - 1 and not finished_good_images ): av_transforms , av_scaling , av_shifts , failed , failed_non_matches = \\ get_average_transform ( transforms , n_matches , matches_thresh , scale_dev_thresh , shift_dev_thresh ) if reg_constant_scale is not None and reg_constant_shift is not None : # reset transforms of those that failed to average transform as starting point for # regularised fitting transforms_outlier [ failed , :, :] = transforms [ failed , :, :] . copy () transforms [ failed , :, :] = av_transforms [ failed , :, :] is_converged [ failed ] = False i = - 1 # Allow n_iter to find regularized best transforms as well. finished_good_images = True pbar . update ( - np . sum ( failed . flatten ())) i += 1 if is_converged . all (): break pbar . close () if i == n_iter : warnings . warn ( f \"Max number of iterations, { n_iter } reached but only \" f \" { np . sum ( is_converged ) } / { np . sum ( is_converged >= 0 ) } transforms converged\" ) debug_info = { 'n_matches' : n_matches , 'error' : error , 'failed' : failed , 'is_converged' : is_converged , 'av_scaling' : av_scaling , 'av_shifts' : av_shifts , 'transforms_outlier' : transforms_outlier } return transforms , debug_info mod_median ( array , ignore , axis = 0 ) This computes the median ignoring values indicated by ignore . Parameters: Name Type Description Default array np . ndarray float [n_dim_1 x n_dim_2 x ... x n_dim_N] . array to compute median from. required ignore np . ndarray bool [n_dim_1 x n_dim_2 x ... x n_dim_N] . True for values in array that should not be used to compute median. required axis Union [ int , List [ int ]] int [n_axis_av] . Which axis to average over. 0 Returns: Type Description Union [ float , np . ndarray ] Median value without using those values indicated by ignore . Source code in coppafish/register/base.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def mod_median ( array : np . ndarray , ignore : np . ndarray , axis : Union [ int , List [ int ]] = 0 ) -> Union [ float , np . ndarray ]: \"\"\" This computes the median ignoring values indicated by ```ignore```. Args: array: ```float [n_dim_1 x n_dim_2 x ... x n_dim_N]```. array to compute median from. ignore: ```bool [n_dim_1 x n_dim_2 x ... x n_dim_N]```. True for values in array that should not be used to compute median. axis: ```int [n_axis_av]```. Which axis to average over. Returns: Median value without using those values indicated by ```ignore```. \"\"\" mod_array = array . copy () mod_array [ ignore ] = np . nan return np . nanmedian ( mod_array , axis = axis ) transform_from_scale_shift ( scale , shift ) Gets [dim+1 x dim] affine transform from scale for each channel and shift for each tile/round. Parameters: Name Type Description Default scale np . ndarray float [n_channels x n_dims] . scale[c, d] is the scaling to account for chromatic aberration from reference channel to channel c for dimension d . Typically, as an initial guess all values in scale will be 1 . required shift np . ndarray float [n_tiles x n_rounds x n_dims] . shift[t, r, d] is the shift to account for the shift between the reference round for tile t and round r for tile t in dimension d . required Returns: Type Description np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . [t, r, c] is the affine transform for tile t , round r , channel c computed from scale[c] and shift[t, r] . Source code in coppafish/register/base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def transform_from_scale_shift ( scale : np . ndarray , shift : np . ndarray ) -> np . ndarray : \"\"\" Gets ```[dim+1 x dim]``` affine transform from scale for each channel and shift for each tile/round. Args: scale: ```float [n_channels x n_dims]```. ```scale[c, d]``` is the scaling to account for chromatic aberration from reference channel to channel ```c``` for dimension ```d```. Typically, as an initial guess all values in scale will be ```1```. shift: ```float [n_tiles x n_rounds x n_dims]```. ```shift[t, r, d]``` is the shift to account for the shift between the reference round for tile ```t``` and round ```r``` for tile ```t``` in dimension ```d```. Returns: ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```. ```[t, r, c]``` is the affine transform for tile ```t```, round ```r```, channel ```c``` computed from ```scale[c]``` and ```shift[t, r]```. \"\"\" n_channels = scale . shape [ 0 ] n_tiles , n_rounds , dim = shift . shape transforms = np . zeros (( n_tiles , n_rounds , n_channels , dim + 1 , dim )) for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): transforms [ t , r , c , : dim , :, ] = np . eye ( dim ) * scale [ c ] transforms [ t , r , c , dim , :] = shift [ t , r ] return transforms","title":"Base"},{"location":"code/register/base/#coppafish.register.base.get_average_transform","text":"This finds all transforms which pass some thresholds and computes the average transform using them. av_transforms[t, r, c] is the average transform for tile t , round r , channel c and has: Zero rotation. Scaling given by median for channel c over all tiles and rounds. I.e. median(av_transforms[:, :, c, 0, 0]) for y scaling. shift given by median for tile t , round r over all channels. I.e. median(av_transforms[t, r, _, 4, 0]) for y shift if dim=3 . Parameters: Name Type Description Default transforms np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms[t, r, c] is the affine transform for tile t from the reference image to round r , channel c . required n_matches np . ndarray int [n_tiles x n_rounds x n_channels] . Number of matches found by point cloud registration. required matches_thresh Union [ int , np . ndarray ] int [n_tiles x n_rounds x n_channels] or single int . n_matches for a particular transform must exceed this to be used when computing av_transforms . Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: 200 . required scale_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: 0.01 . required shift_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: 10 xy pixels in xy direction, 2 z pixels in z direction (normalised to have same units as yx_pixels ). required Returns: Type Description np . ndarray av_transforms - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . av_transforms[t, r, c] is the average transform for tile t , round r , channel c . np . ndarray av_scaling - float [n_channels x dim] . av_scaling[c, d] is the median scaling for channel c , dimension d , over all tiles and rounds. np . ndarray av_shifts - float [n_tiles x n_rounds x dim] . av_shifts[t, r, d] is the median scaling for tile t , round r , dimension d , over all channels. np . ndarray failed - bool [n_tiles x n_rounds x n_channels] . Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating av_transforms . np . ndarray failed_non_matches - bool [n_tiles x n_rounds x n_channels] . Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift or scaling in one or more directions. Source code in coppafish/register/base.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def get_average_transform ( transforms : np . ndarray , n_matches : np . ndarray , matches_thresh : Union [ int , np . ndarray ], scale_thresh : np . ndarray , shift_thresh : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\" This finds all transforms which pass some thresholds and computes the average transform using them. `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c` and has: - Zero rotation. - Scaling given by median for channel `c` over all tiles and rounds. I.e. `median(av_transforms[:, :, c, 0, 0])` for y scaling. - shift given by median for tile `t`, round `r` over all channels. I.e. `median(av_transforms[t, r, _, 4, 0])` for y shift if `dim=3`. Args: transforms: ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```. ```transforms[t, r, c]``` is the affine transform for tile ```t``` from the reference image to round ```r```, channel ```c```. n_matches: ```int [n_tiles x n_rounds x n_channels]```. Number of matches found by point cloud registration. matches_thresh: ```int [n_tiles x n_rounds x n_channels]``` or single ```int```. ```n_matches``` for a particular transform must exceed this to be used when computing ```av_transforms```. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: ```200```. scale_thresh: ```float [n_dim]```. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: `0.01`. shift_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: `10` xy pixels in xy direction, `2` z pixels in z direction (normalised to have same units as `yx_pixels`). Returns: - `av_transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `av_transforms[t, r, c]` is the average transform for tile `t`, round `r`, channel `c`. - `av_scaling` - `float [n_channels x dim]`. `av_scaling[c, d]` is the median scaling for channel `c`, dimension `d`, over all tiles and rounds. - `av_shifts` - `float [n_tiles x n_rounds x dim]`. `av_shifts[t, r, d]` is the median scaling for tile `t`, round `r`, dimension `d`, over all channels. - `failed` - `bool [n_tiles x n_rounds x n_channels]`. Indicates the tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating `av_transforms`. - `failed_non_matches` - `bool [n_tiles x n_rounds x n_channels]`. Indicates the tiles/rounds/channels to which transform was anomalous compared to median either due to shift or scaling in one or more directions. \"\"\" dim = transforms . shape [ - 1 ] failed_matches = n_matches < matches_thresh failed = failed_matches . copy () # Assume scaling is the same for particular channel across all tile and rounds scaling = transforms [:, :, :, np . arange ( dim ), np . arange ( dim )] scaling = np . moveaxis ( scaling , - 1 , 0 ) av_scaling = mod_median ( scaling , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = [ 1 , 2 ]) diff_to_av_scaling = np . abs ( scaling - np . expand_dims ( av_scaling , [ 1 , 2 ])) failed_scale = np . max ( diff_to_av_scaling - np . array ( scale_thresh ) . reshape ( dim , 1 , 1 , 1 ) > 0 , axis = 0 ) failed = np . logical_or ( failed , failed_scale ) # Assume shift the same for particular tile and round across all channels shifts = np . moveaxis ( transforms [:, :, :, 3 ], - 1 , 0 ) av_shifts = mod_median ( shifts , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = 3 ) diff_to_av_shift = np . abs ( shifts - np . expand_dims ( av_shifts , 3 )) failed_shift = np . max ( diff_to_av_shift - np . array ( shift_thresh ) . reshape ( dim , 1 , 1 , 1 ), axis = 0 ) > 0 failed = np . logical_or ( failed , failed_shift ) # find average shifts and scaling again using final failed array av_scaling = mod_median ( scaling , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = [ 1 , 2 ]) av_shifts = mod_median ( shifts , np . expand_dims ( failed , 0 ) . repeat ( dim , 0 ), axis = 3 ) all_failed_scale_c = np . unique ( np . argwhere ( np . isnan ( av_scaling ))[:, 1 :], axis = 0 ) n_failed = len ( all_failed_scale_c ) if n_failed > 0 : # to compute median scale to particular channel, at least one good tile/round. raise ValueError ( f \" \\n No suitable scales found for the following channels across all tiles/rounds \\n \" f \" { [ all_failed_scale_c [ i ][ 0 ] for i in range ( n_failed )] } \\n \" f \"Consider removing these from use_channels.\" ) all_failed_shifts_tr = np . unique ( np . argwhere ( np . isnan ( av_shifts ))[:, 1 :], axis = 0 ) n_failed = len ( all_failed_shifts_tr [:, 0 ]) if n_failed > 0 : # to compute median shift to particular tile/round, at least one good channel is required. raise ValueError ( f \" \\n No suitable shifts found for the following tile/round combinations\" f \" across all colour channels \\n \" f \"t: { [ all_failed_shifts_tr [ i , 0 ] for i in range ( n_failed )] } \\n \" f \"r: { [ all_failed_shifts_tr [ i , 1 ] for i in range ( n_failed )] } \\n \" f \"Look at the following diagnostics to see why registration has few matches for these: \\n \" f \"coppafish.plot.view_register_shift_info \\n coppafish.plot.view_register_search \\n \" f \"coppafish.plot.view_icp \\n \" f \"If it seems to be a single tile/round that is the problem, maybe remove from \" f \"use_tiles/use_rounds and re-run.\" ) av_scaling = np . moveaxis ( av_scaling , 0 , - 1 ) # so get in order channel,dim av_shifts = np . moveaxis ( av_shifts , 0 , - 1 ) # so get in order tile,round,dim av_transforms = transform_from_scale_shift ( av_scaling , av_shifts ) # indicates tiles/rounds/channels which have anomalous transform compared to median independent of number of matches failed_non_matches = np . logical_or ( failed_scale , failed_shift ) return av_transforms , av_scaling , av_shifts , failed , failed_non_matches","title":"get_average_transform()"},{"location":"code/register/base/#coppafish.register.base.get_single_affine_transform","text":"Finds the affine transform taking spot_yxz_base to spot_yxz_transform . Parameters: Name Type Description Default spot_yxz_base np . ndarray Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. required spot_yxz_transform np . ndarray Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. required z_scale_base float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required z_scale_transform float Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. required start_transform np . ndarray float [4 x 3] . Start affine transform for iterative closest point. Typically, start_transform[:3, :] is identity matrix and start_transform[3] is approx yxz shift (z shift in units of xy pixels). required neighb_dist_thresh float Distance between 2 points must be less than this to be constituted a match. required tile_centre np . ndarray int [3]. yxz coordinates of centre of image where spot_yxz found on. required n_iter int Max number of iterations to perform of ICP. 100 reg_constant_scale Optional [ float ] Constant used for scaling and rotation when doing regularized least squares. None means no regularized least squares performed. Typical = 5e8 . None reg_constant_shift Optional [ float ] Constant used for shift when doing regularized least squares. None means no regularized least squares performed. Typical = 500 None reg_transform Optional [ np . ndarray ] float [4 x 3] . Transform to regularize to when doing regularized least squares. None means no regularized least squares performed. None Returns: Type Description np . ndarray transform - float [4 x 3] . transform is the final affine transform found. int n_matches - Number of matches found for each transform. float error - Average distance between neighbours below neighb_dist_thresh . bool is_converged - False if max iterations reached before transform converged. Source code in coppafish/register/base.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 def get_single_affine_transform ( spot_yxz_base : np . ndarray , spot_yxz_transform : np . ndarray , z_scale_base : float , z_scale_transform : float , start_transform : np . ndarray , neighb_dist_thresh : float , tile_centre : np . ndarray , n_iter : int = 100 , reg_constant_scale : Optional [ float ] = None , reg_constant_shift : Optional [ float ] = None , reg_transform : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , int , float , bool ]: \"\"\" Finds the affine transform taking `spot_yxz_base` to `spot_yxz_transform`. Args: spot_yxz_base: Point cloud want to find the shift from. spot_yxz_base[:, 2] is the z coordinate in units of z-pixels. spot_yxz_transform: Point cloud want to find the shift to. spot_yxz_transform[:, 2] is the z coordinate in units of z-pixels. z_scale_base: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. z_scale_transform: Scaling to put z coordinates in same units as yx coordinates for spot_yxz_base. start_transform: `float [4 x 3]`. Start affine transform for iterative closest point. Typically, `start_transform[:3, :]` is identity matrix and `start_transform[3]` is approx yxz shift (z shift in units of xy pixels). neighb_dist_thresh: Distance between 2 points must be less than this to be constituted a match. tile_centre: int [3]. yxz coordinates of centre of image where spot_yxz found on. n_iter: Max number of iterations to perform of ICP. reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. `None` means no regularized least squares performed. Typical = `5e8`. reg_constant_shift: Constant used for shift when doing regularized least squares. `None` means no regularized least squares performed. Typical = `500` reg_transform: `float [4 x 3]`. Transform to regularize to when doing regularized least squares. `None` means no regularized least squares performed. Returns: - `transform` - `float [4 x 3]`. `transform` is the final affine transform found. - `n_matches` - Number of matches found for each transform. - `error` - Average distance between neighbours below `neighb_dist_thresh`. - `is_converged` - `False` if max iterations reached before transform converged. \"\"\" spot_yxz_base = ( spot_yxz_base - tile_centre ) * [ 1 , 1 , z_scale_base ] spot_yxz_transform = ( spot_yxz_transform - tile_centre ) * [ 1 , 1 , z_scale_transform ] tree_transform = KDTree ( spot_yxz_transform ) neighbour = np . zeros ( spot_yxz_base . shape [ 0 ], dtype = int ) transform = start_transform . copy () for i in range ( n_iter ): neighbour_last = neighbour . copy () transform , neighbour , n_matches , error = \\ get_transform ( spot_yxz_base , transform , spot_yxz_transform , neighb_dist_thresh , tree_transform , reg_constant_scale , reg_constant_shift , reg_transform ) is_converged = bool ( np . abs ( neighbour - neighbour_last ) . max () == 0 ) if is_converged : break return transform , n_matches , error , is_converged","title":"get_single_affine_transform()"},{"location":"code/register/base/#coppafish.register.base.get_transform","text":"This finds the affine transform that transforms yxz_base such that the distances between the neighbours with yxz_target are minimised. Parameters: Name Type Description Default yxz_base np . ndarray float [n_base_spots x 3] . Coordinates of spots you want to transform. required transform_old np . ndarray float [4 x 3] . Affine transform found for previous iteration of PCR algorithm. required yxz_target np . ndarray float [n_target_spots x 3] . Coordinates of spots in image that you want to transform yxz_base to. required dist_thresh float If neighbours closer than this, they are used to compute the new transform. Typical: 3 . required yxz_target_tree Optional [ KDTree ] KDTree produced from yxz_target . If None , it will be computed. None reg_constant_scale float Constant used for scaling and rotation when doing regularized least squares. 30000 reg_constant_shift float Constant used for shift when doing regularized least squares. 9 reg_transform Optional [ np . ndarray ] float [4 x 3] . Affine transform which we want final transform to be near when doing regularized least squares. If None , then no regularization is performed. None Returns: Type Description np . ndarray transform - float [4 x 3] . Updated affine transform. np . ndarray neighbour - int [n_base_spots] . neighbour[i] is index of coordinate in yxz_target to which transformation of yxz_base[i] is closest. int n_matches - int . Number of neighbours which have distance less than dist_thresh . float error - float . Average distance between neighbours below dist_thresh . Source code in coppafish/register/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_transform ( yxz_base : np . ndarray , transform_old : np . ndarray , yxz_target : np . ndarray , dist_thresh : float , yxz_target_tree : Optional [ KDTree ] = None , reg_constant_scale : float = 30000 , reg_constant_shift : float = 9 , reg_transform : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , np . ndarray , int , float ]: \"\"\" This finds the affine transform that transforms ```yxz_base``` such that the distances between the neighbours with ```yxz_target``` are minimised. Args: yxz_base: ```float [n_base_spots x 3]```. Coordinates of spots you want to transform. transform_old: ```float [4 x 3]```. Affine transform found for previous iteration of PCR algorithm. yxz_target: ```float [n_target_spots x 3]```. Coordinates of spots in image that you want to transform ```yxz_base``` to. dist_thresh: If neighbours closer than this, they are used to compute the new transform. Typical: ```3```. yxz_target_tree: KDTree produced from ```yxz_target```. If ```None```, it will be computed. reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. reg_constant_shift: Constant used for shift when doing regularized least squares. reg_transform: ```float [4 x 3]```. Affine transform which we want final transform to be near when doing regularized least squares. If ```None```, then no regularization is performed. Returns: - ```transform``` - ```float [4 x 3]```. Updated affine transform. - ```neighbour``` - ```int [n_base_spots]```. ```neighbour[i]``` is index of coordinate in ```yxz_target``` to which transformation of ```yxz_base[i]``` is closest. - ```n_matches``` - ```int```. Number of neighbours which have distance less than ```dist_thresh```. - ```error``` - ```float```. Average distance between ```neighbours``` below ```dist_thresh```. \"\"\" if yxz_target_tree is None : yxz_target_tree = KDTree ( yxz_target ) yxz_base_pad = np . pad ( yxz_base , [( 0 , 0 ), ( 0 , 1 )], constant_values = 1 ) yxz_transform = yxz_base_pad @ transform_old distances , neighbour = yxz_target_tree . query ( yxz_transform , distance_upper_bound = dist_thresh ) neighbour = neighbour . flatten () distances = distances . flatten () use = distances < dist_thresh n_matches = int ( np . sum ( use )) error = float ( np . sqrt ( np . mean ( distances [ use ] ** 2 ))) if reg_transform is None : transform = np . linalg . lstsq ( yxz_base_pad [ use , :], yxz_target [ neighbour [ use ], :], rcond = None )[ 0 ] else : scale = np . array ([ reg_constant_scale , reg_constant_scale , reg_constant_scale , reg_constant_shift ]) . reshape ( 4 , 1 ) yxz_base_regularised = np . concatenate (( yxz_base_pad [ use , :], np . eye ( 4 ) * scale ), axis = 0 ) yxz_target_regularised = np . concatenate (( yxz_target [ neighbour [ use ], :], reg_transform * scale ), axis = 0 ) transform = np . linalg . lstsq ( yxz_base_regularised , yxz_target_regularised , rcond = None )[ 0 ] if np . sum ( transform [ 2 , :] == 0 ) == 3 : transform [ 2 , 2 ] = 1 # if 2d transform, set scaling of z to 1 still return transform , neighbour , n_matches , error","title":"get_transform()"},{"location":"code/register/base/#coppafish.register.base.icp","text":"This gets the affine transforms from yxz_base to yxz_target using iterative closest point until all iterations used or convergence. For transforms that have matches below matches_thresh or are anomalous compared to av_transform , the transforms are recomputed using regularized least squares to ensure they are close to the av_transform . If either reg_constant_rot = None or reg_constant_shift = None then this is not done. Parameters: Name Type Description Default yxz_base np . ndarray object [n_tiles] . yxz_base[t] is a numpy float array [n_base_spots x dim] . coordinates of spots on reference round of tile t . required yxz_target np . ndarray object [n_tiles x n_rounds x n_channels] . yxz_target[t, r, c] is a numpy float array [n_target_spots x 3] . coordinates of spots in tile t , round r , channel c . required transforms_initial np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms_initial[t, r, c] is the starting affine transform for tile t from the reference image to round r , channel c . transforms_initial[t, r, c, :dim, :dim] is probably going to be the identity matrix. transforms_initial[t, r, c, dim, :] is the shift which needs to be pre-computed somehow to get a good result. required n_iter int Max number of iterations to perform of ICP. required dist_thresh float If neighbours closer than this, they are used to compute the new transform. Typical: 3 . required matches_thresh Union [ int , np . ndarray ] int [n_tiles x n_rounds x n_channels] or single int . n_matches for a particular transform must exceed this to be used when computing av_transform . Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: 200 . required scale_dev_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: 0.01 . required shift_dev_thresh np . ndarray float [n_dim] . Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: 10 xy pixels in xy direction, 2 z pixels in z direction (normalised to have same units as yx_pixels ). required reg_constant_scale Optional [ float ] Constant used for scaling and rotation when doing regularized least squares. None means no regularized least squares performed. Typical = 5e8 . None reg_constant_shift Optional [ float ] Constant used for shift when doing regularized least squares. None means no regularized least squares performed. Typical = 500 None Returns: Type Description np . ndarray transforms - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms[t, r, c] is the final affine transform found for tile t , round r , channel c . dict debug_info - dict containing 7 np.ndarray - n_matches - int [n_tiles x n_rounds x n_channels] . Number of matches found for each transform. error - float [n_tiles x n_rounds x n_channels] . Average distance between neighbours below dist_thresh . failed - bool [n_tiles x n_rounds x n_channels] . Indicates tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating av_scalings or av_shifts . is_converged - bool [n_tiles x n_rounds x n_channels] . False if max iterations reached before transform converged. av_scaling - float [n_channels x n_dim] . Chromatic aberration scaling factor to each channel from reference channel. Made using all rounds and tiles. av_shift - float [n_tiles x n_rounds x dim] . av_shift[t, r] is the average shift from reference round to round r for tile t across all colour channels. transforms_outlier - float [n_tiles x n_rounds x n_channels x dim+1 x dim] . transforms_outlier[t, r, c] is the final affine transform found for tile t , round r , channel c without regularization for t , r , c indicated by failed otherwise it is 0 . Source code in coppafish/register/base.py 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def icp ( yxz_base : np . ndarray , yxz_target : np . ndarray , transforms_initial : np . ndarray , n_iter : int , dist_thresh : float , matches_thresh : Union [ int , np . ndarray ], scale_dev_thresh : np . ndarray , shift_dev_thresh : np . ndarray , reg_constant_scale : Optional [ float ] = None , reg_constant_shift : Optional [ float ] = None ) -> Tuple [ np . ndarray , dict ]: \"\"\" This gets the affine `transforms` from `yxz_base` to `yxz_target` using iterative closest point until all iterations used or convergence. For `transforms` that have matches below `matches_thresh` or are anomalous compared to `av_transform`, the `transforms` are recomputed using regularized least squares to ensure they are close to the `av_transform`. If either `reg_constant_rot = None` or `reg_constant_shift = None` then this is not done. Args: yxz_base: `object [n_tiles]`. `yxz_base[t]` is a numpy `float` array `[n_base_spots x dim]`. coordinates of spots on reference round of tile `t`. yxz_target: `object [n_tiles x n_rounds x n_channels]`. `yxz_target[t, r, c]` is a numpy `float` array `[n_target_spots x 3]`. coordinates of spots in tile `t`, round `r`, channel `c`. transforms_initial: `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms_initial[t, r, c]` is the starting affine transform for tile `t` from the reference image to round `r`, channel `c`. `transforms_initial[t, r, c, :dim, :dim]` is probably going to be the identity matrix. `transforms_initial[t, r, c, dim, :]` is the shift which needs to be pre-computed somehow to get a good result. n_iter: Max number of iterations to perform of ICP. dist_thresh: If neighbours closer than this, they are used to compute the new transform. Typical: `3`. matches_thresh: `int [n_tiles x n_rounds x n_channels]` or single `int`. `n_matches` for a particular transform must exceed this to be used when computing `av_transform`. Can specify a single threshold for all transforms or a different threshold for each. E.g. you may give a lower threshold if that tile/round/channel has fewer spots. Typical: `200`. scale_dev_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the scaling to differ from the average scaling in each dimension. Typically, this threshold will be the same in all dimensions as expect chromatic aberration to be same in each. Threshold should be fairly large, it is just to get rid of crazy scalings which sometimes get a lot of matches. Typical: `0.01`. shift_dev_thresh: `float [n_dim]`. Specifies by how much it is acceptable for the shift to differ from the average shift in each dimension. Typically, this threshold will be the same in y and x but different in z. Typical: `10` xy pixels in xy direction, `2` z pixels in z direction (normalised to have same units as `yx_pixels`). reg_constant_scale: Constant used for scaling and rotation when doing regularized least squares. `None` means no regularized least squares performed. Typical = `5e8`. reg_constant_shift: Constant used for shift when doing regularized least squares. `None` means no regularized least squares performed. Typical = `500` Returns: - `transforms` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c`. - `debug_info` - `dict` containing 7 `np.ndarray` - - `n_matches` - `int [n_tiles x n_rounds x n_channels]`. Number of matches found for each transform. - `error` - `float [n_tiles x n_rounds x n_channels]`. Average distance between neighbours below `dist_thresh`. - `failed` - `bool [n_tiles x n_rounds x n_channels]`. Indicates tiles/rounds/channels to which transform had too few matches or transform was anomalous compared to median. These were not included when calculating `av_scalings` or `av_shifts`. - `is_converged` - `bool [n_tiles x n_rounds x n_channels]`. `False` if max iterations reached before transform converged. - `av_scaling` - `float [n_channels x n_dim]`. Chromatic aberration scaling factor to each channel from reference channel. Made using all rounds and tiles. - `av_shift` - `float [n_tiles x n_rounds x dim]`. `av_shift[t, r]` is the average shift from reference round to round `r` for tile `t` across all colour channels. - `transforms_outlier` - `float [n_tiles x n_rounds x n_channels x dim+1 x dim]`. `transforms_outlier[t, r, c]` is the final affine transform found for tile `t`, round `r`, channel `c` without regularization for `t`, `r`, `c` indicated by failed otherwise it is `0`. \"\"\" n_tiles , n_rounds , n_channels = yxz_target . shape if not utils . errors . check_shape ( yxz_base , [ n_tiles ]): raise utils . errors . ShapeError ( \"yxz_base\" , yxz_base . shape , ( n_tiles ,)) tree_target = np . zeros_like ( yxz_target ) for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): tree_target [ t , r , c ] = KDTree ( yxz_target [ t , r , c ]) n_matches = np . zeros_like ( yxz_target , dtype = int ) error = np . zeros_like ( yxz_target , dtype = float ) neighbour = np . zeros_like ( yxz_target ) is_converged = np . zeros_like ( yxz_target , dtype = bool ) transforms = transforms_initial . copy () . astype ( float ) transforms_outlier = np . zeros_like ( transforms ) finished_good_images = False av_transforms = None i_finished_good = 0 i = 0 with tqdm ( total = n_tiles * n_rounds * n_channels ) as pbar : pbar . set_description ( f \"Iterative Closest Point to find affine transforms\" ) while i < n_iter : pbar . set_postfix ({ 'iter' : f ' { i + 1 } / { n_iter } ' , 'regularized' : str ( finished_good_images )}) neighbour_last = neighbour . copy () for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): if is_converged [ t , r , c ]: continue if finished_good_images : reg_transform = av_transforms [ t , r , c ] else : reg_transform = None transforms [ t , r , c ], neighbour [ t , r , c ], n_matches [ t , r , c ], error [ t , r , c ] = \\ get_transform ( yxz_base [ t ], transforms [ t , r , c ], yxz_target [ t , r , c ], dist_thresh , tree_target [ t , r , c ], reg_constant_scale , reg_constant_shift , reg_transform ) if i > i_finished_good : is_converged [ t , r , c ] = np . abs ( neighbour [ t , r , c ] - neighbour_last [ t , r , c ]) . max () == 0 if is_converged [ t , r , c ]: pbar . update ( 1 ) if ( is_converged . all () and not finished_good_images ) or ( i == n_iter - 1 and not finished_good_images ): av_transforms , av_scaling , av_shifts , failed , failed_non_matches = \\ get_average_transform ( transforms , n_matches , matches_thresh , scale_dev_thresh , shift_dev_thresh ) if reg_constant_scale is not None and reg_constant_shift is not None : # reset transforms of those that failed to average transform as starting point for # regularised fitting transforms_outlier [ failed , :, :] = transforms [ failed , :, :] . copy () transforms [ failed , :, :] = av_transforms [ failed , :, :] is_converged [ failed ] = False i = - 1 # Allow n_iter to find regularized best transforms as well. finished_good_images = True pbar . update ( - np . sum ( failed . flatten ())) i += 1 if is_converged . all (): break pbar . close () if i == n_iter : warnings . warn ( f \"Max number of iterations, { n_iter } reached but only \" f \" { np . sum ( is_converged ) } / { np . sum ( is_converged >= 0 ) } transforms converged\" ) debug_info = { 'n_matches' : n_matches , 'error' : error , 'failed' : failed , 'is_converged' : is_converged , 'av_scaling' : av_scaling , 'av_shifts' : av_shifts , 'transforms_outlier' : transforms_outlier } return transforms , debug_info","title":"icp()"},{"location":"code/register/base/#coppafish.register.base.mod_median","text":"This computes the median ignoring values indicated by ignore . Parameters: Name Type Description Default array np . ndarray float [n_dim_1 x n_dim_2 x ... x n_dim_N] . array to compute median from. required ignore np . ndarray bool [n_dim_1 x n_dim_2 x ... x n_dim_N] . True for values in array that should not be used to compute median. required axis Union [ int , List [ int ]] int [n_axis_av] . Which axis to average over. 0 Returns: Type Description Union [ float , np . ndarray ] Median value without using those values indicated by ignore . Source code in coppafish/register/base.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def mod_median ( array : np . ndarray , ignore : np . ndarray , axis : Union [ int , List [ int ]] = 0 ) -> Union [ float , np . ndarray ]: \"\"\" This computes the median ignoring values indicated by ```ignore```. Args: array: ```float [n_dim_1 x n_dim_2 x ... x n_dim_N]```. array to compute median from. ignore: ```bool [n_dim_1 x n_dim_2 x ... x n_dim_N]```. True for values in array that should not be used to compute median. axis: ```int [n_axis_av]```. Which axis to average over. Returns: Median value without using those values indicated by ```ignore```. \"\"\" mod_array = array . copy () mod_array [ ignore ] = np . nan return np . nanmedian ( mod_array , axis = axis )","title":"mod_median()"},{"location":"code/register/base/#coppafish.register.base.transform_from_scale_shift","text":"Gets [dim+1 x dim] affine transform from scale for each channel and shift for each tile/round. Parameters: Name Type Description Default scale np . ndarray float [n_channels x n_dims] . scale[c, d] is the scaling to account for chromatic aberration from reference channel to channel c for dimension d . Typically, as an initial guess all values in scale will be 1 . required shift np . ndarray float [n_tiles x n_rounds x n_dims] . shift[t, r, d] is the shift to account for the shift between the reference round for tile t and round r for tile t in dimension d . required Returns: Type Description np . ndarray float [n_tiles x n_rounds x n_channels x dim+1 x dim] . [t, r, c] is the affine transform for tile t , round r , channel c computed from scale[c] and shift[t, r] . Source code in coppafish/register/base.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def transform_from_scale_shift ( scale : np . ndarray , shift : np . ndarray ) -> np . ndarray : \"\"\" Gets ```[dim+1 x dim]``` affine transform from scale for each channel and shift for each tile/round. Args: scale: ```float [n_channels x n_dims]```. ```scale[c, d]``` is the scaling to account for chromatic aberration from reference channel to channel ```c``` for dimension ```d```. Typically, as an initial guess all values in scale will be ```1```. shift: ```float [n_tiles x n_rounds x n_dims]```. ```shift[t, r, d]``` is the shift to account for the shift between the reference round for tile ```t``` and round ```r``` for tile ```t``` in dimension ```d```. Returns: ```float [n_tiles x n_rounds x n_channels x dim+1 x dim]```. ```[t, r, c]``` is the affine transform for tile ```t```, round ```r```, channel ```c``` computed from ```scale[c]``` and ```shift[t, r]```. \"\"\" n_channels = scale . shape [ 0 ] n_tiles , n_rounds , dim = shift . shape transforms = np . zeros (( n_tiles , n_rounds , n_channels , dim + 1 , dim )) for t in range ( n_tiles ): for r in range ( n_rounds ): for c in range ( n_channels ): transforms [ t , r , c , : dim , :, ] = np . eye ( dim ) * scale [ c ] transforms [ t , r , c , dim , :] = shift [ t , r ] return transforms","title":"transform_from_scale_shift()"},{"location":"code/register/check_transforms/","text":"check_transforms ( nb ) This checks that a decent number of affine transforms computed in the register stage of the pipeline are acceptable ( n_matches > n_matches_thresh ). If for any of the following, the fraction of transforms with n_matches < n_matches_thresh exceeds config['register']['n_transforms_error_fraction'] an error will be raised. Each channel across all rounds and tiles. Each tile across all rounds and channels. Each round across all tile and channels. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required Source code in coppafish/register/check_transforms.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def check_transforms ( nb : Notebook ): \"\"\" This checks that a decent number of affine transforms computed in the register stage of the pipeline are acceptable (`n_matches > n_matches_thresh`). If for any of the following, the fraction of transforms with `n_matches < n_matches_thresh` exceeds `config['register']['n_transforms_error_fraction']` an error will be raised. * Each channel across all rounds and tiles. * Each tile across all rounds and channels. * Each round across all tile and channels. Args: nb: *Notebook* containing `find_spots` page. \"\"\" config = nb . get_config ()[ 'register' ] use_tiles = np . asarray ( nb . basic_info . use_tiles ) use_rounds = np . asarray ( nb . basic_info . use_rounds ) use_channels = np . asarray ( nb . basic_info . use_channels ) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] error_message = \"\" # Consider bad channels first as most likely to have consistently failed transform n_transforms = len ( use_tiles ) * len ( use_rounds ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_channels ), dtype = int ) for c in range ( len ( use_channels )): failed = np . vstack ( np . where ( n_matches [:, :, c ] < n_matches_thresh [:, :, c ])) . T n_transforms_fail [ c ] = failed . shape [ 0 ] if n_transforms_fail [ c ] > 0 : failed_info = np . zeros (( n_transforms_fail [ c ], 4 ), dtype = int ) failed_info [:, 0 ] = use_tiles [ failed [:, 0 ]] failed_info [:, 1 ] = use_rounds [ failed [:, 1 ]] failed_info [:, 2 ] = n_matches [ failed [:, 0 ], failed [:, 1 ], c ] failed_info [:, 3 ] = n_matches_thresh [ failed [:, 0 ], failed [:, 1 ], c ] warnings . warn ( f \" \\n Channel { use_channels [ c ] } - { n_transforms_fail [ c ] } tiles/rounds with n_matches < \" f \"n_matches_thresh: \\n Information for failed transforms \\n \" f \"Tile, Round, n_matches, n_matches_thresh: \\n { failed_info } \" ) fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Channels that failed: { use_channels [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } tiles/rounds, these channels had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n tiles/rounds with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_channels.\" # don't consider failed channels for subsequent warnings/errors use_channels = np . setdiff1d ( use_channels , use_channels [ fail_inds ]) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad tiles next as second most likely to have consistently low spot counts in a tile n_transforms = len ( use_channels ) * len ( use_rounds ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_tiles ), dtype = int ) for t in range ( len ( use_tiles )): failed = np . vstack ( np . where ( n_matches [ t ] < n_matches_thresh [ t ])) . T n_transforms_fail [ t ] = failed . shape [ 0 ] fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Tiles that failed: { use_tiles [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } rounds/channels, these tiles had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n rounds/channels with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_tiles.\" # don't consider failed channels for subsequent warnings/errors use_tiles = np . setdiff1d ( use_tiles , use_tiles [ fail_inds ]) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad rounds last as least likely to have consistently low spot counts in a round n_transforms = len ( use_channels ) * len ( use_tiles ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_rounds ), dtype = int ) for r in range ( len ( use_rounds )): failed = np . vstack ( np . where ( n_matches [:, r ] < n_matches_thresh [:, r ])) . T n_transforms_fail [ r ] = failed . shape [ 0 ] fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Rounds that failed: { use_rounds [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } tiles/channels, these rounds had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n tiles/channels with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_rounds.\" if len ( error_message ) > 0 : error_message = error_message + f \" \\n Look at the following diagnostics to decide if transforms \" \\ f \"are acceptable to continue: \\n \" \\ f \"coppafish.plot.scale_box_plots \\n coppafish.plot.view_affine_shift_info \\n \" \\ f \"coppafish.plot.view_icp\" raise ValueError ( error_message )","title":"Check Transforms"},{"location":"code/register/check_transforms/#coppafish.register.check_transforms.check_transforms","text":"This checks that a decent number of affine transforms computed in the register stage of the pipeline are acceptable ( n_matches > n_matches_thresh ). If for any of the following, the fraction of transforms with n_matches < n_matches_thresh exceeds config['register']['n_transforms_error_fraction'] an error will be raised. Each channel across all rounds and tiles. Each tile across all rounds and channels. Each round across all tile and channels. Parameters: Name Type Description Default nb Notebook Notebook containing find_spots page. required Source code in coppafish/register/check_transforms.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def check_transforms ( nb : Notebook ): \"\"\" This checks that a decent number of affine transforms computed in the register stage of the pipeline are acceptable (`n_matches > n_matches_thresh`). If for any of the following, the fraction of transforms with `n_matches < n_matches_thresh` exceeds `config['register']['n_transforms_error_fraction']` an error will be raised. * Each channel across all rounds and tiles. * Each tile across all rounds and channels. * Each round across all tile and channels. Args: nb: *Notebook* containing `find_spots` page. \"\"\" config = nb . get_config ()[ 'register' ] use_tiles = np . asarray ( nb . basic_info . use_tiles ) use_rounds = np . asarray ( nb . basic_info . use_rounds ) use_channels = np . asarray ( nb . basic_info . use_channels ) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] error_message = \"\" # Consider bad channels first as most likely to have consistently failed transform n_transforms = len ( use_tiles ) * len ( use_rounds ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_channels ), dtype = int ) for c in range ( len ( use_channels )): failed = np . vstack ( np . where ( n_matches [:, :, c ] < n_matches_thresh [:, :, c ])) . T n_transforms_fail [ c ] = failed . shape [ 0 ] if n_transforms_fail [ c ] > 0 : failed_info = np . zeros (( n_transforms_fail [ c ], 4 ), dtype = int ) failed_info [:, 0 ] = use_tiles [ failed [:, 0 ]] failed_info [:, 1 ] = use_rounds [ failed [:, 1 ]] failed_info [:, 2 ] = n_matches [ failed [:, 0 ], failed [:, 1 ], c ] failed_info [:, 3 ] = n_matches_thresh [ failed [:, 0 ], failed [:, 1 ], c ] warnings . warn ( f \" \\n Channel { use_channels [ c ] } - { n_transforms_fail [ c ] } tiles/rounds with n_matches < \" f \"n_matches_thresh: \\n Information for failed transforms \\n \" f \"Tile, Round, n_matches, n_matches_thresh: \\n { failed_info } \" ) fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Channels that failed: { use_channels [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } tiles/rounds, these channels had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n tiles/rounds with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_channels.\" # don't consider failed channels for subsequent warnings/errors use_channels = np . setdiff1d ( use_channels , use_channels [ fail_inds ]) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad tiles next as second most likely to have consistently low spot counts in a tile n_transforms = len ( use_channels ) * len ( use_rounds ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_tiles ), dtype = int ) for t in range ( len ( use_tiles )): failed = np . vstack ( np . where ( n_matches [ t ] < n_matches_thresh [ t ])) . T n_transforms_fail [ t ] = failed . shape [ 0 ] fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Tiles that failed: { use_tiles [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } rounds/channels, these tiles had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n rounds/channels with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_tiles.\" # don't consider failed channels for subsequent warnings/errors use_tiles = np . setdiff1d ( use_tiles , use_tiles [ fail_inds ]) n_matches = nb . register_debug . n_matches [ np . ix_ ( use_tiles , use_rounds , use_channels )] n_matches_thresh = nb . register_debug . n_matches_thresh [ np . ix_ ( use_tiles , use_rounds , use_channels )] # Consider bad rounds last as least likely to have consistently low spot counts in a round n_transforms = len ( use_channels ) * len ( use_tiles ) n_transforms_error = int ( np . clip ( np . floor ( n_transforms * config [ 'n_transforms_error_fraction' ]), 1 , np . inf )) n_transforms_fail = np . zeros ( len ( use_rounds ), dtype = int ) for r in range ( len ( use_rounds )): failed = np . vstack ( np . where ( n_matches [:, r ] < n_matches_thresh [:, r ])) . T n_transforms_fail [ r ] = failed . shape [ 0 ] fail_inds = np . where ( n_transforms_fail >= n_transforms_error )[ 0 ] if len ( fail_inds ) > 0 : error_message = error_message + f \" \\n Rounds that failed: { use_rounds [ fail_inds ] } \\n \" \\ f \"This is because out of { n_transforms } tiles/channels, these rounds had \" \\ f \"respectively: \\n { n_transforms_fail [ fail_inds ] } \\n tiles/channels with \" \\ f \"n_matches < n_matches_thresh. \" \\ f \"These are all more than the error threshold of \" \\ f \" { n_transforms_error } . \\n Consider removing these from use_rounds.\" if len ( error_message ) > 0 : error_message = error_message + f \" \\n Look at the following diagnostics to decide if transforms \" \\ f \"are acceptable to continue: \\n \" \\ f \"coppafish.plot.scale_box_plots \\n coppafish.plot.view_affine_shift_info \\n \" \\ f \"coppafish.plot.view_icp\" raise ValueError ( error_message )","title":"check_transforms()"},{"location":"code/setup/file_names/","text":"set_file_names ( nb , nbp ) Function to set add file_names page to notebook. It requires notebook to be able to access a config file containing a file_names section and also the notebook to contain a basic_info page. Note This will be called every time the notebook is loaded to deal will case when file_names section of config file changed. Parameters: Name Type Description Default nb Notebook containing at least the basic_info page. required nbp NotebookPage with no variables added. This is just to avoid initialising it within the function which would cause a circular import. required Source code in coppafish/setup/file_names.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def set_file_names ( nb , nbp ): \"\"\" Function to set add `file_names` page to notebook. It requires notebook to be able to access a config file containing a `file_names` section and also the notebook to contain a `basic_info` page. !!! note This will be called every time the notebook is loaded to deal will case when `file_names` section of config file changed. Args: nb: *Notebook* containing at least the `basic_info` page. nbp: *NotebookPage* with no variables added. This is just to avoid initialising it within the function which would cause a circular import. \"\"\" config = nb . get_config ()[ 'file_names' ] nbp . name = 'file_names' # make sure name is correct # Copy some variables that are in config to page. nbp . input_dir = config [ 'input_dir' ] nbp . output_dir = config [ 'output_dir' ] nbp . tile_dir = config [ 'tile_dir' ] # remove file extension from round and anchor file names if it is present if config [ 'round' ] is None : if config [ 'anchor' ] is None : raise ValueError ( f 'Neither imaging rounds nor anchor_round provided' ) config [ 'round' ] = [] # Sometimes the case where just want to run the anchor round. config [ 'round' ] = [ r . replace ( config [ 'raw_extension' ], '' ) for r in config [ 'round' ]] nbp . round = config [ 'round' ] if config [ 'anchor' ] is not None : config [ 'anchor' ] = config [ 'anchor' ] . replace ( config [ 'raw_extension' ], '' ) nbp . anchor = config [ 'anchor' ] nbp . raw_extension = config [ 'raw_extension' ] nbp . raw_metadata = config [ 'raw_metadata' ] if config [ 'dye_camera_laser' ] is None : # Default information is project config [ 'dye_camera_laser' ] = os . path . join ( os . path . dirname ( __file__ ), 'dye_camera_laser_raw_intensity.csv' ) nbp . dye_camera_laser = config [ 'dye_camera_laser' ] config [ 'code_book' ] = config [ 'code_book' ] . replace ( '.txt' , '' ) nbp . code_book = config [ 'code_book' ] + '.txt' # where to save scale and scale_anchor values used in extract step. config [ 'scale' ] = config [ 'scale' ] . replace ( '.txt' , '' ) nbp . scale = os . path . join ( config [ 'tile_dir' ], config [ 'scale' ] + '.txt' ) # where to save psf, indicating average spot shape in raw image. Only ever needed in 3D. if nb . basic_info . is_3d : config [ 'psf' ] = config [ 'psf' ] . replace ( '.npy' , '' ) nbp . psf = os . path . join ( config [ 'output_dir' ], config [ 'psf' ] + '.npy' ) else : nbp . psf = None # where to save omp_spot_shape, indicating average spot shape in omp coefficient sign images. config [ 'omp_spot_shape' ] = config [ 'omp_spot_shape' ] . replace ( '.npy' , '' ) omp_spot_shape_file = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_shape' ] + '.npy' ) nbp . omp_spot_shape = omp_spot_shape_file # Add files so save omp results after each tile as security if hit any bugs config [ 'omp_spot_info' ] = config [ 'omp_spot_info' ] . replace ( '.npy' , '' ) nbp . omp_spot_info = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_info' ] + '.npy' ) config [ 'omp_spot_coef' ] = config [ 'omp_spot_coef' ] . replace ( '.npz' , '' ) nbp . omp_spot_coef = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_coef' ] + '.npz' ) # Add files so save plotting information for pciseq config [ 'pciseq' ] = [ val . replace ( '.csv' , '' ) for val in config [ 'pciseq' ]] nbp . pciseq = [ os . path . join ( config [ 'output_dir' ], val + '.csv' ) for val in config [ 'pciseq' ]] # add dapi channel and anchor channel to notebook even if set to None. if config [ 'big_dapi_image' ] is None : nbp . big_dapi_image = None else : config [ 'big_dapi_image' ] = config [ 'big_dapi_image' ] . replace ( '.npz' , '' ) if nb . basic_info . dapi_channel is None : nbp . big_dapi_image = None else : nbp . big_dapi_image = os . path . join ( config [ 'output_dir' ], config [ 'big_dapi_image' ] + '.npz' ) if config [ 'big_anchor_image' ] is None : nbp . big_anchor_image = None else : config [ 'big_anchor_image' ] = config [ 'big_anchor_image' ] . replace ( '.npz' , '' ) nbp . big_anchor_image = os . path . join ( config [ 'output_dir' ], config [ 'big_anchor_image' ] + '.npz' ) if config [ 'anchor' ] is not None : round_files = config [ 'round' ] + [ config [ 'anchor' ]] else : round_files = config [ 'round' ] if nb . basic_info . is_3d : tile_names = get_tile_file_names ( config [ 'tile_dir' ], round_files , nb . basic_info . n_tiles , nb . basic_info . n_channels ) else : tile_names = get_tile_file_names ( config [ 'tile_dir' ], round_files , nb . basic_info . n_tiles ) nbp . tile = tile_names . tolist () # npy tile file paths list [n_tiles x n_rounds (x n_channels if 3D)] nb += nbp","title":"File Names"},{"location":"code/setup/file_names/#coppafish.setup.file_names.set_file_names","text":"Function to set add file_names page to notebook. It requires notebook to be able to access a config file containing a file_names section and also the notebook to contain a basic_info page. Note This will be called every time the notebook is loaded to deal will case when file_names section of config file changed. Parameters: Name Type Description Default nb Notebook containing at least the basic_info page. required nbp NotebookPage with no variables added. This is just to avoid initialising it within the function which would cause a circular import. required Source code in coppafish/setup/file_names.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def set_file_names ( nb , nbp ): \"\"\" Function to set add `file_names` page to notebook. It requires notebook to be able to access a config file containing a `file_names` section and also the notebook to contain a `basic_info` page. !!! note This will be called every time the notebook is loaded to deal will case when `file_names` section of config file changed. Args: nb: *Notebook* containing at least the `basic_info` page. nbp: *NotebookPage* with no variables added. This is just to avoid initialising it within the function which would cause a circular import. \"\"\" config = nb . get_config ()[ 'file_names' ] nbp . name = 'file_names' # make sure name is correct # Copy some variables that are in config to page. nbp . input_dir = config [ 'input_dir' ] nbp . output_dir = config [ 'output_dir' ] nbp . tile_dir = config [ 'tile_dir' ] # remove file extension from round and anchor file names if it is present if config [ 'round' ] is None : if config [ 'anchor' ] is None : raise ValueError ( f 'Neither imaging rounds nor anchor_round provided' ) config [ 'round' ] = [] # Sometimes the case where just want to run the anchor round. config [ 'round' ] = [ r . replace ( config [ 'raw_extension' ], '' ) for r in config [ 'round' ]] nbp . round = config [ 'round' ] if config [ 'anchor' ] is not None : config [ 'anchor' ] = config [ 'anchor' ] . replace ( config [ 'raw_extension' ], '' ) nbp . anchor = config [ 'anchor' ] nbp . raw_extension = config [ 'raw_extension' ] nbp . raw_metadata = config [ 'raw_metadata' ] if config [ 'dye_camera_laser' ] is None : # Default information is project config [ 'dye_camera_laser' ] = os . path . join ( os . path . dirname ( __file__ ), 'dye_camera_laser_raw_intensity.csv' ) nbp . dye_camera_laser = config [ 'dye_camera_laser' ] config [ 'code_book' ] = config [ 'code_book' ] . replace ( '.txt' , '' ) nbp . code_book = config [ 'code_book' ] + '.txt' # where to save scale and scale_anchor values used in extract step. config [ 'scale' ] = config [ 'scale' ] . replace ( '.txt' , '' ) nbp . scale = os . path . join ( config [ 'tile_dir' ], config [ 'scale' ] + '.txt' ) # where to save psf, indicating average spot shape in raw image. Only ever needed in 3D. if nb . basic_info . is_3d : config [ 'psf' ] = config [ 'psf' ] . replace ( '.npy' , '' ) nbp . psf = os . path . join ( config [ 'output_dir' ], config [ 'psf' ] + '.npy' ) else : nbp . psf = None # where to save omp_spot_shape, indicating average spot shape in omp coefficient sign images. config [ 'omp_spot_shape' ] = config [ 'omp_spot_shape' ] . replace ( '.npy' , '' ) omp_spot_shape_file = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_shape' ] + '.npy' ) nbp . omp_spot_shape = omp_spot_shape_file # Add files so save omp results after each tile as security if hit any bugs config [ 'omp_spot_info' ] = config [ 'omp_spot_info' ] . replace ( '.npy' , '' ) nbp . omp_spot_info = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_info' ] + '.npy' ) config [ 'omp_spot_coef' ] = config [ 'omp_spot_coef' ] . replace ( '.npz' , '' ) nbp . omp_spot_coef = os . path . join ( config [ 'output_dir' ], config [ 'omp_spot_coef' ] + '.npz' ) # Add files so save plotting information for pciseq config [ 'pciseq' ] = [ val . replace ( '.csv' , '' ) for val in config [ 'pciseq' ]] nbp . pciseq = [ os . path . join ( config [ 'output_dir' ], val + '.csv' ) for val in config [ 'pciseq' ]] # add dapi channel and anchor channel to notebook even if set to None. if config [ 'big_dapi_image' ] is None : nbp . big_dapi_image = None else : config [ 'big_dapi_image' ] = config [ 'big_dapi_image' ] . replace ( '.npz' , '' ) if nb . basic_info . dapi_channel is None : nbp . big_dapi_image = None else : nbp . big_dapi_image = os . path . join ( config [ 'output_dir' ], config [ 'big_dapi_image' ] + '.npz' ) if config [ 'big_anchor_image' ] is None : nbp . big_anchor_image = None else : config [ 'big_anchor_image' ] = config [ 'big_anchor_image' ] . replace ( '.npz' , '' ) nbp . big_anchor_image = os . path . join ( config [ 'output_dir' ], config [ 'big_anchor_image' ] + '.npz' ) if config [ 'anchor' ] is not None : round_files = config [ 'round' ] + [ config [ 'anchor' ]] else : round_files = config [ 'round' ] if nb . basic_info . is_3d : tile_names = get_tile_file_names ( config [ 'tile_dir' ], round_files , nb . basic_info . n_tiles , nb . basic_info . n_channels ) else : tile_names = get_tile_file_names ( config [ 'tile_dir' ], round_files , nb . basic_info . n_tiles ) nbp . tile = tile_names . tolist () # npy tile file paths list [n_tiles x n_rounds (x n_channels if 3D)] nb += nbp","title":"set_file_names()"},{"location":"code/setup/notebook/","text":"Notebook A write-only file-synchronized class to keep track of coppaFISH results. The Notebook object stores all of the outputs of the script. Almost all information saved in the Notebook is encapsulated within \"pages\" , from the NotebookPage object. To add a NotebookPage object to a Notebook , use the \"add_page\" method. In addition to saving pages, it also saves the contents of the config file, and the time at which the notebook and each page was created. To create a Notebook , pass it the path to the file where the Notebook is to be stored ( notebook_file ), and optionally, the path to the configuration file ( config_file ). If notebook_file already exists, the notebook located at this path will be loaded. If not, a new file will be created as soon as the first data is written to the Notebook . Example With config_file No config_file nb = Notebook ( \"nbfile.npz\" , \"config_file.ini\" ) nbp = NotebookPage ( \"pagename\" ) nbp . var = 1 nb . add_page ( nbp ) or nb += nbp or nb . pagename = nbp assert nb . pagename . var == 1 nb = Notebook ( \"nbfile.npz\" ) nbp = NotebookPage ( \"pagename\" ) nbp . var = 1 nb . add_page ( nbp ) or nb += nbp or nb . pagename = nbp assert nb . pagename . var == 1 Because it is automatically saved to the disk, you can close Python, reopen it, and do the following (Once config_file , added to notebook there is no need to load it again unless it has been changed): nb2 = Notebook ( \"nbfile.npz\" ) assert nb2 . pagename . var == 1 If you create a notebook without specifying notebook_file , i.e. nb = Notebook(config_file=\"config_file.ini\") , the notebook_file will be set to: notebook_file = config [ 'file_names' ][ 'output_dir' ] + config [ 'file_names' ][ 'notebook_name' ]) On using config_file When running the coppafish pipeline, the Notebook requires a config_file to access information required for the different stages of the pipeline through nb.get_config() . But if using the Notebook to store information not in coppafish pipeline, it is not needed. Source code in coppafish/setup/notebook.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 class Notebook : \"\"\"A write-only file-synchronized class to keep track of *coppaFISH* results. The `Notebook` object stores all of the outputs of the script. Almost all information saved in the `Notebook` is encapsulated within `\"pages\"`, from the `NotebookPage` object. To add a `NotebookPage` object to a `Notebook`, use the `\"add_page\"` method. In addition to saving pages, it also saves the contents of the config file, and the time at which the notebook and each page was created. To create a `Notebook`, pass it the path to the file where the `Notebook` is to be stored (`notebook_file`), and optionally, the path to the configuration file (`config_file`). If `notebook_file` already exists, the notebook located at this path will be loaded. If not, a new file will be created as soon as the first data is written to the `Notebook`. !!!example === \"With config_file\" ``` python nb = Notebook(\"nbfile.npz\", \"config_file.ini\") nbp = NotebookPage(\"pagename\") nbp.var = 1 nb.add_page(nbp) or nb += nbp or nb.pagename = nbp assert nb.pagename.var == 1 ``` === \"No config_file\" ``` python nb = Notebook(\"nbfile.npz\") nbp = NotebookPage(\"pagename\") nbp.var = 1 nb.add_page(nbp) or nb += nbp or nb.pagename = nbp assert nb.pagename.var == 1 ``` Because it is automatically saved to the disk, you can close Python, reopen it, and do the following (Once `config_file`, added to notebook there is no need to load it again unless it has been changed): ```python nb2 = Notebook(\"nbfile.npz\") assert nb2.pagename.var == 1 ``` If you create a notebook without specifying `notebook_file`, i.e. ```nb = Notebook(config_file=\"config_file.ini\")```, the `notebook_file` will be set to: ```python notebook_file = config['file_names']['output_dir'] + config['file_names']['notebook_name']) ``` !!!note \"On using config_file\" When running the coppafish pipeline, the `Notebook` requires a `config_file` to access information required for the different stages of the pipeline through `nb.get_config()`. But if using the `Notebook` to store information not in coppafish pipeline, it is not needed. \"\"\" _SEP = \"_-_\" # Separator between notebook page name and item name when saving to file _ADDEDMETA = \"TIME_CREATED\" # Key for notebook created time _CONFIGMETA = \"CONFIGFILE\" # Key for config string _NBMETA = \"NOTEBOOKMETA\" # Key for metadata about the entire notebook # If these sections of config files are different, will not raise error. _no_compare_config_sections = [ 'file_names' ] # When the pages corresponding to the keys are added, a save will not be triggered. # When save does happen, these pages won't be saved, but made on loading using # the corresponding function, load_func, if the notebook contains the pages indicated by # load_func_req. # load_func must only take notebook and page_name as input and has no output but page will be added to notebook. # When last of pages in load_func_req have been added, the page will automatically be added. _no_save_pages = { 'file_names' : { 'load_func' : load_file_names , 'load_func_req' : [ 'basic_info' ]}} def __init__ ( self , notebook_file : Optional [ str ] = None , config_file : Optional [ str ] = None ): # Give option to load with config_file as None so don't have to supply ini_file location every time if # already initialised. # Also, can provide config_file if file_names section changed. # Don't need to provide notebook_file as can determine this from config_file as: # config['file_names']['output_dir'] + config['file_names']['notebook_name'] # numpy isn't compatible with npz files which do not end in the suffix # .npz. If one isn't there, it will add the extension automatically. # We do the same thing here. object . __setattr__ ( self , '_page_times' , {}) if notebook_file is None : if config_file is None : raise ValueError ( 'Both notebook_file and config_file are None' ) else : config_file_names = get_config ( config_file )[ 'file_names' ] notebook_file = os . path . join ( config_file_names [ 'output_dir' ], config_file_names [ 'notebook_name' ]) if not os . path . isdir ( config_file_names [ 'output_dir' ]): raise ValueError ( f \" \\n config['file_names']['output_dir'] = { config_file_names [ 'output_dir' ] } \\n \" f \"is not a valid directory.\" ) if not notebook_file . endswith ( \".npz\" ): notebook_file = notebook_file + \".npz\" # Note that the ordering of _pages may change across saves and loads, # but the order will always correspond to the order of _pages_times self . _file = notebook_file self . _config_file = config_file # Read the config file, but don't assign anything yet. Here, we just # save a copy of the config file. This isn't the main place the config # file should be read from. if config_file is not None : if os . path . isfile ( str ( config_file )): with open ( config_file , 'r' ) as f : read_config = f . read () else : raise ValueError ( f 'Config file given is not valid: { config_file } ' ) else : read_config = None # If the file already exists, initialize the Notebook object from this # file. Otherwise, initialize it empty. if os . path . isfile ( self . _file ): pages , self . _page_times , self . _created_time , self . _config = self . from_file ( self . _file ) for page in pages : object . __setattr__ ( self , page . name , page ) # don't want to set page_time hence use object setattr if read_config is not None : if not self . compare_config ( get_config ( read_config )): raise SystemError ( \"Passed config file is not the same as the saved config file\" ) self . _config = read_config # update config to new one - only difference will be in file_names section self . add_no_save_pages () # add file_names page with new config else : warnings . warn ( \"Notebook file not found, creating a new notebook.\" ) if read_config is None : warnings . warn ( \"Have not passed a config_file so Notebook.get_config() won't work.\" ) self . _created_time = time . time () self . _config = read_config def __repr__ ( self ): # This means that print(nb) gives file location of notebook and # pages in the notebook sorted by time added to the notebook. sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] n_names_per_line = 4 i = n_names_per_line - 1 while i < len ( page_names ) - n_names_per_line / 2 : page_names [ i + 1 ] = \" \\n \" + page_names [ i + 1 ] i = i + n_names_per_line page_names = \", \" . join ( page_names ) return f \"File: { self . _file } \\n Pages: { page_names } \" def get_config ( self ): \"\"\" Returns config as dictionary. \"\"\" if self . _config is not None : return get_config ( self . _config ) else : raise ValueError ( 'Notebook does not contain config parameter.' ) def compare_config ( self , config_2 : dict ) -> bool : \"\"\" Compares whether `config_2` is equal to the config file saved in the notebook. Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook will be checked. Args: config_2: Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. `config_2['basic_info]['param1'] = 5`. Returns: `True` if config dictionaries are equal in required sections. \"\"\" # TODO: issue here that if default settings file changed, the equality here would still be true. config = self . get_config () is_equal = True if config . keys () != config_2 . keys (): warnings . warn ( 'The config files have different sections.' ) is_equal = False else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook # page names are either same as config sections or with _debug suffix page_names = [ name [ 0 ] . replace ( '_debug' , '' ) for name in sort_page_names ] for section in config . keys (): # Only compare sections for which there is a corresponding page in the notebook. if section not in self . _no_compare_config_sections and section in page_names : if config [ section ] != config_2 [ section ]: warnings . warn ( f \"The { section } section of the two config files differ.\" ) is_equal = False return is_equal def describe ( self , key = None ): \"\"\" `describe(var)` will print comments for variables called `var` in each `NotebookPage`. \"\"\" if key is None : print ( self . __repr__ ()) elif len ( self . _page_times ) == 0 : print ( f \"No pages so cannot search for variable { key } \" ) else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] first_page = self . __getattribute__ ( page_names [ 0 ]) with open ( first_page . _comments_file ) as f : json_comments = json . load ( f ) if self . _config is not None : config = self . get_config () n_times_appeared = 0 for page_name in page_names : # if in comments file, then print the comment if key in json_comments [ page_name ]: print ( f \" { key } in { page_name } :\" ) self . __getattribute__ ( page_name ) . describe ( key ) print ( \"\" ) n_times_appeared += 1 elif self . _config is not None : # if in config file, then print the comment # find sections in config file with matching name to current page config_sections_with_name = [ page_name . find ( list ( config . keys ())[ i ]) for i in range ( len ( config . keys ()))] config_sections = np . array ( list ( config . keys ()))[ np . array ( config_sections_with_name ) != - 1 ] for section in config_sections : for param in config [ section ] . keys (): if param . lower () == key . lower (): print ( f \"No variable named { key } in the { page_name } page. \\n \" f \"But it is in the { section } section of the config file and has value: \\n \" f \" { config [ section ][ param ] } \\n \" ) n_times_appeared += 1 if n_times_appeared == 0 : print ( f \" { key } is not in any of the pages in this notebook.\" ) def __eq__ ( self , other ): # Test if two `Notebooks` are identical # # For two `Notebooks` to be identical, all aspects must be the same, # excluding the ordering of the pages, and the filename. All timestamps # must also be identical. if self . _created_time != other . _created_time : return False if self . _config != other . _config : return False if len ( self . _page_times ) != len ( other . _page_times ): return False for k in self . _page_times . keys (): if k not in other . _page_times or getattr ( self , k ) != getattr ( other , k ): return False for k in other . _page_times . keys (): if k not in self . _page_times or getattr ( other , k ) != getattr ( self , k ): return False for k , v in self . _page_times . items (): if k not in other . _page_times or v != other . _page_times [ k ]: return False return True def __len__ ( self ): # Return the number of pages in the Notebook return len ( self . _page_times ) def __setattr__ ( self , key , value ): # Deals with the syntax `nb.key = value` # automatically triggers save if `NotebookPage` is added. # If adding something other than a `NotebookPage`, this syntax does exactly as it is for other classes. if isinstance ( value , NotebookPage ): if self . _SEP in key : raise NameError ( f \"The separator { self . _SEP } may not be in the page's name\" ) if value . finalized : raise ValueError ( \"Page already added to a Notebook, cannot add twice\" ) if key in self . _page_times . keys (): raise ValueError ( \"Cannot add two pages with the same name\" ) if value . name != key : raise ValueError ( f \"Page name is { value . name } but key given is { key } \" ) # ensure all the variables in the comments file are included with open ( value . _comments_file ) as f : json_comments = json . load ( f ) if value . name in json_comments : for var in json_comments [ value . name ]: if var not in value . _times and var != \"DESCRIPTION\" : raise InvalidNotebookPageError ( None , var , value . name ) # ensure all variables in page are in comments file for var in value . _times : if var not in json_comments [ value . name ]: raise InvalidNotebookPageError ( var , None , value . name ) value . finalized = True object . __setattr__ ( self , key , value ) self . _page_times [ key ] = time . time () if value . name not in self . _no_save_pages . keys (): self . save () self . add_no_save_pages () elif key in self . _page_times . keys (): raise ValueError ( f \"Page with name { key } in notebook so can't add variable with this name.\" ) else : object . __setattr__ ( self , key , value ) def __delattr__ ( self , name ): # Method to delete a page or attribute. Deals with del nb.name. object . __delattr__ ( self , name ) if name in self . _page_times : # extra bit if page del self . _page_times [ name ] def add_page ( self , page ): \"\"\"Insert the page `page` into the `Notebook`. This function automatically triggers a save. \"\"\" if not isinstance ( page , NotebookPage ): raise ValueError ( \"Only NotebookPage objects may be added to a notebook.\" ) self . __setattr__ ( page . name , page ) def has_page ( self , page_name ): \"\"\"A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present.\"\"\" if isinstance ( page_name , str ): output = any ( page_name == p for p in self . _page_times ) elif isinstance ( page_name , list ): output = [ any ( page_name [ i ] == p for p in self . _page_times ) for i in range ( len ( page_name ))] else : raise ValueError ( f \"page_name given was { page_name } . This is not a list or a string.\" ) return output def __iadd__ ( self , other ): # Syntactic sugar for the add_page method self . add_page ( other ) return self def add_no_save_pages ( self ): \"\"\" This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']` by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`. At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page has been added. \"\"\" for page_name in self . _no_save_pages . keys (): if self . has_page ( page_name ): continue if all ( self . has_page ( self . _no_save_pages [ page_name ][ 'load_func_req' ])): # If contains all required pages to run load_func, then add the page self . _no_save_pages [ page_name ][ 'load_func' ]( self , page_name ) def change_page_name ( self , old_name : str , new_name : str ): \"\"\" This changes the name of the page `old_name` to `new_name`. It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Args: old_name: new_name: \"\"\" nbp = self . __getattribute__ ( old_name ) warnings . warn ( f \"Changing name of { old_name } page to { new_name } \" ) time_added = self . _page_times [ old_name ] nbp . finalized = False nbp . name = new_name self . __delattr__ ( old_name ) self . add_page ( nbp ) self . _page_times [ new_name ] = time_added # set time to time page initially added self . save () def version_hash ( self ): # A short string representing the file version. # # Since there are many possible page names and entry names within those # pages, that means there are many, many possible file versions based on # different versions of the code. Rather than try to keep track of these # versions and appropriately increment some centralized counter, we # generate a short string which is a hash of the page names and the names # of the entries in that page. This way, it is possible to see if two # notebooks were generated using the same version of the software. (Of # course, it assumes that no fields are ever set conditionally.) s = \"\" for p_name in self . _page_times : s += p_name + \" \\n\\n \" page = getattr ( self , p_name ) s += \" \\n \" . join ( sorted ( page . _times . keys ())) return hashlib . md5 ( bytes ( s , \"utf8\" )) . hexdigest () def save ( self , file : Optional [ str ] = None ): \"\"\" Saves Notebook as a npz file at the path indicated by `file`. Args: file: Where to save *Notebook*. If `None`, will use `self._file`. \"\"\" \"\"\"Save the Notebook to a file\"\"\" if file is not None : if not file . endswith ( \".npz\" ): file = file + \".npz\" self . _file = file d = {} # Diagnostic information about how long the save took. We can probably # take this out, or else set it at a higher debug level via warnings # module. save_start_time = time . time () for p_name in self . _page_times . keys (): if p_name in self . _no_save_pages . keys (): continue p = getattr ( self , p_name ) pd = p . to_serial_dict () for k , v in pd . items (): if v is None : # save None objects as string then convert back to None on loading v = str ( v ) d [ p_name + self . _SEP + k ] = v d [ p_name + self . _SEP + self . _ADDEDMETA ] = self . _page_times [ p_name ] d [ self . _NBMETA + self . _SEP + self . _ADDEDMETA ] = self . _created_time if self . _config is not None : d [ self . _NBMETA + self . _SEP + self . _CONFIGMETA ] = self . _config np . savez_compressed ( self . _file , ** d ) # Finishing the diagnostics described above print ( f \"Notebook saved: took { time . time () - save_start_time } seconds\" ) def from_file ( self , fn : str ) -> Tuple [ List , dict , float , str ]: \"\"\" Read a `Notebook` from a file Args: fn: Filename of the saved `Notebook` to load. Returns: A list of `NotebookPage` objects A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and keys are `page.name` A timestamp for the time the `Notebook` was created. A string of the config file \"\"\" # Right now we won't use lazy loading. One problem with lazy loading # is that we must keep the file handle open. We would rather not do # this, because if we write to the file, it will get screwed up, and if # there is a network issue, it will also mess things up. I can't # imagine that loading the notebook will be a performance bottleneck, # but if it is, we can rethink this decision. It should be pretty easy # to lazy load the pages, but eager load everything in the page. f = np . load ( fn ) keys = list ( f . keys ()) page_items = {} page_times = {} created_time = None config_str = None # If no config saved, will stay as None. Otherwise, will be the config in str form. for pk in keys : p , k = pk . split ( self . _SEP , 1 ) if p in self . _no_save_pages . keys (): # This is to deal with the legacy case from old code where a no_save_page has been saved. # If this is the case, don't load in this page. continue if p == self . _NBMETA : if k == self . _ADDEDMETA : created_time = float ( f [ pk ]) continue if k == self . _CONFIGMETA : config_str = str ( f [ pk ]) continue if k == self . _ADDEDMETA : page_times [ p ] = float ( f [ pk ]) continue if p not in page_items . keys (): page_items [ p ] = {} page_items [ p ][ k ] = f [ pk ] pages = [ NotebookPage . from_serial_dict ( page_items [ d ]) for d in sorted ( page_items . keys ())] for page in pages : page . finalized = True # if loading from file, then all pages are final assert len ( pages ) == len ( page_times ), \"Invalid file, lengths don't match\" assert created_time is not None , \"Invalid file, invalid created date\" return pages , page_times , created_time , config_str add_no_save_pages () This adds the page page_name listed in nb._no_save_pages to the notebook if the notebook already contains the pages listed in nb._no_save_pages['page_name']['load_func_req'] by running the function nb._no_save_pages['page_name']['load_func'](nb, 'page_name') . At the moment, this is only used to add the file_names page to the notebook as soon as the basic_info page has been added. Source code in coppafish/setup/notebook.py 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 def add_no_save_pages ( self ): \"\"\" This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']` by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`. At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page has been added. \"\"\" for page_name in self . _no_save_pages . keys (): if self . has_page ( page_name ): continue if all ( self . has_page ( self . _no_save_pages [ page_name ][ 'load_func_req' ])): # If contains all required pages to run load_func, then add the page self . _no_save_pages [ page_name ][ 'load_func' ]( self , page_name ) add_page ( page ) Insert the page page into the Notebook . This function automatically triggers a save. Source code in coppafish/setup/notebook.py 453 454 455 456 457 458 459 460 def add_page ( self , page ): \"\"\"Insert the page `page` into the `Notebook`. This function automatically triggers a save. \"\"\" if not isinstance ( page , NotebookPage ): raise ValueError ( \"Only NotebookPage objects may be added to a notebook.\" ) self . __setattr__ ( page . name , page ) change_page_name ( old_name , new_name ) This changes the name of the page old_name to new_name . It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Parameters: Name Type Description Default old_name str required new_name str required Source code in coppafish/setup/notebook.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def change_page_name ( self , old_name : str , new_name : str ): \"\"\" This changes the name of the page `old_name` to `new_name`. It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Args: old_name: new_name: \"\"\" nbp = self . __getattribute__ ( old_name ) warnings . warn ( f \"Changing name of { old_name } page to { new_name } \" ) time_added = self . _page_times [ old_name ] nbp . finalized = False nbp . name = new_name self . __delattr__ ( old_name ) self . add_page ( nbp ) self . _page_times [ new_name ] = time_added # set time to time page initially added self . save () compare_config ( config_2 ) Compares whether config_2 is equal to the config file saved in the notebook. Only sections not in _no_compare_config_sections and with a corresponding page saved to the notebook will be checked. Parameters: Name Type Description Default config_2 dict Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. config_2['basic_info]['param1'] = 5 . required Returns: Type Description bool True if config dictionaries are equal in required sections. Source code in coppafish/setup/notebook.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def compare_config ( self , config_2 : dict ) -> bool : \"\"\" Compares whether `config_2` is equal to the config file saved in the notebook. Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook will be checked. Args: config_2: Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. `config_2['basic_info]['param1'] = 5`. Returns: `True` if config dictionaries are equal in required sections. \"\"\" # TODO: issue here that if default settings file changed, the equality here would still be true. config = self . get_config () is_equal = True if config . keys () != config_2 . keys (): warnings . warn ( 'The config files have different sections.' ) is_equal = False else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook # page names are either same as config sections or with _debug suffix page_names = [ name [ 0 ] . replace ( '_debug' , '' ) for name in sort_page_names ] for section in config . keys (): # Only compare sections for which there is a corresponding page in the notebook. if section not in self . _no_compare_config_sections and section in page_names : if config [ section ] != config_2 [ section ]: warnings . warn ( f \"The { section } section of the two config files differ.\" ) is_equal = False return is_equal describe ( key = None ) describe(var) will print comments for variables called var in each NotebookPage . Source code in coppafish/setup/notebook.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def describe ( self , key = None ): \"\"\" `describe(var)` will print comments for variables called `var` in each `NotebookPage`. \"\"\" if key is None : print ( self . __repr__ ()) elif len ( self . _page_times ) == 0 : print ( f \"No pages so cannot search for variable { key } \" ) else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] first_page = self . __getattribute__ ( page_names [ 0 ]) with open ( first_page . _comments_file ) as f : json_comments = json . load ( f ) if self . _config is not None : config = self . get_config () n_times_appeared = 0 for page_name in page_names : # if in comments file, then print the comment if key in json_comments [ page_name ]: print ( f \" { key } in { page_name } :\" ) self . __getattribute__ ( page_name ) . describe ( key ) print ( \"\" ) n_times_appeared += 1 elif self . _config is not None : # if in config file, then print the comment # find sections in config file with matching name to current page config_sections_with_name = [ page_name . find ( list ( config . keys ())[ i ]) for i in range ( len ( config . keys ()))] config_sections = np . array ( list ( config . keys ()))[ np . array ( config_sections_with_name ) != - 1 ] for section in config_sections : for param in config [ section ] . keys (): if param . lower () == key . lower (): print ( f \"No variable named { key } in the { page_name } page. \\n \" f \"But it is in the { section } section of the config file and has value: \\n \" f \" { config [ section ][ param ] } \\n \" ) n_times_appeared += 1 if n_times_appeared == 0 : print ( f \" { key } is not in any of the pages in this notebook.\" ) from_file ( fn ) Read a Notebook from a file Parameters: Name Type Description Default fn str Filename of the saved Notebook to load. required Returns: Type Description List A list of NotebookPage objects dict A dictionary of timestamps, of identical length to the list of NotebookPage objects and keys are page.name float A timestamp for the time the Notebook was created. str A string of the config file Source code in coppafish/setup/notebook.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def from_file ( self , fn : str ) -> Tuple [ List , dict , float , str ]: \"\"\" Read a `Notebook` from a file Args: fn: Filename of the saved `Notebook` to load. Returns: A list of `NotebookPage` objects A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and keys are `page.name` A timestamp for the time the `Notebook` was created. A string of the config file \"\"\" # Right now we won't use lazy loading. One problem with lazy loading # is that we must keep the file handle open. We would rather not do # this, because if we write to the file, it will get screwed up, and if # there is a network issue, it will also mess things up. I can't # imagine that loading the notebook will be a performance bottleneck, # but if it is, we can rethink this decision. It should be pretty easy # to lazy load the pages, but eager load everything in the page. f = np . load ( fn ) keys = list ( f . keys ()) page_items = {} page_times = {} created_time = None config_str = None # If no config saved, will stay as None. Otherwise, will be the config in str form. for pk in keys : p , k = pk . split ( self . _SEP , 1 ) if p in self . _no_save_pages . keys (): # This is to deal with the legacy case from old code where a no_save_page has been saved. # If this is the case, don't load in this page. continue if p == self . _NBMETA : if k == self . _ADDEDMETA : created_time = float ( f [ pk ]) continue if k == self . _CONFIGMETA : config_str = str ( f [ pk ]) continue if k == self . _ADDEDMETA : page_times [ p ] = float ( f [ pk ]) continue if p not in page_items . keys (): page_items [ p ] = {} page_items [ p ][ k ] = f [ pk ] pages = [ NotebookPage . from_serial_dict ( page_items [ d ]) for d in sorted ( page_items . keys ())] for page in pages : page . finalized = True # if loading from file, then all pages are final assert len ( pages ) == len ( page_times ), \"Invalid file, lengths don't match\" assert created_time is not None , \"Invalid file, invalid created date\" return pages , page_times , created_time , config_str get_config () Returns config as dictionary. Source code in coppafish/setup/notebook.py 298 299 300 301 302 303 304 305 def get_config ( self ): \"\"\" Returns config as dictionary. \"\"\" if self . _config is not None : return get_config ( self . _config ) else : raise ValueError ( 'Notebook does not contain config parameter.' ) has_page ( page_name ) A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present. Source code in coppafish/setup/notebook.py 462 463 464 465 466 467 468 469 470 471 472 def has_page ( self , page_name ): \"\"\"A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present.\"\"\" if isinstance ( page_name , str ): output = any ( page_name == p for p in self . _page_times ) elif isinstance ( page_name , list ): output = [ any ( page_name [ i ] == p for p in self . _page_times ) for i in range ( len ( page_name ))] else : raise ValueError ( f \"page_name given was { page_name } . This is not a list or a string.\" ) return output save ( file = None ) Saves Notebook as a npz file at the path indicated by file . Parameters: Name Type Description Default file Optional [ str ] Where to save Notebook . If None , will use self._file . None Source code in coppafish/setup/notebook.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def save ( self , file : Optional [ str ] = None ): \"\"\" Saves Notebook as a npz file at the path indicated by `file`. Args: file: Where to save *Notebook*. If `None`, will use `self._file`. \"\"\" \"\"\"Save the Notebook to a file\"\"\" if file is not None : if not file . endswith ( \".npz\" ): file = file + \".npz\" self . _file = file d = {} # Diagnostic information about how long the save took. We can probably # take this out, or else set it at a higher debug level via warnings # module. save_start_time = time . time () for p_name in self . _page_times . keys (): if p_name in self . _no_save_pages . keys (): continue p = getattr ( self , p_name ) pd = p . to_serial_dict () for k , v in pd . items (): if v is None : # save None objects as string then convert back to None on loading v = str ( v ) d [ p_name + self . _SEP + k ] = v d [ p_name + self . _SEP + self . _ADDEDMETA ] = self . _page_times [ p_name ] d [ self . _NBMETA + self . _SEP + self . _ADDEDMETA ] = self . _created_time if self . _config is not None : d [ self . _NBMETA + self . _SEP + self . _CONFIGMETA ] = self . _config np . savez_compressed ( self . _file , ** d ) # Finishing the diagnostics described above print ( f \"Notebook saved: took { time . time () - save_start_time } seconds\" ) Notebook Page A page, to be added to a Notebook object Expected usage is for a NotebookPage to be created at the beginning of a large step in the analysis pipeline. The name of the page should reflect its function, and it will be used as the indexing key when it is added to a Notebook. The NotebookPage should be created at the beginning of the step in the pipeline, because then the timestamp will be more meaningful. As results are computed, they should be added. This will provide a timestamp for each of the results as well. Then, at the end, the pipeline step should return a NotebookPage , which can then be added to the Notebook . Example nbp = NotebookPage ( \"extract_and_filter\" ) nbp . scale_factor = 10 ... return nbp Source code in coppafish/setup/notebook.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 class NotebookPage : \"\"\"A page, to be added to a `Notebook` object Expected usage is for a `NotebookPage` to be created at the beginning of a large step in the analysis pipeline. The name of the page should reflect its function, and it will be used as the indexing key when it is added to a Notebook. The `NotebookPage` should be created at the beginning of the step in the pipeline, because then the timestamp will be more meaningful. As results are computed, they should be added. This will provide a timestamp for each of the results as well. Then, at the end, the pipeline step should return a `NotebookPage`, which can then be added to the `Notebook`. !!!example ```python nbp = NotebookPage(\"extract_and_filter\") nbp.scale_factor = 10 ... return nbp ``` \"\"\" _PAGEMETA = \"PAGEINFO\" # Filename for metadata about the page _TIMEMETA = \"___TIME\" # Filename suffix for timestamp information _TYPEMETA = \"___TYPE\" # Filename suffix for type information _NON_RESULT_KEYS = [ 'name' , 'finalized' ] _comments_file = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), 'notebook_comments.json' ) def __init__ ( self , name , input_dict = None ): self . finalized = False # Set to true when added to a Notebook self . _times = {} self . name = name self . _time_created = time . time () if isinstance ( input_dict , dict ): self . from_dict ( input_dict ) def __eq__ ( self , other ): # Test for equality using the == syntax. # To be honest, I don't know why you would ever need this, but it is very # useful for writing unit tests, so here it is. if not isinstance ( other , self . __class__ ): return False if self . name != other . name : return False if self . _time_created != other . _time_created : return False for k in self . _times . keys (): if k not in other . _times or not np . array_equal ( getattr ( self , k ), getattr ( other , k )): # second condition in case failed first because of nan == nan is False. # Need first condition as well because equal_nan=True gives error for strings. if k not in other . _times or not np . array_equal ( getattr ( self , k ), getattr ( other , k ), equal_nan = True ): return False for k in other . _times . keys (): if k not in self . _times or not np . array_equal ( getattr ( other , k ), getattr ( self , k )): # second condition in case failed first because of nan == nan is False. # Need first condition as well because equal_nan=True gives error for strings. if k not in self . _times or not np . array_equal ( getattr ( other , k ), getattr ( self , k ), equal_nan = True ): return False for k , v in self . _times . items (): if k not in other . _times or v != other . _times [ k ]: return False return True def __len__ ( self ): # Return the number of results in the NotebookPage return len ( self . _times ) def _is_result_key ( self , key ): # Whether key is a result variable or part of the metadata if key in self . _NON_RESULT_KEYS or key [ 0 ] == '_' : return False else : return True def __repr__ ( self ): # This means that print(nbp) gives description of page if available or name and time created if not. json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : return \" \\n \" . join ( json_comments [ self . name ][ 'DESCRIPTION' ]) else : time_created = time . strftime ( ' %d -%m-%Y- %H:%M:%S' , time . localtime ( self . _time_created )) return f \" { self . name } page created at { time_created } \" def describe ( self , key : Optional [ str ] = None ): \"\"\" Prints a description of the variable indicated by `key`. Args: key: name of variable to describe that must be in `self._times.keys()`. If not specified, will describe the whole page. \"\"\" if key is None : print ( self . __repr__ ()) # describe whole page if no key given else : if key not in self . _times . keys (): print ( f \"No variable named { key } in the { self . name } page.\" ) else : json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : # Remove empty lines while '' in json_comments [ self . name ][ key ]: json_comments [ self . name ][ key ] . remove ( '' ) # replace below removes markdown code indicators print ( \" \\n \" . join ( json_comments [ self . name ][ key ]) . replace ( '`' , '' )) else : print ( f \"No comments available for page called { self . name } .\" ) def __setattr__ ( self , key , value ): # Add an item to the notebook page. # # For a `NotebookPage` object `nbp`, this handles the syntax `nbp.key = value`. # It checks the key and value for validity, and then adds them to the # notebook. Specifically, it implements a write-once mechanism. if self . _is_result_key ( key ): if self . finalized : raise ValueError ( \"This NotebookPage has already been added to a Notebook, no more values can be added.\" ) assert isinstance ( key , str ), f \"NotebookPage key { key !r} must be a string, not { type ( key ) } \" _get_type ( key , value ) if key in self . __dict__ . keys (): raise ValueError ( f \"Cannot assign { key } = { value !r} to the notebook page, key already exists\" ) with open ( self . _comments_file ) as f : json_comments = json . load ( f ) if self . name in json_comments : if key not in json_comments [ self . name ]: raise InvalidNotebookPageError ( key , None , self . name ) if key == 'DESCRIPTION' : raise InvalidNotebookPageError ( key , None , self . name ) self . _times [ key ] = time . time () object . __setattr__ ( self , key , value ) def __delattr__ ( self , name ): # Method to delete a result or attribute. Deals with del nbp.name. # Can only delete attribute if page has not been finalized. if self . finalized : raise ValueError ( \"This NotebookPage has already been added to a Notebook, no values can be deleted.\" ) object . __delattr__ ( self , name ) if name in self . _times : # extra bit if _is_result_key del self . _times [ name ] def has_item ( self , key ): \"\"\"Check to see whether page has attribute `key`\"\"\" return key in self . _times . keys () def from_dict ( self , d ): \"\"\" Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. \"\"\" for key , value in d . items (): if isinstance ( key , ( str , np . str_ )): if value is not None : self . __setattr__ ( key , value ) def to_serial_dict ( self ): \"\"\"Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" keys = {} keys [ self . _PAGEMETA ] = self . name keys [ self . _PAGEMETA + self . _TIMEMETA ] = self . _time_created for rn in self . _times . keys (): r = getattr ( self , rn ) keys [ rn ] = r keys [ rn + self . _TIMEMETA ] = self . _times [ rn ] keys [ rn + self . _TYPEMETA ] = _get_type ( rn , r ) return keys @classmethod def from_serial_dict ( cls , d ): \"\"\"Convert from a dictionary to a `NotebookPage` object In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" # Note that this method will need to be updated if you update the # constructor. name = str ( d [ cls . _PAGEMETA ][()]) n = cls ( name ) n . _time_created = float ( d [ cls . _PAGEMETA + cls . _TIMEMETA ]) # n.finalized = d[cls._FINALIZEDMETA] for k in d . keys (): # If we've already dealt with the key, skip it. if k . startswith ( cls . _PAGEMETA ): continue # Each key has an associated \"time\" and \"type\" key. We deal with # the time and type keys separately when dealing with the main key. if k . endswith ( cls . _TIMEMETA ): continue if k . endswith ( cls . _TYPEMETA ): continue # Now that we have a real key, add it to the page. object . __setattr__ ( n , k , _decode_type ( k , d [ k ], str ( d [ k + cls . _TYPEMETA ][()]))) n . _times [ k ] = float ( d [ k + cls . _TIMEMETA ]) return n describe ( key = None ) Prints a description of the variable indicated by key . Parameters: Name Type Description Default key Optional [ str ] name of variable to describe that must be in self._times.keys() . If not specified, will describe the whole page. None Source code in coppafish/setup/notebook.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 def describe ( self , key : Optional [ str ] = None ): \"\"\" Prints a description of the variable indicated by `key`. Args: key: name of variable to describe that must be in `self._times.keys()`. If not specified, will describe the whole page. \"\"\" if key is None : print ( self . __repr__ ()) # describe whole page if no key given else : if key not in self . _times . keys (): print ( f \"No variable named { key } in the { self . name } page.\" ) else : json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : # Remove empty lines while '' in json_comments [ self . name ][ key ]: json_comments [ self . name ][ key ] . remove ( '' ) # replace below removes markdown code indicators print ( \" \\n \" . join ( json_comments [ self . name ][ key ]) . replace ( '`' , '' )) else : print ( f \"No comments available for page called { self . name } .\" ) from_dict ( d ) Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. Source code in coppafish/setup/notebook.py 765 766 767 768 769 770 771 772 773 def from_dict ( self , d ): \"\"\" Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. \"\"\" for key , value in d . items (): if isinstance ( key , ( str , np . str_ )): if value is not None : self . __setattr__ ( key , value ) from_serial_dict ( d ) classmethod Convert from a dictionary to a NotebookPage object In general, this function shouldn't need to be called other than within a Notebook object. Source code in coppafish/setup/notebook.py 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 @classmethod def from_serial_dict ( cls , d ): \"\"\"Convert from a dictionary to a `NotebookPage` object In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" # Note that this method will need to be updated if you update the # constructor. name = str ( d [ cls . _PAGEMETA ][()]) n = cls ( name ) n . _time_created = float ( d [ cls . _PAGEMETA + cls . _TIMEMETA ]) # n.finalized = d[cls._FINALIZEDMETA] for k in d . keys (): # If we've already dealt with the key, skip it. if k . startswith ( cls . _PAGEMETA ): continue # Each key has an associated \"time\" and \"type\" key. We deal with # the time and type keys separately when dealing with the main key. if k . endswith ( cls . _TIMEMETA ): continue if k . endswith ( cls . _TYPEMETA ): continue # Now that we have a real key, add it to the page. object . __setattr__ ( n , k , _decode_type ( k , d [ k ], str ( d [ k + cls . _TYPEMETA ][()]))) n . _times [ k ] = float ( d [ k + cls . _TIMEMETA ]) return n has_item ( key ) Check to see whether page has attribute key Source code in coppafish/setup/notebook.py 761 762 763 def has_item ( self , key ): \"\"\"Check to see whether page has attribute `key`\"\"\" return key in self . _times . keys () to_serial_dict () Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a Notebook object. Source code in coppafish/setup/notebook.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 def to_serial_dict ( self ): \"\"\"Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" keys = {} keys [ self . _PAGEMETA ] = self . name keys [ self . _PAGEMETA + self . _TIMEMETA ] = self . _time_created for rn in self . _times . keys (): r = getattr ( self , rn ) keys [ rn ] = r keys [ rn + self . _TIMEMETA ] = self . _times [ rn ] keys [ rn + self . _TYPEMETA ] = _get_type ( rn , r ) return keys","title":"Notebook"},{"location":"code/setup/notebook/#notebook","text":"A write-only file-synchronized class to keep track of coppaFISH results. The Notebook object stores all of the outputs of the script. Almost all information saved in the Notebook is encapsulated within \"pages\" , from the NotebookPage object. To add a NotebookPage object to a Notebook , use the \"add_page\" method. In addition to saving pages, it also saves the contents of the config file, and the time at which the notebook and each page was created. To create a Notebook , pass it the path to the file where the Notebook is to be stored ( notebook_file ), and optionally, the path to the configuration file ( config_file ). If notebook_file already exists, the notebook located at this path will be loaded. If not, a new file will be created as soon as the first data is written to the Notebook . Example With config_file No config_file nb = Notebook ( \"nbfile.npz\" , \"config_file.ini\" ) nbp = NotebookPage ( \"pagename\" ) nbp . var = 1 nb . add_page ( nbp ) or nb += nbp or nb . pagename = nbp assert nb . pagename . var == 1 nb = Notebook ( \"nbfile.npz\" ) nbp = NotebookPage ( \"pagename\" ) nbp . var = 1 nb . add_page ( nbp ) or nb += nbp or nb . pagename = nbp assert nb . pagename . var == 1 Because it is automatically saved to the disk, you can close Python, reopen it, and do the following (Once config_file , added to notebook there is no need to load it again unless it has been changed): nb2 = Notebook ( \"nbfile.npz\" ) assert nb2 . pagename . var == 1 If you create a notebook without specifying notebook_file , i.e. nb = Notebook(config_file=\"config_file.ini\") , the notebook_file will be set to: notebook_file = config [ 'file_names' ][ 'output_dir' ] + config [ 'file_names' ][ 'notebook_name' ]) On using config_file When running the coppafish pipeline, the Notebook requires a config_file to access information required for the different stages of the pipeline through nb.get_config() . But if using the Notebook to store information not in coppafish pipeline, it is not needed. Source code in coppafish/setup/notebook.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 class Notebook : \"\"\"A write-only file-synchronized class to keep track of *coppaFISH* results. The `Notebook` object stores all of the outputs of the script. Almost all information saved in the `Notebook` is encapsulated within `\"pages\"`, from the `NotebookPage` object. To add a `NotebookPage` object to a `Notebook`, use the `\"add_page\"` method. In addition to saving pages, it also saves the contents of the config file, and the time at which the notebook and each page was created. To create a `Notebook`, pass it the path to the file where the `Notebook` is to be stored (`notebook_file`), and optionally, the path to the configuration file (`config_file`). If `notebook_file` already exists, the notebook located at this path will be loaded. If not, a new file will be created as soon as the first data is written to the `Notebook`. !!!example === \"With config_file\" ``` python nb = Notebook(\"nbfile.npz\", \"config_file.ini\") nbp = NotebookPage(\"pagename\") nbp.var = 1 nb.add_page(nbp) or nb += nbp or nb.pagename = nbp assert nb.pagename.var == 1 ``` === \"No config_file\" ``` python nb = Notebook(\"nbfile.npz\") nbp = NotebookPage(\"pagename\") nbp.var = 1 nb.add_page(nbp) or nb += nbp or nb.pagename = nbp assert nb.pagename.var == 1 ``` Because it is automatically saved to the disk, you can close Python, reopen it, and do the following (Once `config_file`, added to notebook there is no need to load it again unless it has been changed): ```python nb2 = Notebook(\"nbfile.npz\") assert nb2.pagename.var == 1 ``` If you create a notebook without specifying `notebook_file`, i.e. ```nb = Notebook(config_file=\"config_file.ini\")```, the `notebook_file` will be set to: ```python notebook_file = config['file_names']['output_dir'] + config['file_names']['notebook_name']) ``` !!!note \"On using config_file\" When running the coppafish pipeline, the `Notebook` requires a `config_file` to access information required for the different stages of the pipeline through `nb.get_config()`. But if using the `Notebook` to store information not in coppafish pipeline, it is not needed. \"\"\" _SEP = \"_-_\" # Separator between notebook page name and item name when saving to file _ADDEDMETA = \"TIME_CREATED\" # Key for notebook created time _CONFIGMETA = \"CONFIGFILE\" # Key for config string _NBMETA = \"NOTEBOOKMETA\" # Key for metadata about the entire notebook # If these sections of config files are different, will not raise error. _no_compare_config_sections = [ 'file_names' ] # When the pages corresponding to the keys are added, a save will not be triggered. # When save does happen, these pages won't be saved, but made on loading using # the corresponding function, load_func, if the notebook contains the pages indicated by # load_func_req. # load_func must only take notebook and page_name as input and has no output but page will be added to notebook. # When last of pages in load_func_req have been added, the page will automatically be added. _no_save_pages = { 'file_names' : { 'load_func' : load_file_names , 'load_func_req' : [ 'basic_info' ]}} def __init__ ( self , notebook_file : Optional [ str ] = None , config_file : Optional [ str ] = None ): # Give option to load with config_file as None so don't have to supply ini_file location every time if # already initialised. # Also, can provide config_file if file_names section changed. # Don't need to provide notebook_file as can determine this from config_file as: # config['file_names']['output_dir'] + config['file_names']['notebook_name'] # numpy isn't compatible with npz files which do not end in the suffix # .npz. If one isn't there, it will add the extension automatically. # We do the same thing here. object . __setattr__ ( self , '_page_times' , {}) if notebook_file is None : if config_file is None : raise ValueError ( 'Both notebook_file and config_file are None' ) else : config_file_names = get_config ( config_file )[ 'file_names' ] notebook_file = os . path . join ( config_file_names [ 'output_dir' ], config_file_names [ 'notebook_name' ]) if not os . path . isdir ( config_file_names [ 'output_dir' ]): raise ValueError ( f \" \\n config['file_names']['output_dir'] = { config_file_names [ 'output_dir' ] } \\n \" f \"is not a valid directory.\" ) if not notebook_file . endswith ( \".npz\" ): notebook_file = notebook_file + \".npz\" # Note that the ordering of _pages may change across saves and loads, # but the order will always correspond to the order of _pages_times self . _file = notebook_file self . _config_file = config_file # Read the config file, but don't assign anything yet. Here, we just # save a copy of the config file. This isn't the main place the config # file should be read from. if config_file is not None : if os . path . isfile ( str ( config_file )): with open ( config_file , 'r' ) as f : read_config = f . read () else : raise ValueError ( f 'Config file given is not valid: { config_file } ' ) else : read_config = None # If the file already exists, initialize the Notebook object from this # file. Otherwise, initialize it empty. if os . path . isfile ( self . _file ): pages , self . _page_times , self . _created_time , self . _config = self . from_file ( self . _file ) for page in pages : object . __setattr__ ( self , page . name , page ) # don't want to set page_time hence use object setattr if read_config is not None : if not self . compare_config ( get_config ( read_config )): raise SystemError ( \"Passed config file is not the same as the saved config file\" ) self . _config = read_config # update config to new one - only difference will be in file_names section self . add_no_save_pages () # add file_names page with new config else : warnings . warn ( \"Notebook file not found, creating a new notebook.\" ) if read_config is None : warnings . warn ( \"Have not passed a config_file so Notebook.get_config() won't work.\" ) self . _created_time = time . time () self . _config = read_config def __repr__ ( self ): # This means that print(nb) gives file location of notebook and # pages in the notebook sorted by time added to the notebook. sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] n_names_per_line = 4 i = n_names_per_line - 1 while i < len ( page_names ) - n_names_per_line / 2 : page_names [ i + 1 ] = \" \\n \" + page_names [ i + 1 ] i = i + n_names_per_line page_names = \", \" . join ( page_names ) return f \"File: { self . _file } \\n Pages: { page_names } \" def get_config ( self ): \"\"\" Returns config as dictionary. \"\"\" if self . _config is not None : return get_config ( self . _config ) else : raise ValueError ( 'Notebook does not contain config parameter.' ) def compare_config ( self , config_2 : dict ) -> bool : \"\"\" Compares whether `config_2` is equal to the config file saved in the notebook. Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook will be checked. Args: config_2: Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. `config_2['basic_info]['param1'] = 5`. Returns: `True` if config dictionaries are equal in required sections. \"\"\" # TODO: issue here that if default settings file changed, the equality here would still be true. config = self . get_config () is_equal = True if config . keys () != config_2 . keys (): warnings . warn ( 'The config files have different sections.' ) is_equal = False else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook # page names are either same as config sections or with _debug suffix page_names = [ name [ 0 ] . replace ( '_debug' , '' ) for name in sort_page_names ] for section in config . keys (): # Only compare sections for which there is a corresponding page in the notebook. if section not in self . _no_compare_config_sections and section in page_names : if config [ section ] != config_2 [ section ]: warnings . warn ( f \"The { section } section of the two config files differ.\" ) is_equal = False return is_equal def describe ( self , key = None ): \"\"\" `describe(var)` will print comments for variables called `var` in each `NotebookPage`. \"\"\" if key is None : print ( self . __repr__ ()) elif len ( self . _page_times ) == 0 : print ( f \"No pages so cannot search for variable { key } \" ) else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] first_page = self . __getattribute__ ( page_names [ 0 ]) with open ( first_page . _comments_file ) as f : json_comments = json . load ( f ) if self . _config is not None : config = self . get_config () n_times_appeared = 0 for page_name in page_names : # if in comments file, then print the comment if key in json_comments [ page_name ]: print ( f \" { key } in { page_name } :\" ) self . __getattribute__ ( page_name ) . describe ( key ) print ( \"\" ) n_times_appeared += 1 elif self . _config is not None : # if in config file, then print the comment # find sections in config file with matching name to current page config_sections_with_name = [ page_name . find ( list ( config . keys ())[ i ]) for i in range ( len ( config . keys ()))] config_sections = np . array ( list ( config . keys ()))[ np . array ( config_sections_with_name ) != - 1 ] for section in config_sections : for param in config [ section ] . keys (): if param . lower () == key . lower (): print ( f \"No variable named { key } in the { page_name } page. \\n \" f \"But it is in the { section } section of the config file and has value: \\n \" f \" { config [ section ][ param ] } \\n \" ) n_times_appeared += 1 if n_times_appeared == 0 : print ( f \" { key } is not in any of the pages in this notebook.\" ) def __eq__ ( self , other ): # Test if two `Notebooks` are identical # # For two `Notebooks` to be identical, all aspects must be the same, # excluding the ordering of the pages, and the filename. All timestamps # must also be identical. if self . _created_time != other . _created_time : return False if self . _config != other . _config : return False if len ( self . _page_times ) != len ( other . _page_times ): return False for k in self . _page_times . keys (): if k not in other . _page_times or getattr ( self , k ) != getattr ( other , k ): return False for k in other . _page_times . keys (): if k not in self . _page_times or getattr ( other , k ) != getattr ( self , k ): return False for k , v in self . _page_times . items (): if k not in other . _page_times or v != other . _page_times [ k ]: return False return True def __len__ ( self ): # Return the number of pages in the Notebook return len ( self . _page_times ) def __setattr__ ( self , key , value ): # Deals with the syntax `nb.key = value` # automatically triggers save if `NotebookPage` is added. # If adding something other than a `NotebookPage`, this syntax does exactly as it is for other classes. if isinstance ( value , NotebookPage ): if self . _SEP in key : raise NameError ( f \"The separator { self . _SEP } may not be in the page's name\" ) if value . finalized : raise ValueError ( \"Page already added to a Notebook, cannot add twice\" ) if key in self . _page_times . keys (): raise ValueError ( \"Cannot add two pages with the same name\" ) if value . name != key : raise ValueError ( f \"Page name is { value . name } but key given is { key } \" ) # ensure all the variables in the comments file are included with open ( value . _comments_file ) as f : json_comments = json . load ( f ) if value . name in json_comments : for var in json_comments [ value . name ]: if var not in value . _times and var != \"DESCRIPTION\" : raise InvalidNotebookPageError ( None , var , value . name ) # ensure all variables in page are in comments file for var in value . _times : if var not in json_comments [ value . name ]: raise InvalidNotebookPageError ( var , None , value . name ) value . finalized = True object . __setattr__ ( self , key , value ) self . _page_times [ key ] = time . time () if value . name not in self . _no_save_pages . keys (): self . save () self . add_no_save_pages () elif key in self . _page_times . keys (): raise ValueError ( f \"Page with name { key } in notebook so can't add variable with this name.\" ) else : object . __setattr__ ( self , key , value ) def __delattr__ ( self , name ): # Method to delete a page or attribute. Deals with del nb.name. object . __delattr__ ( self , name ) if name in self . _page_times : # extra bit if page del self . _page_times [ name ] def add_page ( self , page ): \"\"\"Insert the page `page` into the `Notebook`. This function automatically triggers a save. \"\"\" if not isinstance ( page , NotebookPage ): raise ValueError ( \"Only NotebookPage objects may be added to a notebook.\" ) self . __setattr__ ( page . name , page ) def has_page ( self , page_name ): \"\"\"A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present.\"\"\" if isinstance ( page_name , str ): output = any ( page_name == p for p in self . _page_times ) elif isinstance ( page_name , list ): output = [ any ( page_name [ i ] == p for p in self . _page_times ) for i in range ( len ( page_name ))] else : raise ValueError ( f \"page_name given was { page_name } . This is not a list or a string.\" ) return output def __iadd__ ( self , other ): # Syntactic sugar for the add_page method self . add_page ( other ) return self def add_no_save_pages ( self ): \"\"\" This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']` by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`. At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page has been added. \"\"\" for page_name in self . _no_save_pages . keys (): if self . has_page ( page_name ): continue if all ( self . has_page ( self . _no_save_pages [ page_name ][ 'load_func_req' ])): # If contains all required pages to run load_func, then add the page self . _no_save_pages [ page_name ][ 'load_func' ]( self , page_name ) def change_page_name ( self , old_name : str , new_name : str ): \"\"\" This changes the name of the page `old_name` to `new_name`. It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Args: old_name: new_name: \"\"\" nbp = self . __getattribute__ ( old_name ) warnings . warn ( f \"Changing name of { old_name } page to { new_name } \" ) time_added = self . _page_times [ old_name ] nbp . finalized = False nbp . name = new_name self . __delattr__ ( old_name ) self . add_page ( nbp ) self . _page_times [ new_name ] = time_added # set time to time page initially added self . save () def version_hash ( self ): # A short string representing the file version. # # Since there are many possible page names and entry names within those # pages, that means there are many, many possible file versions based on # different versions of the code. Rather than try to keep track of these # versions and appropriately increment some centralized counter, we # generate a short string which is a hash of the page names and the names # of the entries in that page. This way, it is possible to see if two # notebooks were generated using the same version of the software. (Of # course, it assumes that no fields are ever set conditionally.) s = \"\" for p_name in self . _page_times : s += p_name + \" \\n\\n \" page = getattr ( self , p_name ) s += \" \\n \" . join ( sorted ( page . _times . keys ())) return hashlib . md5 ( bytes ( s , \"utf8\" )) . hexdigest () def save ( self , file : Optional [ str ] = None ): \"\"\" Saves Notebook as a npz file at the path indicated by `file`. Args: file: Where to save *Notebook*. If `None`, will use `self._file`. \"\"\" \"\"\"Save the Notebook to a file\"\"\" if file is not None : if not file . endswith ( \".npz\" ): file = file + \".npz\" self . _file = file d = {} # Diagnostic information about how long the save took. We can probably # take this out, or else set it at a higher debug level via warnings # module. save_start_time = time . time () for p_name in self . _page_times . keys (): if p_name in self . _no_save_pages . keys (): continue p = getattr ( self , p_name ) pd = p . to_serial_dict () for k , v in pd . items (): if v is None : # save None objects as string then convert back to None on loading v = str ( v ) d [ p_name + self . _SEP + k ] = v d [ p_name + self . _SEP + self . _ADDEDMETA ] = self . _page_times [ p_name ] d [ self . _NBMETA + self . _SEP + self . _ADDEDMETA ] = self . _created_time if self . _config is not None : d [ self . _NBMETA + self . _SEP + self . _CONFIGMETA ] = self . _config np . savez_compressed ( self . _file , ** d ) # Finishing the diagnostics described above print ( f \"Notebook saved: took { time . time () - save_start_time } seconds\" ) def from_file ( self , fn : str ) -> Tuple [ List , dict , float , str ]: \"\"\" Read a `Notebook` from a file Args: fn: Filename of the saved `Notebook` to load. Returns: A list of `NotebookPage` objects A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and keys are `page.name` A timestamp for the time the `Notebook` was created. A string of the config file \"\"\" # Right now we won't use lazy loading. One problem with lazy loading # is that we must keep the file handle open. We would rather not do # this, because if we write to the file, it will get screwed up, and if # there is a network issue, it will also mess things up. I can't # imagine that loading the notebook will be a performance bottleneck, # but if it is, we can rethink this decision. It should be pretty easy # to lazy load the pages, but eager load everything in the page. f = np . load ( fn ) keys = list ( f . keys ()) page_items = {} page_times = {} created_time = None config_str = None # If no config saved, will stay as None. Otherwise, will be the config in str form. for pk in keys : p , k = pk . split ( self . _SEP , 1 ) if p in self . _no_save_pages . keys (): # This is to deal with the legacy case from old code where a no_save_page has been saved. # If this is the case, don't load in this page. continue if p == self . _NBMETA : if k == self . _ADDEDMETA : created_time = float ( f [ pk ]) continue if k == self . _CONFIGMETA : config_str = str ( f [ pk ]) continue if k == self . _ADDEDMETA : page_times [ p ] = float ( f [ pk ]) continue if p not in page_items . keys (): page_items [ p ] = {} page_items [ p ][ k ] = f [ pk ] pages = [ NotebookPage . from_serial_dict ( page_items [ d ]) for d in sorted ( page_items . keys ())] for page in pages : page . finalized = True # if loading from file, then all pages are final assert len ( pages ) == len ( page_times ), \"Invalid file, lengths don't match\" assert created_time is not None , \"Invalid file, invalid created date\" return pages , page_times , created_time , config_str","title":"Notebook"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.add_no_save_pages","text":"This adds the page page_name listed in nb._no_save_pages to the notebook if the notebook already contains the pages listed in nb._no_save_pages['page_name']['load_func_req'] by running the function nb._no_save_pages['page_name']['load_func'](nb, 'page_name') . At the moment, this is only used to add the file_names page to the notebook as soon as the basic_info page has been added. Source code in coppafish/setup/notebook.py 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 def add_no_save_pages ( self ): \"\"\" This adds the page `page_name` listed in `nb._no_save_pages` to the notebook if the notebook already contains the pages listed in `nb._no_save_pages['page_name']['load_func_req']` by running the function `nb._no_save_pages['page_name']['load_func'](nb, 'page_name')`. At the moment, this is only used to add the `file_names` page to the notebook as soon as the `basic_info` page has been added. \"\"\" for page_name in self . _no_save_pages . keys (): if self . has_page ( page_name ): continue if all ( self . has_page ( self . _no_save_pages [ page_name ][ 'load_func_req' ])): # If contains all required pages to run load_func, then add the page self . _no_save_pages [ page_name ][ 'load_func' ]( self , page_name )","title":"add_no_save_pages()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.add_page","text":"Insert the page page into the Notebook . This function automatically triggers a save. Source code in coppafish/setup/notebook.py 453 454 455 456 457 458 459 460 def add_page ( self , page ): \"\"\"Insert the page `page` into the `Notebook`. This function automatically triggers a save. \"\"\" if not isinstance ( page , NotebookPage ): raise ValueError ( \"Only NotebookPage objects may be added to a notebook.\" ) self . __setattr__ ( page . name , page )","title":"add_page()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.change_page_name","text":"This changes the name of the page old_name to new_name . It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Parameters: Name Type Description Default old_name str required new_name str required Source code in coppafish/setup/notebook.py 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 def change_page_name ( self , old_name : str , new_name : str ): \"\"\" This changes the name of the page `old_name` to `new_name`. It will trigger two saves, one after changing the new and one after changing the time the page was added to be the time the initial page was added. Args: old_name: new_name: \"\"\" nbp = self . __getattribute__ ( old_name ) warnings . warn ( f \"Changing name of { old_name } page to { new_name } \" ) time_added = self . _page_times [ old_name ] nbp . finalized = False nbp . name = new_name self . __delattr__ ( old_name ) self . add_page ( nbp ) self . _page_times [ new_name ] = time_added # set time to time page initially added self . save ()","title":"change_page_name()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.compare_config","text":"Compares whether config_2 is equal to the config file saved in the notebook. Only sections not in _no_compare_config_sections and with a corresponding page saved to the notebook will be checked. Parameters: Name Type Description Default config_2 dict Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. config_2['basic_info]['param1'] = 5 . required Returns: Type Description bool True if config dictionaries are equal in required sections. Source code in coppafish/setup/notebook.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def compare_config ( self , config_2 : dict ) -> bool : \"\"\" Compares whether `config_2` is equal to the config file saved in the notebook. Only sections not in `_no_compare_config_sections` and with a corresponding page saved to the notebook will be checked. Args: config_2: Dictionary with keys corresponding to sections where a section is also a dictionary containing parameters. E.g. `config_2['basic_info]['param1'] = 5`. Returns: `True` if config dictionaries are equal in required sections. \"\"\" # TODO: issue here that if default settings file changed, the equality here would still be true. config = self . get_config () is_equal = True if config . keys () != config_2 . keys (): warnings . warn ( 'The config files have different sections.' ) is_equal = False else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook # page names are either same as config sections or with _debug suffix page_names = [ name [ 0 ] . replace ( '_debug' , '' ) for name in sort_page_names ] for section in config . keys (): # Only compare sections for which there is a corresponding page in the notebook. if section not in self . _no_compare_config_sections and section in page_names : if config [ section ] != config_2 [ section ]: warnings . warn ( f \"The { section } section of the two config files differ.\" ) is_equal = False return is_equal","title":"compare_config()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.describe","text":"describe(var) will print comments for variables called var in each NotebookPage . Source code in coppafish/setup/notebook.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 def describe ( self , key = None ): \"\"\" `describe(var)` will print comments for variables called `var` in each `NotebookPage`. \"\"\" if key is None : print ( self . __repr__ ()) elif len ( self . _page_times ) == 0 : print ( f \"No pages so cannot search for variable { key } \" ) else : sort_page_names = sorted ( self . _page_times . items (), key = lambda x : x [ 1 ]) # sort by time added to notebook page_names = [ name [ 0 ] for name in sort_page_names ] first_page = self . __getattribute__ ( page_names [ 0 ]) with open ( first_page . _comments_file ) as f : json_comments = json . load ( f ) if self . _config is not None : config = self . get_config () n_times_appeared = 0 for page_name in page_names : # if in comments file, then print the comment if key in json_comments [ page_name ]: print ( f \" { key } in { page_name } :\" ) self . __getattribute__ ( page_name ) . describe ( key ) print ( \"\" ) n_times_appeared += 1 elif self . _config is not None : # if in config file, then print the comment # find sections in config file with matching name to current page config_sections_with_name = [ page_name . find ( list ( config . keys ())[ i ]) for i in range ( len ( config . keys ()))] config_sections = np . array ( list ( config . keys ()))[ np . array ( config_sections_with_name ) != - 1 ] for section in config_sections : for param in config [ section ] . keys (): if param . lower () == key . lower (): print ( f \"No variable named { key } in the { page_name } page. \\n \" f \"But it is in the { section } section of the config file and has value: \\n \" f \" { config [ section ][ param ] } \\n \" ) n_times_appeared += 1 if n_times_appeared == 0 : print ( f \" { key } is not in any of the pages in this notebook.\" )","title":"describe()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.from_file","text":"Read a Notebook from a file Parameters: Name Type Description Default fn str Filename of the saved Notebook to load. required Returns: Type Description List A list of NotebookPage objects dict A dictionary of timestamps, of identical length to the list of NotebookPage objects and keys are page.name float A timestamp for the time the Notebook was created. str A string of the config file Source code in coppafish/setup/notebook.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 def from_file ( self , fn : str ) -> Tuple [ List , dict , float , str ]: \"\"\" Read a `Notebook` from a file Args: fn: Filename of the saved `Notebook` to load. Returns: A list of `NotebookPage` objects A dictionary of timestamps, of identical length to the list of `NotebookPage` objects and keys are `page.name` A timestamp for the time the `Notebook` was created. A string of the config file \"\"\" # Right now we won't use lazy loading. One problem with lazy loading # is that we must keep the file handle open. We would rather not do # this, because if we write to the file, it will get screwed up, and if # there is a network issue, it will also mess things up. I can't # imagine that loading the notebook will be a performance bottleneck, # but if it is, we can rethink this decision. It should be pretty easy # to lazy load the pages, but eager load everything in the page. f = np . load ( fn ) keys = list ( f . keys ()) page_items = {} page_times = {} created_time = None config_str = None # If no config saved, will stay as None. Otherwise, will be the config in str form. for pk in keys : p , k = pk . split ( self . _SEP , 1 ) if p in self . _no_save_pages . keys (): # This is to deal with the legacy case from old code where a no_save_page has been saved. # If this is the case, don't load in this page. continue if p == self . _NBMETA : if k == self . _ADDEDMETA : created_time = float ( f [ pk ]) continue if k == self . _CONFIGMETA : config_str = str ( f [ pk ]) continue if k == self . _ADDEDMETA : page_times [ p ] = float ( f [ pk ]) continue if p not in page_items . keys (): page_items [ p ] = {} page_items [ p ][ k ] = f [ pk ] pages = [ NotebookPage . from_serial_dict ( page_items [ d ]) for d in sorted ( page_items . keys ())] for page in pages : page . finalized = True # if loading from file, then all pages are final assert len ( pages ) == len ( page_times ), \"Invalid file, lengths don't match\" assert created_time is not None , \"Invalid file, invalid created date\" return pages , page_times , created_time , config_str","title":"from_file()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.get_config","text":"Returns config as dictionary. Source code in coppafish/setup/notebook.py 298 299 300 301 302 303 304 305 def get_config ( self ): \"\"\" Returns config as dictionary. \"\"\" if self . _config is not None : return get_config ( self . _config ) else : raise ValueError ( 'Notebook does not contain config parameter.' )","title":"get_config()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.has_page","text":"A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present. Source code in coppafish/setup/notebook.py 462 463 464 465 466 467 468 469 470 471 472 def has_page ( self , page_name ): \"\"\"A check to see if notebook includes a page called page_name. If page_name is a list, a boolean list of equal size will be returned indicating whether each page is present.\"\"\" if isinstance ( page_name , str ): output = any ( page_name == p for p in self . _page_times ) elif isinstance ( page_name , list ): output = [ any ( page_name [ i ] == p for p in self . _page_times ) for i in range ( len ( page_name ))] else : raise ValueError ( f \"page_name given was { page_name } . This is not a list or a string.\" ) return output","title":"has_page()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.Notebook.save","text":"Saves Notebook as a npz file at the path indicated by file . Parameters: Name Type Description Default file Optional [ str ] Where to save Notebook . If None , will use self._file . None Source code in coppafish/setup/notebook.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 def save ( self , file : Optional [ str ] = None ): \"\"\" Saves Notebook as a npz file at the path indicated by `file`. Args: file: Where to save *Notebook*. If `None`, will use `self._file`. \"\"\" \"\"\"Save the Notebook to a file\"\"\" if file is not None : if not file . endswith ( \".npz\" ): file = file + \".npz\" self . _file = file d = {} # Diagnostic information about how long the save took. We can probably # take this out, or else set it at a higher debug level via warnings # module. save_start_time = time . time () for p_name in self . _page_times . keys (): if p_name in self . _no_save_pages . keys (): continue p = getattr ( self , p_name ) pd = p . to_serial_dict () for k , v in pd . items (): if v is None : # save None objects as string then convert back to None on loading v = str ( v ) d [ p_name + self . _SEP + k ] = v d [ p_name + self . _SEP + self . _ADDEDMETA ] = self . _page_times [ p_name ] d [ self . _NBMETA + self . _SEP + self . _ADDEDMETA ] = self . _created_time if self . _config is not None : d [ self . _NBMETA + self . _SEP + self . _CONFIGMETA ] = self . _config np . savez_compressed ( self . _file , ** d ) # Finishing the diagnostics described above print ( f \"Notebook saved: took { time . time () - save_start_time } seconds\" )","title":"save()"},{"location":"code/setup/notebook/#notebook-page","text":"A page, to be added to a Notebook object Expected usage is for a NotebookPage to be created at the beginning of a large step in the analysis pipeline. The name of the page should reflect its function, and it will be used as the indexing key when it is added to a Notebook. The NotebookPage should be created at the beginning of the step in the pipeline, because then the timestamp will be more meaningful. As results are computed, they should be added. This will provide a timestamp for each of the results as well. Then, at the end, the pipeline step should return a NotebookPage , which can then be added to the Notebook . Example nbp = NotebookPage ( \"extract_and_filter\" ) nbp . scale_factor = 10 ... return nbp Source code in coppafish/setup/notebook.py 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 class NotebookPage : \"\"\"A page, to be added to a `Notebook` object Expected usage is for a `NotebookPage` to be created at the beginning of a large step in the analysis pipeline. The name of the page should reflect its function, and it will be used as the indexing key when it is added to a Notebook. The `NotebookPage` should be created at the beginning of the step in the pipeline, because then the timestamp will be more meaningful. As results are computed, they should be added. This will provide a timestamp for each of the results as well. Then, at the end, the pipeline step should return a `NotebookPage`, which can then be added to the `Notebook`. !!!example ```python nbp = NotebookPage(\"extract_and_filter\") nbp.scale_factor = 10 ... return nbp ``` \"\"\" _PAGEMETA = \"PAGEINFO\" # Filename for metadata about the page _TIMEMETA = \"___TIME\" # Filename suffix for timestamp information _TYPEMETA = \"___TYPE\" # Filename suffix for type information _NON_RESULT_KEYS = [ 'name' , 'finalized' ] _comments_file = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), 'notebook_comments.json' ) def __init__ ( self , name , input_dict = None ): self . finalized = False # Set to true when added to a Notebook self . _times = {} self . name = name self . _time_created = time . time () if isinstance ( input_dict , dict ): self . from_dict ( input_dict ) def __eq__ ( self , other ): # Test for equality using the == syntax. # To be honest, I don't know why you would ever need this, but it is very # useful for writing unit tests, so here it is. if not isinstance ( other , self . __class__ ): return False if self . name != other . name : return False if self . _time_created != other . _time_created : return False for k in self . _times . keys (): if k not in other . _times or not np . array_equal ( getattr ( self , k ), getattr ( other , k )): # second condition in case failed first because of nan == nan is False. # Need first condition as well because equal_nan=True gives error for strings. if k not in other . _times or not np . array_equal ( getattr ( self , k ), getattr ( other , k ), equal_nan = True ): return False for k in other . _times . keys (): if k not in self . _times or not np . array_equal ( getattr ( other , k ), getattr ( self , k )): # second condition in case failed first because of nan == nan is False. # Need first condition as well because equal_nan=True gives error for strings. if k not in self . _times or not np . array_equal ( getattr ( other , k ), getattr ( self , k ), equal_nan = True ): return False for k , v in self . _times . items (): if k not in other . _times or v != other . _times [ k ]: return False return True def __len__ ( self ): # Return the number of results in the NotebookPage return len ( self . _times ) def _is_result_key ( self , key ): # Whether key is a result variable or part of the metadata if key in self . _NON_RESULT_KEYS or key [ 0 ] == '_' : return False else : return True def __repr__ ( self ): # This means that print(nbp) gives description of page if available or name and time created if not. json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : return \" \\n \" . join ( json_comments [ self . name ][ 'DESCRIPTION' ]) else : time_created = time . strftime ( ' %d -%m-%Y- %H:%M:%S' , time . localtime ( self . _time_created )) return f \" { self . name } page created at { time_created } \" def describe ( self , key : Optional [ str ] = None ): \"\"\" Prints a description of the variable indicated by `key`. Args: key: name of variable to describe that must be in `self._times.keys()`. If not specified, will describe the whole page. \"\"\" if key is None : print ( self . __repr__ ()) # describe whole page if no key given else : if key not in self . _times . keys (): print ( f \"No variable named { key } in the { self . name } page.\" ) else : json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : # Remove empty lines while '' in json_comments [ self . name ][ key ]: json_comments [ self . name ][ key ] . remove ( '' ) # replace below removes markdown code indicators print ( \" \\n \" . join ( json_comments [ self . name ][ key ]) . replace ( '`' , '' )) else : print ( f \"No comments available for page called { self . name } .\" ) def __setattr__ ( self , key , value ): # Add an item to the notebook page. # # For a `NotebookPage` object `nbp`, this handles the syntax `nbp.key = value`. # It checks the key and value for validity, and then adds them to the # notebook. Specifically, it implements a write-once mechanism. if self . _is_result_key ( key ): if self . finalized : raise ValueError ( \"This NotebookPage has already been added to a Notebook, no more values can be added.\" ) assert isinstance ( key , str ), f \"NotebookPage key { key !r} must be a string, not { type ( key ) } \" _get_type ( key , value ) if key in self . __dict__ . keys (): raise ValueError ( f \"Cannot assign { key } = { value !r} to the notebook page, key already exists\" ) with open ( self . _comments_file ) as f : json_comments = json . load ( f ) if self . name in json_comments : if key not in json_comments [ self . name ]: raise InvalidNotebookPageError ( key , None , self . name ) if key == 'DESCRIPTION' : raise InvalidNotebookPageError ( key , None , self . name ) self . _times [ key ] = time . time () object . __setattr__ ( self , key , value ) def __delattr__ ( self , name ): # Method to delete a result or attribute. Deals with del nbp.name. # Can only delete attribute if page has not been finalized. if self . finalized : raise ValueError ( \"This NotebookPage has already been added to a Notebook, no values can be deleted.\" ) object . __delattr__ ( self , name ) if name in self . _times : # extra bit if _is_result_key del self . _times [ name ] def has_item ( self , key ): \"\"\"Check to see whether page has attribute `key`\"\"\" return key in self . _times . keys () def from_dict ( self , d ): \"\"\" Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. \"\"\" for key , value in d . items (): if isinstance ( key , ( str , np . str_ )): if value is not None : self . __setattr__ ( key , value ) def to_serial_dict ( self ): \"\"\"Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" keys = {} keys [ self . _PAGEMETA ] = self . name keys [ self . _PAGEMETA + self . _TIMEMETA ] = self . _time_created for rn in self . _times . keys (): r = getattr ( self , rn ) keys [ rn ] = r keys [ rn + self . _TIMEMETA ] = self . _times [ rn ] keys [ rn + self . _TYPEMETA ] = _get_type ( rn , r ) return keys @classmethod def from_serial_dict ( cls , d ): \"\"\"Convert from a dictionary to a `NotebookPage` object In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" # Note that this method will need to be updated if you update the # constructor. name = str ( d [ cls . _PAGEMETA ][()]) n = cls ( name ) n . _time_created = float ( d [ cls . _PAGEMETA + cls . _TIMEMETA ]) # n.finalized = d[cls._FINALIZEDMETA] for k in d . keys (): # If we've already dealt with the key, skip it. if k . startswith ( cls . _PAGEMETA ): continue # Each key has an associated \"time\" and \"type\" key. We deal with # the time and type keys separately when dealing with the main key. if k . endswith ( cls . _TIMEMETA ): continue if k . endswith ( cls . _TYPEMETA ): continue # Now that we have a real key, add it to the page. object . __setattr__ ( n , k , _decode_type ( k , d [ k ], str ( d [ k + cls . _TYPEMETA ][()]))) n . _times [ k ] = float ( d [ k + cls . _TIMEMETA ]) return n","title":"Notebook Page"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.describe","text":"Prints a description of the variable indicated by key . Parameters: Name Type Description Default key Optional [ str ] name of variable to describe that must be in self._times.keys() . If not specified, will describe the whole page. None Source code in coppafish/setup/notebook.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 def describe ( self , key : Optional [ str ] = None ): \"\"\" Prints a description of the variable indicated by `key`. Args: key: name of variable to describe that must be in `self._times.keys()`. If not specified, will describe the whole page. \"\"\" if key is None : print ( self . __repr__ ()) # describe whole page if no key given else : if key not in self . _times . keys (): print ( f \"No variable named { key } in the { self . name } page.\" ) else : json_comments = json . load ( open ( self . _comments_file )) if self . name in json_comments : # Remove empty lines while '' in json_comments [ self . name ][ key ]: json_comments [ self . name ][ key ] . remove ( '' ) # replace below removes markdown code indicators print ( \" \\n \" . join ( json_comments [ self . name ][ key ]) . replace ( '`' , '' )) else : print ( f \"No comments available for page called { self . name } .\" )","title":"describe()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.from_dict","text":"Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. Source code in coppafish/setup/notebook.py 765 766 767 768 769 770 771 772 773 def from_dict ( self , d ): \"\"\" Adds all string keys of dictionary d to page. Keys whose value is None will be ignored. \"\"\" for key , value in d . items (): if isinstance ( key , ( str , np . str_ )): if value is not None : self . __setattr__ ( key , value )","title":"from_dict()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.from_serial_dict","text":"Convert from a dictionary to a NotebookPage object In general, this function shouldn't need to be called other than within a Notebook object. Source code in coppafish/setup/notebook.py 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 @classmethod def from_serial_dict ( cls , d ): \"\"\"Convert from a dictionary to a `NotebookPage` object In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" # Note that this method will need to be updated if you update the # constructor. name = str ( d [ cls . _PAGEMETA ][()]) n = cls ( name ) n . _time_created = float ( d [ cls . _PAGEMETA + cls . _TIMEMETA ]) # n.finalized = d[cls._FINALIZEDMETA] for k in d . keys (): # If we've already dealt with the key, skip it. if k . startswith ( cls . _PAGEMETA ): continue # Each key has an associated \"time\" and \"type\" key. We deal with # the time and type keys separately when dealing with the main key. if k . endswith ( cls . _TIMEMETA ): continue if k . endswith ( cls . _TYPEMETA ): continue # Now that we have a real key, add it to the page. object . __setattr__ ( n , k , _decode_type ( k , d [ k ], str ( d [ k + cls . _TYPEMETA ][()]))) n . _times [ k ] = float ( d [ k + cls . _TIMEMETA ]) return n","title":"from_serial_dict()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.has_item","text":"Check to see whether page has attribute key Source code in coppafish/setup/notebook.py 761 762 763 def has_item ( self , key ): \"\"\"Check to see whether page has attribute `key`\"\"\" return key in self . _times . keys ()","title":"has_item()"},{"location":"code/setup/notebook/#coppafish.setup.notebook.NotebookPage.to_serial_dict","text":"Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a Notebook object. Source code in coppafish/setup/notebook.py 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 def to_serial_dict ( self ): \"\"\"Convert to a dictionary which can be written to a file. In general, this function shouldn't need to be called other than within a `Notebook` object. \"\"\" keys = {} keys [ self . _PAGEMETA ] = self . name keys [ self . _PAGEMETA + self . _TIMEMETA ] = self . _time_created for rn in self . _times . keys (): r = getattr ( self , rn ) keys [ rn ] = r keys [ rn + self . _TIMEMETA ] = self . _times [ rn ] keys [ rn + self . _TYPEMETA ] = _get_type ( rn , r ) return keys","title":"to_serial_dict()"},{"location":"code/setup/tile_details/","text":"get_tile_file_names ( tile_directory , file_base , n_tiles , n_channels = 0 ) Gets array of all tile file paths which will be saved in tile directory. Parameters: Name Type Description Default tile_directory str Path to folder where tiles npy files saved. required file_base List [ str ] str [n_rounds] . file_base[r] is identifier for round r . required n_tiles int Number of tiles in data set. required n_channels int Total number of imaging channels if using 3D. 0 if using 2D pipeline as all channels saved in same file. 0 Returns: Type Description np . ndarray object [n_tiles x n_rounds (x n_channels)] . np . ndarray tile_files such that np . ndarray If 2D so n_channels = 0 , tile_files[t, r] is the full path to npy file containing all channels of tile t , round r . np . ndarray If 3D so n_channels > 0 , tile_files[t, r] is the full path to npy file containing all z-planes of np . ndarray tile t , round r , channel c . Source code in coppafish/setup/tile_details.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_tile_file_names ( tile_directory : str , file_base : List [ str ], n_tiles : int , n_channels : int = 0 ) -> np . ndarray : \"\"\" Gets array of all tile file paths which will be saved in tile directory. Args: tile_directory: Path to folder where tiles npy files saved. file_base: `str [n_rounds]`. `file_base[r]` is identifier for round `r`. n_tiles: Number of tiles in data set. n_channels: Total number of imaging channels if using 3D. `0` if using 2D pipeline as all channels saved in same file. Returns: `object [n_tiles x n_rounds (x n_channels)]`. `tile_files` such that - If 2D so `n_channels = 0`, `tile_files[t, r]` is the full path to npy file containing all channels of tile `t`, round `r`. - If 3D so `n_channels > 0`, `tile_files[t, r]` is the full path to npy file containing all z-planes of tile `t`, round `r`, channel `c`. \"\"\" n_rounds = len ( file_base ) if n_channels == 0 : # 2D tile_files = np . zeros (( n_tiles , n_rounds ), dtype = object ) for r in range ( n_rounds ): for t in range ( n_tiles ): tile_files [ t , r ] = \\ get_tile_name ( tile_directory , file_base , r , t ) else : # 3D tile_files = np . zeros (( n_tiles , n_rounds , n_channels ), dtype = object ) for r in range ( n_rounds ): for t in range ( n_tiles ): for c in range ( n_channels ): tile_files [ t , r , c ] = \\ get_tile_name ( tile_directory , file_base , r , t , c ) return tile_files get_tile_name ( tile_directory , file_base , r , t , c = None ) Finds the full path to tile, t , of particular round, r , and channel, c , in tile_directory . Parameters: Name Type Description Default tile_directory str Path to folder where tiles npy files saved. required file_base List [ str ] str [n_rounds] . file_base[r] is identifier for round r . required r int Round of desired npy image. required t int Tile of desired npy image. required c Optional [ int ] Channel of desired npy image. None Returns: Type Description str Full path of tile npy file. Source code in coppafish/setup/tile_details.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_tile_name ( tile_directory : str , file_base : List [ str ], r : int , t : int , c : Optional [ int ] = None ) -> str : \"\"\" Finds the full path to tile, `t`, of particular round, `r`, and channel, `c`, in `tile_directory`. Args: tile_directory: Path to folder where tiles npy files saved. file_base: `str [n_rounds]`. `file_base[r]` is identifier for round `r`. r: Round of desired npy image. t: Tile of desired npy image. c: Channel of desired npy image. Returns: Full path of tile npy file. \"\"\" if c is None : tile_name = os . path . join ( tile_directory , ' {} _t {} .npy' . format ( file_base [ r ], t )) else : tile_name = os . path . join ( tile_directory , ' {} _t {} c {} .npy' . format ( file_base [ r ], t , c )) return tile_name get_tilepos ( xy_pos , tile_sz ) Using xy_pos from nd2 metadata, this obtains the yx position of each tile. I.e. how tiles are arranged with respect to each other. Note that this is indexed differently in nd2 file and npy files in the tile directory. Parameters: Name Type Description Default xy_pos np . ndarray float [n_tiles x 2] . xy position of tiles in pixels. Obtained from nd2 metadata. required tile_sz int xy dimension of tile in pixels. required Returns: Type Description np . ndarray tilepos_yx_nd2 - int [n_tiles x 2] . tilepos_yx_nd2[i] is yx index of tile with fov index i in nd2 file. Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . np . ndarray tilepos_yx_npy - int [n_tiles x 2] . tilepos_yx_npy[i, 0] is yx index of tile with tile directory (npy files) index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . Source code in coppafish/setup/tile_details.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_tilepos ( xy_pos : np . ndarray , tile_sz : int ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Using `xy_pos` from nd2 metadata, this obtains the yx position of each tile. I.e. how tiles are arranged with respect to each other. Note that this is indexed differently in nd2 file and npy files in the tile directory. Args: xy_pos: `float [n_tiles x 2]`. xy position of tiles in pixels. Obtained from nd2 metadata. tile_sz: xy dimension of tile in pixels. Returns: - `tilepos_yx_nd2` - `int [n_tiles x 2]`. `tilepos_yx_nd2[i]` is yx index of tile with fov index `i` in nd2 file. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. - `tilepos_yx_npy` - `int [n_tiles x 2]`. `tilepos_yx_npy[i, 0]` is yx index of tile with tile directory (npy files) index `i`. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. \"\"\" tilepos_yx_nd2 = np . zeros_like ( xy_pos , dtype = int ) tilepos_yx_npy = np . zeros_like ( xy_pos , dtype = int ) if np . shape ( xy_pos )[ 0 ] != 1 : # say y coordinate changes when successive tiles have pixel separation of more than tile_sz/2 change_y_coord = np . abs ( np . ediff1d ( xy_pos [:, 1 ])) > tile_sz / 2 if False in change_y_coord : # sometimes get faulty first xy_pos # know that if there are more than one y coordinates, then the first # and second tile must have the same y coordinate. change_y_coord [ 0 ] = False ny = sum ( change_y_coord ) + 1 nx = np . shape ( xy_pos )[ 0 ] / ny if round ( nx ) != nx : raise ValueError ( 'nx is not an integer' ) tilepos_yx_nd2 [:, 0 ] = np . arange ( ny ) . repeat ( nx ) tilepos_yx_nd2 [:, 1 ] = np . tile ( np . concatenate (( np . arange ( nx ), np . flip ( np . arange ( nx )))), np . ceil ( ny / 2 ) . astype ( int ))[: np . shape ( xy_pos )[ 0 ]] tilepos_yx_npy [:, 0 ] = np . flip ( np . arange ( ny ) . repeat ( nx )) tilepos_yx_npy [:, 1 ] = np . tile ( np . concatenate (( np . flip ( np . arange ( nx )), np . flip ( np . arange ( nx )))), np . ceil ( ny / 2 ) . astype ( int ))[: np . shape ( xy_pos )[ 0 ]] return tilepos_yx_nd2 , tilepos_yx_npy","title":"Tile Detail"},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tile_file_names","text":"Gets array of all tile file paths which will be saved in tile directory. Parameters: Name Type Description Default tile_directory str Path to folder where tiles npy files saved. required file_base List [ str ] str [n_rounds] . file_base[r] is identifier for round r . required n_tiles int Number of tiles in data set. required n_channels int Total number of imaging channels if using 3D. 0 if using 2D pipeline as all channels saved in same file. 0 Returns: Type Description np . ndarray object [n_tiles x n_rounds (x n_channels)] . np . ndarray tile_files such that np . ndarray If 2D so n_channels = 0 , tile_files[t, r] is the full path to npy file containing all channels of tile t , round r . np . ndarray If 3D so n_channels > 0 , tile_files[t, r] is the full path to npy file containing all z-planes of np . ndarray tile t , round r , channel c . Source code in coppafish/setup/tile_details.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def get_tile_file_names ( tile_directory : str , file_base : List [ str ], n_tiles : int , n_channels : int = 0 ) -> np . ndarray : \"\"\" Gets array of all tile file paths which will be saved in tile directory. Args: tile_directory: Path to folder where tiles npy files saved. file_base: `str [n_rounds]`. `file_base[r]` is identifier for round `r`. n_tiles: Number of tiles in data set. n_channels: Total number of imaging channels if using 3D. `0` if using 2D pipeline as all channels saved in same file. Returns: `object [n_tiles x n_rounds (x n_channels)]`. `tile_files` such that - If 2D so `n_channels = 0`, `tile_files[t, r]` is the full path to npy file containing all channels of tile `t`, round `r`. - If 3D so `n_channels > 0`, `tile_files[t, r]` is the full path to npy file containing all z-planes of tile `t`, round `r`, channel `c`. \"\"\" n_rounds = len ( file_base ) if n_channels == 0 : # 2D tile_files = np . zeros (( n_tiles , n_rounds ), dtype = object ) for r in range ( n_rounds ): for t in range ( n_tiles ): tile_files [ t , r ] = \\ get_tile_name ( tile_directory , file_base , r , t ) else : # 3D tile_files = np . zeros (( n_tiles , n_rounds , n_channels ), dtype = object ) for r in range ( n_rounds ): for t in range ( n_tiles ): for c in range ( n_channels ): tile_files [ t , r , c ] = \\ get_tile_name ( tile_directory , file_base , r , t , c ) return tile_files","title":"get_tile_file_names()"},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tile_name","text":"Finds the full path to tile, t , of particular round, r , and channel, c , in tile_directory . Parameters: Name Type Description Default tile_directory str Path to folder where tiles npy files saved. required file_base List [ str ] str [n_rounds] . file_base[r] is identifier for round r . required r int Round of desired npy image. required t int Tile of desired npy image. required c Optional [ int ] Channel of desired npy image. None Returns: Type Description str Full path of tile npy file. Source code in coppafish/setup/tile_details.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_tile_name ( tile_directory : str , file_base : List [ str ], r : int , t : int , c : Optional [ int ] = None ) -> str : \"\"\" Finds the full path to tile, `t`, of particular round, `r`, and channel, `c`, in `tile_directory`. Args: tile_directory: Path to folder where tiles npy files saved. file_base: `str [n_rounds]`. `file_base[r]` is identifier for round `r`. r: Round of desired npy image. t: Tile of desired npy image. c: Channel of desired npy image. Returns: Full path of tile npy file. \"\"\" if c is None : tile_name = os . path . join ( tile_directory , ' {} _t {} .npy' . format ( file_base [ r ], t )) else : tile_name = os . path . join ( tile_directory , ' {} _t {} c {} .npy' . format ( file_base [ r ], t , c )) return tile_name","title":"get_tile_name()"},{"location":"code/setup/tile_details/#coppafish.setup.tile_details.get_tilepos","text":"Using xy_pos from nd2 metadata, this obtains the yx position of each tile. I.e. how tiles are arranged with respect to each other. Note that this is indexed differently in nd2 file and npy files in the tile directory. Parameters: Name Type Description Default xy_pos np . ndarray float [n_tiles x 2] . xy position of tiles in pixels. Obtained from nd2 metadata. required tile_sz int xy dimension of tile in pixels. required Returns: Type Description np . ndarray tilepos_yx_nd2 - int [n_tiles x 2] . tilepos_yx_nd2[i] is yx index of tile with fov index i in nd2 file. Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . np . ndarray tilepos_yx_npy - int [n_tiles x 2] . tilepos_yx_npy[i, 0] is yx index of tile with tile directory (npy files) index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . Source code in coppafish/setup/tile_details.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_tilepos ( xy_pos : np . ndarray , tile_sz : int ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Using `xy_pos` from nd2 metadata, this obtains the yx position of each tile. I.e. how tiles are arranged with respect to each other. Note that this is indexed differently in nd2 file and npy files in the tile directory. Args: xy_pos: `float [n_tiles x 2]`. xy position of tiles in pixels. Obtained from nd2 metadata. tile_sz: xy dimension of tile in pixels. Returns: - `tilepos_yx_nd2` - `int [n_tiles x 2]`. `tilepos_yx_nd2[i]` is yx index of tile with fov index `i` in nd2 file. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. - `tilepos_yx_npy` - `int [n_tiles x 2]`. `tilepos_yx_npy[i, 0]` is yx index of tile with tile directory (npy files) index `i`. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. \"\"\" tilepos_yx_nd2 = np . zeros_like ( xy_pos , dtype = int ) tilepos_yx_npy = np . zeros_like ( xy_pos , dtype = int ) if np . shape ( xy_pos )[ 0 ] != 1 : # say y coordinate changes when successive tiles have pixel separation of more than tile_sz/2 change_y_coord = np . abs ( np . ediff1d ( xy_pos [:, 1 ])) > tile_sz / 2 if False in change_y_coord : # sometimes get faulty first xy_pos # know that if there are more than one y coordinates, then the first # and second tile must have the same y coordinate. change_y_coord [ 0 ] = False ny = sum ( change_y_coord ) + 1 nx = np . shape ( xy_pos )[ 0 ] / ny if round ( nx ) != nx : raise ValueError ( 'nx is not an integer' ) tilepos_yx_nd2 [:, 0 ] = np . arange ( ny ) . repeat ( nx ) tilepos_yx_nd2 [:, 1 ] = np . tile ( np . concatenate (( np . arange ( nx ), np . flip ( np . arange ( nx )))), np . ceil ( ny / 2 ) . astype ( int ))[: np . shape ( xy_pos )[ 0 ]] tilepos_yx_npy [:, 0 ] = np . flip ( np . arange ( ny ) . repeat ( nx )) tilepos_yx_npy [:, 1 ] = np . tile ( np . concatenate (( np . flip ( np . arange ( nx )), np . flip ( np . arange ( nx )))), np . ceil ( ny / 2 ) . astype ( int ))[: np . shape ( xy_pos )[ 0 ]] return tilepos_yx_nd2 , tilepos_yx_npy","title":"get_tilepos()"},{"location":"code/spot_colors/base/","text":"all_pixel_yxz ( y_size , x_size , z_planes ) Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Parameters: Name Type Description Default y_size int number of pixels in y direction of image. required x_size int number of pixels in x direction of image. required z_planes Union [ List , int , np . ndarray ] int [n_z_planes] z_planes, coordinates are desired for. required Returns: Type Description np . ndarray int16 [y_size * x_size * n_z_planes, 3] yxz coordinates of all pixels on z_planes . Source code in coppafish/spot_colors/base.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def all_pixel_yxz ( y_size : int , x_size : int , z_planes : Union [ List , int , np . ndarray ]) -> np . ndarray : \"\"\" Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Args: y_size: number of pixels in y direction of image. x_size: number of pixels in x direction of image. z_planes: `int [n_z_planes]` z_planes, coordinates are desired for. Returns: `int16 [y_size * x_size * n_z_planes, 3]` yxz coordinates of all pixels on `z_planes`. \"\"\" if isinstance ( z_planes , int ): z_planes = np . array ([ z_planes ]) elif isinstance ( z_planes , list ): z_planes = np . array ( z_planes ) return np . array ( np . meshgrid ( np . arange ( y_size ), np . arange ( x_size ), z_planes ), dtype = np . int16 ) . T . reshape ( - 1 , 3 ) apply_transform ( yxz , transform , tile_centre , z_scale , tile_sz ) This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Parameters: Name Type Description Default yxz np . ndarray int [n_spots x 3] . yxz[i, :2] are the non-centered yx coordinates in yx_pixels for spot i . yxz[i, 2] is the non-centered z coordinate in z_pixels for spot i . E.g. these are the coordinates stored in nb['find_spots']['spot_details'] . required transform np . ndarray float [4 x 3] . Affine transform to apply to yxz , once centered and z units changed to yx_pixels . transform[3, 2] is approximately the z shift in units of yx_pixels . E.g. this is one of the transforms stored in nb['register']['transform'] . required tile_centre np . ndarray float [3] . tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required tile_sz np . ndarray int16 [3] . YXZ dimensions of tile required Returns: Type Description np . ndarray int [n_spots x 3] . yxz_transform such that yxz_transform[i, [1,2]] are the transformed non-centered yx coordinates in yx_pixels for spot i . yxz_transform[i, 2] is the transformed non-centered z coordinate in z_pixels for spot i . np . ndarray in_range - bool [n_spots] . Whether spot s was in the bounds of the tile when transformed to round r , channel c . Source code in coppafish/spot_colors/base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def apply_transform ( yxz : np . ndarray , transform : np . ndarray , tile_centre : np . ndarray , z_scale : float , tile_sz : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Args: yxz: ```int [n_spots x 3]```. ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```. E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```. transform: ```float [4 x 3]```. Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```. ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```. E.g. this is one of the transforms stored in ```nb['register']['transform']```. tile_centre: ```float [3]```. ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. tile_sz: ```int16 [3]```. YXZ dimensions of tile Returns: ```int [n_spots x 3]```. ```yxz_transform``` such that ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```. - ```in_range``` - ```bool [n_spots]```. Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`. \"\"\" if ( utils . round_any ( tile_centre , 0.5 ) == tile_centre ) . min () == False : raise ValueError ( f \"tile_centre given, { tile_centre } , is not a multiple of 0.5 in each dimension.\" ) yxz_pad = np . pad (( yxz - tile_centre ) * [ 1 , 1 , z_scale ], [( 0 , 0 ), ( 0 , 1 )], constant_values = 1 ) yxz_transform = yxz_pad @ transform yxz_transform = np . round (( yxz_transform / [ 1 , 1 , z_scale ]) + tile_centre ) . astype ( np . int16 ) in_range = np . logical_and (( yxz_transform >= np . array ([ 0 , 0 , 0 ])) . all ( axis = 1 ), ( yxz_transform < tile_sz ) . all ( axis = 1 )) # set color to nan if out range return yxz_transform , in_range get_spot_colors ( yxz_base , t , transforms , nbp_file , nbp_basic , use_rounds = None , use_channels = None , return_in_bounds = False ) Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on nbp_basic.use_rounds and nbp_basic.use_channels . Note Returned spot colors have dimension n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) not n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels . Note invalid_value = -nbp_basic.tile_pixel_value_shift is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence, I use this as integer nan. It will be invalid_value if the registered coordinate of spot s is outside the tile in round r , channel c . Parameters: Name Type Description Default yxz_base np . ndarray int16 [n_spots x 3] . Local yxz coordinates of spots found in the reference round/reference channel of tile t yx coordinates are in units of yx_pixels . z coordinates are in units of z_pixels . required t int Tile that spots were found on. required transforms np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transforms[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required use_rounds Optional [ List [ int ]] int [n_use_rounds] . Rounds you would like to find the spot_color for. Error will raise if transform is zero for particular round. If None , all rounds in nbp_basic.use_rounds used. None use_channels Optional [ List [ int ]] int [n_use_channels] . Channels you would like to find the spot_color for. Error will raise if transform is zero for particular channel. If None , all channels in nbp_basic.use_channels used. None return_in_bounds bool if True , then only spot_colors which are within the tile bounds in all use_rounds / use_channels will be returned. The corresponding yxz_base coordinates will also be returned in this case. Otherwise, spot_colors will be returned for all the given yxz_base but if spot s is out of bounds on round r , channel c , then spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift . This is the only scenario for which spot_colors = invalid_value due to clipping in the extract step. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_colors - int32 [n_spots x n_rounds_use x n_channels_use] or int32 [n_spots_in_bounds x n_rounds_use x n_channels_use] . spot_colors[s, r, c] is the spot color for spot s in round use_rounds[r] , channel use_channels[c] . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] yxz_base - int16 [n_spots_in_bounds x 3] . If return_in_bounds , the yxz_base corresponding to spots in bounds for all use_rounds / use_channels will be returned. It is likely that n_spots_in_bounds won't be the same as n_spots . Source code in coppafish/spot_colors/base.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_spot_colors ( yxz_base : np . ndarray , t : int , transforms : np . ndarray , nbp_file : NotebookPage , nbp_basic : NotebookPage , use_rounds : Optional [ List [ int ]] = None , use_channels : Optional [ List [ int ]] = None , return_in_bounds : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`. !!! note Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`. !!! note `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence, I use this as integer nan. It will be `invalid_value` if the registered coordinate of spot `s` is outside the tile in round `r`, channel `c`. Args: yxz_base: `int16 [n_spots x 3]`. Local yxz coordinates of spots found in the reference round/reference channel of tile `t` yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`. t: Tile that spots were found on. transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page use_rounds: `int [n_use_rounds]`. Rounds you would like to find the `spot_color` for. Error will raise if transform is zero for particular round. If `None`, all rounds in `nbp_basic.use_rounds` used. use_channels: `int [n_use_channels]`. Channels you would like to find the `spot_color` for. Error will raise if transform is zero for particular channel. If `None`, all channels in `nbp_basic.use_channels` used. return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all `use_rounds` / `use_channels` will be returned. The corresponding `yxz_base` coordinates will also be returned in this case. Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`. This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step. Returns: - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`. `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`. - `yxz_base` - `int16 [n_spots_in_bounds x 3]`. If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels` will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`. \"\"\" if use_rounds is None : use_rounds = nbp_basic . use_rounds if use_channels is None : use_channels = nbp_basic . use_channels z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy n_spots = yxz_base . shape [ 0 ] no_verbose = n_spots < 10000 # note using nan means can't use integer even though data is integer n_use_rounds = len ( use_rounds ) n_use_channels = len ( use_channels ) # spots outside tile bounds on particular r/c will initially be set to 0. spot_colors = np . zeros (( n_spots , n_use_rounds , n_use_channels ), dtype = np . int32 ) tile_centre = np . array ( nbp_basic . tile_centre ) if not nbp_basic . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ], dtype = np . int16 ) with tqdm ( total = n_use_rounds * n_use_channels , disable = no_verbose ) as pbar : pbar . set_description ( f \"Reading { n_spots } spot_colors found on tile { t } from npy files\" ) for r in range ( n_use_rounds ): if not nbp_basic . is_3d : # If 2D, load in all channels first image_all_channels = np . load ( nbp_file . tile [ t ][ use_rounds [ r ]], mmap_mode = 'r' ) for c in range ( n_use_channels ): transform_rc = transforms [ t , use_rounds [ r ], use_channels [ c ]] pbar . set_postfix ({ 'round' : use_rounds [ r ], 'channel' : use_channels [ c ]}) if transform_rc [ 0 , 0 ] == 0 : raise ValueError ( f \"Transform for tile { t } , round { use_rounds [ r ] } , channel { use_channels [ c ] } is zero:\" f \" \\n { transform_rc } \" ) yxz_transform , in_range = apply_transform ( yxz_base , transform_rc , tile_centre , z_scale , tile_sz ) yxz_transform = yxz_transform [ in_range ] if yxz_transform . shape [ 0 ] > 0 : # Read in the shifted uint16 colors here, and remove shift later. if nbp_basic . is_3d : spot_colors [ in_range , r , c ] = utils . npy . load_tile ( nbp_file , nbp_basic , t , use_rounds [ r ], use_channels [ c ], yxz_transform , apply_shift = False ) else : spot_colors [ in_range , r , c ] = image_all_channels [ use_channels [ c ]][ tuple ( np . asarray ( yxz_transform [:, i ]) for i in range ( 2 ))] pbar . update ( 1 ) # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value. # It is impossible for any actual spot color to be this due to clipping at the extract stage. spot_colors = spot_colors - nbp_basic . tile_pixel_value_shift invalid_value = - nbp_basic . tile_pixel_value_shift if return_in_bounds : good = ~ np . any ( spot_colors == invalid_value , axis = ( 1 , 2 )) return spot_colors [ good ], yxz_base [ good ] else : return spot_colors Optimised all_pixel_yxz ( y_size , x_size , z_planes ) Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Parameters: Name Type Description Default y_size int number of pixels in y direction of image. required x_size int number of pixels in x direction of image. required z_planes Union [ List , int , np . ndarray ] int [n_z_planes] z_planes, coordinates are desired for. required Returns: Type Description jnp . ndarray int16 [y_size * x_size * n_z_planes, 3] yxz coordinates of all pixels on z_planes . Source code in coppafish/spot_colors/base_optimised.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def all_pixel_yxz ( y_size : int , x_size : int , z_planes : Union [ List , int , np . ndarray ]) -> jnp . ndarray : \"\"\" Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Args: y_size: number of pixels in y direction of image. x_size: number of pixels in x direction of image. z_planes: `int [n_z_planes]` z_planes, coordinates are desired for. Returns: `int16 [y_size * x_size * n_z_planes, 3]` yxz coordinates of all pixels on `z_planes`. \"\"\" if isinstance ( z_planes , int ): z_planes = jnp . array ([ z_planes ]) elif isinstance ( z_planes , list ): z_planes = jnp . array ( z_planes ) return jnp . array ( jnp . meshgrid ( jnp . arange ( y_size ), jnp . arange ( x_size ), z_planes ), dtype = jnp . int16 ) . T . reshape ( - 1 , 3 ) apply_transform ( yxz , transform , tile_centre , z_scale , tile_sz ) This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Parameters: Name Type Description Default yxz jnp . ndarray int16 [n_spots x 3] . yxz[i, :2] are the non-centered yx coordinates in yx_pixels for spot i . yxz[i, 2] is the non-centered z coordinate in z_pixels for spot i . E.g. these are the coordinates stored in nb['find_spots']['spot_details'] . required transform jnp . ndarray float [4 x 3] . Affine transform to apply to yxz , once centered and z units changed to yx_pixels . transform[3, 2] is approximately the z shift in units of yx_pixels . E.g. this is one of the transforms stored in nb['register']['transform'] . required tile_centre jnp . ndarray float [3] . tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required tile_sz jnp . ndarray int16 [3] . YXZ dimensions of tile required Returns: Type Description jnp . ndarray yxz_transform - int [n_spots x 3] . yxz_transform such that yxz_transform[i, [1,2]] are the transformed non-centered yx coordinates in yx_pixels for spot i . yxz_transform[i, 2] is the transformed non-centered z coordinate in z_pixels for spot i . jnp . ndarray in_range - bool [n_spots] . Whether spot s was in the bounds of the tile when transformed to round r , channel c . Source code in coppafish/spot_colors/base_optimised.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @partial ( jax . jit , static_argnums = 3 ) def apply_transform ( yxz : jnp . ndarray , transform : jnp . ndarray , tile_centre : jnp . ndarray , z_scale : float , tile_sz : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Args: yxz: ```int16 [n_spots x 3]```. ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```. E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```. transform: ```float [4 x 3]```. Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```. ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```. E.g. this is one of the transforms stored in ```nb['register']['transform']```. tile_centre: ```float [3]```. ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. tile_sz: ```int16 [3]```. YXZ dimensions of tile Returns: - `yxz_transform` - ```int [n_spots x 3]```. ```yxz_transform``` such that ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```. - ```in_range``` - ```bool [n_spots]```. Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`. \"\"\" return jax . vmap ( apply_transform_single , in_axes = ( 0 , None , None , None , None ), out_axes = ( 0 , 0 ))( yxz , transform , tile_centre , z_scale , tile_sz ) get_spot_colors ( yxz_base , t , transforms , nbp_file , nbp_basic , use_rounds = None , use_channels = None , return_in_bounds = False ) Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on nbp_basic.use_rounds and nbp_basic.use_channels . Note Returned spot colors have dimension n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) not n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels . Note invalid_value = -nbp_basic.tile_pixel_value_shift is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence I use this as integer nan. It will be invalid_value if the registered coordinate of spot s is outside the tile in round r , channel c . Parameters: Name Type Description Default yxz_base jnp . ndarray int16 [n_spots x 3] . Local yxz coordinates of spots found in the reference round/reference channel of tile t yx coordinates are in units of yx_pixels . z coordinates are in units of z_pixels . required t int Tile that spots were found on. required transforms jnp . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transforms[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required use_rounds Optional [ List [ int ]] int [n_use_rounds] . Rounds you would like to find the spot_color for. Error will raise if transform is zero for particular round. If None , all rounds in nbp_basic.use_rounds used. None use_channels Optional [ List [ int ]] int [n_use_channels] . Channels you would like to find the spot_color for. Error will raise if transform is zero for particular channel. If None , all channels in nbp_basic.use_channels used. None return_in_bounds bool if True , then only spot_colors which are within the tile bounds in all use_rounds / use_channels will be returned. The corresponding yxz_base coordinates will also be returned in this case. Otherwise, spot_colors will be returned for all the given yxz_base but if spot s is out of bounds on round r , channel c , then spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift . This is the only scenario for which spot_colors = invalid_value due to clipping in the extract step. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]] spot_colors - int32 [n_spots x n_rounds_use x n_channels_use] or int32 [n_spots_in_bounds x n_rounds_use x n_channels_use] . spot_colors[s, r, c] is the spot color for spot s in round use_rounds[r] , channel use_channels[c] . Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]] yxz_base - int16 [n_spots_in_bounds x 3] . If return_in_bounds , the yxz_base corresponding to spots in bounds for all use_rounds / use_channels will be returned. It is likely that n_spots_in_bounds won't be the same as n_spots . Source code in coppafish/spot_colors/base_optimised.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def get_spot_colors ( yxz_base : jnp . ndarray , t : int , transforms : jnp . ndarray , nbp_file : NotebookPage , nbp_basic : NotebookPage , use_rounds : Optional [ List [ int ]] = None , use_channels : Optional [ List [ int ]] = None , return_in_bounds : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]]: \"\"\" Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`. !!! note Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`. !!! note `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence I use this as integer nan. It will be `invalid_value` if the registered coordinate of spot `s` is outside the tile in round `r`, channel `c`. Args: yxz_base: `int16 [n_spots x 3]`. Local yxz coordinates of spots found in the reference round/reference channel of tile `t` yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`. t: Tile that spots were found on. transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page use_rounds: `int [n_use_rounds]`. Rounds you would like to find the `spot_color` for. Error will raise if transform is zero for particular round. If `None`, all rounds in `nbp_basic.use_rounds` used. use_channels: `int [n_use_channels]`. Channels you would like to find the `spot_color` for. Error will raise if transform is zero for particular channel. If `None`, all channels in `nbp_basic.use_channels` used. return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all `use_rounds` / `use_channels` will be returned. The corresponding `yxz_base` coordinates will also be returned in this case. Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`. This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step. Returns: - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`. `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`. - `yxz_base` - `int16 [n_spots_in_bounds x 3]`. If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels` will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`. \"\"\" if use_rounds is None : use_rounds = nbp_basic . use_rounds if use_channels is None : use_channels = nbp_basic . use_channels z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy n_spots = yxz_base . shape [ 0 ] no_verbose = n_spots < 10000 # note using nan means can't use integer even though data is integer n_use_rounds = len ( use_rounds ) n_use_channels = len ( use_channels ) # spots outside tile bounds on particular r/c will initially be set to 0. spot_colors = np . zeros (( n_spots , n_use_rounds , n_use_channels ), dtype = np . int32 ) tile_centre = jnp . array ( nbp_basic . tile_centre ) if not nbp_basic . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = jnp . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , 1 ], dtype = jnp . int16 ) else : tile_sz = jnp . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ], dtype = jnp . int16 ) with tqdm ( total = n_use_rounds * n_use_channels , disable = no_verbose ) as pbar : pbar . set_description ( f \"Reading { n_spots } spot_colors found on tile { t } from npy files\" ) for r in range ( n_use_rounds ): if not nbp_basic . is_3d : # If 2D, load in all channels first image_all_channels = np . load ( nbp_file . tile [ t ][ use_rounds [ r ]], mmap_mode = 'r' ) for c in range ( n_use_channels ): transform_rc = transforms [ t , use_rounds [ r ], use_channels [ c ]] pbar . set_postfix ({ 'round' : use_rounds [ r ], 'channel' : use_channels [ c ]}) if transform_rc [ 0 , 0 ] == 0 : raise ValueError ( f \"Transform for tile { t } , round { use_rounds [ r ] } , channel { use_channels [ c ] } is zero:\" f \" \\n { transform_rc } \" ) yxz_transform , in_range = apply_transform ( yxz_base , transform_rc , tile_centre , z_scale , tile_sz ) yxz_transform = np . asarray ( yxz_transform ) in_range = np . asarray ( in_range ) yxz_transform = yxz_transform [ in_range ] if yxz_transform . shape [ 0 ] > 0 : # Read in the shifted uint16 colors here, and remove shift later. if nbp_basic . is_3d : spot_colors [ in_range , r , c ] = utils . npy . load_tile ( nbp_file , nbp_basic , t , use_rounds [ r ], use_channels [ c ], yxz_transform , apply_shift = False ) else : spot_colors [ in_range , r , c ] = image_all_channels [ use_channels [ c ]][ tuple ( np . asarray ( yxz_transform [:, i ]) for i in range ( 2 ))] pbar . update ( 1 ) # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value. # It is impossible for any actual spot color to be this due to clipping at the extract stage. spot_colors = spot_colors - nbp_basic . tile_pixel_value_shift invalid_value = - nbp_basic . tile_pixel_value_shift if return_in_bounds : good = ~ np . any ( spot_colors == invalid_value , axis = ( 1 , 2 )) return spot_colors [ good ], yxz_base [ good ] else : return spot_colors","title":"Base"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.all_pixel_yxz","text":"Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Parameters: Name Type Description Default y_size int number of pixels in y direction of image. required x_size int number of pixels in x direction of image. required z_planes Union [ List , int , np . ndarray ] int [n_z_planes] z_planes, coordinates are desired for. required Returns: Type Description np . ndarray int16 [y_size * x_size * n_z_planes, 3] yxz coordinates of all pixels on z_planes . Source code in coppafish/spot_colors/base.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def all_pixel_yxz ( y_size : int , x_size : int , z_planes : Union [ List , int , np . ndarray ]) -> np . ndarray : \"\"\" Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Args: y_size: number of pixels in y direction of image. x_size: number of pixels in x direction of image. z_planes: `int [n_z_planes]` z_planes, coordinates are desired for. Returns: `int16 [y_size * x_size * n_z_planes, 3]` yxz coordinates of all pixels on `z_planes`. \"\"\" if isinstance ( z_planes , int ): z_planes = np . array ([ z_planes ]) elif isinstance ( z_planes , list ): z_planes = np . array ( z_planes ) return np . array ( np . meshgrid ( np . arange ( y_size ), np . arange ( x_size ), z_planes ), dtype = np . int16 ) . T . reshape ( - 1 , 3 )","title":"all_pixel_yxz()"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.apply_transform","text":"This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Parameters: Name Type Description Default yxz np . ndarray int [n_spots x 3] . yxz[i, :2] are the non-centered yx coordinates in yx_pixels for spot i . yxz[i, 2] is the non-centered z coordinate in z_pixels for spot i . E.g. these are the coordinates stored in nb['find_spots']['spot_details'] . required transform np . ndarray float [4 x 3] . Affine transform to apply to yxz , once centered and z units changed to yx_pixels . transform[3, 2] is approximately the z shift in units of yx_pixels . E.g. this is one of the transforms stored in nb['register']['transform'] . required tile_centre np . ndarray float [3] . tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required tile_sz np . ndarray int16 [3] . YXZ dimensions of tile required Returns: Type Description np . ndarray int [n_spots x 3] . yxz_transform such that yxz_transform[i, [1,2]] are the transformed non-centered yx coordinates in yx_pixels for spot i . yxz_transform[i, 2] is the transformed non-centered z coordinate in z_pixels for spot i . np . ndarray in_range - bool [n_spots] . Whether spot s was in the bounds of the tile when transformed to round r , channel c . Source code in coppafish/spot_colors/base.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def apply_transform ( yxz : np . ndarray , transform : np . ndarray , tile_centre : np . ndarray , z_scale : float , tile_sz : np . ndarray ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Args: yxz: ```int [n_spots x 3]```. ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```. E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```. transform: ```float [4 x 3]```. Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```. ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```. E.g. this is one of the transforms stored in ```nb['register']['transform']```. tile_centre: ```float [3]```. ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. tile_sz: ```int16 [3]```. YXZ dimensions of tile Returns: ```int [n_spots x 3]```. ```yxz_transform``` such that ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```. - ```in_range``` - ```bool [n_spots]```. Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`. \"\"\" if ( utils . round_any ( tile_centre , 0.5 ) == tile_centre ) . min () == False : raise ValueError ( f \"tile_centre given, { tile_centre } , is not a multiple of 0.5 in each dimension.\" ) yxz_pad = np . pad (( yxz - tile_centre ) * [ 1 , 1 , z_scale ], [( 0 , 0 ), ( 0 , 1 )], constant_values = 1 ) yxz_transform = yxz_pad @ transform yxz_transform = np . round (( yxz_transform / [ 1 , 1 , z_scale ]) + tile_centre ) . astype ( np . int16 ) in_range = np . logical_and (( yxz_transform >= np . array ([ 0 , 0 , 0 ])) . all ( axis = 1 ), ( yxz_transform < tile_sz ) . all ( axis = 1 )) # set color to nan if out range return yxz_transform , in_range","title":"apply_transform()"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base.get_spot_colors","text":"Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on nbp_basic.use_rounds and nbp_basic.use_channels . Note Returned spot colors have dimension n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) not n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels . Note invalid_value = -nbp_basic.tile_pixel_value_shift is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence, I use this as integer nan. It will be invalid_value if the registered coordinate of spot s is outside the tile in round r , channel c . Parameters: Name Type Description Default yxz_base np . ndarray int16 [n_spots x 3] . Local yxz coordinates of spots found in the reference round/reference channel of tile t yx coordinates are in units of yx_pixels . z coordinates are in units of z_pixels . required t int Tile that spots were found on. required transforms np . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transforms[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required use_rounds Optional [ List [ int ]] int [n_use_rounds] . Rounds you would like to find the spot_color for. Error will raise if transform is zero for particular round. If None , all rounds in nbp_basic.use_rounds used. None use_channels Optional [ List [ int ]] int [n_use_channels] . Channels you would like to find the spot_color for. Error will raise if transform is zero for particular channel. If None , all channels in nbp_basic.use_channels used. None return_in_bounds bool if True , then only spot_colors which are within the tile bounds in all use_rounds / use_channels will be returned. The corresponding yxz_base coordinates will also be returned in this case. Otherwise, spot_colors will be returned for all the given yxz_base but if spot s is out of bounds on round r , channel c , then spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift . This is the only scenario for which spot_colors = invalid_value due to clipping in the extract step. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] spot_colors - int32 [n_spots x n_rounds_use x n_channels_use] or int32 [n_spots_in_bounds x n_rounds_use x n_channels_use] . spot_colors[s, r, c] is the spot color for spot s in round use_rounds[r] , channel use_channels[c] . Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] yxz_base - int16 [n_spots_in_bounds x 3] . If return_in_bounds , the yxz_base corresponding to spots in bounds for all use_rounds / use_channels will be returned. It is likely that n_spots_in_bounds won't be the same as n_spots . Source code in coppafish/spot_colors/base.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_spot_colors ( yxz_base : np . ndarray , t : int , transforms : np . ndarray , nbp_file : NotebookPage , nbp_basic : NotebookPage , use_rounds : Optional [ List [ int ]] = None , use_channels : Optional [ List [ int ]] = None , return_in_bounds : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`. !!! note Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`. !!! note `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence, I use this as integer nan. It will be `invalid_value` if the registered coordinate of spot `s` is outside the tile in round `r`, channel `c`. Args: yxz_base: `int16 [n_spots x 3]`. Local yxz coordinates of spots found in the reference round/reference channel of tile `t` yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`. t: Tile that spots were found on. transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page use_rounds: `int [n_use_rounds]`. Rounds you would like to find the `spot_color` for. Error will raise if transform is zero for particular round. If `None`, all rounds in `nbp_basic.use_rounds` used. use_channels: `int [n_use_channels]`. Channels you would like to find the `spot_color` for. Error will raise if transform is zero for particular channel. If `None`, all channels in `nbp_basic.use_channels` used. return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all `use_rounds` / `use_channels` will be returned. The corresponding `yxz_base` coordinates will also be returned in this case. Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`. This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step. Returns: - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`. `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`. - `yxz_base` - `int16 [n_spots_in_bounds x 3]`. If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels` will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`. \"\"\" if use_rounds is None : use_rounds = nbp_basic . use_rounds if use_channels is None : use_channels = nbp_basic . use_channels z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy n_spots = yxz_base . shape [ 0 ] no_verbose = n_spots < 10000 # note using nan means can't use integer even though data is integer n_use_rounds = len ( use_rounds ) n_use_channels = len ( use_channels ) # spots outside tile bounds on particular r/c will initially be set to 0. spot_colors = np . zeros (( n_spots , n_use_rounds , n_use_channels ), dtype = np . int32 ) tile_centre = np . array ( nbp_basic . tile_centre ) if not nbp_basic . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , 1 ], dtype = np . int16 ) else : tile_sz = np . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ], dtype = np . int16 ) with tqdm ( total = n_use_rounds * n_use_channels , disable = no_verbose ) as pbar : pbar . set_description ( f \"Reading { n_spots } spot_colors found on tile { t } from npy files\" ) for r in range ( n_use_rounds ): if not nbp_basic . is_3d : # If 2D, load in all channels first image_all_channels = np . load ( nbp_file . tile [ t ][ use_rounds [ r ]], mmap_mode = 'r' ) for c in range ( n_use_channels ): transform_rc = transforms [ t , use_rounds [ r ], use_channels [ c ]] pbar . set_postfix ({ 'round' : use_rounds [ r ], 'channel' : use_channels [ c ]}) if transform_rc [ 0 , 0 ] == 0 : raise ValueError ( f \"Transform for tile { t } , round { use_rounds [ r ] } , channel { use_channels [ c ] } is zero:\" f \" \\n { transform_rc } \" ) yxz_transform , in_range = apply_transform ( yxz_base , transform_rc , tile_centre , z_scale , tile_sz ) yxz_transform = yxz_transform [ in_range ] if yxz_transform . shape [ 0 ] > 0 : # Read in the shifted uint16 colors here, and remove shift later. if nbp_basic . is_3d : spot_colors [ in_range , r , c ] = utils . npy . load_tile ( nbp_file , nbp_basic , t , use_rounds [ r ], use_channels [ c ], yxz_transform , apply_shift = False ) else : spot_colors [ in_range , r , c ] = image_all_channels [ use_channels [ c ]][ tuple ( np . asarray ( yxz_transform [:, i ]) for i in range ( 2 ))] pbar . update ( 1 ) # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value. # It is impossible for any actual spot color to be this due to clipping at the extract stage. spot_colors = spot_colors - nbp_basic . tile_pixel_value_shift invalid_value = - nbp_basic . tile_pixel_value_shift if return_in_bounds : good = ~ np . any ( spot_colors == invalid_value , axis = ( 1 , 2 )) return spot_colors [ good ], yxz_base [ good ] else : return spot_colors","title":"get_spot_colors()"},{"location":"code/spot_colors/base/#optimised","text":"","title":"Optimised"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.all_pixel_yxz","text":"Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Parameters: Name Type Description Default y_size int number of pixels in y direction of image. required x_size int number of pixels in x direction of image. required z_planes Union [ List , int , np . ndarray ] int [n_z_planes] z_planes, coordinates are desired for. required Returns: Type Description jnp . ndarray int16 [y_size * x_size * n_z_planes, 3] yxz coordinates of all pixels on z_planes . Source code in coppafish/spot_colors/base_optimised.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def all_pixel_yxz ( y_size : int , x_size : int , z_planes : Union [ List , int , np . ndarray ]) -> jnp . ndarray : \"\"\" Returns the yxz coordinates of all pixels on the indicated z-planes of an image. Args: y_size: number of pixels in y direction of image. x_size: number of pixels in x direction of image. z_planes: `int [n_z_planes]` z_planes, coordinates are desired for. Returns: `int16 [y_size * x_size * n_z_planes, 3]` yxz coordinates of all pixels on `z_planes`. \"\"\" if isinstance ( z_planes , int ): z_planes = jnp . array ([ z_planes ]) elif isinstance ( z_planes , list ): z_planes = jnp . array ( z_planes ) return jnp . array ( jnp . meshgrid ( jnp . arange ( y_size ), jnp . arange ( x_size ), z_planes ), dtype = jnp . int16 ) . T . reshape ( - 1 , 3 )","title":"all_pixel_yxz()"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.apply_transform","text":"This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Parameters: Name Type Description Default yxz jnp . ndarray int16 [n_spots x 3] . yxz[i, :2] are the non-centered yx coordinates in yx_pixels for spot i . yxz[i, 2] is the non-centered z coordinate in z_pixels for spot i . E.g. these are the coordinates stored in nb['find_spots']['spot_details'] . required transform jnp . ndarray float [4 x 3] . Affine transform to apply to yxz , once centered and z units changed to yx_pixels . transform[3, 2] is approximately the z shift in units of yx_pixels . E.g. this is one of the transforms stored in nb['register']['transform'] . required tile_centre jnp . ndarray float [3] . tile_centre[:2] are yx coordinates in yx_pixels of the centre of the tile that spots in yxz were found on. tile_centre[2] is the z coordinate in z_pixels of the centre of the tile. E.g. for tile of yxz dimensions [2048, 2048, 51] , tile_centre = [1023.5, 1023.5, 25] Each entry in tile_centre must be an integer multiple of 0.5 . required z_scale float Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. z_scale = pixel_size_z / pixel_size_yx where both are measured in microns. typically, z_scale > 1 because z_pixels are larger than the yx_pixels . required tile_sz jnp . ndarray int16 [3] . YXZ dimensions of tile required Returns: Type Description jnp . ndarray yxz_transform - int [n_spots x 3] . yxz_transform such that yxz_transform[i, [1,2]] are the transformed non-centered yx coordinates in yx_pixels for spot i . yxz_transform[i, 2] is the transformed non-centered z coordinate in z_pixels for spot i . jnp . ndarray in_range - bool [n_spots] . Whether spot s was in the bounds of the tile when transformed to round r , channel c . Source code in coppafish/spot_colors/base_optimised.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 @partial ( jax . jit , static_argnums = 3 ) def apply_transform ( yxz : jnp . ndarray , transform : jnp . ndarray , tile_centre : jnp . ndarray , z_scale : float , tile_sz : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray ]: \"\"\" This transforms the coordinates yxz based on an affine transform. E.g. to find coordinates of spots on the same tile but on a different round and channel. Args: yxz: ```int16 [n_spots x 3]```. ```yxz[i, :2]``` are the non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz[i, 2]``` is the non-centered z coordinate in ```z_pixels``` for spot ```i```. E.g. these are the coordinates stored in ```nb['find_spots']['spot_details']```. transform: ```float [4 x 3]```. Affine transform to apply to ```yxz```, once centered and z units changed to ```yx_pixels```. ```transform[3, 2]``` is approximately the z shift in units of ```yx_pixels```. E.g. this is one of the transforms stored in ```nb['register']['transform']```. tile_centre: ```float [3]```. ```tile_centre[:2]``` are yx coordinates in ```yx_pixels``` of the centre of the tile that spots in ```yxz``` were found on. ```tile_centre[2]``` is the z coordinate in ```z_pixels``` of the centre of the tile. E.g. for tile of ```yxz``` dimensions ```[2048, 2048, 51]```, ```tile_centre = [1023.5, 1023.5, 25]``` Each entry in ```tile_centre``` must be an integer multiple of ```0.5```. z_scale: Scale factor to multiply z coordinates to put them in units of yx pixels. I.e. ```z_scale = pixel_size_z / pixel_size_yx``` where both are measured in microns. typically, ```z_scale > 1``` because ```z_pixels``` are larger than the ```yx_pixels```. tile_sz: ```int16 [3]```. YXZ dimensions of tile Returns: - `yxz_transform` - ```int [n_spots x 3]```. ```yxz_transform``` such that ```yxz_transform[i, [1,2]]``` are the transformed non-centered yx coordinates in ```yx_pixels``` for spot ```i```. ```yxz_transform[i, 2]``` is the transformed non-centered z coordinate in ```z_pixels``` for spot ```i```. - ```in_range``` - ```bool [n_spots]```. Whether spot s was in the bounds of the tile when transformed to round `r`, channel `c`. \"\"\" return jax . vmap ( apply_transform_single , in_axes = ( 0 , None , None , None , None ), out_axes = ( 0 , 0 ))( yxz , transform , tile_centre , z_scale , tile_sz )","title":"apply_transform()"},{"location":"code/spot_colors/base/#coppafish.spot_colors.base_optimised.get_spot_colors","text":"Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on nbp_basic.use_rounds and nbp_basic.use_channels . Note Returned spot colors have dimension n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels) not n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels . Note invalid_value = -nbp_basic.tile_pixel_value_shift is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence I use this as integer nan. It will be invalid_value if the registered coordinate of spot s is outside the tile in round r , channel c . Parameters: Name Type Description Default yxz_base jnp . ndarray int16 [n_spots x 3] . Local yxz coordinates of spots found in the reference round/reference channel of tile t yx coordinates are in units of yx_pixels . z coordinates are in units of z_pixels . required t int Tile that spots were found on. required transforms jnp . ndarray float [n_tiles x n_rounds x n_channels x 4 x 3] . transforms[t, r, c] is the affine transform to get from tile t , ref_round , ref_channel to tile t , round r , channel c . required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required use_rounds Optional [ List [ int ]] int [n_use_rounds] . Rounds you would like to find the spot_color for. Error will raise if transform is zero for particular round. If None , all rounds in nbp_basic.use_rounds used. None use_channels Optional [ List [ int ]] int [n_use_channels] . Channels you would like to find the spot_color for. Error will raise if transform is zero for particular channel. If None , all channels in nbp_basic.use_channels used. None return_in_bounds bool if True , then only spot_colors which are within the tile bounds in all use_rounds / use_channels will be returned. The corresponding yxz_base coordinates will also be returned in this case. Otherwise, spot_colors will be returned for all the given yxz_base but if spot s is out of bounds on round r , channel c , then spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift . This is the only scenario for which spot_colors = invalid_value due to clipping in the extract step. False Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]] spot_colors - int32 [n_spots x n_rounds_use x n_channels_use] or int32 [n_spots_in_bounds x n_rounds_use x n_channels_use] . spot_colors[s, r, c] is the spot color for spot s in round use_rounds[r] , channel use_channels[c] . Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]] yxz_base - int16 [n_spots_in_bounds x 3] . If return_in_bounds , the yxz_base corresponding to spots in bounds for all use_rounds / use_channels will be returned. It is likely that n_spots_in_bounds won't be the same as n_spots . Source code in coppafish/spot_colors/base_optimised.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def get_spot_colors ( yxz_base : jnp . ndarray , t : int , transforms : jnp . ndarray , nbp_file : NotebookPage , nbp_basic : NotebookPage , use_rounds : Optional [ List [ int ]] = None , use_channels : Optional [ List [ int ]] = None , return_in_bounds : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , jnp . ndarray ]]: \"\"\" Takes some spots found on the reference round, and computes the corresponding spot intensity in specified imaging rounds/channels. By default, will run on `nbp_basic.use_rounds` and `nbp_basic.use_channels`. !!! note Returned spot colors have dimension `n_spots x len(nbp_basic.use_rounds) x len(nbp_basic.use_channels)` not `n_pixels x nbp_basic.n_rounds x nbp_basic.n_channels`. !!! note `invalid_value = -nbp_basic.tile_pixel_value_shift` is the lowest possible value saved in the npy file minus 1 (due to clipping in extract step), so it is impossible for spot_color to be this. Hence I use this as integer nan. It will be `invalid_value` if the registered coordinate of spot `s` is outside the tile in round `r`, channel `c`. Args: yxz_base: `int16 [n_spots x 3]`. Local yxz coordinates of spots found in the reference round/reference channel of tile `t` yx coordinates are in units of `yx_pixels`. z coordinates are in units of `z_pixels`. t: Tile that spots were found on. transforms: `float [n_tiles x n_rounds x n_channels x 4 x 3]`. `transforms[t, r, c]` is the affine transform to get from tile `t`, `ref_round`, `ref_channel` to tile `t`, round `r`, channel `c`. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page use_rounds: `int [n_use_rounds]`. Rounds you would like to find the `spot_color` for. Error will raise if transform is zero for particular round. If `None`, all rounds in `nbp_basic.use_rounds` used. use_channels: `int [n_use_channels]`. Channels you would like to find the `spot_color` for. Error will raise if transform is zero for particular channel. If `None`, all channels in `nbp_basic.use_channels` used. return_in_bounds: if `True`, then only `spot_colors` which are within the tile bounds in all `use_rounds` / `use_channels` will be returned. The corresponding `yxz_base` coordinates will also be returned in this case. Otherwise, `spot_colors` will be returned for all the given `yxz_base` but if spot `s` is out of bounds on round `r`, channel `c`, then `spot_colors[s, r, c] = invalid_value = -nbp_basic.tile_pixel_value_shift`. This is the only scenario for which `spot_colors = invalid_value` due to clipping in the extract step. Returns: - `spot_colors` - `int32 [n_spots x n_rounds_use x n_channels_use]` or `int32 [n_spots_in_bounds x n_rounds_use x n_channels_use]`. `spot_colors[s, r, c]` is the spot color for spot `s` in round `use_rounds[r]`, channel `use_channels[c]`. - `yxz_base` - `int16 [n_spots_in_bounds x 3]`. If `return_in_bounds`, the `yxz_base` corresponding to spots in bounds for all `use_rounds` / `use_channels` will be returned. It is likely that `n_spots_in_bounds` won't be the same as `n_spots`. \"\"\" if use_rounds is None : use_rounds = nbp_basic . use_rounds if use_channels is None : use_channels = nbp_basic . use_channels z_scale = nbp_basic . pixel_size_z / nbp_basic . pixel_size_xy n_spots = yxz_base . shape [ 0 ] no_verbose = n_spots < 10000 # note using nan means can't use integer even though data is integer n_use_rounds = len ( use_rounds ) n_use_channels = len ( use_channels ) # spots outside tile bounds on particular r/c will initially be set to 0. spot_colors = np . zeros (( n_spots , n_use_rounds , n_use_channels ), dtype = np . int32 ) tile_centre = jnp . array ( nbp_basic . tile_centre ) if not nbp_basic . is_3d : # use numpy not jax.numpy as reading in tiff is done in numpy. tile_sz = jnp . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , 1 ], dtype = jnp . int16 ) else : tile_sz = jnp . array ([ nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ], dtype = jnp . int16 ) with tqdm ( total = n_use_rounds * n_use_channels , disable = no_verbose ) as pbar : pbar . set_description ( f \"Reading { n_spots } spot_colors found on tile { t } from npy files\" ) for r in range ( n_use_rounds ): if not nbp_basic . is_3d : # If 2D, load in all channels first image_all_channels = np . load ( nbp_file . tile [ t ][ use_rounds [ r ]], mmap_mode = 'r' ) for c in range ( n_use_channels ): transform_rc = transforms [ t , use_rounds [ r ], use_channels [ c ]] pbar . set_postfix ({ 'round' : use_rounds [ r ], 'channel' : use_channels [ c ]}) if transform_rc [ 0 , 0 ] == 0 : raise ValueError ( f \"Transform for tile { t } , round { use_rounds [ r ] } , channel { use_channels [ c ] } is zero:\" f \" \\n { transform_rc } \" ) yxz_transform , in_range = apply_transform ( yxz_base , transform_rc , tile_centre , z_scale , tile_sz ) yxz_transform = np . asarray ( yxz_transform ) in_range = np . asarray ( in_range ) yxz_transform = yxz_transform [ in_range ] if yxz_transform . shape [ 0 ] > 0 : # Read in the shifted uint16 colors here, and remove shift later. if nbp_basic . is_3d : spot_colors [ in_range , r , c ] = utils . npy . load_tile ( nbp_file , nbp_basic , t , use_rounds [ r ], use_channels [ c ], yxz_transform , apply_shift = False ) else : spot_colors [ in_range , r , c ] = image_all_channels [ use_channels [ c ]][ tuple ( np . asarray ( yxz_transform [:, i ]) for i in range ( 2 ))] pbar . update ( 1 ) # Remove shift so now spots outside bounds have color equal to - nbp_basic.tile_pixel_shift_value. # It is impossible for any actual spot color to be this due to clipping at the extract stage. spot_colors = spot_colors - nbp_basic . tile_pixel_value_shift invalid_value = - nbp_basic . tile_pixel_value_shift if return_in_bounds : good = ~ np . any ( spot_colors == invalid_value , axis = ( 1 , 2 )) return spot_colors [ good ], yxz_base [ good ] else : return spot_colors","title":"get_spot_colors()"},{"location":"code/stitch/check_shifts/","text":"check_shifts_register ( nb ) This checks that a decent number of shifts computed in the register_initial stage of the pipeline are acceptable ( score > score_thresh ). An error will be raised if the fraction of shifts with score < score_thresh exceeds config['register_initial']['n_shifts_error_fraction'] . Parameters: Name Type Description Default nb Notebook Notebook containing stitch page. required Source code in coppafish/stitch/check_shifts.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def check_shifts_register ( nb : Notebook ): \"\"\" This checks that a decent number of shifts computed in the `register_initial` stage of the pipeline are acceptable (`score > score_thresh`). An error will be raised if the fraction of shifts with `score < score_thresh` exceeds `config['register_initial']['n_shifts_error_fraction']`. Args: nb: *Notebook* containing `stitch` page. \"\"\" r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel c_shift = nb . register_initial . shift_channel use_rounds = np . asarray ( nb . basic_info . use_rounds ) use_tiles = np . asarray ( nb . basic_info . use_tiles ) n_shifts = len ( use_rounds ) * len ( use_tiles ) n_fail = 0 config = nb . get_config ()[ 'register_initial' ] shift = nb . register_initial . shift score = nb . register_initial . shift_score score_thresh = nb . register_initial . shift_score_thresh fail_info = np . zeros (( 0 , 7 ), dtype = int ) for r in nb . basic_info . use_rounds : fail_tiles = use_tiles [ np . where (( score [ use_tiles , r ] < score_thresh [ use_tiles , r ]) . flatten ())[ 0 ]] n_fail += len ( fail_tiles ) if len ( fail_tiles ) > 0 : fail_info_r = np . zeros (( len ( fail_tiles ), 7 ), dtype = int ) fail_info_r [:, 0 ] = r fail_info_r [:, 1 ] = fail_tiles fail_info_r [:, 2 : 5 ] = shift [ fail_tiles , r ] fail_info_r [:, 5 ] = score [ fail_tiles , r ] . flatten () fail_info_r [:, 6 ] = score_thresh [ fail_tiles , r ] . flatten () fail_info = np . append ( fail_info , fail_info_r , axis = 0 ) if n_fail >= 1 : message = f \" \\n Info for the { n_fail } shifts from round { r_ref } /channel { c_ref } to channel { c_shift } \" \\ f \" with score < score_thresh: \\n \" \\ f \"Round, Tile, Y shift, X shift, Z shift, score, score_thresh \\n \" \\ f \" { fail_info } \" n_error_thresh = int ( np . floor ( config [ 'n_shifts_error_fraction' ] * n_shifts )) if n_fail > n_error_thresh : message = message + f \" \\n { n_fail } / { n_shifts } shifts have score < score_thresh. \\n \" \\ f \"This exceeds error threshold of { n_error_thresh } . \\n Look at the following \" \\ f \"diagnostics to decide if shifts are acceptable to continue: \\n \" \\ f \"coppafish.plot.view_register_shift_info \\n coppafish.plot.view_register_search \\n \" \\ f \"coppafish.plot.view_icp \\n \" \\ f \"If shifts looks wrong, maybe try re-running with \" \\ f \"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\" # Recommend channel with the most spots on the tile/round for which it has the least. spot_no = nb . find_spots . spot_no [ np . ix_ ( nb . basic_info . use_tiles , nb . basic_info . use_rounds )] # For each channel this is number of spots on tile/round with the least spots. spot_no = np . min ( spot_no , axis = ( 0 , 1 )) c_most_spots = np . argmax ( spot_no ) if c_most_spots != nb . register_initial . shift_channel : message = message + f \" \\n Also consider changing config['register_initial']['shift_channel']. \" \\ f \"Current channel { c_ref } has at least { spot_no [ c_ref ] } on all tiles and rounds \" \\ f \"but channel { c_most_spots } has at least { spot_no [ c_most_spots ] } .\" raise ValueError ( f \" { message } \" ) else : warnings . warn ( message ) check_shifts_stitch ( nb ) This checks that a decent number of shifts computed in the stitch stage of the pipeline are acceptable ( score > score_thresh ). An error will be raised if the fraction of shifts with score < score_thresh exceeds config['stitch']['n_shifts_error_fraction'] . Parameters: Name Type Description Default nb Notebook Notebook containing stitch page. required Source code in coppafish/stitch/check_shifts.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def check_shifts_stitch ( nb : Notebook ): \"\"\" This checks that a decent number of shifts computed in the stitch stage of the pipeline are acceptable (`score > score_thresh`). An error will be raised if the fraction of shifts with `score < score_thresh` exceeds `config['stitch']['n_shifts_error_fraction']`. Args: nb: *Notebook* containing `stitch` page. \"\"\" n_shifts = 0 n_fail = 0 message = \"\" config = nb . get_config ()[ 'stitch' ] directions = [ 'south' , 'west' ] dir_opp = { 'south' : 'north' , 'west' : 'east' } for j in directions : n_shifts += len ( nb . stitch . __getattribute__ ( f \" { j } _score\" )) fail_ind = np . where (( nb . stitch . __getattribute__ ( f \" { j } _score\" ) < nb . stitch . __getattribute__ ( f \" { j } _score_thresh\" )) . flatten ())[ 0 ] n_fail += len ( fail_ind ) if len ( fail_ind ) > 0 : fail_info = np . zeros (( len ( fail_ind ), 7 ), dtype = int ) fail_info [:, : 2 ] = nb . stitch . __getattribute__ ( f \" { j } _pairs\" )[ fail_ind ] fail_info [:, 2 : 5 ] = nb . stitch . __getattribute__ ( f \" { j } _shifts\" )[ fail_ind ] fail_info [:, 5 ] = nb . stitch . __getattribute__ ( f \" { j } _score\" )[ fail_ind ] . flatten () fail_info [:, 6 ] = nb . stitch . __getattribute__ ( f \" { j } _score_thresh\" )[ fail_ind ] . flatten () message = message + f \" \\n Info for the { len ( fail_ind ) } shifts with score < score_thresh in { j } direction: \\n \" \\ f \"Tile, Tile to { dir_opp [ j ] } , Y shift, X shift, Z shift, score, score_thresh \\n \" \\ f \" { fail_info } \" n_error_thresh = int ( np . floor ( config [ 'n_shifts_error_fraction' ] * n_shifts )) if n_fail > n_error_thresh : message = message + f \" \\n { n_fail } / { n_shifts } shifts have score < score_thresh. \\n \" \\ f \"This exceeds error threshold of { n_error_thresh } . \\n Look at the following diagnostics \" \\ f \"to decide if stitching is acceptable to continue: \\n \" \\ f \"coppafish.plot.view_stitch_shift_info \\n coppafish.plot.view_stitch \\n \" \\ f \"coppafish.plot.view_stitch_overlap \\n \" \\ f \"coppafish.plot.view_stitch_search \\n If stitching looks wrong, maybe try re-running with \" \\ f \"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\" raise ValueError ( f \" { message } \" ) elif n_fail >= 1 : warnings . warn ( message )","title":"Check Shifts"},{"location":"code/stitch/check_shifts/#coppafish.stitch.check_shifts.check_shifts_register","text":"This checks that a decent number of shifts computed in the register_initial stage of the pipeline are acceptable ( score > score_thresh ). An error will be raised if the fraction of shifts with score < score_thresh exceeds config['register_initial']['n_shifts_error_fraction'] . Parameters: Name Type Description Default nb Notebook Notebook containing stitch page. required Source code in coppafish/stitch/check_shifts.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def check_shifts_register ( nb : Notebook ): \"\"\" This checks that a decent number of shifts computed in the `register_initial` stage of the pipeline are acceptable (`score > score_thresh`). An error will be raised if the fraction of shifts with `score < score_thresh` exceeds `config['register_initial']['n_shifts_error_fraction']`. Args: nb: *Notebook* containing `stitch` page. \"\"\" r_ref = nb . basic_info . ref_round c_ref = nb . basic_info . ref_channel c_shift = nb . register_initial . shift_channel use_rounds = np . asarray ( nb . basic_info . use_rounds ) use_tiles = np . asarray ( nb . basic_info . use_tiles ) n_shifts = len ( use_rounds ) * len ( use_tiles ) n_fail = 0 config = nb . get_config ()[ 'register_initial' ] shift = nb . register_initial . shift score = nb . register_initial . shift_score score_thresh = nb . register_initial . shift_score_thresh fail_info = np . zeros (( 0 , 7 ), dtype = int ) for r in nb . basic_info . use_rounds : fail_tiles = use_tiles [ np . where (( score [ use_tiles , r ] < score_thresh [ use_tiles , r ]) . flatten ())[ 0 ]] n_fail += len ( fail_tiles ) if len ( fail_tiles ) > 0 : fail_info_r = np . zeros (( len ( fail_tiles ), 7 ), dtype = int ) fail_info_r [:, 0 ] = r fail_info_r [:, 1 ] = fail_tiles fail_info_r [:, 2 : 5 ] = shift [ fail_tiles , r ] fail_info_r [:, 5 ] = score [ fail_tiles , r ] . flatten () fail_info_r [:, 6 ] = score_thresh [ fail_tiles , r ] . flatten () fail_info = np . append ( fail_info , fail_info_r , axis = 0 ) if n_fail >= 1 : message = f \" \\n Info for the { n_fail } shifts from round { r_ref } /channel { c_ref } to channel { c_shift } \" \\ f \" with score < score_thresh: \\n \" \\ f \"Round, Tile, Y shift, X shift, Z shift, score, score_thresh \\n \" \\ f \" { fail_info } \" n_error_thresh = int ( np . floor ( config [ 'n_shifts_error_fraction' ] * n_shifts )) if n_fail > n_error_thresh : message = message + f \" \\n { n_fail } / { n_shifts } shifts have score < score_thresh. \\n \" \\ f \"This exceeds error threshold of { n_error_thresh } . \\n Look at the following \" \\ f \"diagnostics to decide if shifts are acceptable to continue: \\n \" \\ f \"coppafish.plot.view_register_shift_info \\n coppafish.plot.view_register_search \\n \" \\ f \"coppafish.plot.view_icp \\n \" \\ f \"If shifts looks wrong, maybe try re-running with \" \\ f \"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\" # Recommend channel with the most spots on the tile/round for which it has the least. spot_no = nb . find_spots . spot_no [ np . ix_ ( nb . basic_info . use_tiles , nb . basic_info . use_rounds )] # For each channel this is number of spots on tile/round with the least spots. spot_no = np . min ( spot_no , axis = ( 0 , 1 )) c_most_spots = np . argmax ( spot_no ) if c_most_spots != nb . register_initial . shift_channel : message = message + f \" \\n Also consider changing config['register_initial']['shift_channel']. \" \\ f \"Current channel { c_ref } has at least { spot_no [ c_ref ] } on all tiles and rounds \" \\ f \"but channel { c_most_spots } has at least { spot_no [ c_most_spots ] } .\" raise ValueError ( f \" { message } \" ) else : warnings . warn ( message )","title":"check_shifts_register()"},{"location":"code/stitch/check_shifts/#coppafish.stitch.check_shifts.check_shifts_stitch","text":"This checks that a decent number of shifts computed in the stitch stage of the pipeline are acceptable ( score > score_thresh ). An error will be raised if the fraction of shifts with score < score_thresh exceeds config['stitch']['n_shifts_error_fraction'] . Parameters: Name Type Description Default nb Notebook Notebook containing stitch page. required Source code in coppafish/stitch/check_shifts.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def check_shifts_stitch ( nb : Notebook ): \"\"\" This checks that a decent number of shifts computed in the stitch stage of the pipeline are acceptable (`score > score_thresh`). An error will be raised if the fraction of shifts with `score < score_thresh` exceeds `config['stitch']['n_shifts_error_fraction']`. Args: nb: *Notebook* containing `stitch` page. \"\"\" n_shifts = 0 n_fail = 0 message = \"\" config = nb . get_config ()[ 'stitch' ] directions = [ 'south' , 'west' ] dir_opp = { 'south' : 'north' , 'west' : 'east' } for j in directions : n_shifts += len ( nb . stitch . __getattribute__ ( f \" { j } _score\" )) fail_ind = np . where (( nb . stitch . __getattribute__ ( f \" { j } _score\" ) < nb . stitch . __getattribute__ ( f \" { j } _score_thresh\" )) . flatten ())[ 0 ] n_fail += len ( fail_ind ) if len ( fail_ind ) > 0 : fail_info = np . zeros (( len ( fail_ind ), 7 ), dtype = int ) fail_info [:, : 2 ] = nb . stitch . __getattribute__ ( f \" { j } _pairs\" )[ fail_ind ] fail_info [:, 2 : 5 ] = nb . stitch . __getattribute__ ( f \" { j } _shifts\" )[ fail_ind ] fail_info [:, 5 ] = nb . stitch . __getattribute__ ( f \" { j } _score\" )[ fail_ind ] . flatten () fail_info [:, 6 ] = nb . stitch . __getattribute__ ( f \" { j } _score_thresh\" )[ fail_ind ] . flatten () message = message + f \" \\n Info for the { len ( fail_ind ) } shifts with score < score_thresh in { j } direction: \\n \" \\ f \"Tile, Tile to { dir_opp [ j ] } , Y shift, X shift, Z shift, score, score_thresh \\n \" \\ f \" { fail_info } \" n_error_thresh = int ( np . floor ( config [ 'n_shifts_error_fraction' ] * n_shifts )) if n_fail > n_error_thresh : message = message + f \" \\n { n_fail } / { n_shifts } shifts have score < score_thresh. \\n \" \\ f \"This exceeds error threshold of { n_error_thresh } . \\n Look at the following diagnostics \" \\ f \"to decide if stitching is acceptable to continue: \\n \" \\ f \"coppafish.plot.view_stitch_shift_info \\n coppafish.plot.view_stitch \\n \" \\ f \"coppafish.plot.view_stitch_overlap \\n \" \\ f \"coppafish.plot.view_stitch_search \\n If stitching looks wrong, maybe try re-running with \" \\ f \"different configuration parameters e.g. smaller shift_step or larger shift_max_range.\" raise ValueError ( f \" { message } \" ) elif n_fail >= 1 : warnings . warn ( message )","title":"check_shifts_stitch()"},{"location":"code/stitch/shift/","text":"compute_shift ( yxz_base , yxz_transform , min_score_2d , min_score_multiplier , min_score_min_dist , min_score_max_dist , neighb_dist_thresh , y_shifts , x_shifts , z_shifts = None , widen = None , max_range = None , z_scale = 1 , nz_collapse = None , z_step = 3 ) This finds the shift from those given that is best applied to yxz_base to match yxz_transform . If the score of this is below min_score_2d , a widened search is performed. If the score is above min_score_2d , a refined search is done about the best shift so as to find the absolute best shift, not the best shift among those given. Parameters: Name Type Description Default yxz_base np . ndarray int [n_spots_base x 3] . Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). required yxz_transform np . ndarray int [n_spots_transform x 3] . Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). required min_score_2d Optional [ float ] If score of best shift is below this, will search among the widened shifts. If None , min_score_2d will be computed using get_score_thresh . required min_score_multiplier Optional [ float ] Parameter used to find min_score_2d and min_score_3d if not given. Typical = 1.5 (definitely more than 1 ). required min_score_min_dist Optional [ float ] min_score_2d is set to max score of those scores for shifts a distance between min_dist and max_dist from the best_shift. required min_score_max_dist Optional [ float ] min_score_2d is set to max score of those scores for shifts a distance between min_dist and max_dist from the best_shift. required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required z_shifts Optional [ np . ndarray ] float [n_z_shifts] . All possible shifts to test in z direction, probably made with np.arange . If not given, will compute automatically from initial guess when making slices and z_step . None widen Optional [ List [ int ]] int [3] . By how many shifts to extend search in [y, x, z] direction if score below min_score . This many are added above and below current range. If all widen parameters are 0 , widened search is never performed. If None , set to [0, 0, 0] . None max_range Optional [ List [ int ]] int [3] . The range of shifts searched over will continue to be increased according to widen until the max_range is reached in each dimension. If a good shift is still not found, the best shift will still be returned without error. If None and widen supplied, range will only be widened once. None z_scale Union [ float , List ] By what scale factor to multiply z coordinates to make them same units as xy. I.e. z_pixel_size / xy_pixel_size . If one value, given same scale used for yxz_base and yxz_transform. Otherwise, first value used for yxz_base and second for yxz_transform. 1 nz_collapse Optional [ int ] Maximum number of z-planes allowed to be flattened into a 2D slice. If None , n_slices =1. Should be None for 2D data. None z_step int int . Step of shift search in z direction in uints of z_pixels . z_shifts are computed automatically as 1 shift either side of an initial guess. 3 Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x, shift_z] . Best shift found. float best_score - float . Score of best shift. float min_score_3d - float . Same as min_score_2d , unless input was None in which case this is the calculated value. dict debug_info - dict containing debugging information: shifts_2d : int [n_shifts_2d x 2] All yx shifts searched to get best yx_shift . scores_2d : float [n_shifts_2d] Score corresponding to each 2d shift. shifts_3d : int [n_shifts_3d x 3] All yxz shifts searched to get best yxz_shift . None if nz_collapse is None i.e. 2D point cloud. scores_3d : float [n_shifts_3d] Score corresponding to each 3d shift. None if nz_collapse is None i.e. 2D point cloud. shift_2d_initial : float [2] Best shift found after first 2D search. I.e. annulus around this shift was used to compute min_score_2d and shift_thresh . shift_thresh : int [3] yxz shift corresponding to min_score_3d . Will be None if min_score_2d provided in advance. shift_thresh[:2] is the yx shift corresponding to min_score_2d min_score_2d : Same as input min_score_2d , unless was None in which case this is the calculated value. Source code in coppafish/stitch/shift.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 def compute_shift ( yxz_base : np . ndarray , yxz_transform : np . ndarray , min_score_2d : Optional [ float ], min_score_multiplier : Optional [ float ], min_score_min_dist : Optional [ float ], min_score_max_dist : Optional [ float ], neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , z_shifts : Optional [ np . ndarray ] = None , widen : Optional [ List [ int ]] = None , max_range : Optional [ List [ int ]] = None , z_scale : Union [ float , List ] = 1 , nz_collapse : Optional [ int ] = None , z_step : int = 3 ) -> Tuple [ np . ndarray , float , float , dict ]: \"\"\" This finds the shift from those given that is best applied to `yxz_base` to match `yxz_transform`. If the `score` of this is below `min_score_2d`, a widened search is performed. If the `score` is above `min_score_2d`, a refined search is done about the best shift so as to find the absolute best shift, not the best shift among those given. Args: yxz_base: `int [n_spots_base x 3]`. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). yxz_transform: `int [n_spots_transform x 3]`. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). min_score_2d: If score of best shift is below this, will search among the widened shifts. If `None`, `min_score_2d` will be computed using `get_score_thresh`. min_score_multiplier: Parameter used to find `min_score_2d` and `min_score_3d` if not given. Typical = `1.5` (definitely more than `1`). min_score_min_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist` and `max_dist` from the best_shift. min_score_max_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist` and `max_dist` from the best_shift. neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. z_shifts: `float [n_z_shifts]`. All possible shifts to test in z direction, probably made with `np.arange`. If not given, will compute automatically from initial guess when making slices and `z_step`. widen: `int [3]`. By how many shifts to extend search in `[y, x, z]` direction if score below `min_score`. This many are added above and below current range. If all widen parameters are `0`, widened search is never performed. If `None`, set to `[0, 0, 0]`. max_range: `int [3]`. The range of shifts searched over will continue to be increased according to `widen` until the `max_range` is reached in each dimension. If a good shift is still not found, the best shift will still be returned without error. If None and widen supplied, range will only be widened once. z_scale: By what scale factor to multiply z coordinates to make them same units as xy. I.e. `z_pixel_size / xy_pixel_size`. If one value, given same scale used for yxz_base and yxz_transform. Otherwise, first value used for yxz_base and second for yxz_transform. nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice. If `None`, `n_slices`=1. Should be `None` for 2D data. z_step: `int`. Step of shift search in z direction in uints of `z_pixels`. `z_shifts` are computed automatically as 1 shift either side of an initial guess. Returns: - `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. - `best_score` - `float`. Score of best shift. - `min_score_3d` - `float`. Same as `min_score_2d`, unless input was `None` in which case this is the calculated value. - `debug_info` - dict containing debugging information: - `shifts_2d`: `int [n_shifts_2d x 2]` All yx shifts searched to get best `yx_shift`. - `scores_2d`: `float [n_shifts_2d]` Score corresponding to each 2d shift. - `shifts_3d`: `int [n_shifts_3d x 3]` All yxz shifts searched to get best `yxz_shift`. `None` if `nz_collapse is None` i.e. 2D point cloud. - `scores_3d`: `float [n_shifts_3d]` Score corresponding to each 3d shift. `None` if `nz_collapse is None` i.e. 2D point cloud. - `shift_2d_initial`: `float [2]` Best shift found after first 2D search. I.e. annulus around this shift was used to compute `min_score_2d` and `shift_thresh`. - `shift_thresh`: `int [3]` yxz shift corresponding to `min_score_3d`. Will be `None` if `min_score_2d` provided in advance. `shift_thresh[:2]` is the yx shift corresponding to `min_score_2d` - `min_score_2d`: Same as input `min_score_2d`, unless was `None` in which case this is the calculated value. \"\"\" if widen is None : widen = [ 0 , 0 , 0 ] if np . asarray ( z_scale ) . size == 1 : z_scale = [ z_scale , z_scale ] if len ( z_scale ) > 2 : raise ValueError ( f 'Only 2 z_scale values should be provided but z_scale given was { z_scale } .' ) yx_base_slices , yx_transform_trees , z_shift_guess = get_2d_slices ( yxz_base , yxz_transform , nz_collapse ) if nz_collapse is not None : # Only do z-scaling in 3D case yxz_base = yxz_base * [ 1 , 1 , z_scale [ 0 ]] yxz_transform = yxz_transform * [ 1 , 1 , z_scale [ 1 ]] yxz_transform_tree = KDTree ( yxz_transform ) shift_2d , score_2d , all_shifts_2d , all_scores_2d = get_best_shift_2d ( yx_base_slices , yx_transform_trees , neighb_dist_thresh , y_shifts , x_shifts ) # Only look at 3 shifts in z to start with about guess from getting the 2d slices. if z_shifts is None : z_shifts = np . arange ( z_shift_guess - z_step , z_shift_guess + z_step + 1 , z_step ) # save initial_shifts so don't look over same shifts twice # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2) shift_2d_initial = shift_2d . copy () if min_score_2d is None : min_score_2d , shift_thresh = get_score_thresh ( all_shifts_2d , all_scores_2d , shift_2d , min_score_min_dist , min_score_max_dist , min_score_multiplier ) shift_thresh = np . pad ( shift_thresh , ( 0 , 1 )) # add z shift = 0 else : shift_thresh = None if score_2d <= min_score_2d and np . max ( widen [: 2 ]) > 0 : shift_ranges = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts ]]) if max_range is None : # If don't specify max_range, only widen once. max_range = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts , z_shifts ]]) * ( np . array ( widen [: 2 ]) > 0 ) max_range [ max_range > 0 ] += 1 max_range_2d = max_range [: 2 ] else : max_range_2d = np . asarray ( max_range [: 2 ]) # keep extending range of shifts in yx until good score reached or hit max shift_range. while score_2d <= min_score_2d : if np . all ( shift_ranges >= max_range_2d ): warnings . warn ( f \"Shift search range exceeds max_range = { max_range_2d } in yxz directions but \\n \" f \"best score is only { round ( score_2d , 2 ) } which is below \" f \"min_score = { round ( min_score_2d , 2 ) } .\" f \" \\n Best shift found was { shift_2d } .\" ) break else : warnings . warn ( f \"Best shift found ( { shift_2d } ) has score of { round ( score_2d , 2 ) } which is below \" f \"min_score = { round ( min_score_2d , 2 ) } .\" f \" \\n Running again with extended shift search range in yx.\" ) if shift_ranges [ 0 ] < max_range_2d [ 0 ]: y_shifts = extend_array ( y_shifts , widen [ 0 ]) if shift_ranges [ 1 ] < max_range_2d [ 1 ]: x_shifts = extend_array ( x_shifts , widen [ 1 ]) shift_2d_new , score_2d_new , all_shifts_new , all_scores_new = \\ get_best_shift_2d ( yx_base_slices , yx_transform_trees , neighb_dist_thresh , y_shifts , x_shifts , all_shifts_2d ) if score_2d_new > score_2d : score_2d = score_2d_new shift_2d = shift_2d_new # update initial_shifts so don't look over same shifts twice all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new , axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2) shift_ranges = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts ]]) if nz_collapse is None : # nz_collapse not provided for 2D data. shift = np . append ( shift_2d , 0 ) score = score_2d all_shifts_3d = None all_scores_3d = None min_score_3d = min_score_2d else : ignore_shifts = None if shift_thresh is None : y_shift_2d = np . array ( shift_2d [ 0 ]) x_shift_2d = np . array ( shift_2d [ 1 ]) else : y_shift_2d = np . array ([ shift_2d [ 0 ], shift_thresh [ 0 ]]) x_shift_2d = np . array ([ shift_2d [ 1 ], shift_thresh [ 1 ]]) if len ( np . unique ( y_shift_2d )) == 2 and len ( np . unique ( x_shift_2d )) == 2 : # Only find shifts for the shift_2d and shift_thresh, get rid of cross terms. ignore_shifts = np . array ([[ shift_2d [ 0 ], shift_thresh [ 1 ]], [ shift_thresh [ 0 ], shift_2d [ 1 ]]]) ignore_shifts = np . tile ( np . pad ( ignore_shifts , [( 0 , 0 ), ( 0 , 1 )]), [ len ( z_shifts ), 1 ]) ignore_shifts [:, 2 ] = np . repeat ( z_shifts * z_scale [ 0 ], len ( y_shift_2d )) # z_scale for yxz_base used from now on as we are finding the shift from yxz_base to yxz_transform. shift , score , all_shifts_3d , all_scores_3d = get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shift_2d , x_shift_2d , z_shifts * z_scale [ 0 ], ignore_shifts = ignore_shifts ) if shift_thresh is not None : # Set min_score_3d to max score at shift used to find min_score_2d across all z planes # multiplied by min_score_multiplier shift_thresh_ind = np . where ( numpy_indexed . indices ( shift_thresh [ np . newaxis , : 2 ] . astype ( int ), all_shifts_3d [:, : 2 ] . astype ( int ), missing =- 10 ) == 0 )[ 0 ] shift_thresh_best_ind = shift_thresh_ind [ np . argmax ( all_scores_3d [ shift_thresh_ind ])] min_score_3d = all_scores_3d [ shift_thresh_best_ind ] * min_score_multiplier shift_thresh = ( all_shifts_3d [ shift_thresh_best_ind ] / [ 1 , 1 , z_scale [ 0 ]]) . astype ( int ) else : min_score_3d = min_score_2d if score < min_score_2d and widen [ 2 ] > 0 : # keep extending range of shifts in z until good score reached or hit max shift_range. # yx shift is kept as 2d shift found when using slices. max_range_z = np . asarray ( max_range [ 2 ]) z_shift_range = np . ptp ( z_shifts ) while score < min_score_3d : if z_shift_range > max_range_z : warnings . warn ( f \"Shift search range exceeds max_range = { max_range_z } in z directions but \\n \" f \"best score is only { np . around ( score , 2 ) } which is below \" f \"min_score = { np . around ( min_score_3d , 2 ) } .\" f \" \\n Best shift found was { shift } .\" ) break else : warnings . warn ( f \"Best shift found ( { shift } ) has score of { round ( score , 2 ) } which is below \" f \"min_score = { np . around ( min_score_3d , 2 ) } .\" f \" \\n Running again with extended shift search range in z.\" ) z_shifts = extend_array ( z_shifts , widen [ 2 ]) shift_new , score_new , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shift_2d , x_shift_2d , z_shifts * z_scale [ 0 ], all_shifts_3d ) if score_new > score : score = score_new shift = shift_new # update initial_shifts so don't look over same shifts twice all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) z_shift_range = np . ptp ( z_shifts ) # refined search near maxima with half the step y_shifts = refined_shifts ( y_shifts , shift [ 0 ]) x_shifts = refined_shifts ( x_shifts , shift [ 1 ]) z_shifts = refined_shifts ( z_shifts , shift [ 2 ] / z_scale [ 0 ]) shift2 , score2 , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shifts , x_shifts , z_shifts * z_scale [ 0 ], all_shifts_3d ) if score2 > score : shift = shift2 if nz_collapse is None : all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new [:, : 2 ], axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) else : all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) # final search with a step of 1 y_shifts = refined_shifts ( y_shifts , shift [ 0 ], refined_scale = 1e-50 , extend_scale = 1 ) x_shifts = refined_shifts ( x_shifts , shift [ 1 ], refined_scale = 1e-50 , extend_scale = 1 ) z_shifts = refined_shifts ( z_shifts , shift [ 2 ] / z_scale [ 0 ], refined_scale = 1e-50 , extend_scale = 1 ) shift , score , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shifts , x_shifts , z_shifts * z_scale [ 0 ], all_shifts_3d ) if nz_collapse is None : all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new [:, : 2 ], axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) else : all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) all_shifts_3d [:, 2 ] = all_shifts_3d [:, 2 ] / z_scale [ 0 ] all_shifts_3d = all_shifts_3d . astype ( np . int16 ) shift [ 2 ] = shift [ 2 ] / z_scale [ 0 ] return shift . astype ( int ), score , min_score_3d , { 'shifts_2d' : all_shifts_2d , 'scores_2d' : all_scores_2d , 'shifts_3d' : all_shifts_3d , 'scores_3d' : all_scores_3d , 'shift_2d_initial' : shift_2d_initial , 'shift_thresh' : shift_thresh , 'min_score_2d' : min_score_2d } extend_array ( array , extend_scale , direction = 'both' ) Extrapolates array using its mean spacing in the direction specified by extend_sz values. Parameters: Name Type Description Default array np . ndarray float [n_values] . Array probably produced using np.arange . It is expected to be in ascending order with constant step. required extend_scale int By how many values to extend the array. required direction str One of the following, specifying how to extend the array - 'below' - array extended below the min value. 'above' - array extended above the max value 'both' - array extended in both directions (by extend_sz in each direction). 'both' Returns: Type Description np . ndarray float [n_values + extend_scale * (direction == 'both' + 1)] . array extrapolated in direction specified. Source code in coppafish/stitch/shift.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def extend_array ( array : np . ndarray , extend_scale : int , direction : str = 'both' ) -> np . ndarray : \"\"\" Extrapolates array using its mean spacing in the direction specified by `extend_sz` values. Args: array: `float [n_values]`. Array probably produced using `np.arange`. It is expected to be in ascending order with constant step. extend_scale: By how many values to extend the array. direction: One of the following, specifying how to extend the `array` - - `'below'` - `array` extended below the min value. - `'above'` - `array` extended above the max value - `'both'` - `array` extended in both directions (by `extend_sz` in each direction). Returns: `float [n_values + extend_scale * (direction == 'both' + 1)]`. `array` extrapolated in `direction` specified. \"\"\" if extend_scale == 0 : ext_array = array else : step = np . mean ( np . ediff1d ( array )) ext_below = np . arange ( array . min () - extend_scale * step , array . min (), step ) ext_above = np . arange ( array . max () + step , array . max () + extend_scale * step + step / 2 , step ) if direction == 'below' : ext_array = np . concatenate (( ext_below , array )) elif direction == 'above' : ext_array = np . concatenate (( array , ext_above )) elif direction == 'both' : ext_array = np . concatenate (( ext_below , array , ext_above )) else : raise ValueError ( f \"direction specified was { direction } , whereas it should be 'below', 'above' or 'both'\" ) return ext_array get_2d_slices ( yxz_base , yxz_transform , nz_collapse ) This splits yxz_base and yxz_transform into n_slices = nz / nz_collapse 2D slices. Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search. Parameters: Name Type Description Default yxz_base np . ndarray float [n_spots_base x 3] . Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). required yxz_transform np . ndarray float [n_spots_transform x 3] . Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). required nz_collapse Optional [ int ] Maximum number of z-planes allowed to be flattened into a 2D slice. If None , n_slices =1. required Returns: Type Description List [ np . ndarray ] yx_base_slices - List of n_slices arrays indicating yx_base coordinates of spots in that slice. List [ KDTree ] yx_transform_trees - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. int transform_min_z - Guess of z shift from yxz_base to yxz_transform in units of z_pixels . Source code in coppafish/stitch/shift.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def get_2d_slices ( yxz_base : np . ndarray , yxz_transform : np . ndarray , nz_collapse : Optional [ int ]) -> Tuple [ List [ np . ndarray ], List [ KDTree ], int ]: \"\"\" This splits `yxz_base` and `yxz_transform` into `n_slices = nz / nz_collapse` 2D slices. Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search. Args: yxz_base: `float [n_spots_base x 3]`. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). yxz_transform: `float [n_spots_transform x 3]`. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice. If `None`, `n_slices`=1. Returns: - `yx_base_slices` - List of n_slices arrays indicating yx_base coordinates of spots in that slice. - `yx_transform_trees` - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. - transform_min_z - Guess of z shift from `yxz_base` to `yxz_transform` in units of `z_pixels`. \"\"\" if nz_collapse is not None : nz = int ( np . ceil ( yxz_base [:, 2 ] . max () + 1 )) n_slices = int ( np . ceil ( nz / nz_collapse )) base_z_slices = np . array_split ( np . arange ( nz ), n_slices ) slice_max_z_base = 0 # min z for slice 0 transform_max_z = int ( np . ceil ( yxz_transform [:, 2 ] . max () + 1 )) # transform_min_z provides an approx guess to the z shift. transform_min_z = np . min ([ int ( np . floor ( yxz_transform [:, 2 ] . min ())), transform_max_z - nz ]) slice_max_z_transform = transform_min_z # min z for slice 0 yx_base_slices = [] yx_transform_trees = [] for i in range ( n_slices ): slice_min_z_base = slice_max_z_base # set min z to the max z of the last slice slice_max_z_base = base_z_slices [ i ][ - 1 ] + 1 in_slice_base = np . array ([ yxz_base [:, 2 ] >= slice_min_z_base , yxz_base [:, 2 ] < slice_max_z_base ]) . all ( axis = 0 ) yx_base_slices . append ( yxz_base [ in_slice_base , : 2 ]) # transform z coords may have systematic z shift so start from min_z not 0. slice_min_z_transform = slice_max_z_transform # set min z to the max z of the last slice if i == n_slices - 1 : # For final slice, ensure all z planes in yxz_transform included. slice_max_z_transform = transform_max_z + 1 else : slice_max_z_transform = base_z_slices [ i ][ - 1 ] + 1 + transform_min_z in_slice_transform = np . array ([ yxz_transform [:, 2 ] >= slice_min_z_transform , yxz_transform [:, 2 ] < slice_max_z_transform ]) . all ( axis = 0 ) yx_transform_trees . append ( KDTree ( yxz_transform [ in_slice_transform , : 2 ])) else : transform_min_z = 0 yx_base_slices = [ yxz_base [:, : 2 ]] yx_transform_trees = [ KDTree ( yxz_transform [:, : 2 ])] return yx_base_slices , yx_transform_trees , transform_min_z get_best_shift_2d ( yx_base_slices , yx_transform_trees , neighb_dist_thresh , y_shifts , x_shifts , ignore_shifts = None ) Parameters: Name Type Description Default yx_base_slices List [ np . ndarray ] List of n_slices arrays indicating yx_base coordinates of spots in that slice. required yx_transform_trees List [ KDTree ] List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required ignore_shifts Optional [ np . ndarray ] float [n_ignore x 2] . Contains yx shifts to not search over. If None , all permutations of y_shifts , x_shifts used. None Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x] . Best shift found. float best_score - float . Score of best shift. np . ndarray all_shifts - float [n_shifts x 2] . yx shifts searched over. np . ndarray score - float [n_shifts] . Score of all shifts. Source code in coppafish/stitch/shift.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_best_shift_2d ( yx_base_slices : List [ np . ndarray ], yx_transform_trees : List [ KDTree ], neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , ignore_shifts : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , float , np . ndarray , np . ndarray ]: \"\"\" Args: yx_base_slices: List of n_slices arrays indicating yx_base coordinates of spots in that slice. yx_transform_trees: List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. ignore_shifts: `float [n_ignore x 2]`. Contains yx shifts to not search over. If `None`, all permutations of `y_shifts`, `x_shifts` used. Returns: - `best_shift` - `float [shift_y, shift_x]`. Best shift found. - `best_score` - `float`. Score of best shift. - `all_shifts` - `float [n_shifts x 2]`. yx shifts searched over. - `score` - `float [n_shifts]`. Score of all shifts. \"\"\" all_shifts = np . array ( np . meshgrid ( y_shifts , x_shifts )) . T . reshape ( - 1 , 2 ) if ignore_shifts is not None : all_shifts = setdiff2d ( all_shifts , ignore_shifts ) score = np . zeros ( all_shifts . shape [ 0 ]) n_trees = len ( yx_transform_trees ) dist_upper_bound = 3 * neighb_dist_thresh # beyond this, score < exp(-4.5) and quicker to use this. for i in range ( all_shifts . shape [ 0 ]): for j in range ( n_trees ): yx_shifted = yx_base_slices [ j ] + all_shifts [ i ] distances = yx_transform_trees [ j ] . query ( yx_shifted , distance_upper_bound = dist_upper_bound )[ 0 ] score [ i ] += shift_score ( distances , neighb_dist_thresh ) best_shift_ind = score . argmax () return all_shifts [ best_shift_ind ], score [ best_shift_ind ], all_shifts , score get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shifts , x_shifts , z_shifts , ignore_shifts = None ) Finds the shift from those given that is best applied to yx_base to match yx_transform . Parameters: Name Type Description Default yxz_base np . ndarray float [n_spots_base x 3] . Coordinates of spots on base image (yxz units must be same). required yxz_transform_tree KDTree KDTree built from coordinates of spots on transformed image ( float [n_spots_transform x 3] , yxz units must be same). required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required z_shifts np . ndarray float [n_z_shifts] . All possible shifts to test in z direction, probably made with np.arange . required ignore_shifts Optional [ np . ndarray ] float [n_ignore x 3] . Contains yxz shifts to not search over. If None , all permutations of y_shifts , x_shifts , z_shifts used. None Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x, shift_z] . Best shift found. float best_score - float . Score of best shift. np . ndarray all_shifts - float [n_shifts x 3] . yxz shifts searched over. np . ndarray score - float [n_shifts] . Score of all shifts. Source code in coppafish/stitch/shift.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def get_best_shift_3d ( yxz_base : np . ndarray , yxz_transform_tree : KDTree , neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , z_shifts : np . ndarray , ignore_shifts : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , float , np . ndarray , np . ndarray ]: \"\"\" Finds the shift from those given that is best applied to `yx_base` to match `yx_transform`. Args: yxz_base: `float [n_spots_base x 3]`. Coordinates of spots on base image (yxz units must be same). yxz_transform_tree: KDTree built from coordinates of spots on transformed image (`float [n_spots_transform x 3]`, yxz units must be same). neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. z_shifts: `float [n_z_shifts]`. All possible shifts to test in z direction, probably made with `np.arange`. ignore_shifts: `float [n_ignore x 3]`. Contains yxz shifts to not search over. If `None`, all permutations of `y_shifts`, `x_shifts`, `z_shifts` used. Returns: - `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. - `best_score` - `float`. Score of best shift. - `all_shifts` - `float [n_shifts x 3]`. yxz shifts searched over. - `score` - `float [n_shifts]`. Score of all shifts. \"\"\" all_shifts = np . array ( np . meshgrid ( y_shifts , x_shifts , z_shifts )) . T . reshape ( - 1 , 3 ) if ignore_shifts is not None : all_shifts = setdiff2d ( all_shifts , ignore_shifts ) score = np . zeros ( all_shifts . shape [ 0 ]) dist_upper_bound = 3 * neighb_dist_thresh # beyond this, score < exp(-4.5) and quicker to use this. for i in range ( all_shifts . shape [ 0 ]): yxz_shifted = yxz_base + all_shifts [ i ] distances = yxz_transform_tree . query ( yxz_shifted , distance_upper_bound = dist_upper_bound )[ 0 ] score [ i ] = shift_score ( distances , neighb_dist_thresh ) best_shift_ind = score . argmax () return all_shifts [ best_shift_ind ], score [ best_shift_ind ], all_shifts , score get_score_thresh ( all_shifts , all_scores , best_shift , min_dist , max_dist , thresh_multiplier ) Score thresh is the max of all scores from transforms between a distance=min_dist and distance=max_dist from the best_shift . I.e. we expect just for actual shift, there will be sharp gradient in score near it, so threshold is multiple of nearby score. If not the actual shift, then expect scores in this annulus will also be quite large. Parameters: Name Type Description Default all_shifts np . ndarray float [n_shifts x 2] . yx shifts searched over. required all_scores np . ndarray float [n_shifts] . all_scores[s] is the score corresponding to all_shifts[s] . required best_shift Union [ np . ndarray , List ] float [2] . yx shift with the best score. required min_dist float score_thresh computed from all_shifts a distance between min_shift and max_shift from best_shifts . required max_dist float score_thresh computed from all_shifts a distance between min_shift and max_shift from best_shifts . required thresh_multiplier float score_thresh is thresh_multiplier * mean of scores of shifts the correct distance from best_shift . required Returns: Type Description float score_thresh - Threshold used to determine if best_shift found is legitimate. Optional [ np . ndarray ] shift_thresh - float [2] shift corresponding to score_thresh . Will be None if there were no shifts in the range set by min_dist and max_dist . Source code in coppafish/stitch/shift.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def get_score_thresh ( all_shifts : np . ndarray , all_scores : np . ndarray , best_shift : Union [ np . ndarray , List ], min_dist : float , max_dist : float , thresh_multiplier : float ) -> Tuple [ float , Optional [ np . ndarray ]]: \"\"\" Score thresh is the max of all scores from transforms between a `distance=min_dist` and `distance=max_dist` from the `best_shift`. I.e. we expect just for actual shift, there will be sharp gradient in score near it, so threshold is multiple of nearby score. If not the actual shift, then expect scores in this annulus will also be quite large. Args: all_shifts: `float [n_shifts x 2]`. yx shifts searched over. all_scores: `float [n_shifts]`. `all_scores[s]` is the score corresponding to `all_shifts[s]`. best_shift: `float [2]`. yx shift with the best score. min_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift` from `best_shifts`. max_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift` from `best_shifts`. thresh_multiplier: `score_thresh` is `thresh_multiplier` * mean of scores of shifts the correct distance from `best_shift`. Returns: score_thresh - Threshold used to determine if `best_shift` found is legitimate. shift_thresh - `float [2]` shift corresponding to `score_thresh`. Will be None if there were no shifts in the range set by `min_dist` and `max_dist`. \"\"\" dist_to_best = pairwise_distances ( np . array ( all_shifts ), np . array ( best_shift )[ np . newaxis ]) . squeeze () use = np . where ( np . logical_and ( dist_to_best <= max_dist , dist_to_best >= min_dist ))[ 0 ] if len ( use ) > 0 : thresh_ind = use [ np . argmax ( all_scores [ use ])] score_thresh = thresh_multiplier * all_scores [ thresh_ind ] shift_thresh = all_shifts [ thresh_ind ] else : score_thresh = thresh_multiplier * np . median ( all_scores ) shift_thresh = None return score_thresh , shift_thresh refined_shifts ( shifts , best_shift , refined_scale = 0.5 , extend_scale = 2 ) If shifts is an array with mean spacing step then this builds array that covers from best_shift - extend_scale * step to best_shift + extend_scale * step with a spacing of step*refined_scale . The new step, step*refined_scale , is forced to be an integer. If only one shift provided, doesn't do anything. Parameters: Name Type Description Default shifts np . ndarray float [n_shifts] . Array probably produced using np.arange . It is expected to be in ascending order with constant step. required best_shift float Value in shifts to build new shifts around. required refined_scale float Scaling to apply to find new shift spacing. 0.5 extend_scale float By how many steps to build new shifts. 2 Returns: Type Description np . ndarray float [n_new_shifts] . Array covering from np . ndarray best_shift - extend_scale * step to best_shift + extend_scale * step with a spacing of step*refined_scale . Source code in coppafish/stitch/shift.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def refined_shifts ( shifts : np . ndarray , best_shift : float , refined_scale : float = 0.5 , extend_scale : float = 2 ) -> np . ndarray : \"\"\" If `shifts` is an array with mean spacing `step` then this builds array that covers from `best_shift - extend_scale * step` to `best_shift + extend_scale * step` with a spacing of `step*refined_scale`. The new step, `step*refined_scale`, is forced to be an integer. If only one `shift` provided, doesn't do anything. Args: shifts: `float [n_shifts]`. Array probably produced using `np.arange`. It is expected to be in ascending order with constant step. best_shift: Value in `shifts` to build new shifts around. refined_scale: Scaling to apply to find new shift spacing. extend_scale: By how many steps to build new shifts. Returns: `float [n_new_shifts]`. Array covering from `best_shift - extend_scale * step` to `best_shift + extend_scale * step` with a spacing of `step*refined_scale`. \"\"\" if np . size ( shifts ) == 1 : refined_shifts = shifts else : step = np . mean ( np . ediff1d ( shifts )) refined_step = np . ceil ( refined_scale * step ) . astype ( int ) refined_shifts = np . arange ( best_shift - extend_scale * step , best_shift + extend_scale * step + refined_step / 2 , refined_step ) return refined_shifts shift_score ( distances , thresh ) Computes a score to quantify how good a shift is based on the distances between the neighbours found. the value of this score is approximately the number of close neighbours found. Parameters: Name Type Description Default distances np . ndarray float [n_neighbours] . Distances between each pair of neighbours. required thresh float Basically the distance in pixels below which neighbours are a good match. Typical = 2 . required Returns: Type Description float Score to quantify how good a shift is based on the distances between the neighbours found. Source code in coppafish/stitch/shift.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def shift_score ( distances : np . ndarray , thresh : float ) -> float : \"\"\" Computes a score to quantify how good a shift is based on the distances between the neighbours found. the value of this score is approximately the number of close neighbours found. Args: distances: `float [n_neighbours]`. Distances between each pair of neighbours. thresh: Basically the distance in pixels below which neighbours are a good match. Typical = `2`. Returns: Score to quantify how good a shift is based on the distances between the neighbours found. \"\"\" return np . sum ( np . exp ( - distances ** 2 / ( 2 * thresh ** 2 ))) update_shifts ( search_shifts , prev_found_shifts ) Returns a new array of search_shifts around the mean of prev_found_shifts if new array has fewer entries or if mean of prev_found_shifts is outside initial range of search_shifts . If more than one prev_found_shifts is outside the search_shifts in the same way i.e. too high or too low, search_shifts will be updated too. Parameters: Name Type Description Default search_shifts np . ndarray int [n_shifts] . Indicates all shifts currently searched over. required prev_found_shifts np . ndarray int [n_shifts_found] . Indicate shifts found on all previous runs of compute_shift . required Returns: Type Description np . ndarray int [n_new_shifts] . np . ndarray New set of shifts around mean of previously found shifts. np . ndarray Will only return updated shifts if new array has fewer entries than before or mean of prev_found_shifts np . ndarray is outside range of search_shifts . Source code in coppafish/stitch/shift.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def update_shifts ( search_shifts : np . ndarray , prev_found_shifts : np . ndarray ) -> np . ndarray : \"\"\" Returns a new array of `search_shifts` around the mean of `prev_found_shifts` if new array has fewer entries or if mean of `prev_found_shifts` is outside initial range of `search_shifts`. If more than one `prev_found_shifts` is outside the `search_shifts` in the same way i.e. too high or too low, `search_shifts` will be updated too. Args: search_shifts: `int [n_shifts]`. Indicates all shifts currently searched over. prev_found_shifts: `int [n_shifts_found]`. Indicate shifts found on all previous runs of `compute_shift`. Returns: `int [n_new_shifts]`. New set of shifts around mean of previously found shifts. Will only return updated shifts if new array has fewer entries than before or mean of `prev_found_shifts` is outside range of `search_shifts`. \"\"\" n_shifts = len ( search_shifts ) n_prev_shifts = len ( prev_found_shifts ) if n_shifts > 1 and n_prev_shifts > 0 : step = np . mean ( np . ediff1d ( search_shifts )) mean_shift = np . mean ( prev_found_shifts , dtype = int ) n_shifts_new = 2 * np . ceil (( np . max ( prev_found_shifts ) - mean_shift ) / step + 1 ) . astype ( int ) + 1 if n_shifts_new < n_shifts or mean_shift <= search_shifts . min () or mean_shift >= search_shifts . max (): # only update shifts if results in less to search over. search_shifts = refined_shifts ( search_shifts , mean_shift , 1 , (( n_shifts_new - 1 ) / 2 ) . astype ( int )) if np . sum ( prev_found_shifts > search_shifts . max ()) > 1 : search_shifts = np . arange ( search_shifts . min (), prev_found_shifts . max () + step , step ) if np . sum ( prev_found_shifts < search_shifts . min ()) > 1 : search_shifts = np . arange ( prev_found_shifts . min (), search_shifts . max () + step , step ) return search_shifts","title":"Shift"},{"location":"code/stitch/shift/#coppafish.stitch.shift.compute_shift","text":"This finds the shift from those given that is best applied to yxz_base to match yxz_transform . If the score of this is below min_score_2d , a widened search is performed. If the score is above min_score_2d , a refined search is done about the best shift so as to find the absolute best shift, not the best shift among those given. Parameters: Name Type Description Default yxz_base np . ndarray int [n_spots_base x 3] . Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). required yxz_transform np . ndarray int [n_spots_transform x 3] . Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). required min_score_2d Optional [ float ] If score of best shift is below this, will search among the widened shifts. If None , min_score_2d will be computed using get_score_thresh . required min_score_multiplier Optional [ float ] Parameter used to find min_score_2d and min_score_3d if not given. Typical = 1.5 (definitely more than 1 ). required min_score_min_dist Optional [ float ] min_score_2d is set to max score of those scores for shifts a distance between min_dist and max_dist from the best_shift. required min_score_max_dist Optional [ float ] min_score_2d is set to max score of those scores for shifts a distance between min_dist and max_dist from the best_shift. required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required z_shifts Optional [ np . ndarray ] float [n_z_shifts] . All possible shifts to test in z direction, probably made with np.arange . If not given, will compute automatically from initial guess when making slices and z_step . None widen Optional [ List [ int ]] int [3] . By how many shifts to extend search in [y, x, z] direction if score below min_score . This many are added above and below current range. If all widen parameters are 0 , widened search is never performed. If None , set to [0, 0, 0] . None max_range Optional [ List [ int ]] int [3] . The range of shifts searched over will continue to be increased according to widen until the max_range is reached in each dimension. If a good shift is still not found, the best shift will still be returned without error. If None and widen supplied, range will only be widened once. None z_scale Union [ float , List ] By what scale factor to multiply z coordinates to make them same units as xy. I.e. z_pixel_size / xy_pixel_size . If one value, given same scale used for yxz_base and yxz_transform. Otherwise, first value used for yxz_base and second for yxz_transform. 1 nz_collapse Optional [ int ] Maximum number of z-planes allowed to be flattened into a 2D slice. If None , n_slices =1. Should be None for 2D data. None z_step int int . Step of shift search in z direction in uints of z_pixels . z_shifts are computed automatically as 1 shift either side of an initial guess. 3 Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x, shift_z] . Best shift found. float best_score - float . Score of best shift. float min_score_3d - float . Same as min_score_2d , unless input was None in which case this is the calculated value. dict debug_info - dict containing debugging information: shifts_2d : int [n_shifts_2d x 2] All yx shifts searched to get best yx_shift . scores_2d : float [n_shifts_2d] Score corresponding to each 2d shift. shifts_3d : int [n_shifts_3d x 3] All yxz shifts searched to get best yxz_shift . None if nz_collapse is None i.e. 2D point cloud. scores_3d : float [n_shifts_3d] Score corresponding to each 3d shift. None if nz_collapse is None i.e. 2D point cloud. shift_2d_initial : float [2] Best shift found after first 2D search. I.e. annulus around this shift was used to compute min_score_2d and shift_thresh . shift_thresh : int [3] yxz shift corresponding to min_score_3d . Will be None if min_score_2d provided in advance. shift_thresh[:2] is the yx shift corresponding to min_score_2d min_score_2d : Same as input min_score_2d , unless was None in which case this is the calculated value. Source code in coppafish/stitch/shift.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 def compute_shift ( yxz_base : np . ndarray , yxz_transform : np . ndarray , min_score_2d : Optional [ float ], min_score_multiplier : Optional [ float ], min_score_min_dist : Optional [ float ], min_score_max_dist : Optional [ float ], neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , z_shifts : Optional [ np . ndarray ] = None , widen : Optional [ List [ int ]] = None , max_range : Optional [ List [ int ]] = None , z_scale : Union [ float , List ] = 1 , nz_collapse : Optional [ int ] = None , z_step : int = 3 ) -> Tuple [ np . ndarray , float , float , dict ]: \"\"\" This finds the shift from those given that is best applied to `yxz_base` to match `yxz_transform`. If the `score` of this is below `min_score_2d`, a widened search is performed. If the `score` is above `min_score_2d`, a refined search is done about the best shift so as to find the absolute best shift, not the best shift among those given. Args: yxz_base: `int [n_spots_base x 3]`. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). yxz_transform: `int [n_spots_transform x 3]`. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). min_score_2d: If score of best shift is below this, will search among the widened shifts. If `None`, `min_score_2d` will be computed using `get_score_thresh`. min_score_multiplier: Parameter used to find `min_score_2d` and `min_score_3d` if not given. Typical = `1.5` (definitely more than `1`). min_score_min_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist` and `max_dist` from the best_shift. min_score_max_dist: `min_score_2d` is set to max score of those scores for shifts a distance between `min_dist` and `max_dist` from the best_shift. neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. z_shifts: `float [n_z_shifts]`. All possible shifts to test in z direction, probably made with `np.arange`. If not given, will compute automatically from initial guess when making slices and `z_step`. widen: `int [3]`. By how many shifts to extend search in `[y, x, z]` direction if score below `min_score`. This many are added above and below current range. If all widen parameters are `0`, widened search is never performed. If `None`, set to `[0, 0, 0]`. max_range: `int [3]`. The range of shifts searched over will continue to be increased according to `widen` until the `max_range` is reached in each dimension. If a good shift is still not found, the best shift will still be returned without error. If None and widen supplied, range will only be widened once. z_scale: By what scale factor to multiply z coordinates to make them same units as xy. I.e. `z_pixel_size / xy_pixel_size`. If one value, given same scale used for yxz_base and yxz_transform. Otherwise, first value used for yxz_base and second for yxz_transform. nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice. If `None`, `n_slices`=1. Should be `None` for 2D data. z_step: `int`. Step of shift search in z direction in uints of `z_pixels`. `z_shifts` are computed automatically as 1 shift either side of an initial guess. Returns: - `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. - `best_score` - `float`. Score of best shift. - `min_score_3d` - `float`. Same as `min_score_2d`, unless input was `None` in which case this is the calculated value. - `debug_info` - dict containing debugging information: - `shifts_2d`: `int [n_shifts_2d x 2]` All yx shifts searched to get best `yx_shift`. - `scores_2d`: `float [n_shifts_2d]` Score corresponding to each 2d shift. - `shifts_3d`: `int [n_shifts_3d x 3]` All yxz shifts searched to get best `yxz_shift`. `None` if `nz_collapse is None` i.e. 2D point cloud. - `scores_3d`: `float [n_shifts_3d]` Score corresponding to each 3d shift. `None` if `nz_collapse is None` i.e. 2D point cloud. - `shift_2d_initial`: `float [2]` Best shift found after first 2D search. I.e. annulus around this shift was used to compute `min_score_2d` and `shift_thresh`. - `shift_thresh`: `int [3]` yxz shift corresponding to `min_score_3d`. Will be `None` if `min_score_2d` provided in advance. `shift_thresh[:2]` is the yx shift corresponding to `min_score_2d` - `min_score_2d`: Same as input `min_score_2d`, unless was `None` in which case this is the calculated value. \"\"\" if widen is None : widen = [ 0 , 0 , 0 ] if np . asarray ( z_scale ) . size == 1 : z_scale = [ z_scale , z_scale ] if len ( z_scale ) > 2 : raise ValueError ( f 'Only 2 z_scale values should be provided but z_scale given was { z_scale } .' ) yx_base_slices , yx_transform_trees , z_shift_guess = get_2d_slices ( yxz_base , yxz_transform , nz_collapse ) if nz_collapse is not None : # Only do z-scaling in 3D case yxz_base = yxz_base * [ 1 , 1 , z_scale [ 0 ]] yxz_transform = yxz_transform * [ 1 , 1 , z_scale [ 1 ]] yxz_transform_tree = KDTree ( yxz_transform ) shift_2d , score_2d , all_shifts_2d , all_scores_2d = get_best_shift_2d ( yx_base_slices , yx_transform_trees , neighb_dist_thresh , y_shifts , x_shifts ) # Only look at 3 shifts in z to start with about guess from getting the 2d slices. if z_shifts is None : z_shifts = np . arange ( z_shift_guess - z_step , z_shift_guess + z_step + 1 , z_step ) # save initial_shifts so don't look over same shifts twice # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2) shift_2d_initial = shift_2d . copy () if min_score_2d is None : min_score_2d , shift_thresh = get_score_thresh ( all_shifts_2d , all_scores_2d , shift_2d , min_score_min_dist , min_score_max_dist , min_score_multiplier ) shift_thresh = np . pad ( shift_thresh , ( 0 , 1 )) # add z shift = 0 else : shift_thresh = None if score_2d <= min_score_2d and np . max ( widen [: 2 ]) > 0 : shift_ranges = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts ]]) if max_range is None : # If don't specify max_range, only widen once. max_range = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts , z_shifts ]]) * ( np . array ( widen [: 2 ]) > 0 ) max_range [ max_range > 0 ] += 1 max_range_2d = max_range [: 2 ] else : max_range_2d = np . asarray ( max_range [: 2 ]) # keep extending range of shifts in yx until good score reached or hit max shift_range. while score_2d <= min_score_2d : if np . all ( shift_ranges >= max_range_2d ): warnings . warn ( f \"Shift search range exceeds max_range = { max_range_2d } in yxz directions but \\n \" f \"best score is only { round ( score_2d , 2 ) } which is below \" f \"min_score = { round ( min_score_2d , 2 ) } .\" f \" \\n Best shift found was { shift_2d } .\" ) break else : warnings . warn ( f \"Best shift found ( { shift_2d } ) has score of { round ( score_2d , 2 ) } which is below \" f \"min_score = { round ( min_score_2d , 2 ) } .\" f \" \\n Running again with extended shift search range in yx.\" ) if shift_ranges [ 0 ] < max_range_2d [ 0 ]: y_shifts = extend_array ( y_shifts , widen [ 0 ]) if shift_ranges [ 1 ] < max_range_2d [ 1 ]: x_shifts = extend_array ( x_shifts , widen [ 1 ]) shift_2d_new , score_2d_new , all_shifts_new , all_scores_new = \\ get_best_shift_2d ( yx_base_slices , yx_transform_trees , neighb_dist_thresh , y_shifts , x_shifts , all_shifts_2d ) if score_2d_new > score_2d : score_2d = score_2d_new shift_2d = shift_2d_new # update initial_shifts so don't look over same shifts twice all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new , axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) # initial_shifts = np.array(np.meshgrid(y_shifts, x_shifts)).T.reshape(-1, 2) shift_ranges = np . array ([ np . ptp ( i ) for i in [ y_shifts , x_shifts ]]) if nz_collapse is None : # nz_collapse not provided for 2D data. shift = np . append ( shift_2d , 0 ) score = score_2d all_shifts_3d = None all_scores_3d = None min_score_3d = min_score_2d else : ignore_shifts = None if shift_thresh is None : y_shift_2d = np . array ( shift_2d [ 0 ]) x_shift_2d = np . array ( shift_2d [ 1 ]) else : y_shift_2d = np . array ([ shift_2d [ 0 ], shift_thresh [ 0 ]]) x_shift_2d = np . array ([ shift_2d [ 1 ], shift_thresh [ 1 ]]) if len ( np . unique ( y_shift_2d )) == 2 and len ( np . unique ( x_shift_2d )) == 2 : # Only find shifts for the shift_2d and shift_thresh, get rid of cross terms. ignore_shifts = np . array ([[ shift_2d [ 0 ], shift_thresh [ 1 ]], [ shift_thresh [ 0 ], shift_2d [ 1 ]]]) ignore_shifts = np . tile ( np . pad ( ignore_shifts , [( 0 , 0 ), ( 0 , 1 )]), [ len ( z_shifts ), 1 ]) ignore_shifts [:, 2 ] = np . repeat ( z_shifts * z_scale [ 0 ], len ( y_shift_2d )) # z_scale for yxz_base used from now on as we are finding the shift from yxz_base to yxz_transform. shift , score , all_shifts_3d , all_scores_3d = get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shift_2d , x_shift_2d , z_shifts * z_scale [ 0 ], ignore_shifts = ignore_shifts ) if shift_thresh is not None : # Set min_score_3d to max score at shift used to find min_score_2d across all z planes # multiplied by min_score_multiplier shift_thresh_ind = np . where ( numpy_indexed . indices ( shift_thresh [ np . newaxis , : 2 ] . astype ( int ), all_shifts_3d [:, : 2 ] . astype ( int ), missing =- 10 ) == 0 )[ 0 ] shift_thresh_best_ind = shift_thresh_ind [ np . argmax ( all_scores_3d [ shift_thresh_ind ])] min_score_3d = all_scores_3d [ shift_thresh_best_ind ] * min_score_multiplier shift_thresh = ( all_shifts_3d [ shift_thresh_best_ind ] / [ 1 , 1 , z_scale [ 0 ]]) . astype ( int ) else : min_score_3d = min_score_2d if score < min_score_2d and widen [ 2 ] > 0 : # keep extending range of shifts in z until good score reached or hit max shift_range. # yx shift is kept as 2d shift found when using slices. max_range_z = np . asarray ( max_range [ 2 ]) z_shift_range = np . ptp ( z_shifts ) while score < min_score_3d : if z_shift_range > max_range_z : warnings . warn ( f \"Shift search range exceeds max_range = { max_range_z } in z directions but \\n \" f \"best score is only { np . around ( score , 2 ) } which is below \" f \"min_score = { np . around ( min_score_3d , 2 ) } .\" f \" \\n Best shift found was { shift } .\" ) break else : warnings . warn ( f \"Best shift found ( { shift } ) has score of { round ( score , 2 ) } which is below \" f \"min_score = { np . around ( min_score_3d , 2 ) } .\" f \" \\n Running again with extended shift search range in z.\" ) z_shifts = extend_array ( z_shifts , widen [ 2 ]) shift_new , score_new , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shift_2d , x_shift_2d , z_shifts * z_scale [ 0 ], all_shifts_3d ) if score_new > score : score = score_new shift = shift_new # update initial_shifts so don't look over same shifts twice all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) z_shift_range = np . ptp ( z_shifts ) # refined search near maxima with half the step y_shifts = refined_shifts ( y_shifts , shift [ 0 ]) x_shifts = refined_shifts ( x_shifts , shift [ 1 ]) z_shifts = refined_shifts ( z_shifts , shift [ 2 ] / z_scale [ 0 ]) shift2 , score2 , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shifts , x_shifts , z_shifts * z_scale [ 0 ], all_shifts_3d ) if score2 > score : shift = shift2 if nz_collapse is None : all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new [:, : 2 ], axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) else : all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) # final search with a step of 1 y_shifts = refined_shifts ( y_shifts , shift [ 0 ], refined_scale = 1e-50 , extend_scale = 1 ) x_shifts = refined_shifts ( x_shifts , shift [ 1 ], refined_scale = 1e-50 , extend_scale = 1 ) z_shifts = refined_shifts ( z_shifts , shift [ 2 ] / z_scale [ 0 ], refined_scale = 1e-50 , extend_scale = 1 ) shift , score , all_shifts_new , all_scores_new = \\ get_best_shift_3d ( yxz_base , yxz_transform_tree , neighb_dist_thresh , y_shifts , x_shifts , z_shifts * z_scale [ 0 ], all_shifts_3d ) if nz_collapse is None : all_shifts_2d = np . append ( all_shifts_2d , all_shifts_new [:, : 2 ], axis = 0 ) all_scores_2d = np . append ( all_scores_2d , all_scores_new , axis = 0 ) else : all_shifts_3d = np . append ( all_shifts_3d , all_shifts_new , axis = 0 ) all_scores_3d = np . append ( all_scores_3d , all_scores_new , axis = 0 ) all_shifts_3d [:, 2 ] = all_shifts_3d [:, 2 ] / z_scale [ 0 ] all_shifts_3d = all_shifts_3d . astype ( np . int16 ) shift [ 2 ] = shift [ 2 ] / z_scale [ 0 ] return shift . astype ( int ), score , min_score_3d , { 'shifts_2d' : all_shifts_2d , 'scores_2d' : all_scores_2d , 'shifts_3d' : all_shifts_3d , 'scores_3d' : all_scores_3d , 'shift_2d_initial' : shift_2d_initial , 'shift_thresh' : shift_thresh , 'min_score_2d' : min_score_2d }","title":"compute_shift()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.extend_array","text":"Extrapolates array using its mean spacing in the direction specified by extend_sz values. Parameters: Name Type Description Default array np . ndarray float [n_values] . Array probably produced using np.arange . It is expected to be in ascending order with constant step. required extend_scale int By how many values to extend the array. required direction str One of the following, specifying how to extend the array - 'below' - array extended below the min value. 'above' - array extended above the max value 'both' - array extended in both directions (by extend_sz in each direction). 'both' Returns: Type Description np . ndarray float [n_values + extend_scale * (direction == 'both' + 1)] . array extrapolated in direction specified. Source code in coppafish/stitch/shift.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def extend_array ( array : np . ndarray , extend_scale : int , direction : str = 'both' ) -> np . ndarray : \"\"\" Extrapolates array using its mean spacing in the direction specified by `extend_sz` values. Args: array: `float [n_values]`. Array probably produced using `np.arange`. It is expected to be in ascending order with constant step. extend_scale: By how many values to extend the array. direction: One of the following, specifying how to extend the `array` - - `'below'` - `array` extended below the min value. - `'above'` - `array` extended above the max value - `'both'` - `array` extended in both directions (by `extend_sz` in each direction). Returns: `float [n_values + extend_scale * (direction == 'both' + 1)]`. `array` extrapolated in `direction` specified. \"\"\" if extend_scale == 0 : ext_array = array else : step = np . mean ( np . ediff1d ( array )) ext_below = np . arange ( array . min () - extend_scale * step , array . min (), step ) ext_above = np . arange ( array . max () + step , array . max () + extend_scale * step + step / 2 , step ) if direction == 'below' : ext_array = np . concatenate (( ext_below , array )) elif direction == 'above' : ext_array = np . concatenate (( array , ext_above )) elif direction == 'both' : ext_array = np . concatenate (( ext_below , array , ext_above )) else : raise ValueError ( f \"direction specified was { direction } , whereas it should be 'below', 'above' or 'both'\" ) return ext_array","title":"extend_array()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_2d_slices","text":"This splits yxz_base and yxz_transform into n_slices = nz / nz_collapse 2D slices. Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search. Parameters: Name Type Description Default yxz_base np . ndarray float [n_spots_base x 3] . Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). required yxz_transform np . ndarray float [n_spots_transform x 3] . Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). required nz_collapse Optional [ int ] Maximum number of z-planes allowed to be flattened into a 2D slice. If None , n_slices =1. required Returns: Type Description List [ np . ndarray ] yx_base_slices - List of n_slices arrays indicating yx_base coordinates of spots in that slice. List [ KDTree ] yx_transform_trees - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. int transform_min_z - Guess of z shift from yxz_base to yxz_transform in units of z_pixels . Source code in coppafish/stitch/shift.py 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def get_2d_slices ( yxz_base : np . ndarray , yxz_transform : np . ndarray , nz_collapse : Optional [ int ]) -> Tuple [ List [ np . ndarray ], List [ KDTree ], int ]: \"\"\" This splits `yxz_base` and `yxz_transform` into `n_slices = nz / nz_collapse` 2D slices. Then can do a 2D exhaustive search over multiple 2D slices instead of 3D exhaustive search. Args: yxz_base: `float [n_spots_base x 3]`. Coordinates of spots on base image (yx in yx pixel units, z in z pixel units). yxz_transform: `float [n_spots_transform x 3]`. Coordinates of spots on transformed image (yx in yx pixel units, z in z pixel units). nz_collapse: Maximum number of z-planes allowed to be flattened into a 2D slice. If `None`, `n_slices`=1. Returns: - `yx_base_slices` - List of n_slices arrays indicating yx_base coordinates of spots in that slice. - `yx_transform_trees` - List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. - transform_min_z - Guess of z shift from `yxz_base` to `yxz_transform` in units of `z_pixels`. \"\"\" if nz_collapse is not None : nz = int ( np . ceil ( yxz_base [:, 2 ] . max () + 1 )) n_slices = int ( np . ceil ( nz / nz_collapse )) base_z_slices = np . array_split ( np . arange ( nz ), n_slices ) slice_max_z_base = 0 # min z for slice 0 transform_max_z = int ( np . ceil ( yxz_transform [:, 2 ] . max () + 1 )) # transform_min_z provides an approx guess to the z shift. transform_min_z = np . min ([ int ( np . floor ( yxz_transform [:, 2 ] . min ())), transform_max_z - nz ]) slice_max_z_transform = transform_min_z # min z for slice 0 yx_base_slices = [] yx_transform_trees = [] for i in range ( n_slices ): slice_min_z_base = slice_max_z_base # set min z to the max z of the last slice slice_max_z_base = base_z_slices [ i ][ - 1 ] + 1 in_slice_base = np . array ([ yxz_base [:, 2 ] >= slice_min_z_base , yxz_base [:, 2 ] < slice_max_z_base ]) . all ( axis = 0 ) yx_base_slices . append ( yxz_base [ in_slice_base , : 2 ]) # transform z coords may have systematic z shift so start from min_z not 0. slice_min_z_transform = slice_max_z_transform # set min z to the max z of the last slice if i == n_slices - 1 : # For final slice, ensure all z planes in yxz_transform included. slice_max_z_transform = transform_max_z + 1 else : slice_max_z_transform = base_z_slices [ i ][ - 1 ] + 1 + transform_min_z in_slice_transform = np . array ([ yxz_transform [:, 2 ] >= slice_min_z_transform , yxz_transform [:, 2 ] < slice_max_z_transform ]) . all ( axis = 0 ) yx_transform_trees . append ( KDTree ( yxz_transform [ in_slice_transform , : 2 ])) else : transform_min_z = 0 yx_base_slices = [ yxz_base [:, : 2 ]] yx_transform_trees = [ KDTree ( yxz_transform [:, : 2 ])] return yx_base_slices , yx_transform_trees , transform_min_z","title":"get_2d_slices()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_best_shift_2d","text":"Parameters: Name Type Description Default yx_base_slices List [ np . ndarray ] List of n_slices arrays indicating yx_base coordinates of spots in that slice. required yx_transform_trees List [ KDTree ] List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required ignore_shifts Optional [ np . ndarray ] float [n_ignore x 2] . Contains yx shifts to not search over. If None , all permutations of y_shifts , x_shifts used. None Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x] . Best shift found. float best_score - float . Score of best shift. np . ndarray all_shifts - float [n_shifts x 2] . yx shifts searched over. np . ndarray score - float [n_shifts] . Score of all shifts. Source code in coppafish/stitch/shift.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def get_best_shift_2d ( yx_base_slices : List [ np . ndarray ], yx_transform_trees : List [ KDTree ], neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , ignore_shifts : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , float , np . ndarray , np . ndarray ]: \"\"\" Args: yx_base_slices: List of n_slices arrays indicating yx_base coordinates of spots in that slice. yx_transform_trees: List of n_slices KDTrees, each built from the yx_transform coordinates of spots in that slice. neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. ignore_shifts: `float [n_ignore x 2]`. Contains yx shifts to not search over. If `None`, all permutations of `y_shifts`, `x_shifts` used. Returns: - `best_shift` - `float [shift_y, shift_x]`. Best shift found. - `best_score` - `float`. Score of best shift. - `all_shifts` - `float [n_shifts x 2]`. yx shifts searched over. - `score` - `float [n_shifts]`. Score of all shifts. \"\"\" all_shifts = np . array ( np . meshgrid ( y_shifts , x_shifts )) . T . reshape ( - 1 , 2 ) if ignore_shifts is not None : all_shifts = setdiff2d ( all_shifts , ignore_shifts ) score = np . zeros ( all_shifts . shape [ 0 ]) n_trees = len ( yx_transform_trees ) dist_upper_bound = 3 * neighb_dist_thresh # beyond this, score < exp(-4.5) and quicker to use this. for i in range ( all_shifts . shape [ 0 ]): for j in range ( n_trees ): yx_shifted = yx_base_slices [ j ] + all_shifts [ i ] distances = yx_transform_trees [ j ] . query ( yx_shifted , distance_upper_bound = dist_upper_bound )[ 0 ] score [ i ] += shift_score ( distances , neighb_dist_thresh ) best_shift_ind = score . argmax () return all_shifts [ best_shift_ind ], score [ best_shift_ind ], all_shifts , score","title":"get_best_shift_2d()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_best_shift_3d","text":"Finds the shift from those given that is best applied to yx_base to match yx_transform . Parameters: Name Type Description Default yxz_base np . ndarray float [n_spots_base x 3] . Coordinates of spots on base image (yxz units must be same). required yxz_transform_tree KDTree KDTree built from coordinates of spots on transformed image ( float [n_spots_transform x 3] , yxz units must be same). required neighb_dist_thresh float Basically the distance below which neighbours are a good match. Typical = 2 . required y_shifts np . ndarray float [n_y_shifts] . All possible shifts to test in y direction, probably made with np.arange . required x_shifts np . ndarray float [n_x_shifts] . All possible shifts to test in x direction, probably made with np.arange . required z_shifts np . ndarray float [n_z_shifts] . All possible shifts to test in z direction, probably made with np.arange . required ignore_shifts Optional [ np . ndarray ] float [n_ignore x 3] . Contains yxz shifts to not search over. If None , all permutations of y_shifts , x_shifts , z_shifts used. None Returns: Type Description np . ndarray best_shift - float [shift_y, shift_x, shift_z] . Best shift found. float best_score - float . Score of best shift. np . ndarray all_shifts - float [n_shifts x 3] . yxz shifts searched over. np . ndarray score - float [n_shifts] . Score of all shifts. Source code in coppafish/stitch/shift.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def get_best_shift_3d ( yxz_base : np . ndarray , yxz_transform_tree : KDTree , neighb_dist_thresh : float , y_shifts : np . ndarray , x_shifts : np . ndarray , z_shifts : np . ndarray , ignore_shifts : Optional [ np . ndarray ] = None ) -> Tuple [ np . ndarray , float , np . ndarray , np . ndarray ]: \"\"\" Finds the shift from those given that is best applied to `yx_base` to match `yx_transform`. Args: yxz_base: `float [n_spots_base x 3]`. Coordinates of spots on base image (yxz units must be same). yxz_transform_tree: KDTree built from coordinates of spots on transformed image (`float [n_spots_transform x 3]`, yxz units must be same). neighb_dist_thresh: Basically the distance below which neighbours are a good match. Typical = `2`. y_shifts: `float [n_y_shifts]`. All possible shifts to test in y direction, probably made with `np.arange`. x_shifts: `float [n_x_shifts]`. All possible shifts to test in x direction, probably made with `np.arange`. z_shifts: `float [n_z_shifts]`. All possible shifts to test in z direction, probably made with `np.arange`. ignore_shifts: `float [n_ignore x 3]`. Contains yxz shifts to not search over. If `None`, all permutations of `y_shifts`, `x_shifts`, `z_shifts` used. Returns: - `best_shift` - `float [shift_y, shift_x, shift_z]`. Best shift found. - `best_score` - `float`. Score of best shift. - `all_shifts` - `float [n_shifts x 3]`. yxz shifts searched over. - `score` - `float [n_shifts]`. Score of all shifts. \"\"\" all_shifts = np . array ( np . meshgrid ( y_shifts , x_shifts , z_shifts )) . T . reshape ( - 1 , 3 ) if ignore_shifts is not None : all_shifts = setdiff2d ( all_shifts , ignore_shifts ) score = np . zeros ( all_shifts . shape [ 0 ]) dist_upper_bound = 3 * neighb_dist_thresh # beyond this, score < exp(-4.5) and quicker to use this. for i in range ( all_shifts . shape [ 0 ]): yxz_shifted = yxz_base + all_shifts [ i ] distances = yxz_transform_tree . query ( yxz_shifted , distance_upper_bound = dist_upper_bound )[ 0 ] score [ i ] = shift_score ( distances , neighb_dist_thresh ) best_shift_ind = score . argmax () return all_shifts [ best_shift_ind ], score [ best_shift_ind ], all_shifts , score","title":"get_best_shift_3d()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.get_score_thresh","text":"Score thresh is the max of all scores from transforms between a distance=min_dist and distance=max_dist from the best_shift . I.e. we expect just for actual shift, there will be sharp gradient in score near it, so threshold is multiple of nearby score. If not the actual shift, then expect scores in this annulus will also be quite large. Parameters: Name Type Description Default all_shifts np . ndarray float [n_shifts x 2] . yx shifts searched over. required all_scores np . ndarray float [n_shifts] . all_scores[s] is the score corresponding to all_shifts[s] . required best_shift Union [ np . ndarray , List ] float [2] . yx shift with the best score. required min_dist float score_thresh computed from all_shifts a distance between min_shift and max_shift from best_shifts . required max_dist float score_thresh computed from all_shifts a distance between min_shift and max_shift from best_shifts . required thresh_multiplier float score_thresh is thresh_multiplier * mean of scores of shifts the correct distance from best_shift . required Returns: Type Description float score_thresh - Threshold used to determine if best_shift found is legitimate. Optional [ np . ndarray ] shift_thresh - float [2] shift corresponding to score_thresh . Will be None if there were no shifts in the range set by min_dist and max_dist . Source code in coppafish/stitch/shift.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 def get_score_thresh ( all_shifts : np . ndarray , all_scores : np . ndarray , best_shift : Union [ np . ndarray , List ], min_dist : float , max_dist : float , thresh_multiplier : float ) -> Tuple [ float , Optional [ np . ndarray ]]: \"\"\" Score thresh is the max of all scores from transforms between a `distance=min_dist` and `distance=max_dist` from the `best_shift`. I.e. we expect just for actual shift, there will be sharp gradient in score near it, so threshold is multiple of nearby score. If not the actual shift, then expect scores in this annulus will also be quite large. Args: all_shifts: `float [n_shifts x 2]`. yx shifts searched over. all_scores: `float [n_shifts]`. `all_scores[s]` is the score corresponding to `all_shifts[s]`. best_shift: `float [2]`. yx shift with the best score. min_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift` from `best_shifts`. max_dist: `score_thresh` computed from `all_shifts` a distance between `min_shift` and `max_shift` from `best_shifts`. thresh_multiplier: `score_thresh` is `thresh_multiplier` * mean of scores of shifts the correct distance from `best_shift`. Returns: score_thresh - Threshold used to determine if `best_shift` found is legitimate. shift_thresh - `float [2]` shift corresponding to `score_thresh`. Will be None if there were no shifts in the range set by `min_dist` and `max_dist`. \"\"\" dist_to_best = pairwise_distances ( np . array ( all_shifts ), np . array ( best_shift )[ np . newaxis ]) . squeeze () use = np . where ( np . logical_and ( dist_to_best <= max_dist , dist_to_best >= min_dist ))[ 0 ] if len ( use ) > 0 : thresh_ind = use [ np . argmax ( all_scores [ use ])] score_thresh = thresh_multiplier * all_scores [ thresh_ind ] shift_thresh = all_shifts [ thresh_ind ] else : score_thresh = thresh_multiplier * np . median ( all_scores ) shift_thresh = None return score_thresh , shift_thresh","title":"get_score_thresh()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.refined_shifts","text":"If shifts is an array with mean spacing step then this builds array that covers from best_shift - extend_scale * step to best_shift + extend_scale * step with a spacing of step*refined_scale . The new step, step*refined_scale , is forced to be an integer. If only one shift provided, doesn't do anything. Parameters: Name Type Description Default shifts np . ndarray float [n_shifts] . Array probably produced using np.arange . It is expected to be in ascending order with constant step. required best_shift float Value in shifts to build new shifts around. required refined_scale float Scaling to apply to find new shift spacing. 0.5 extend_scale float By how many steps to build new shifts. 2 Returns: Type Description np . ndarray float [n_new_shifts] . Array covering from np . ndarray best_shift - extend_scale * step to best_shift + extend_scale * step with a spacing of step*refined_scale . Source code in coppafish/stitch/shift.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def refined_shifts ( shifts : np . ndarray , best_shift : float , refined_scale : float = 0.5 , extend_scale : float = 2 ) -> np . ndarray : \"\"\" If `shifts` is an array with mean spacing `step` then this builds array that covers from `best_shift - extend_scale * step` to `best_shift + extend_scale * step` with a spacing of `step*refined_scale`. The new step, `step*refined_scale`, is forced to be an integer. If only one `shift` provided, doesn't do anything. Args: shifts: `float [n_shifts]`. Array probably produced using `np.arange`. It is expected to be in ascending order with constant step. best_shift: Value in `shifts` to build new shifts around. refined_scale: Scaling to apply to find new shift spacing. extend_scale: By how many steps to build new shifts. Returns: `float [n_new_shifts]`. Array covering from `best_shift - extend_scale * step` to `best_shift + extend_scale * step` with a spacing of `step*refined_scale`. \"\"\" if np . size ( shifts ) == 1 : refined_shifts = shifts else : step = np . mean ( np . ediff1d ( shifts )) refined_step = np . ceil ( refined_scale * step ) . astype ( int ) refined_shifts = np . arange ( best_shift - extend_scale * step , best_shift + extend_scale * step + refined_step / 2 , refined_step ) return refined_shifts","title":"refined_shifts()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.shift_score","text":"Computes a score to quantify how good a shift is based on the distances between the neighbours found. the value of this score is approximately the number of close neighbours found. Parameters: Name Type Description Default distances np . ndarray float [n_neighbours] . Distances between each pair of neighbours. required thresh float Basically the distance in pixels below which neighbours are a good match. Typical = 2 . required Returns: Type Description float Score to quantify how good a shift is based on the distances between the neighbours found. Source code in coppafish/stitch/shift.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 def shift_score ( distances : np . ndarray , thresh : float ) -> float : \"\"\" Computes a score to quantify how good a shift is based on the distances between the neighbours found. the value of this score is approximately the number of close neighbours found. Args: distances: `float [n_neighbours]`. Distances between each pair of neighbours. thresh: Basically the distance in pixels below which neighbours are a good match. Typical = `2`. Returns: Score to quantify how good a shift is based on the distances between the neighbours found. \"\"\" return np . sum ( np . exp ( - distances ** 2 / ( 2 * thresh ** 2 )))","title":"shift_score()"},{"location":"code/stitch/shift/#coppafish.stitch.shift.update_shifts","text":"Returns a new array of search_shifts around the mean of prev_found_shifts if new array has fewer entries or if mean of prev_found_shifts is outside initial range of search_shifts . If more than one prev_found_shifts is outside the search_shifts in the same way i.e. too high or too low, search_shifts will be updated too. Parameters: Name Type Description Default search_shifts np . ndarray int [n_shifts] . Indicates all shifts currently searched over. required prev_found_shifts np . ndarray int [n_shifts_found] . Indicate shifts found on all previous runs of compute_shift . required Returns: Type Description np . ndarray int [n_new_shifts] . np . ndarray New set of shifts around mean of previously found shifts. np . ndarray Will only return updated shifts if new array has fewer entries than before or mean of prev_found_shifts np . ndarray is outside range of search_shifts . Source code in coppafish/stitch/shift.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def update_shifts ( search_shifts : np . ndarray , prev_found_shifts : np . ndarray ) -> np . ndarray : \"\"\" Returns a new array of `search_shifts` around the mean of `prev_found_shifts` if new array has fewer entries or if mean of `prev_found_shifts` is outside initial range of `search_shifts`. If more than one `prev_found_shifts` is outside the `search_shifts` in the same way i.e. too high or too low, `search_shifts` will be updated too. Args: search_shifts: `int [n_shifts]`. Indicates all shifts currently searched over. prev_found_shifts: `int [n_shifts_found]`. Indicate shifts found on all previous runs of `compute_shift`. Returns: `int [n_new_shifts]`. New set of shifts around mean of previously found shifts. Will only return updated shifts if new array has fewer entries than before or mean of `prev_found_shifts` is outside range of `search_shifts`. \"\"\" n_shifts = len ( search_shifts ) n_prev_shifts = len ( prev_found_shifts ) if n_shifts > 1 and n_prev_shifts > 0 : step = np . mean ( np . ediff1d ( search_shifts )) mean_shift = np . mean ( prev_found_shifts , dtype = int ) n_shifts_new = 2 * np . ceil (( np . max ( prev_found_shifts ) - mean_shift ) / step + 1 ) . astype ( int ) + 1 if n_shifts_new < n_shifts or mean_shift <= search_shifts . min () or mean_shift >= search_shifts . max (): # only update shifts if results in less to search over. search_shifts = refined_shifts ( search_shifts , mean_shift , 1 , (( n_shifts_new - 1 ) / 2 ) . astype ( int )) if np . sum ( prev_found_shifts > search_shifts . max ()) > 1 : search_shifts = np . arange ( search_shifts . min (), prev_found_shifts . max () + step , step ) if np . sum ( prev_found_shifts < search_shifts . min ()) > 1 : search_shifts = np . arange ( prev_found_shifts . min (), search_shifts . max () + step , step ) return search_shifts","title":"update_shifts()"},{"location":"code/stitch/starting_shifts/","text":"get_shifts_to_search ( config , nbp_basic , nbp_debug = None ) Using information in config dictionary to get range of shifts to search over when finding overlap between overlapping tiles in south and west directions. Parameters: Name Type Description Default config dict 'stitch' section of .ini document. required nbp_basic NotebookPage basic_info notebook page required nbp_debug Optional [ NotebookPage ] stitch notebook page where debugging information for stitching is kept. If provided, south_start_shift_search and west_start_shift_search variables added to page. None Returns: Type Description dict shifts[j][i] contains the shifts to search over for overlap in the j direction for coordinate i where: j = 'south', 'west' . i = 'y', 'x', 'z' . Source code in coppafish/stitch/starting_shifts.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_shifts_to_search ( config : dict , nbp_basic : NotebookPage , nbp_debug : Optional [ NotebookPage ] = None ) -> dict : \"\"\" Using information in config dictionary to get range of shifts to search over when finding overlap between overlapping tiles in south and west directions. Args: config: 'stitch' section of .ini document. nbp_basic: `basic_info` notebook page nbp_debug: `stitch` notebook page where debugging information for stitching is kept. If provided, south_start_shift_search and west_start_shift_search variables added to page. Returns: `shifts[j][i]` contains the shifts to search over for overlap in the `j` direction for coordinate `i` where: `j = 'south', 'west'`. `i = 'y', 'x', 'z'`. \"\"\" expected_shift_south = np . array ([ - ( 1 - config [ 'expected_overlap' ]) * nbp_basic . tile_sz , 0 , 0 ]) . astype ( int ) auto_shift_south_extent = np . array ( config [ 'auto_n_shifts' ]) * np . array ( config [ 'shift_step' ]) expected_shift_west = expected_shift_south [[ 1 , 0 , 2 ]] auto_shift_west_extent = auto_shift_south_extent [[ 1 , 0 , 2 ]] if config [ 'shift_south_min' ] is None : config [ 'shift_south_min' ] = list ( expected_shift_south - auto_shift_south_extent ) if config [ 'shift_south_max' ] is None : config [ 'shift_south_max' ] = list ( expected_shift_south + auto_shift_south_extent ) if config [ 'shift_west_min' ] is None : config [ 'shift_west_min' ] = list ( expected_shift_west - auto_shift_west_extent ) if config [ 'shift_west_max' ] is None : config [ 'shift_west_max' ] = list ( expected_shift_west + auto_shift_west_extent ) directions = [ 'south' , 'west' ] coords = [ 'y' , 'x' , 'z' ] shifts = { 'south' : {}, 'west' : {}} for j in directions : if nbp_debug is not None : nbp_debug . __setattr__ ( j + '_' + 'start_shift_search' , np . zeros (( 3 , 3 ), dtype = int )) for i in range ( len ( coords )): shifts [ j ][ coords [ i ]] = np . arange ( config [ 'shift_' + j + '_min' ][ i ], config [ 'shift_' + j + '_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if nbp_debug is not None : nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[ i , :] = [ config [ 'shift_' + j + '_min' ][ i ], config [ 'shift_' + j + '_max' ][ i ], config [ 'shift_step' ][ i ]] if not nbp_basic . is_3d : shifts [ 'south' ][ 'z' ] = np . array ([ 0 ], dtype = int ) shifts [ 'west' ][ 'z' ] = np . array ([ 0 ], dtype = int ) if nbp_debug is not None : for j in directions : nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[ 2 , : 2 ] = 0 return shifts","title":"Starting Shifts"},{"location":"code/stitch/starting_shifts/#coppafish.stitch.starting_shifts.get_shifts_to_search","text":"Using information in config dictionary to get range of shifts to search over when finding overlap between overlapping tiles in south and west directions. Parameters: Name Type Description Default config dict 'stitch' section of .ini document. required nbp_basic NotebookPage basic_info notebook page required nbp_debug Optional [ NotebookPage ] stitch notebook page where debugging information for stitching is kept. If provided, south_start_shift_search and west_start_shift_search variables added to page. None Returns: Type Description dict shifts[j][i] contains the shifts to search over for overlap in the j direction for coordinate i where: j = 'south', 'west' . i = 'y', 'x', 'z' . Source code in coppafish/stitch/starting_shifts.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def get_shifts_to_search ( config : dict , nbp_basic : NotebookPage , nbp_debug : Optional [ NotebookPage ] = None ) -> dict : \"\"\" Using information in config dictionary to get range of shifts to search over when finding overlap between overlapping tiles in south and west directions. Args: config: 'stitch' section of .ini document. nbp_basic: `basic_info` notebook page nbp_debug: `stitch` notebook page where debugging information for stitching is kept. If provided, south_start_shift_search and west_start_shift_search variables added to page. Returns: `shifts[j][i]` contains the shifts to search over for overlap in the `j` direction for coordinate `i` where: `j = 'south', 'west'`. `i = 'y', 'x', 'z'`. \"\"\" expected_shift_south = np . array ([ - ( 1 - config [ 'expected_overlap' ]) * nbp_basic . tile_sz , 0 , 0 ]) . astype ( int ) auto_shift_south_extent = np . array ( config [ 'auto_n_shifts' ]) * np . array ( config [ 'shift_step' ]) expected_shift_west = expected_shift_south [[ 1 , 0 , 2 ]] auto_shift_west_extent = auto_shift_south_extent [[ 1 , 0 , 2 ]] if config [ 'shift_south_min' ] is None : config [ 'shift_south_min' ] = list ( expected_shift_south - auto_shift_south_extent ) if config [ 'shift_south_max' ] is None : config [ 'shift_south_max' ] = list ( expected_shift_south + auto_shift_south_extent ) if config [ 'shift_west_min' ] is None : config [ 'shift_west_min' ] = list ( expected_shift_west - auto_shift_west_extent ) if config [ 'shift_west_max' ] is None : config [ 'shift_west_max' ] = list ( expected_shift_west + auto_shift_west_extent ) directions = [ 'south' , 'west' ] coords = [ 'y' , 'x' , 'z' ] shifts = { 'south' : {}, 'west' : {}} for j in directions : if nbp_debug is not None : nbp_debug . __setattr__ ( j + '_' + 'start_shift_search' , np . zeros (( 3 , 3 ), dtype = int )) for i in range ( len ( coords )): shifts [ j ][ coords [ i ]] = np . arange ( config [ 'shift_' + j + '_min' ][ i ], config [ 'shift_' + j + '_max' ][ i ] + config [ 'shift_step' ][ i ] / 2 , config [ 'shift_step' ][ i ]) . astype ( int ) if nbp_debug is not None : nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[ i , :] = [ config [ 'shift_' + j + '_min' ][ i ], config [ 'shift_' + j + '_max' ][ i ], config [ 'shift_step' ][ i ]] if not nbp_basic . is_3d : shifts [ 'south' ][ 'z' ] = np . array ([ 0 ], dtype = int ) shifts [ 'west' ][ 'z' ] = np . array ([ 0 ], dtype = int ) if nbp_debug is not None : for j in directions : nbp_debug . __getattribute__ ( j + '_' + 'start_shift_search' )[ 2 , : 2 ] = 0 return shifts","title":"get_shifts_to_search()"},{"location":"code/stitch/tile_origin/","text":"get_tile_origin ( v_pairs , v_shifts , h_pairs , h_shifts , n_tiles , home_tile ) This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles. Parameters: Name Type Description Default v_pairs np . ndarray int [n_v_pairs x 2] . v_pairs[i,1] is the tile index of the tile to the south of v_pairs[i,0] . required v_shifts np . ndarray int [n_v_pairs x 3] . v_shifts[i, :] is the yxz shift from v_pairs[i,0] to v_pairs[i,1] . v_shifts[:, 0] should all be negative. required h_pairs np . ndarray int [n_h_pairs x 2] . h_pairs[i,1] is the tile index of the tile to the west of h_pairs[i,0] . required h_shifts np . ndarray int [n_h_pairs x 3] . h_shifts[i, :] is the yxz shift from h_pairs[i,0] to h_pairs[i,1] . h_shifts[:, 1] should all be negative. required n_tiles int Number of tiles (including those not used) in data set. required home_tile int Index of tile that is anchored to a fixed coordinate when finding tile origins. It should be the tile nearest to the centre. required Returns: Type Description np . ndarray float [n_tiles x 3] . yxz origin of each tile. Source code in coppafish/stitch/tile_origin.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def get_tile_origin ( v_pairs : np . ndarray , v_shifts : np . ndarray , h_pairs : np . ndarray , h_shifts : np . ndarray , n_tiles : int , home_tile : int ) -> np . ndarray : \"\"\" This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles. Args: v_pairs: `int [n_v_pairs x 2]`. `v_pairs[i,1]` is the tile index of the tile to the south of `v_pairs[i,0]`. v_shifts: `int [n_v_pairs x 3]`. `v_shifts[i, :]` is the yxz shift from `v_pairs[i,0]` to `v_pairs[i,1]`. `v_shifts[:, 0]` should all be negative. h_pairs: `int [n_h_pairs x 2]`. `h_pairs[i,1]` is the tile index of the tile to the west of `h_pairs[i,0]`. h_shifts: `int [n_h_pairs x 3]`. `h_shifts[i, :]` is the yxz shift from `h_pairs[i,0]` to `h_pairs[i,1]`. `h_shifts[:, 1]` should all be negative. n_tiles: Number of tiles (including those not used) in data set. home_tile: Index of tile that is anchored to a fixed coordinate when finding tile origins. It should be the tile nearest to the centre. Returns: `float [n_tiles x 3]`. yxz origin of each tile. \"\"\" # solve a set of linear equations for each shift, # This will be of the form M*x = c, where x and c are both of length n_tiles. # The t'th row is the equation for tile t. c has columns for y, x and z coordinates pairs = { 'v' : v_pairs , 'h' : h_pairs } shifts = { 'v' : v_shifts , 'h' : h_shifts } M = np . zeros (( n_tiles + 1 , n_tiles )) c = np . zeros (( n_tiles + 1 , 3 )) for j in [ 'v' , 'h' ]: for i in range ( pairs [ j ] . shape [ 0 ]): t1 = pairs [ j ][ i , 0 ] t2 = pairs [ j ][ i , 1 ] M [ t1 , t1 ] = M [ t1 , t1 ] + 1 M [ t1 , t2 ] = M [ t1 , t2 ] - 1 c [ t1 , :] = c [ t1 , :] + shifts [ j ][ i , :] # this is -shifts in MATLAB, but t1, t2 flipped in python M [ t2 , t2 ] = M [ t2 , t2 ] + 1 M [ t2 , t1 ] = M [ t2 , t1 ] - 1 c [ t2 , :] = c [ t2 , :] - shifts [ j ][ i , :] # this is +shifts in MATLAB, but t1, t2 flipped in python # now we want to anchor one of the tiles to a fixed coordinate. We do this # for a home tile in the middle, because it is going to be connected; and we set # its coordinate to a large value, so any non-connected ones can be detected. # (BTW this is why spectral clustering works!!) huge = 1e6 M [ n_tiles , home_tile ] = 1 c [ n_tiles , :] = huge tiny = 1e-4 # for regularization tile_offset0 = np . linalg . lstsq ( M + tiny * np . eye ( n_tiles + 1 , n_tiles ), c , rcond = None )[ 0 ] # find tiles that are connected to the home tile aligned_ok = tile_offset0 [:, 0 ] > huge / 2 tile_offset1 = np . ones (( n_tiles , 3 )) * np . nan tile_offset1 [ aligned_ok ] = tile_offset0 [ aligned_ok ] - huge tile_origin = tile_offset1 - np . nanmin ( tile_offset1 , axis = 0 ) return tile_origin","title":"Tile Origin"},{"location":"code/stitch/tile_origin/#coppafish.stitch.tile_origin.get_tile_origin","text":"This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles. Parameters: Name Type Description Default v_pairs np . ndarray int [n_v_pairs x 2] . v_pairs[i,1] is the tile index of the tile to the south of v_pairs[i,0] . required v_shifts np . ndarray int [n_v_pairs x 3] . v_shifts[i, :] is the yxz shift from v_pairs[i,0] to v_pairs[i,1] . v_shifts[:, 0] should all be negative. required h_pairs np . ndarray int [n_h_pairs x 2] . h_pairs[i,1] is the tile index of the tile to the west of h_pairs[i,0] . required h_shifts np . ndarray int [n_h_pairs x 3] . h_shifts[i, :] is the yxz shift from h_pairs[i,0] to h_pairs[i,1] . h_shifts[:, 1] should all be negative. required n_tiles int Number of tiles (including those not used) in data set. required home_tile int Index of tile that is anchored to a fixed coordinate when finding tile origins. It should be the tile nearest to the centre. required Returns: Type Description np . ndarray float [n_tiles x 3] . yxz origin of each tile. Source code in coppafish/stitch/tile_origin.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def get_tile_origin ( v_pairs : np . ndarray , v_shifts : np . ndarray , h_pairs : np . ndarray , h_shifts : np . ndarray , n_tiles : int , home_tile : int ) -> np . ndarray : \"\"\" This finds the origin of each tile in a global coordinate system based on the shifts between overlapping tiles. Args: v_pairs: `int [n_v_pairs x 2]`. `v_pairs[i,1]` is the tile index of the tile to the south of `v_pairs[i,0]`. v_shifts: `int [n_v_pairs x 3]`. `v_shifts[i, :]` is the yxz shift from `v_pairs[i,0]` to `v_pairs[i,1]`. `v_shifts[:, 0]` should all be negative. h_pairs: `int [n_h_pairs x 2]`. `h_pairs[i,1]` is the tile index of the tile to the west of `h_pairs[i,0]`. h_shifts: `int [n_h_pairs x 3]`. `h_shifts[i, :]` is the yxz shift from `h_pairs[i,0]` to `h_pairs[i,1]`. `h_shifts[:, 1]` should all be negative. n_tiles: Number of tiles (including those not used) in data set. home_tile: Index of tile that is anchored to a fixed coordinate when finding tile origins. It should be the tile nearest to the centre. Returns: `float [n_tiles x 3]`. yxz origin of each tile. \"\"\" # solve a set of linear equations for each shift, # This will be of the form M*x = c, where x and c are both of length n_tiles. # The t'th row is the equation for tile t. c has columns for y, x and z coordinates pairs = { 'v' : v_pairs , 'h' : h_pairs } shifts = { 'v' : v_shifts , 'h' : h_shifts } M = np . zeros (( n_tiles + 1 , n_tiles )) c = np . zeros (( n_tiles + 1 , 3 )) for j in [ 'v' , 'h' ]: for i in range ( pairs [ j ] . shape [ 0 ]): t1 = pairs [ j ][ i , 0 ] t2 = pairs [ j ][ i , 1 ] M [ t1 , t1 ] = M [ t1 , t1 ] + 1 M [ t1 , t2 ] = M [ t1 , t2 ] - 1 c [ t1 , :] = c [ t1 , :] + shifts [ j ][ i , :] # this is -shifts in MATLAB, but t1, t2 flipped in python M [ t2 , t2 ] = M [ t2 , t2 ] + 1 M [ t2 , t1 ] = M [ t2 , t1 ] - 1 c [ t2 , :] = c [ t2 , :] - shifts [ j ][ i , :] # this is +shifts in MATLAB, but t1, t2 flipped in python # now we want to anchor one of the tiles to a fixed coordinate. We do this # for a home tile in the middle, because it is going to be connected; and we set # its coordinate to a large value, so any non-connected ones can be detected. # (BTW this is why spectral clustering works!!) huge = 1e6 M [ n_tiles , home_tile ] = 1 c [ n_tiles , :] = huge tiny = 1e-4 # for regularization tile_offset0 = np . linalg . lstsq ( M + tiny * np . eye ( n_tiles + 1 , n_tiles ), c , rcond = None )[ 0 ] # find tiles that are connected to the home tile aligned_ok = tile_offset0 [:, 0 ] > huge / 2 tile_offset1 = np . ones (( n_tiles , 3 )) * np . nan tile_offset1 [ aligned_ok ] = tile_offset0 [ aligned_ok ] - huge tile_origin = tile_offset1 - np . nanmin ( tile_offset1 , axis = 0 ) return tile_origin","title":"get_tile_origin()"},{"location":"code/utils/base/","text":"round_any ( x , base , round_type = 'round' ) Rounds x to the nearest multiple of base with the rounding done according to round_type . Parameters: Name Type Description Default x Union [ float , np . ndarray ] Number or array to round. required base float Rounds x to nearest integer multiple of value of base . required round_type str One of the following, indicating how to round x - 'round' 'ceil' 'float' 'round' Returns: Type Description Union [ float , np . ndarray ] Rounded version of x . Example round_any(3, 5) = 5 round_any(3, 5, 'floor') = 0 Source code in coppafish/utils/base.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def round_any ( x : Union [ float , np . ndarray ], base : float , round_type : str = 'round' ) -> Union [ float , np . ndarray ]: \"\"\" Rounds `x` to the nearest multiple of `base` with the rounding done according to `round_type`. Args: x: Number or array to round. base: Rounds `x` to nearest integer multiple of value of `base`. round_type: One of the following, indicating how to round `x` - - `'round'` - `'ceil'` - `'float'` Returns: Rounded version of `x`. Example: ``` round_any(3, 5) = 5 round_any(3, 5, 'floor') = 0 ``` \"\"\" if round_type == 'round' : return base * np . round ( x / base ) elif round_type == 'ceil' : return base * np . ceil ( x / base ) elif round_type == 'floor' : return base * np . floor ( x / base ) else : raise ValueError ( f \"round_type specified was { round_type } but it should be one of the following: \\n \" f \"round, ceil, floor\" ) setdiff2d ( array1 , array2 ) Finds all elements in array1 that are not in array2 . Returned array will only contain unique elements E.g. If array1 has [4,0] twice, array2 has [4,0] once, returned array will not have [4,0] . If array1 has [4,0] twice, array2 does not have [4,0] , returned array will have [4,0] once. Parameters: Name Type Description Default array1 np . ndarray float [n_elements1 x element_dim] . required array2 np . ndarray float [n_elements2 x element_dim] . required Returns: Type Description np . ndarray float [n_elements_diff x element_dim] . Source code in coppafish/utils/base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def setdiff2d ( array1 : np . ndarray , array2 : np . ndarray ) -> np . ndarray : \"\"\" Finds all elements in `array1` that are not in `array2`. Returned array will only contain unique elements E.g. If `array1` has `[4,0]` twice, `array2` has `[4,0]` once, returned array will not have `[4,0]`. If `array1` has `[4,0]` twice, `array2` does not have `[4,0]`, returned array will have `[4,0]` once. Args: array1: `float [n_elements1 x element_dim]`. array2: `float [n_elements2 x element_dim]`. Returns: `float [n_elements_diff x element_dim]`. \"\"\" set1 = set ([ tuple ( x ) for x in array1 ]) set2 = set ([ tuple ( x ) for x in array2 ]) return np . array ( list ( set1 - set2 ))","title":"Base"},{"location":"code/utils/base/#coppafish.utils.base.round_any","text":"Rounds x to the nearest multiple of base with the rounding done according to round_type . Parameters: Name Type Description Default x Union [ float , np . ndarray ] Number or array to round. required base float Rounds x to nearest integer multiple of value of base . required round_type str One of the following, indicating how to round x - 'round' 'ceil' 'float' 'round' Returns: Type Description Union [ float , np . ndarray ] Rounded version of x . Example round_any(3, 5) = 5 round_any(3, 5, 'floor') = 0 Source code in coppafish/utils/base.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 def round_any ( x : Union [ float , np . ndarray ], base : float , round_type : str = 'round' ) -> Union [ float , np . ndarray ]: \"\"\" Rounds `x` to the nearest multiple of `base` with the rounding done according to `round_type`. Args: x: Number or array to round. base: Rounds `x` to nearest integer multiple of value of `base`. round_type: One of the following, indicating how to round `x` - - `'round'` - `'ceil'` - `'float'` Returns: Rounded version of `x`. Example: ``` round_any(3, 5) = 5 round_any(3, 5, 'floor') = 0 ``` \"\"\" if round_type == 'round' : return base * np . round ( x / base ) elif round_type == 'ceil' : return base * np . ceil ( x / base ) elif round_type == 'floor' : return base * np . floor ( x / base ) else : raise ValueError ( f \"round_type specified was { round_type } but it should be one of the following: \\n \" f \"round, ceil, floor\" )","title":"round_any()"},{"location":"code/utils/base/#coppafish.utils.base.setdiff2d","text":"Finds all elements in array1 that are not in array2 . Returned array will only contain unique elements E.g. If array1 has [4,0] twice, array2 has [4,0] once, returned array will not have [4,0] . If array1 has [4,0] twice, array2 does not have [4,0] , returned array will have [4,0] once. Parameters: Name Type Description Default array1 np . ndarray float [n_elements1 x element_dim] . required array2 np . ndarray float [n_elements2 x element_dim] . required Returns: Type Description np . ndarray float [n_elements_diff x element_dim] . Source code in coppafish/utils/base.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def setdiff2d ( array1 : np . ndarray , array2 : np . ndarray ) -> np . ndarray : \"\"\" Finds all elements in `array1` that are not in `array2`. Returned array will only contain unique elements E.g. If `array1` has `[4,0]` twice, `array2` has `[4,0]` once, returned array will not have `[4,0]`. If `array1` has `[4,0]` twice, `array2` does not have `[4,0]`, returned array will have `[4,0]` once. Args: array1: `float [n_elements1 x element_dim]`. array2: `float [n_elements2 x element_dim]`. Returns: `float [n_elements_diff x element_dim]`. \"\"\" set1 = set ([ tuple ( x ) for x in array1 ]) set2 = set ([ tuple ( x ) for x in array2 ]) return np . array ( list ( set1 - set2 ))","title":"setdiff2d()"},{"location":"code/utils/errors/","text":"ColorInvalidWarning Source code in coppafish/utils/errors.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class ColorInvalidWarning : def __init__ ( self , colors : np . ndarray , nbp_basic : NotebookPage , invalid_value : float , round_no : Optional [ int ] = None , channel_no : Optional [ int ] = None , code_no : Optional [ int ] = None ): \"\"\" Warning raised because `spot_colors` contains a `invalid_value` where it should not. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage invalid_value: This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. round_no: round to flag error for. channel_no: channel to flag error for. code_no: Spot or gene index to flag error for. \"\"\" n_spots , n_rounds , n_channels = colors . shape if round_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in round { round_no } \\n \" \\ f \"which is not in use_rounds = { nbp_basic . use_rounds } .\" elif channel_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in channel { channel_no } \\n \" \\ f \"which is not in use_channels = { nbp_basic . use_channels } .\" elif round_no is not None and channel_no is not None and code_no is not None : self . message = f \"colors contains a invalid_value= { invalid_value } for code { code_no } , round { round_no } , \" \\ f \"channel { channel_no } . \\n \" \\ f \"There should be no invalid_values in this round and channel.\" else : self . message = f \"colors has n_rounds = { n_rounds } and n_channels = { n_channels } . \\n \" \\ f \"This is neither matches the total_rounds = { nbp_basic . n_rounds } and \" \\ f \"total_channels = { nbp_basic . n_channels } \\n \" \\ f \"nor the number of use_rounds = { len ( nbp_basic . use_rounds ) } and use_channels = \" \\ f \" { len ( nbp_basic . use_channels ) } \" warnings . warn ( self . message ) __init__ ( colors , nbp_basic , invalid_value , round_no = None , channel_no = None , code_no = None ) Warning raised because spot_colors contains a invalid_value where it should not. Parameters: Name Type Description Default colors np . ndarray int or float [n_codes x n_rounds x n_channels] colors[s, r, c] is the color for code s in round r , channel c . This is likely to be spot_colors if int or bled_codes if float . required nbp_basic NotebookPage basic_info NotebookPage required invalid_value float This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. required round_no Optional [ int ] round to flag error for. None channel_no Optional [ int ] channel to flag error for. None code_no Optional [ int ] Spot or gene index to flag error for. None Source code in coppafish/utils/errors.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def __init__ ( self , colors : np . ndarray , nbp_basic : NotebookPage , invalid_value : float , round_no : Optional [ int ] = None , channel_no : Optional [ int ] = None , code_no : Optional [ int ] = None ): \"\"\" Warning raised because `spot_colors` contains a `invalid_value` where it should not. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage invalid_value: This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. round_no: round to flag error for. channel_no: channel to flag error for. code_no: Spot or gene index to flag error for. \"\"\" n_spots , n_rounds , n_channels = colors . shape if round_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in round { round_no } \\n \" \\ f \"which is not in use_rounds = { nbp_basic . use_rounds } .\" elif channel_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in channel { channel_no } \\n \" \\ f \"which is not in use_channels = { nbp_basic . use_channels } .\" elif round_no is not None and channel_no is not None and code_no is not None : self . message = f \"colors contains a invalid_value= { invalid_value } for code { code_no } , round { round_no } , \" \\ f \"channel { channel_no } . \\n \" \\ f \"There should be no invalid_values in this round and channel.\" else : self . message = f \"colors has n_rounds = { n_rounds } and n_channels = { n_channels } . \\n \" \\ f \"This is neither matches the total_rounds = { nbp_basic . n_rounds } and \" \\ f \"total_channels = { nbp_basic . n_channels } \\n \" \\ f \"nor the number of use_rounds = { len ( nbp_basic . use_rounds ) } and use_channels = \" \\ f \" { len ( nbp_basic . use_channels ) } \" warnings . warn ( self . message ) EmptyListError Bases: Exception Source code in coppafish/utils/errors.py 36 37 38 39 40 41 42 43 44 45 class EmptyListError ( Exception ): def __init__ ( self , var_name : str ): \"\"\" Error raised because the variable indicated by `var_name` contains no data. Args: var_name: Name of list or numpy array \"\"\" self . message = f \" \\n { var_name } contains no data\" super () . __init__ ( self . message ) __init__ ( var_name ) Error raised because the variable indicated by var_name contains no data. Parameters: Name Type Description Default var_name str Name of list or numpy array required Source code in coppafish/utils/errors.py 37 38 39 40 41 42 43 44 45 def __init__ ( self , var_name : str ): \"\"\" Error raised because the variable indicated by `var_name` contains no data. Args: var_name: Name of list or numpy array \"\"\" self . message = f \" \\n { var_name } contains no data\" super () . __init__ ( self . message ) NoFileError Bases: Exception Source code in coppafish/utils/errors.py 24 25 26 27 28 29 30 31 32 33 class NoFileError ( Exception ): def __init__ ( self , file_path : str ): \"\"\" Error raised because `file_path` does not exist. Args: file_path: Path to file of interest. \"\"\" self . message = f \" \\n No file with the following path: \\n { file_path } \\n exists\" super () . __init__ ( self . message ) __init__ ( file_path ) Error raised because file_path does not exist. Parameters: Name Type Description Default file_path str Path to file of interest. required Source code in coppafish/utils/errors.py 25 26 27 28 29 30 31 32 33 def __init__ ( self , file_path : str ): \"\"\" Error raised because `file_path` does not exist. Args: file_path: Path to file of interest. \"\"\" self . message = f \" \\n No file with the following path: \\n { file_path } \\n exists\" super () . __init__ ( self . message ) OutOfBoundsError Bases: Exception Source code in coppafish/utils/errors.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class OutOfBoundsError ( Exception ): def __init__ ( self , var_name : str , oob_val : float , min_allowed : float , max_allowed : float ): \"\"\" Error raised because `oob_val` is outside expected range between `min_allowed` and `max_allowed` inclusive. Args: var_name: Name of variable testing. oob_val: Value in array that is not in expected range. min_allowed: Smallest allowed value i.e. `>= min_allowed`. max_allowed: Largest allowed value i.e. `<= max_allowed`. \"\"\" self . message = f \" \\n { var_name } contains the value { oob_val } .\" \\ f \" \\n This is outside the expected inclusive range between { min_allowed } and { max_allowed } \" super () . __init__ ( self . message ) __init__ ( var_name , oob_val , min_allowed , max_allowed ) Error raised because oob_val is outside expected range between min_allowed and max_allowed inclusive. Parameters: Name Type Description Default var_name str Name of variable testing. required oob_val float Value in array that is not in expected range. required min_allowed float Smallest allowed value i.e. >= min_allowed . required max_allowed float Largest allowed value i.e. <= max_allowed . required Source code in coppafish/utils/errors.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , var_name : str , oob_val : float , min_allowed : float , max_allowed : float ): \"\"\" Error raised because `oob_val` is outside expected range between `min_allowed` and `max_allowed` inclusive. Args: var_name: Name of variable testing. oob_val: Value in array that is not in expected range. min_allowed: Smallest allowed value i.e. `>= min_allowed`. max_allowed: Largest allowed value i.e. `<= max_allowed`. \"\"\" self . message = f \" \\n { var_name } contains the value { oob_val } .\" \\ f \" \\n This is outside the expected inclusive range between { min_allowed } and { max_allowed } \" super () . __init__ ( self . message ) ShapeError Bases: Exception Source code in coppafish/utils/errors.py 66 67 68 69 70 71 72 73 74 75 76 77 class ShapeError ( Exception ): def __init__ ( self , var_name : str , var_shape : tuple , expected_shape : tuple ): \"\"\" Error raised because variable indicated by `var_name` has wrong shape. Args: var_name: Name of numpy array. var_shape: Shape of numpy array. expected_shape: Expected shape of numpy array. \"\"\" self . message = f \" \\n Shape of { var_name } is { var_shape } but should be { expected_shape } \" super () . __init__ ( self . message ) __init__ ( var_name , var_shape , expected_shape ) Error raised because variable indicated by var_name has wrong shape. Parameters: Name Type Description Default var_name str Name of numpy array. required var_shape tuple Shape of numpy array. required expected_shape tuple Expected shape of numpy array. required Source code in coppafish/utils/errors.py 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , var_name : str , var_shape : tuple , expected_shape : tuple ): \"\"\" Error raised because variable indicated by `var_name` has wrong shape. Args: var_name: Name of numpy array. var_shape: Shape of numpy array. expected_shape: Expected shape of numpy array. \"\"\" self . message = f \" \\n Shape of { var_name } is { var_shape } but should be { expected_shape } \" super () . __init__ ( self . message ) TiffError Bases: Exception Source code in coppafish/utils/errors.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class TiffError ( Exception ): def __init__ ( self , scale_tiff : float , scale_nbp : float , shift_tiff : int , shift_nbp : int ): \"\"\" Error raised because parameters used to produce tiff files are different to those in the current notebook. Args: scale_tiff: Scale factor applied to tiff. Found from tiff description. scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`. shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description. shift_nbp: Shift applied to tiff to ensure pixel values positive. Found from `nb.basic_info.tile_pixel_value_shift`. \"\"\" self . message = f \" \\n There are differences between the parameters used to make the tiffs and the parameters \" \\ f \"in the Notebook:\" if scale_tiff != scale_nbp : self . message = self . message + f \" \\n Scale used to make tiff was { scale_tiff } .\" \\ f \" \\n Current scale in extract_params notebook page is { scale_nbp } .\" if shift_tiff != shift_nbp : self . message = self . message + f \" \\n Shift used to make tiff was { shift_tiff } .\" \\ f \" \\n Current tile_pixel_value_shift in basic_info notebook page is \" \\ f \" { shift_nbp } .\" super () . __init__ ( self . message ) __init__ ( scale_tiff , scale_nbp , shift_tiff , shift_nbp ) Error raised because parameters used to produce tiff files are different to those in the current notebook. Parameters: Name Type Description Default scale_tiff float Scale factor applied to tiff. Found from tiff description. required scale_nbp float Scale factor applied to tiff. Found from nb.extract_debug.scale . required shift_tiff int Shift applied to tiff to ensure pixel values positive. Found from tiff description. required shift_nbp int Shift applied to tiff to ensure pixel values positive. Found from nb.basic_info.tile_pixel_value_shift . required Source code in coppafish/utils/errors.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , scale_tiff : float , scale_nbp : float , shift_tiff : int , shift_nbp : int ): \"\"\" Error raised because parameters used to produce tiff files are different to those in the current notebook. Args: scale_tiff: Scale factor applied to tiff. Found from tiff description. scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`. shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description. shift_nbp: Shift applied to tiff to ensure pixel values positive. Found from `nb.basic_info.tile_pixel_value_shift`. \"\"\" self . message = f \" \\n There are differences between the parameters used to make the tiffs and the parameters \" \\ f \"in the Notebook:\" if scale_tiff != scale_nbp : self . message = self . message + f \" \\n Scale used to make tiff was { scale_tiff } .\" \\ f \" \\n Current scale in extract_params notebook page is { scale_nbp } .\" if shift_tiff != shift_nbp : self . message = self . message + f \" \\n Shift used to make tiff was { shift_tiff } .\" \\ f \" \\n Current tile_pixel_value_shift in basic_info notebook page is \" \\ f \" { shift_nbp } .\" super () . __init__ ( self . message ) check_color_nan ( colors , nbp_basic ) colors should only contain the invalid_value in rounds/channels not in use_rounds/channels. This raises an error if this is not the case or if a round/channel not in use_rounds/channels contains a value other than invalid_value . invalid_value = -nbp_basic.tile_pixel_value_shift - 1 if colors is integer i.e. the non-normalised colors, usually spot_colors. invalid_value = -np.nan if colors is float i.e. the normalised colors or most likely the bled_codes. Parameters: Name Type Description Default colors np . ndarray int or float [n_codes x n_rounds x n_channels] colors[s, r, c] is the color for code s in round r , channel c . This is likely to be spot_colors if int or bled_codes if float . required nbp_basic NotebookPage basic_info NotebookPage required Source code in coppafish/utils/errors.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def check_color_nan ( colors : np . ndarray , nbp_basic : NotebookPage ): \"\"\" `colors` should only contain the `invalid_value` in rounds/channels not in use_rounds/channels. This raises an error if this is not the case or if a round/channel not in use_rounds/channels contains a value other than `invalid_value`. `invalid_value = -nbp_basic.tile_pixel_value_shift - 1` if colors is integer i.e. the non-normalised colors, usually spot_colors. `invalid_value = -np.nan` if colors is float i.e. the normalised colors or most likely the bled_codes. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage \"\"\" diff_to_int = np . round ( colors ) . astype ( int ) - colors if np . abs ( diff_to_int ) . max () == 0 : # if not normalised, then invalid_value is an integer value that is impossible for a spot_color to be invalid_value = - nbp_basic . tile_pixel_value_shift else : # if is normalised then expect nan value to be normal np.nan. invalid_value = np . nan # decide which rounds/channels should be ignored i.e. only contain invalid_value. n_spots , n_rounds , n_channels = colors . shape if n_rounds == nbp_basic . n_rounds and n_channels == nbp_basic . n_channels : use_rounds = nbp_basic . use_rounds use_channels = nbp_basic . use_channels elif n_rounds == len ( nbp_basic . use_rounds ) and n_channels == len ( nbp_basic . use_channels ): use_rounds = np . arange ( n_rounds ) use_channels = np . arange ( n_channels ) else : ColorInvalidWarning ( colors , nbp_basic , invalid_value ) ignore_rounds = np . setdiff1d ( np . arange ( n_rounds ), use_rounds ) for r in ignore_rounds : unique_vals = np . unique ( colors [:, r , :]) for val in unique_vals : if not invalid_value in unique_vals : ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r ) if not np . array_equal ( val , invalid_value , equal_nan = True ): ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r ) ignore_channels = np . setdiff1d ( np . arange ( n_channels ), use_channels ) for c in ignore_channels : unique_vals = np . unique ( colors [:, :, c ]) for val in unique_vals : if not invalid_value in unique_vals : ColorInvalidWarning ( colors , nbp_basic , invalid_value , channel_no = c ) if not np . array_equal ( val , invalid_value , equal_nan = True ): ColorInvalidWarning ( colors , nbp_basic , invalid_value , channel_no = c ) # see if any spots contain invalid_values. use_colors = colors [ np . ix_ ( np . arange ( n_spots ), use_rounds , use_channels )] if np . array_equal ( invalid_value , np . nan , equal_nan = True ): nan_codes = np . where ( np . isnan ( use_colors )) else : nan_codes = np . where ( use_colors == invalid_value ) n_nan_spots = nan_codes [ 0 ] . size if n_nan_spots > 0 : s = nan_codes [ 0 ][ 0 ] # round, channel number in spot_colors different from in use_spot_colors. r = np . arange ( n_rounds )[ nan_codes [ 1 ][ 0 ]] c = np . arange ( n_channels )[ nan_codes [ 2 ][ 0 ]] ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r , channel_no = c , code_no = s ) check_shape ( array , expected_shape ) Checks to see if array has the shape indicated by expected_shape . Parameters: Name Type Description Default array np . ndarray Array to check the shape of. required expected_shape Union [ list , tuple , np . ndarray ] int [n_array_dims] . Expected shape of array. required Returns: Type Description bool True if shape of array is correct. Source code in coppafish/utils/errors.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def check_shape ( array : np . ndarray , expected_shape : Union [ list , tuple , np . ndarray ]) -> bool : \"\"\" Checks to see if `array` has the shape indicated by `expected_shape`. Args: array: Array to check the shape of. expected_shape: `int [n_array_dims]`. Expected shape of array. Returns: `True` if shape of array is correct. \"\"\" correct_shape = array . ndim == len ( expected_shape ) # first check if number of dimensions are correct if correct_shape : correct_shape = np . abs ( np . array ( array . shape ) - np . array ( expected_shape )) . max () == 0 return correct_shape","title":"Errors"},{"location":"code/utils/errors/#coppafish.utils.errors.ColorInvalidWarning","text":"Source code in coppafish/utils/errors.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class ColorInvalidWarning : def __init__ ( self , colors : np . ndarray , nbp_basic : NotebookPage , invalid_value : float , round_no : Optional [ int ] = None , channel_no : Optional [ int ] = None , code_no : Optional [ int ] = None ): \"\"\" Warning raised because `spot_colors` contains a `invalid_value` where it should not. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage invalid_value: This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. round_no: round to flag error for. channel_no: channel to flag error for. code_no: Spot or gene index to flag error for. \"\"\" n_spots , n_rounds , n_channels = colors . shape if round_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in round { round_no } \\n \" \\ f \"which is not in use_rounds = { nbp_basic . use_rounds } .\" elif channel_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in channel { channel_no } \\n \" \\ f \"which is not in use_channels = { nbp_basic . use_channels } .\" elif round_no is not None and channel_no is not None and code_no is not None : self . message = f \"colors contains a invalid_value= { invalid_value } for code { code_no } , round { round_no } , \" \\ f \"channel { channel_no } . \\n \" \\ f \"There should be no invalid_values in this round and channel.\" else : self . message = f \"colors has n_rounds = { n_rounds } and n_channels = { n_channels } . \\n \" \\ f \"This is neither matches the total_rounds = { nbp_basic . n_rounds } and \" \\ f \"total_channels = { nbp_basic . n_channels } \\n \" \\ f \"nor the number of use_rounds = { len ( nbp_basic . use_rounds ) } and use_channels = \" \\ f \" { len ( nbp_basic . use_channels ) } \" warnings . warn ( self . message )","title":"ColorInvalidWarning"},{"location":"code/utils/errors/#coppafish.utils.errors.ColorInvalidWarning.__init__","text":"Warning raised because spot_colors contains a invalid_value where it should not. Parameters: Name Type Description Default colors np . ndarray int or float [n_codes x n_rounds x n_channels] colors[s, r, c] is the color for code s in round r , channel c . This is likely to be spot_colors if int or bled_codes if float . required nbp_basic NotebookPage basic_info NotebookPage required invalid_value float This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. required round_no Optional [ int ] round to flag error for. None channel_no Optional [ int ] channel to flag error for. None code_no Optional [ int ] Spot or gene index to flag error for. None Source code in coppafish/utils/errors.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def __init__ ( self , colors : np . ndarray , nbp_basic : NotebookPage , invalid_value : float , round_no : Optional [ int ] = None , channel_no : Optional [ int ] = None , code_no : Optional [ int ] = None ): \"\"\" Warning raised because `spot_colors` contains a `invalid_value` where it should not. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage invalid_value: This is the value that colors should only be in rounds/channels not used. Likely to be np.nan if colors is float or -nbp_basic.tile_pixel_value_shift if integer. round_no: round to flag error for. channel_no: channel to flag error for. code_no: Spot or gene index to flag error for. \"\"\" n_spots , n_rounds , n_channels = colors . shape if round_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in round { round_no } \\n \" \\ f \"which is not in use_rounds = { nbp_basic . use_rounds } .\" elif channel_no is not None and code_no is None : self . message = f \"colors contains a value other than invalid_value= { invalid_value } in channel { channel_no } \\n \" \\ f \"which is not in use_channels = { nbp_basic . use_channels } .\" elif round_no is not None and channel_no is not None and code_no is not None : self . message = f \"colors contains a invalid_value= { invalid_value } for code { code_no } , round { round_no } , \" \\ f \"channel { channel_no } . \\n \" \\ f \"There should be no invalid_values in this round and channel.\" else : self . message = f \"colors has n_rounds = { n_rounds } and n_channels = { n_channels } . \\n \" \\ f \"This is neither matches the total_rounds = { nbp_basic . n_rounds } and \" \\ f \"total_channels = { nbp_basic . n_channels } \\n \" \\ f \"nor the number of use_rounds = { len ( nbp_basic . use_rounds ) } and use_channels = \" \\ f \" { len ( nbp_basic . use_channels ) } \" warnings . warn ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.EmptyListError","text":"Bases: Exception Source code in coppafish/utils/errors.py 36 37 38 39 40 41 42 43 44 45 class EmptyListError ( Exception ): def __init__ ( self , var_name : str ): \"\"\" Error raised because the variable indicated by `var_name` contains no data. Args: var_name: Name of list or numpy array \"\"\" self . message = f \" \\n { var_name } contains no data\" super () . __init__ ( self . message )","title":"EmptyListError"},{"location":"code/utils/errors/#coppafish.utils.errors.EmptyListError.__init__","text":"Error raised because the variable indicated by var_name contains no data. Parameters: Name Type Description Default var_name str Name of list or numpy array required Source code in coppafish/utils/errors.py 37 38 39 40 41 42 43 44 45 def __init__ ( self , var_name : str ): \"\"\" Error raised because the variable indicated by `var_name` contains no data. Args: var_name: Name of list or numpy array \"\"\" self . message = f \" \\n { var_name } contains no data\" super () . __init__ ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.NoFileError","text":"Bases: Exception Source code in coppafish/utils/errors.py 24 25 26 27 28 29 30 31 32 33 class NoFileError ( Exception ): def __init__ ( self , file_path : str ): \"\"\" Error raised because `file_path` does not exist. Args: file_path: Path to file of interest. \"\"\" self . message = f \" \\n No file with the following path: \\n { file_path } \\n exists\" super () . __init__ ( self . message )","title":"NoFileError"},{"location":"code/utils/errors/#coppafish.utils.errors.NoFileError.__init__","text":"Error raised because file_path does not exist. Parameters: Name Type Description Default file_path str Path to file of interest. required Source code in coppafish/utils/errors.py 25 26 27 28 29 30 31 32 33 def __init__ ( self , file_path : str ): \"\"\" Error raised because `file_path` does not exist. Args: file_path: Path to file of interest. \"\"\" self . message = f \" \\n No file with the following path: \\n { file_path } \\n exists\" super () . __init__ ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.OutOfBoundsError","text":"Bases: Exception Source code in coppafish/utils/errors.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class OutOfBoundsError ( Exception ): def __init__ ( self , var_name : str , oob_val : float , min_allowed : float , max_allowed : float ): \"\"\" Error raised because `oob_val` is outside expected range between `min_allowed` and `max_allowed` inclusive. Args: var_name: Name of variable testing. oob_val: Value in array that is not in expected range. min_allowed: Smallest allowed value i.e. `>= min_allowed`. max_allowed: Largest allowed value i.e. `<= max_allowed`. \"\"\" self . message = f \" \\n { var_name } contains the value { oob_val } .\" \\ f \" \\n This is outside the expected inclusive range between { min_allowed } and { max_allowed } \" super () . __init__ ( self . message )","title":"OutOfBoundsError"},{"location":"code/utils/errors/#coppafish.utils.errors.OutOfBoundsError.__init__","text":"Error raised because oob_val is outside expected range between min_allowed and max_allowed inclusive. Parameters: Name Type Description Default var_name str Name of variable testing. required oob_val float Value in array that is not in expected range. required min_allowed float Smallest allowed value i.e. >= min_allowed . required max_allowed float Largest allowed value i.e. <= max_allowed . required Source code in coppafish/utils/errors.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , var_name : str , oob_val : float , min_allowed : float , max_allowed : float ): \"\"\" Error raised because `oob_val` is outside expected range between `min_allowed` and `max_allowed` inclusive. Args: var_name: Name of variable testing. oob_val: Value in array that is not in expected range. min_allowed: Smallest allowed value i.e. `>= min_allowed`. max_allowed: Largest allowed value i.e. `<= max_allowed`. \"\"\" self . message = f \" \\n { var_name } contains the value { oob_val } .\" \\ f \" \\n This is outside the expected inclusive range between { min_allowed } and { max_allowed } \" super () . __init__ ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.ShapeError","text":"Bases: Exception Source code in coppafish/utils/errors.py 66 67 68 69 70 71 72 73 74 75 76 77 class ShapeError ( Exception ): def __init__ ( self , var_name : str , var_shape : tuple , expected_shape : tuple ): \"\"\" Error raised because variable indicated by `var_name` has wrong shape. Args: var_name: Name of numpy array. var_shape: Shape of numpy array. expected_shape: Expected shape of numpy array. \"\"\" self . message = f \" \\n Shape of { var_name } is { var_shape } but should be { expected_shape } \" super () . __init__ ( self . message )","title":"ShapeError"},{"location":"code/utils/errors/#coppafish.utils.errors.ShapeError.__init__","text":"Error raised because variable indicated by var_name has wrong shape. Parameters: Name Type Description Default var_name str Name of numpy array. required var_shape tuple Shape of numpy array. required expected_shape tuple Expected shape of numpy array. required Source code in coppafish/utils/errors.py 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , var_name : str , var_shape : tuple , expected_shape : tuple ): \"\"\" Error raised because variable indicated by `var_name` has wrong shape. Args: var_name: Name of numpy array. var_shape: Shape of numpy array. expected_shape: Expected shape of numpy array. \"\"\" self . message = f \" \\n Shape of { var_name } is { var_shape } but should be { expected_shape } \" super () . __init__ ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.TiffError","text":"Bases: Exception Source code in coppafish/utils/errors.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class TiffError ( Exception ): def __init__ ( self , scale_tiff : float , scale_nbp : float , shift_tiff : int , shift_nbp : int ): \"\"\" Error raised because parameters used to produce tiff files are different to those in the current notebook. Args: scale_tiff: Scale factor applied to tiff. Found from tiff description. scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`. shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description. shift_nbp: Shift applied to tiff to ensure pixel values positive. Found from `nb.basic_info.tile_pixel_value_shift`. \"\"\" self . message = f \" \\n There are differences between the parameters used to make the tiffs and the parameters \" \\ f \"in the Notebook:\" if scale_tiff != scale_nbp : self . message = self . message + f \" \\n Scale used to make tiff was { scale_tiff } .\" \\ f \" \\n Current scale in extract_params notebook page is { scale_nbp } .\" if shift_tiff != shift_nbp : self . message = self . message + f \" \\n Shift used to make tiff was { shift_tiff } .\" \\ f \" \\n Current tile_pixel_value_shift in basic_info notebook page is \" \\ f \" { shift_nbp } .\" super () . __init__ ( self . message )","title":"TiffError"},{"location":"code/utils/errors/#coppafish.utils.errors.TiffError.__init__","text":"Error raised because parameters used to produce tiff files are different to those in the current notebook. Parameters: Name Type Description Default scale_tiff float Scale factor applied to tiff. Found from tiff description. required scale_nbp float Scale factor applied to tiff. Found from nb.extract_debug.scale . required shift_tiff int Shift applied to tiff to ensure pixel values positive. Found from tiff description. required shift_nbp int Shift applied to tiff to ensure pixel values positive. Found from nb.basic_info.tile_pixel_value_shift . required Source code in coppafish/utils/errors.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __init__ ( self , scale_tiff : float , scale_nbp : float , shift_tiff : int , shift_nbp : int ): \"\"\" Error raised because parameters used to produce tiff files are different to those in the current notebook. Args: scale_tiff: Scale factor applied to tiff. Found from tiff description. scale_nbp: Scale factor applied to tiff. Found from `nb.extract_debug.scale`. shift_tiff: Shift applied to tiff to ensure pixel values positive. Found from tiff description. shift_nbp: Shift applied to tiff to ensure pixel values positive. Found from `nb.basic_info.tile_pixel_value_shift`. \"\"\" self . message = f \" \\n There are differences between the parameters used to make the tiffs and the parameters \" \\ f \"in the Notebook:\" if scale_tiff != scale_nbp : self . message = self . message + f \" \\n Scale used to make tiff was { scale_tiff } .\" \\ f \" \\n Current scale in extract_params notebook page is { scale_nbp } .\" if shift_tiff != shift_nbp : self . message = self . message + f \" \\n Shift used to make tiff was { shift_tiff } .\" \\ f \" \\n Current tile_pixel_value_shift in basic_info notebook page is \" \\ f \" { shift_nbp } .\" super () . __init__ ( self . message )","title":"__init__()"},{"location":"code/utils/errors/#coppafish.utils.errors.check_color_nan","text":"colors should only contain the invalid_value in rounds/channels not in use_rounds/channels. This raises an error if this is not the case or if a round/channel not in use_rounds/channels contains a value other than invalid_value . invalid_value = -nbp_basic.tile_pixel_value_shift - 1 if colors is integer i.e. the non-normalised colors, usually spot_colors. invalid_value = -np.nan if colors is float i.e. the normalised colors or most likely the bled_codes. Parameters: Name Type Description Default colors np . ndarray int or float [n_codes x n_rounds x n_channels] colors[s, r, c] is the color for code s in round r , channel c . This is likely to be spot_colors if int or bled_codes if float . required nbp_basic NotebookPage basic_info NotebookPage required Source code in coppafish/utils/errors.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def check_color_nan ( colors : np . ndarray , nbp_basic : NotebookPage ): \"\"\" `colors` should only contain the `invalid_value` in rounds/channels not in use_rounds/channels. This raises an error if this is not the case or if a round/channel not in use_rounds/channels contains a value other than `invalid_value`. `invalid_value = -nbp_basic.tile_pixel_value_shift - 1` if colors is integer i.e. the non-normalised colors, usually spot_colors. `invalid_value = -np.nan` if colors is float i.e. the normalised colors or most likely the bled_codes. Args: colors: `int or float [n_codes x n_rounds x n_channels]` `colors[s, r, c]` is the color for code `s` in round `r`, channel `c`. This is likely to be `spot_colors` if `int` or `bled_codes` if `float`. nbp_basic: basic_info NotebookPage \"\"\" diff_to_int = np . round ( colors ) . astype ( int ) - colors if np . abs ( diff_to_int ) . max () == 0 : # if not normalised, then invalid_value is an integer value that is impossible for a spot_color to be invalid_value = - nbp_basic . tile_pixel_value_shift else : # if is normalised then expect nan value to be normal np.nan. invalid_value = np . nan # decide which rounds/channels should be ignored i.e. only contain invalid_value. n_spots , n_rounds , n_channels = colors . shape if n_rounds == nbp_basic . n_rounds and n_channels == nbp_basic . n_channels : use_rounds = nbp_basic . use_rounds use_channels = nbp_basic . use_channels elif n_rounds == len ( nbp_basic . use_rounds ) and n_channels == len ( nbp_basic . use_channels ): use_rounds = np . arange ( n_rounds ) use_channels = np . arange ( n_channels ) else : ColorInvalidWarning ( colors , nbp_basic , invalid_value ) ignore_rounds = np . setdiff1d ( np . arange ( n_rounds ), use_rounds ) for r in ignore_rounds : unique_vals = np . unique ( colors [:, r , :]) for val in unique_vals : if not invalid_value in unique_vals : ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r ) if not np . array_equal ( val , invalid_value , equal_nan = True ): ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r ) ignore_channels = np . setdiff1d ( np . arange ( n_channels ), use_channels ) for c in ignore_channels : unique_vals = np . unique ( colors [:, :, c ]) for val in unique_vals : if not invalid_value in unique_vals : ColorInvalidWarning ( colors , nbp_basic , invalid_value , channel_no = c ) if not np . array_equal ( val , invalid_value , equal_nan = True ): ColorInvalidWarning ( colors , nbp_basic , invalid_value , channel_no = c ) # see if any spots contain invalid_values. use_colors = colors [ np . ix_ ( np . arange ( n_spots ), use_rounds , use_channels )] if np . array_equal ( invalid_value , np . nan , equal_nan = True ): nan_codes = np . where ( np . isnan ( use_colors )) else : nan_codes = np . where ( use_colors == invalid_value ) n_nan_spots = nan_codes [ 0 ] . size if n_nan_spots > 0 : s = nan_codes [ 0 ][ 0 ] # round, channel number in spot_colors different from in use_spot_colors. r = np . arange ( n_rounds )[ nan_codes [ 1 ][ 0 ]] c = np . arange ( n_channels )[ nan_codes [ 2 ][ 0 ]] ColorInvalidWarning ( colors , nbp_basic , invalid_value , round_no = r , channel_no = c , code_no = s )","title":"check_color_nan()"},{"location":"code/utils/errors/#coppafish.utils.errors.check_shape","text":"Checks to see if array has the shape indicated by expected_shape . Parameters: Name Type Description Default array np . ndarray Array to check the shape of. required expected_shape Union [ list , tuple , np . ndarray ] int [n_array_dims] . Expected shape of array. required Returns: Type Description bool True if shape of array is correct. Source code in coppafish/utils/errors.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def check_shape ( array : np . ndarray , expected_shape : Union [ list , tuple , np . ndarray ]) -> bool : \"\"\" Checks to see if `array` has the shape indicated by `expected_shape`. Args: array: Array to check the shape of. expected_shape: `int [n_array_dims]`. Expected shape of array. Returns: `True` if shape of array is correct. \"\"\" correct_shape = array . ndim == len ( expected_shape ) # first check if number of dimensions are correct if correct_shape : correct_shape = np . abs ( np . array ( array . shape ) - np . array ( expected_shape )) . max () == 0 return correct_shape","title":"check_shape()"},{"location":"code/utils/matlab/","text":"load_array ( file_name , var_names ) This is used to load info from v7.3 or later matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If var_names is str , one value is returned, otherwise tuple of all values requested is returned. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_names Union [ str , List [ str ]] str [n_vars] . Names of variables desired. required Returns: Type Description Union [ tuple , np . ndarray ] Tuple of n_vars numpy arrays. Source code in coppafish/utils/matlab.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def load_array ( file_name : str , var_names : Union [ str , List [ str ]]) -> Union [ tuple , np . ndarray ]: \"\"\" This is used to load info from v7.3 or later matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If `var_names` is `str`, one value is returned, otherwise `tuple` of all values requested is returned. Args: file_name: Path of MATLAB file. var_names: `str [n_vars]`. Names of variables desired. Returns: `Tuple` of `n_vars` numpy arrays. \"\"\" f = h5py . File ( file_name ) if not isinstance ( var_names , list ): output = np . array ( f [ var_names ]) . transpose () else : output = [] for var_name in var_names : output . append ( np . array ( f [ var_name ]) . transpose ()) output = tuple ( output ) return output load_cell ( file_name , var_name ) If cell is M x N , will return list of length M where each entry is another list of length N and each element of this list is a numpy array. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_name str Names of variable in MATLAB file. required Returns: Type Description list MATLAB cell var_name as a list of numpy arrays. Source code in coppafish/utils/matlab.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def load_cell ( file_name : str , var_name : str ) -> list : \"\"\" If cell is `M x N`, will return list of length `M` where each entry is another list of length `N` and each element of this list is a numpy array. Args: file_name: Path of MATLAB file. var_name: Names of variable in MATLAB file. Returns: MATLAB cell `var_name` as a list of numpy arrays. \"\"\" # MAYBE CHANGE THIS TO OBJECT NUMPY ARRAY f = h5py . File ( file_name ) data = [] for column in np . transpose ( f [ var_name ]): row_data = [] for row_number in range ( len ( column )): row_data . append ( np . array ( f [ column [ row_number ]]) . transpose ()) data . append ( row_data ) return data load_v_less_7_3 ( file_name , var_names ) This is used to load info from earlier than v7.3 matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If var_names is str , one value is returned, otherwise tuple of all values requested is returned. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_names Union [ str , List [ str ]] str [n_vars] . Names of variables desired. required Returns: Type Description Union [ tuple , np . ndarray ] Tuple of n_vars numpy arrays. Source code in coppafish/utils/matlab.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_v_less_7_3 ( file_name : str , var_names : Union [ str , List [ str ]]) -> Union [ tuple , np . ndarray ]: \"\"\" This is used to load info from earlier than v7.3 matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If `var_names` is `str`, one value is returned, otherwise tuple of all values requested is returned. Args: file_name: Path of MATLAB file. var_names: `str [n_vars]`. Names of variables desired. Returns: `Tuple` of `n_vars` numpy arrays. \"\"\" f = io . loadmat ( file_name ) if not isinstance ( var_names , list ): output = f [ var_names ] else : output = [] for var_name in var_names : output . append ( f [ var_name ]) output = tuple ( output ) return output save_nb_results ( nb , file_name , score_thresh_ref_spots = 0.15 , score_thresh_omp = 0.15 ) Saves important information in notebook as a .mat file so can load in MATLAB and plot using python_testing/iss_object_from_python.m scripts Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required file_name str Path to save matlab version of Notebook. required score_thresh_ref_spots float Only ref_round spots with score exceeding this will be saved. 0.15 score_thresh_omp float Only omp spots with score exceeding this will be saved. 0.15 Source code in coppafish/utils/matlab.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def save_nb_results ( nb : Notebook , file_name : str , score_thresh_ref_spots : float = 0.15 , score_thresh_omp : float = 0.15 ): \"\"\" Saves important information in notebook as a .mat file so can load in MATLAB and plot using python_testing/iss_object_from_python.m scripts Args: nb: Notebook containing at least `call_spots` page. file_name: Path to save matlab version of Notebook. score_thresh_ref_spots: Only `ref_round` spots with score exceeding this will be saved. score_thresh_omp: Only `omp` spots with score exceeding this will be saved. \"\"\" mdic = {} if nb . has_page ( 'file_names' ): mdic [ 'tile_file_names' ] = nb . file_names . tile if nb . has_page ( 'stitch' ): mdic [ 'tile_origin' ] = nb . stitch . tile_origin if nb . has_page ( 'register' ): mdic [ 'transform' ] = nb . register . transform if nb . has_page ( 'call_spots' ): call_spots_dict = nb . call_spots . to_serial_dict () call_spots_dict = { k : v for k , v in call_spots_dict . items () if not '___' in k } del call_spots_dict [ 'PAGEINFO' ] mdic . update ( call_spots_dict ) if nb . has_page ( 'ref_spots' ): ref_spots_dict = nb . ref_spots . to_serial_dict () # Give 'ref_spots' prefix to key names as same variables in omp page. ref_spots_dict = { ref_spots_dict [ 'PAGEINFO' ] + '_' + k : v for k , v in ref_spots_dict . items () if not '___' in k } del ref_spots_dict [ 'ref_spots_PAGEINFO' ] ref_spots_dict = update_dict ( nb . ref_spots , nb , ref_spots_dict , score_thresh_ref_spots ) mdic . update ( ref_spots_dict ) if nb . has_page ( 'omp' ): omp_dict = nb . omp . to_serial_dict () omp_dict = { omp_dict [ 'PAGEINFO' ] + '_' + k : v for k , v in omp_dict . items () if not '___' in k } del omp_dict [ 'omp_PAGEINFO' ] omp_dict = update_dict ( nb . omp , nb , omp_dict , score_thresh_omp ) mdic . update ( omp_dict ) for k , v in mdic . items (): if v is None : mdic [ k ] = [] # Can't save None values io . savemat ( file_name , mdic ) update_dict ( nbp , nb , spots_info_dict , score_thresh ) Used in save_nb_results to reduced amount of spots saved. Only spots with score > score_thresh kept. Parameters: Name Type Description Default nbp NotebookPage Either omp or ref_spots NotebookPage. required nb Notebook Full notebook required spots_info_dict dict Dictionary produced in save_nb_results containing information to save about each spot. required score_thresh float All spots with a score above this threshold will be returned. required Returns: Type Description dict spots_info_dict is returned, just including spots for which score > score_thresh . Source code in coppafish/utils/matlab.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def update_dict ( nbp : NotebookPage , nb : Notebook , spots_info_dict : dict , score_thresh : float ) -> dict : \"\"\" Used in `save_nb_results` to reduced amount of spots saved. Only spots with `score > score_thresh` kept. Args: nbp: Either `omp` or `ref_spots` NotebookPage. nb: Full notebook spots_info_dict: Dictionary produced in `save_nb_results` containing information to save about each spot. score_thresh: All spots with a score above this threshold will be returned. Returns: `spots_info_dict` is returned, just including spots for which `score > score_thresh`. \"\"\" pf = nbp . name + '_' if pf != 'omp_' and pf != 'ref_spots_' : raise ValueError ( \"Wrong page given, should be 'omp' or 'ref_spots'\" ) nbp . finalized = False score_thresh_old = nbp . score_thresh del nbp . score_thresh nbp . score_thresh = score_thresh if pf == 'omp_' : keep = quality_threshold ( nb , 'omp' ) else : keep = quality_threshold ( nb , 'ref' ) del nbp . score_thresh nbp . score_thresh = score_thresh_old nbp . finalized = True for var in [ 'local_yxz' , 'tile' , 'colors' , 'intensity' , 'background_coef' , 'gene_no' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] if pf == 'ref_spots_' : del spots_info_dict [ pf + 'background_coef' ] for var in [ 'isolated' , 'score' , 'score_diff' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] if pf == 'omp_' : for var in [ 'shape_spot_local_yxz' , 'shape_spot_gene_no' , 'spot_shape_float' ]: del spots_info_dict [ pf + var ] for var in [ 'coef' , 'n_neighbours_pos' , 'n_neighbours_neg' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] return spots_info_dict","title":"Matlab"},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_array","text":"This is used to load info from v7.3 or later matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If var_names is str , one value is returned, otherwise tuple of all values requested is returned. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_names Union [ str , List [ str ]] str [n_vars] . Names of variables desired. required Returns: Type Description Union [ tuple , np . ndarray ] Tuple of n_vars numpy arrays. Source code in coppafish/utils/matlab.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def load_array ( file_name : str , var_names : Union [ str , List [ str ]]) -> Union [ tuple , np . ndarray ]: \"\"\" This is used to load info from v7.3 or later matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If `var_names` is `str`, one value is returned, otherwise `tuple` of all values requested is returned. Args: file_name: Path of MATLAB file. var_names: `str [n_vars]`. Names of variables desired. Returns: `Tuple` of `n_vars` numpy arrays. \"\"\" f = h5py . File ( file_name ) if not isinstance ( var_names , list ): output = np . array ( f [ var_names ]) . transpose () else : output = [] for var_name in var_names : output . append ( np . array ( f [ var_name ]) . transpose ()) output = tuple ( output ) return output","title":"load_array()"},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_cell","text":"If cell is M x N , will return list of length M where each entry is another list of length N and each element of this list is a numpy array. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_name str Names of variable in MATLAB file. required Returns: Type Description list MATLAB cell var_name as a list of numpy arrays. Source code in coppafish/utils/matlab.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def load_cell ( file_name : str , var_name : str ) -> list : \"\"\" If cell is `M x N`, will return list of length `M` where each entry is another list of length `N` and each element of this list is a numpy array. Args: file_name: Path of MATLAB file. var_name: Names of variable in MATLAB file. Returns: MATLAB cell `var_name` as a list of numpy arrays. \"\"\" # MAYBE CHANGE THIS TO OBJECT NUMPY ARRAY f = h5py . File ( file_name ) data = [] for column in np . transpose ( f [ var_name ]): row_data = [] for row_number in range ( len ( column )): row_data . append ( np . array ( f [ column [ row_number ]]) . transpose ()) data . append ( row_data ) return data","title":"load_cell()"},{"location":"code/utils/matlab/#coppafish.utils.matlab.load_v_less_7_3","text":"This is used to load info from earlier than v7.3 matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If var_names is str , one value is returned, otherwise tuple of all values requested is returned. Parameters: Name Type Description Default file_name str Path of MATLAB file. required var_names Union [ str , List [ str ]] str [n_vars] . Names of variables desired. required Returns: Type Description Union [ tuple , np . ndarray ] Tuple of n_vars numpy arrays. Source code in coppafish/utils/matlab.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_v_less_7_3 ( file_name : str , var_names : Union [ str , List [ str ]]) -> Union [ tuple , np . ndarray ]: \"\"\" This is used to load info from earlier than v7.3 matlab files. It is also good at dealing with complicated matlab cell arrays which are loaded as numpy object arrays. If `var_names` is `str`, one value is returned, otherwise tuple of all values requested is returned. Args: file_name: Path of MATLAB file. var_names: `str [n_vars]`. Names of variables desired. Returns: `Tuple` of `n_vars` numpy arrays. \"\"\" f = io . loadmat ( file_name ) if not isinstance ( var_names , list ): output = f [ var_names ] else : output = [] for var_name in var_names : output . append ( f [ var_name ]) output = tuple ( output ) return output","title":"load_v_less_7_3()"},{"location":"code/utils/matlab/#coppafish.utils.matlab.save_nb_results","text":"Saves important information in notebook as a .mat file so can load in MATLAB and plot using python_testing/iss_object_from_python.m scripts Parameters: Name Type Description Default nb Notebook Notebook containing at least call_spots page. required file_name str Path to save matlab version of Notebook. required score_thresh_ref_spots float Only ref_round spots with score exceeding this will be saved. 0.15 score_thresh_omp float Only omp spots with score exceeding this will be saved. 0.15 Source code in coppafish/utils/matlab.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def save_nb_results ( nb : Notebook , file_name : str , score_thresh_ref_spots : float = 0.15 , score_thresh_omp : float = 0.15 ): \"\"\" Saves important information in notebook as a .mat file so can load in MATLAB and plot using python_testing/iss_object_from_python.m scripts Args: nb: Notebook containing at least `call_spots` page. file_name: Path to save matlab version of Notebook. score_thresh_ref_spots: Only `ref_round` spots with score exceeding this will be saved. score_thresh_omp: Only `omp` spots with score exceeding this will be saved. \"\"\" mdic = {} if nb . has_page ( 'file_names' ): mdic [ 'tile_file_names' ] = nb . file_names . tile if nb . has_page ( 'stitch' ): mdic [ 'tile_origin' ] = nb . stitch . tile_origin if nb . has_page ( 'register' ): mdic [ 'transform' ] = nb . register . transform if nb . has_page ( 'call_spots' ): call_spots_dict = nb . call_spots . to_serial_dict () call_spots_dict = { k : v for k , v in call_spots_dict . items () if not '___' in k } del call_spots_dict [ 'PAGEINFO' ] mdic . update ( call_spots_dict ) if nb . has_page ( 'ref_spots' ): ref_spots_dict = nb . ref_spots . to_serial_dict () # Give 'ref_spots' prefix to key names as same variables in omp page. ref_spots_dict = { ref_spots_dict [ 'PAGEINFO' ] + '_' + k : v for k , v in ref_spots_dict . items () if not '___' in k } del ref_spots_dict [ 'ref_spots_PAGEINFO' ] ref_spots_dict = update_dict ( nb . ref_spots , nb , ref_spots_dict , score_thresh_ref_spots ) mdic . update ( ref_spots_dict ) if nb . has_page ( 'omp' ): omp_dict = nb . omp . to_serial_dict () omp_dict = { omp_dict [ 'PAGEINFO' ] + '_' + k : v for k , v in omp_dict . items () if not '___' in k } del omp_dict [ 'omp_PAGEINFO' ] omp_dict = update_dict ( nb . omp , nb , omp_dict , score_thresh_omp ) mdic . update ( omp_dict ) for k , v in mdic . items (): if v is None : mdic [ k ] = [] # Can't save None values io . savemat ( file_name , mdic )","title":"save_nb_results()"},{"location":"code/utils/matlab/#coppafish.utils.matlab.update_dict","text":"Used in save_nb_results to reduced amount of spots saved. Only spots with score > score_thresh kept. Parameters: Name Type Description Default nbp NotebookPage Either omp or ref_spots NotebookPage. required nb Notebook Full notebook required spots_info_dict dict Dictionary produced in save_nb_results containing information to save about each spot. required score_thresh float All spots with a score above this threshold will be returned. required Returns: Type Description dict spots_info_dict is returned, just including spots for which score > score_thresh . Source code in coppafish/utils/matlab.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def update_dict ( nbp : NotebookPage , nb : Notebook , spots_info_dict : dict , score_thresh : float ) -> dict : \"\"\" Used in `save_nb_results` to reduced amount of spots saved. Only spots with `score > score_thresh` kept. Args: nbp: Either `omp` or `ref_spots` NotebookPage. nb: Full notebook spots_info_dict: Dictionary produced in `save_nb_results` containing information to save about each spot. score_thresh: All spots with a score above this threshold will be returned. Returns: `spots_info_dict` is returned, just including spots for which `score > score_thresh`. \"\"\" pf = nbp . name + '_' if pf != 'omp_' and pf != 'ref_spots_' : raise ValueError ( \"Wrong page given, should be 'omp' or 'ref_spots'\" ) nbp . finalized = False score_thresh_old = nbp . score_thresh del nbp . score_thresh nbp . score_thresh = score_thresh if pf == 'omp_' : keep = quality_threshold ( nb , 'omp' ) else : keep = quality_threshold ( nb , 'ref' ) del nbp . score_thresh nbp . score_thresh = score_thresh_old nbp . finalized = True for var in [ 'local_yxz' , 'tile' , 'colors' , 'intensity' , 'background_coef' , 'gene_no' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] if pf == 'ref_spots_' : del spots_info_dict [ pf + 'background_coef' ] for var in [ 'isolated' , 'score' , 'score_diff' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] if pf == 'omp_' : for var in [ 'shape_spot_local_yxz' , 'shape_spot_gene_no' , 'spot_shape_float' ]: del spots_info_dict [ pf + var ] for var in [ 'coef' , 'n_neighbours_pos' , 'n_neighbours_neg' ]: spots_info_dict [ pf + var ] = spots_info_dict [ pf + var ][ keep ] return spots_info_dict","title":"update_dict()"},{"location":"code/utils/morphology/","text":"Base convolve_2d ( image , kernel ) Convolves image with kernel , padding by replicating border pixels. Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2] . Image to convolve. required kernel np . ndarray float [kernel_sz1 x kernel_sz2] . 2D kernel required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being convolved with kernel . Note np.flip is used to give same result as convn with replicate padding in MATLAB. Source code in coppafish/utils/morphology/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def convolve_2d ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Convolves `image` with `kernel`, padding by replicating border pixels. Args: image: `float [image_sz1 x image_sz2]`. Image to convolve. kernel: `float [kernel_sz1 x kernel_sz2]`. 2D kernel Returns: `float [image_sz1 x image_sz2]`. `image` after being convolved with `kernel`. !!! note `np.flip` is used to give same result as `convn` with replicate padding in MATLAB. \"\"\" return cv2 . filter2D ( image . astype ( float ), - 1 , np . flip ( kernel ), borderType = cv2 . BORDER_REPLICATE ) dilate ( image , kernel ) Dilates image with kernel , using zero padding. Parameters: Name Type Description Default image np . ndarray float [image_sz1 x ... x image_szN] . Image to be dilated. required kernel np . ndarray int [kernel_sz1 x ... x kernel_szN] . Dilation kernel containing only zeros or ones. required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being dilated with kernel . Source code in coppafish/utils/morphology/base.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def dilate ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Dilates `image` with `kernel`, using zero padding. Args: image: `float [image_sz1 x ... x image_szN]`. Image to be dilated. kernel: `int [kernel_sz1 x ... x kernel_szN]`. Dilation kernel containing only zeros or ones. Returns: `float [image_sz1 x image_sz2]`. `image` after being dilated with `kernel`. \"\"\" kernel = ensure_odd_kernel ( kernel ) # mode refers to the padding. We pad with zeros to keep results the same as MATLAB return grey_dilation ( image , footprint = kernel , mode = 'constant' ) ensure_odd_kernel ( kernel , pad_location = 'start' ) This ensures all dimensions of kernel are odd by padding even dimensions with zeros. Replicates MATLAB way of dealing with even kernels. Parameters: Name Type Description Default kernel np . ndarray float [kernel_sz1 x kernel_sz2 x ... x kernel_szN] . required pad_location str One of the following, indicating where to pad with zeros - 'start' - Zeros at start of kernel. 'end' - Zeros at end of kernel. 'start' Returns: Type Description np . ndarray float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN] . kernel padded with zeros so each dimension is odd. Example If pad_location is 'start' then [[5,4];[3,1]] becomes [[0,0,0],[0,5,4],[0,3,1]] . Source code in coppafish/utils/morphology/base.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def ensure_odd_kernel ( kernel : np . ndarray , pad_location : str = 'start' ) -> np . ndarray : \"\"\" This ensures all dimensions of `kernel` are odd by padding even dimensions with zeros. Replicates MATLAB way of dealing with even kernels. Args: kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`. pad_location: One of the following, indicating where to pad with zeros - - `'start'` - Zeros at start of kernel. - `'end'` - Zeros at end of kernel. Returns: `float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN]`. `kernel` padded with zeros so each dimension is odd. Example: If `pad_location` is `'start'` then `[[5,4];[3,1]]` becomes `[[0,0,0],[0,5,4],[0,3,1]]`. \"\"\" even_dims = ( np . mod ( kernel . shape , 2 ) == 0 ) . astype ( int ) if max ( even_dims ) == 1 : if pad_location == 'start' : pad_dims = [ tuple ( np . array ([ 1 , 0 ]) * val ) for val in even_dims ] elif pad_location == 'end' : pad_dims = [ tuple ( np . array ([ 0 , 1 ]) * val ) for val in even_dims ] else : raise ValueError ( f \"pad_location has to be either 'start' or 'end' but value given was { pad_location } .\" ) return np . pad ( kernel , pad_dims , mode = 'constant' ) else : return kernel ftrans2 ( b , t = None ) Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, b , using the transform, t . Copied from MATLAB ftrans2 . Parameters: Name Type Description Default b np . ndarray float [Q] . 1D convolve kernel. required t Optional [ np . ndarray ] float [M x N] . Transform to make b a 2D convolve kernel. If None , McClellan transform used. None Returns: Type Description np . ndarray float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1] . 2D convolve kernel. Source code in coppafish/utils/morphology/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def ftrans2 ( b : np . ndarray , t : Optional [ np . ndarray ] = None ) -> np . ndarray : \"\"\" Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, `b`, using the transform, `t`. Copied from [MATLAB `ftrans2`](https://www.mathworks.com/help/images/ref/ftrans2.html). Args: b: `float [Q]`. 1D convolve kernel. t: `float [M x N]`. Transform to make `b` a 2D convolve kernel. If `None`, McClellan transform used. Returns: `float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1]`. 2D convolve kernel. \"\"\" if t is None : # McClellan transformation t = np . array ([[ 1 , 2 , 1 ], [ 2 , - 4 , 2 ], [ 1 , 2 , 1 ]]) / 8 # Convert the 1-D convolve_2d b to SUM_n a(n) cos(wn) form n = int ( round (( len ( b ) - 1 ) / 2 )) b = b . reshape ( - 1 , 1 ) b = np . rot90 ( np . fft . fftshift ( np . rot90 ( b ))) a = np . concatenate (( b [: 1 ], 2 * b [ 1 : n + 1 ])) inset = np . floor (( np . array ( t . shape ) - 1 ) / 2 ) . astype ( int ) # Use Chebyshev polynomials to compute h p0 = 1 p1 = t h = a [ 1 ] * p1 rows = inset [ 0 ] cols = inset [ 1 ] h [ rows , cols ] += a [ 0 ] * p0 for i in range ( 2 , n + 1 ): p2 = 2 * scipy . signal . convolve2d ( t , p1 ) rows = rows + inset [ 0 ] cols = cols + inset [ 1 ] p2 [ rows , cols ] -= p0 rows = inset [ 0 ] + np . arange ( p1 . shape [ 0 ]) cols = ( inset [ 1 ] + np . arange ( p1 . shape [ 1 ])) . reshape ( - 1 , 1 ) hh = h . copy () h = a [ i ] * p2 h [ rows , cols ] += hh p0 = p1 . copy () p1 = p2 . copy () h = np . rot90 ( h ) return h hanning_diff ( r1 , r2 ) Gets difference of two hanning window 2D convolve kernel. Central positive, outer negative with sum of 0 . Parameters: Name Type Description Default r1 int radius in pixels of central positive hanning convolve kernel. required r2 int radius in pixels of outer negative hanning convolve kernel. required Returns: Type Description np . ndarray float [2*r2+1 x 2*r2+1] . Difference of two hanning window 2D convolve kernel. Source code in coppafish/utils/morphology/base.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def hanning_diff ( r1 : int , r2 : int ) -> np . ndarray : \"\"\" Gets difference of two hanning window 2D convolve kernel. Central positive, outer negative with sum of `0`. Args: r1: radius in pixels of central positive hanning convolve kernel. r2: radius in pixels of outer negative hanning convolve kernel. Returns: `float [2*r2+1 x 2*r2+1]`. Difference of two hanning window 2D convolve kernel. \"\"\" if not 0 <= r1 <= r2 - 1 : raise errors . OutOfBoundsError ( \"r1\" , r1 , 0 , r2 - 1 ) if not r1 + 1 <= r2 <= np . inf : raise errors . OutOfBoundsError ( \"r2\" , r1 + 1 , np . inf ) h_outer = np . hanning ( 2 * r2 + 3 )[ 1 : - 1 ] # ignore zero values at first and last index h_outer = - h_outer / h_outer . sum () h_inner = np . hanning ( 2 * r1 + 3 )[ 1 : - 1 ] h_inner = h_inner / h_inner . sum () h = h_outer . copy () h [ r2 - r1 : r2 + r1 + 1 ] += h_inner h = ftrans2 ( h ) return h top_hat ( image , kernel ) Does tophat filtering of image with kernel . Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2] . Image to filter. required kernel np . ndarray np.uint8 [kernel_sz1 x kernel_sz2] . Top hat kernel containing only zeros or ones. required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being top hat filtered with kernel . Source code in coppafish/utils/morphology/base.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def top_hat ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Does tophat filtering of `image` with `kernel`. Args: image: `float [image_sz1 x image_sz2]`. Image to filter. kernel: `np.uint8 [kernel_sz1 x kernel_sz2]`. Top hat `kernel` containing only zeros or ones. Returns: `float [image_sz1 x image_sz2]`. `image` after being top hat filtered with `kernel`. \"\"\" if kernel . dtype != np . uint8 : if sum ( np . unique ( kernel ) == [ 0 , 1 ]) == len ( np . unique ( kernel )): kernel = kernel . astype ( np . uint8 ) # kernel must be uint8 else : raise ValueError ( f 'kernel is of type { kernel . dtype } but must be of data type np.uint8.' ) image_dtype = image . dtype # so returned image is of same dtype as input if image . dtype == int : if image . min () >= 0 and image . max () <= np . iinfo ( np . uint16 ) . max : image = image . astype ( np . uint16 ) if not ( image . dtype == float or image . dtype == np . uint16 ): raise ValueError ( f 'image is of type { image . dtype } but must be of data type np.uint16 or float.' ) if np . max ( np . mod ( kernel . shape , 2 ) == 0 ): # With even kernel, gives different results to MATLAB raise ValueError ( f 'kernel dimensions are { kernel . shape } . Require all dimensions to be odd.' ) # kernel = ensure_odd_kernel(kernel) # doesn't work for tophat at start or end. return cv2 . morphologyEx ( image , cv2 . MORPH_TOPHAT , kernel ) . astype ( image_dtype ) Filter imfilter ( image , kernel , padding = 0 , corr_or_conv = 'corr' , oa = True ) Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2 x ... x image_szN] . Image to be filtered. required kernel np . ndarray float [kernel_sz1 x kernel_sz2 x ... x kernel_szN] . Multidimensional filter. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. 'conv' - Performs multidimensional filtering using convolution. 'corr' oa bool Whether to use oaconvolve or scipy.ndimage.convolve. scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes). True Returns: Type Description np . ndarray float [image_sz1 x image_sz2 x ... x image_szN] . image after being filtered. Source code in coppafish/utils/morphology/filter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def imfilter ( image : np . ndarray , kernel : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' , oa : bool = True ) -> np . ndarray : \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Args: image: `float [image_sz1 x image_sz2 x ... x image_szN]`. Image to be filtered. kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`. Multidimensional filter. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. - `'conv'` - Performs multidimensional filtering using convolution. oa: Whether to use oaconvolve or scipy.ndimage.convolve. scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes). Returns: `float [image_sz1 x image_sz2 x ... x image_szN]`. `image` after being filtered. \"\"\" if oa : if corr_or_conv == 'corr' : kernel = np . flip ( kernel ) elif corr_or_conv != 'conv' : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" ) kernel = ensure_odd_kernel ( kernel , 'end' ) if kernel . ndim < image . ndim : kernel = np . expand_dims ( kernel , axis = tuple ( np . arange ( kernel . ndim , image . ndim ))) pad_size = [( int (( ax_size - 1 ) / 2 ),) * 2 for ax_size in kernel . shape ] if isinstance ( padding , numbers . Number ): return oaconvolve ( np . pad ( image , pad_size , 'constant' , constant_values = padding ), kernel , 'valid' ) else : return oaconvolve ( np . pad ( image , pad_size , padding ), kernel , 'valid' ) else : if padding == 'symmetric' : padding = 'reflect' elif padding == 'edge' : padding = 'nearest' # Old method, about 3x slower for filtering large 3d image with small 3d kernel if isinstance ( padding , numbers . Number ): pad_value = padding padding = 'constant' else : pad_value = 0.0 # doesn't do anything for non-constant padding if corr_or_conv == 'corr' : kernel = ensure_odd_kernel ( kernel , 'start' ) return correlate ( image , kernel , mode = padding , cval = pad_value ) elif corr_or_conv == 'conv' : kernel = ensure_odd_kernel ( kernel , 'end' ) return convolve ( image , kernel , mode = padding , cval = pad_value ) else : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" ) imfilter_coords ( image , kernel , coords , padding = 0 , corr_or_conv = 'corr' ) Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Only finds result of filtering at specific locations but still filters entire image. Note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Parameters: Name Type Description Default image np . ndarray int [image_szY x image_szX (x image_szZ)] . Image to be filtered. Must be 2D or 3D. required kernel np . ndarray int [kernel_szY x kernel_szX (x kernel_szZ)] . Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. required coords np . ndarray int [n_points x image.ndims] . Coordinates where result of filtering is desired. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. 'conv' - Performs multidimensional filtering using convolution. 'corr' Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def imfilter_coords ( image : np . ndarray , kernel : np . ndarray , coords : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Only finds result of filtering at specific locations but still filters entire image. !!! note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Args: image: `int [image_szY x image_szX (x image_szZ)]`. Image to be filtered. Must be 2D or 3D. kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. coords: `int [n_points x image.ndims]`. Coordinates where result of filtering is desired. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. - `'conv'` - Performs multidimensional filtering using convolution. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" im_filt = imfilter ( image . astype ( int ), kernel , padding , corr_or_conv , oa = False ) return im_filt [ tuple ([ coords [:, j ] for j in range ( im_filt . ndim )])] Filter Optimised get_shifts_from_kernel ( kernel ) Returns where kernel is positive as shifts in y, x and z. I.e. kernel=jnp.ones((3,3,3)) would return y_shifts = x_shifts = z_shifts = -1, 0, 1 . Parameters: Name Type Description Default kernel jnp . ndarray int [kernel_szY x kernel_szX x kernel_szY] required Returns: Type Description jnp . ndarray int [n_shifts] . y_shifts. jnp . ndarray int [n_shifts] . x_shifts. jnp . ndarray int [n_shifts] . z_shifts. Source code in coppafish/utils/morphology/filter_optimised.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_shifts_from_kernel ( kernel : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Returns where kernel is positive as shifts in y, x and z. I.e. `kernel=jnp.ones((3,3,3))` would return `y_shifts = x_shifts = z_shifts = -1, 0, 1`. Args: kernel: int [kernel_szY x kernel_szX x kernel_szY] Returns: - `int [n_shifts]`. y_shifts. - `int [n_shifts]`. x_shifts. - `int [n_shifts]`. z_shifts. \"\"\" shifts = list ( jnp . where ( kernel > 0 )) for i in range ( kernel . ndim ): shifts [ i ] = ( shifts [ i ] - ( kernel . shape [ i ] - 1 ) / 2 ) . astype ( int ) return tuple ( shifts ) imfilter_coords ( image , kernel , coords , padding = 0 , corr_or_conv = 'corr' ) Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Only finds result of filtering at specific locations. Note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Parameters: Name Type Description Default image np . ndarray int [image_szY x image_szX (x image_szZ)] . Image to be filtered. Must be 2D or 3D. required kernel np . ndarray int [kernel_szY x kernel_szX (x kernel_szZ)] . Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. required coords np . ndarray int [n_points x image.ndims] . Coordinates where result of filtering is desired. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. This is the default when no option specified. 'conv' - Performs multidimensional filtering using convolution. 'corr' Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter_optimised.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def imfilter_coords ( image : np . ndarray , kernel : np . ndarray , coords : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Only finds result of filtering at specific locations. !!! note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Args: image: `int [image_szY x image_szX (x image_szZ)]`. Image to be filtered. Must be 2D or 3D. kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. coords: `int [n_points x image.ndims]`. Coordinates where result of filtering is desired. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. This is the default when no option specified. - `'conv'` - Performs multidimensional filtering using convolution. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" if corr_or_conv == 'corr' : kernel = np . flip ( kernel ) elif corr_or_conv != 'conv' : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" ) kernel = ensure_odd_kernel ( kernel , 'end' ) # Ensure shape of image and kernel correct if image . ndim != coords . shape [ 1 ]: raise ValueError ( f \"Image has { image . ndim } dimensions but coords only have { coords . shape [ 1 ] } dimensions.\" ) if image . ndim == 2 : image = np . expand_dims ( image , 2 ) elif image . ndim != 3 : raise ValueError ( f \"image must have 2 or 3 dimensions but given image has { image . ndim } .\" ) if kernel . ndim == 2 : kernel = np . expand_dims ( kernel , 2 ) elif kernel . ndim != 3 : raise ValueError ( f \"kernel must have 2 or 3 dimensions but given image has { image . ndim } .\" ) if kernel . max () > 1 : raise ValueError ( f 'kernel is expected to be binary, only containing 0 or 1 but kernel.max = { kernel . max () } ' ) if coords . shape [ 1 ] == 2 : # set all z coordinates to 0 if 2D. coords = np . append ( coords , np . zeros (( coords . shape [ 0 ], 1 ), dtype = int ), axis = 1 ) if ( coords . max ( axis = 0 ) >= np . array ( image . shape )) . any (): raise ValueError ( f \"Max yxz coordinates provided are { coords . max ( axis = 0 ) } but image has shape { image . shape } .\" ) pad_size = [( int (( ax_size - 1 ) / 2 ),) * 2 for ax_size in kernel . shape ] pad_coords = jnp . asarray ( coords ) + jnp . array ([ val [ 0 ] for val in pad_size ]) if isinstance ( padding , numbers . Number ): image_pad = jnp . pad ( jnp . asarray ( image ), pad_size , 'constant' , constant_values = padding ) . astype ( int ) else : image_pad = jnp . pad ( jnp . asarray ( image ), pad_size , padding ) . astype ( int ) y_shifts , x_shifts , z_shifts = get_shifts_from_kernel ( jnp . asarray ( np . flip ( kernel ))) return np . asarray ( manual_convolve ( image_pad , y_shifts , x_shifts , z_shifts , pad_coords )) manual_convolve ( image , y_kernel_shifts , x_kernel_shifts , z_kernel_shifts , coords ) Finds result of convolution at specific locations indicated by coords with binary kernel. I.e. instead of convolving whole image , just find result at these points . Note image needs to be padded before this function is called otherwise get an error when go out of bounds. Parameters: Name Type Description Default image jnp . ndarray int [image_szY x image_szX x image_szZ] . Image to be filtered. Must be 3D. required y_kernel_shifts jnp . ndarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required x_kernel_shifts jnp . asarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required z_kernel_shifts jnp . ndarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required coords jnp . ndarray int [n_points x 3] . yxz coordinates where result of filtering is desired. required Returns: Type Description jnp . ndarray int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter_optimised.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @jax . jit def manual_convolve ( image : jnp . ndarray , y_kernel_shifts : jnp . ndarray , x_kernel_shifts : jnp . asarray , z_kernel_shifts : jnp . ndarray , coords : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds result of convolution at specific locations indicated by `coords` with binary kernel. I.e. instead of convolving whole `image`, just find result at these `points`. !!! note image needs to be padded before this function is called otherwise get an error when go out of bounds. Args: image: `int [image_szY x image_szX x image_szZ]`. Image to be filtered. Must be 3D. y_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. x_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. z_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. coords: `int [n_points x 3]`. yxz coordinates where result of filtering is desired. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" return jax . vmap ( manual_convolve_single , in_axes = ( None , None , None , None , 0 ), out_axes = 0 )( image , y_kernel_shifts , x_kernel_shifts , z_kernel_shifts , coords )","title":"Morphology"},{"location":"code/utils/morphology/#base","text":"","title":"Base"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.convolve_2d","text":"Convolves image with kernel , padding by replicating border pixels. Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2] . Image to convolve. required kernel np . ndarray float [kernel_sz1 x kernel_sz2] . 2D kernel required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being convolved with kernel . Note np.flip is used to give same result as convn with replicate padding in MATLAB. Source code in coppafish/utils/morphology/base.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 def convolve_2d ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Convolves `image` with `kernel`, padding by replicating border pixels. Args: image: `float [image_sz1 x image_sz2]`. Image to convolve. kernel: `float [kernel_sz1 x kernel_sz2]`. 2D kernel Returns: `float [image_sz1 x image_sz2]`. `image` after being convolved with `kernel`. !!! note `np.flip` is used to give same result as `convn` with replicate padding in MATLAB. \"\"\" return cv2 . filter2D ( image . astype ( float ), - 1 , np . flip ( kernel ), borderType = cv2 . BORDER_REPLICATE )","title":"convolve_2d()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.dilate","text":"Dilates image with kernel , using zero padding. Parameters: Name Type Description Default image np . ndarray float [image_sz1 x ... x image_szN] . Image to be dilated. required kernel np . ndarray int [kernel_sz1 x ... x kernel_szN] . Dilation kernel containing only zeros or ones. required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being dilated with kernel . Source code in coppafish/utils/morphology/base.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def dilate ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Dilates `image` with `kernel`, using zero padding. Args: image: `float [image_sz1 x ... x image_szN]`. Image to be dilated. kernel: `int [kernel_sz1 x ... x kernel_szN]`. Dilation kernel containing only zeros or ones. Returns: `float [image_sz1 x image_sz2]`. `image` after being dilated with `kernel`. \"\"\" kernel = ensure_odd_kernel ( kernel ) # mode refers to the padding. We pad with zeros to keep results the same as MATLAB return grey_dilation ( image , footprint = kernel , mode = 'constant' )","title":"dilate()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.ensure_odd_kernel","text":"This ensures all dimensions of kernel are odd by padding even dimensions with zeros. Replicates MATLAB way of dealing with even kernels. Parameters: Name Type Description Default kernel np . ndarray float [kernel_sz1 x kernel_sz2 x ... x kernel_szN] . required pad_location str One of the following, indicating where to pad with zeros - 'start' - Zeros at start of kernel. 'end' - Zeros at end of kernel. 'start' Returns: Type Description np . ndarray float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN] . kernel padded with zeros so each dimension is odd. Example If pad_location is 'start' then [[5,4];[3,1]] becomes [[0,0,0],[0,5,4],[0,3,1]] . Source code in coppafish/utils/morphology/base.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def ensure_odd_kernel ( kernel : np . ndarray , pad_location : str = 'start' ) -> np . ndarray : \"\"\" This ensures all dimensions of `kernel` are odd by padding even dimensions with zeros. Replicates MATLAB way of dealing with even kernels. Args: kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`. pad_location: One of the following, indicating where to pad with zeros - - `'start'` - Zeros at start of kernel. - `'end'` - Zeros at end of kernel. Returns: `float [odd_kernel_sz1 x odd_kernel_sz2 x ... x odd_kernel_szN]`. `kernel` padded with zeros so each dimension is odd. Example: If `pad_location` is `'start'` then `[[5,4];[3,1]]` becomes `[[0,0,0],[0,5,4],[0,3,1]]`. \"\"\" even_dims = ( np . mod ( kernel . shape , 2 ) == 0 ) . astype ( int ) if max ( even_dims ) == 1 : if pad_location == 'start' : pad_dims = [ tuple ( np . array ([ 1 , 0 ]) * val ) for val in even_dims ] elif pad_location == 'end' : pad_dims = [ tuple ( np . array ([ 0 , 1 ]) * val ) for val in even_dims ] else : raise ValueError ( f \"pad_location has to be either 'start' or 'end' but value given was { pad_location } .\" ) return np . pad ( kernel , pad_dims , mode = 'constant' ) else : return kernel","title":"ensure_odd_kernel()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.ftrans2","text":"Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, b , using the transform, t . Copied from MATLAB ftrans2 . Parameters: Name Type Description Default b np . ndarray float [Q] . 1D convolve kernel. required t Optional [ np . ndarray ] float [M x N] . Transform to make b a 2D convolve kernel. If None , McClellan transform used. None Returns: Type Description np . ndarray float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1] . 2D convolve kernel. Source code in coppafish/utils/morphology/base.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def ftrans2 ( b : np . ndarray , t : Optional [ np . ndarray ] = None ) -> np . ndarray : \"\"\" Produces a 2D convolve kernel that corresponds to the 1D convolve kernel, `b`, using the transform, `t`. Copied from [MATLAB `ftrans2`](https://www.mathworks.com/help/images/ref/ftrans2.html). Args: b: `float [Q]`. 1D convolve kernel. t: `float [M x N]`. Transform to make `b` a 2D convolve kernel. If `None`, McClellan transform used. Returns: `float [(M-1)*(Q-1)/2+1 x (N-1)*(Q-1)/2+1]`. 2D convolve kernel. \"\"\" if t is None : # McClellan transformation t = np . array ([[ 1 , 2 , 1 ], [ 2 , - 4 , 2 ], [ 1 , 2 , 1 ]]) / 8 # Convert the 1-D convolve_2d b to SUM_n a(n) cos(wn) form n = int ( round (( len ( b ) - 1 ) / 2 )) b = b . reshape ( - 1 , 1 ) b = np . rot90 ( np . fft . fftshift ( np . rot90 ( b ))) a = np . concatenate (( b [: 1 ], 2 * b [ 1 : n + 1 ])) inset = np . floor (( np . array ( t . shape ) - 1 ) / 2 ) . astype ( int ) # Use Chebyshev polynomials to compute h p0 = 1 p1 = t h = a [ 1 ] * p1 rows = inset [ 0 ] cols = inset [ 1 ] h [ rows , cols ] += a [ 0 ] * p0 for i in range ( 2 , n + 1 ): p2 = 2 * scipy . signal . convolve2d ( t , p1 ) rows = rows + inset [ 0 ] cols = cols + inset [ 1 ] p2 [ rows , cols ] -= p0 rows = inset [ 0 ] + np . arange ( p1 . shape [ 0 ]) cols = ( inset [ 1 ] + np . arange ( p1 . shape [ 1 ])) . reshape ( - 1 , 1 ) hh = h . copy () h = a [ i ] * p2 h [ rows , cols ] += hh p0 = p1 . copy () p1 = p2 . copy () h = np . rot90 ( h ) return h","title":"ftrans2()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.hanning_diff","text":"Gets difference of two hanning window 2D convolve kernel. Central positive, outer negative with sum of 0 . Parameters: Name Type Description Default r1 int radius in pixels of central positive hanning convolve kernel. required r2 int radius in pixels of outer negative hanning convolve kernel. required Returns: Type Description np . ndarray float [2*r2+1 x 2*r2+1] . Difference of two hanning window 2D convolve kernel. Source code in coppafish/utils/morphology/base.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def hanning_diff ( r1 : int , r2 : int ) -> np . ndarray : \"\"\" Gets difference of two hanning window 2D convolve kernel. Central positive, outer negative with sum of `0`. Args: r1: radius in pixels of central positive hanning convolve kernel. r2: radius in pixels of outer negative hanning convolve kernel. Returns: `float [2*r2+1 x 2*r2+1]`. Difference of two hanning window 2D convolve kernel. \"\"\" if not 0 <= r1 <= r2 - 1 : raise errors . OutOfBoundsError ( \"r1\" , r1 , 0 , r2 - 1 ) if not r1 + 1 <= r2 <= np . inf : raise errors . OutOfBoundsError ( \"r2\" , r1 + 1 , np . inf ) h_outer = np . hanning ( 2 * r2 + 3 )[ 1 : - 1 ] # ignore zero values at first and last index h_outer = - h_outer / h_outer . sum () h_inner = np . hanning ( 2 * r1 + 3 )[ 1 : - 1 ] h_inner = h_inner / h_inner . sum () h = h_outer . copy () h [ r2 - r1 : r2 + r1 + 1 ] += h_inner h = ftrans2 ( h ) return h","title":"hanning_diff()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.base.top_hat","text":"Does tophat filtering of image with kernel . Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2] . Image to filter. required kernel np . ndarray np.uint8 [kernel_sz1 x kernel_sz2] . Top hat kernel containing only zeros or ones. required Returns: Type Description np . ndarray float [image_sz1 x image_sz2] . image after being top hat filtered with kernel . Source code in coppafish/utils/morphology/base.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def top_hat ( image : np . ndarray , kernel : np . ndarray ) -> np . ndarray : \"\"\" Does tophat filtering of `image` with `kernel`. Args: image: `float [image_sz1 x image_sz2]`. Image to filter. kernel: `np.uint8 [kernel_sz1 x kernel_sz2]`. Top hat `kernel` containing only zeros or ones. Returns: `float [image_sz1 x image_sz2]`. `image` after being top hat filtered with `kernel`. \"\"\" if kernel . dtype != np . uint8 : if sum ( np . unique ( kernel ) == [ 0 , 1 ]) == len ( np . unique ( kernel )): kernel = kernel . astype ( np . uint8 ) # kernel must be uint8 else : raise ValueError ( f 'kernel is of type { kernel . dtype } but must be of data type np.uint8.' ) image_dtype = image . dtype # so returned image is of same dtype as input if image . dtype == int : if image . min () >= 0 and image . max () <= np . iinfo ( np . uint16 ) . max : image = image . astype ( np . uint16 ) if not ( image . dtype == float or image . dtype == np . uint16 ): raise ValueError ( f 'image is of type { image . dtype } but must be of data type np.uint16 or float.' ) if np . max ( np . mod ( kernel . shape , 2 ) == 0 ): # With even kernel, gives different results to MATLAB raise ValueError ( f 'kernel dimensions are { kernel . shape } . Require all dimensions to be odd.' ) # kernel = ensure_odd_kernel(kernel) # doesn't work for tophat at start or end. return cv2 . morphologyEx ( image , cv2 . MORPH_TOPHAT , kernel ) . astype ( image_dtype )","title":"top_hat()"},{"location":"code/utils/morphology/#filter","text":"","title":"Filter"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter.imfilter","text":"Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Parameters: Name Type Description Default image np . ndarray float [image_sz1 x image_sz2 x ... x image_szN] . Image to be filtered. required kernel np . ndarray float [kernel_sz1 x kernel_sz2 x ... x kernel_szN] . Multidimensional filter. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. 'conv' - Performs multidimensional filtering using convolution. 'corr' oa bool Whether to use oaconvolve or scipy.ndimage.convolve. scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes). True Returns: Type Description np . ndarray float [image_sz1 x image_sz2 x ... x image_szN] . image after being filtered. Source code in coppafish/utils/morphology/filter.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def imfilter ( image : np . ndarray , kernel : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' , oa : bool = True ) -> np . ndarray : \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Args: image: `float [image_sz1 x image_sz2 x ... x image_szN]`. Image to be filtered. kernel: `float [kernel_sz1 x kernel_sz2 x ... x kernel_szN]`. Multidimensional filter. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. - `'conv'` - Performs multidimensional filtering using convolution. oa: Whether to use oaconvolve or scipy.ndimage.convolve. scipy.ndimage.convolve seems to be quicker for smoothing in extract step (3s vs 20s for 50 z-planes). Returns: `float [image_sz1 x image_sz2 x ... x image_szN]`. `image` after being filtered. \"\"\" if oa : if corr_or_conv == 'corr' : kernel = np . flip ( kernel ) elif corr_or_conv != 'conv' : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" ) kernel = ensure_odd_kernel ( kernel , 'end' ) if kernel . ndim < image . ndim : kernel = np . expand_dims ( kernel , axis = tuple ( np . arange ( kernel . ndim , image . ndim ))) pad_size = [( int (( ax_size - 1 ) / 2 ),) * 2 for ax_size in kernel . shape ] if isinstance ( padding , numbers . Number ): return oaconvolve ( np . pad ( image , pad_size , 'constant' , constant_values = padding ), kernel , 'valid' ) else : return oaconvolve ( np . pad ( image , pad_size , padding ), kernel , 'valid' ) else : if padding == 'symmetric' : padding = 'reflect' elif padding == 'edge' : padding = 'nearest' # Old method, about 3x slower for filtering large 3d image with small 3d kernel if isinstance ( padding , numbers . Number ): pad_value = padding padding = 'constant' else : pad_value = 0.0 # doesn't do anything for non-constant padding if corr_or_conv == 'corr' : kernel = ensure_odd_kernel ( kernel , 'start' ) return correlate ( image , kernel , mode = padding , cval = pad_value ) elif corr_or_conv == 'conv' : kernel = ensure_odd_kernel ( kernel , 'end' ) return convolve ( image , kernel , mode = padding , cval = pad_value ) else : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" )","title":"imfilter()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter.imfilter_coords","text":"Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Only finds result of filtering at specific locations but still filters entire image. Note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Parameters: Name Type Description Default image np . ndarray int [image_szY x image_szX (x image_szZ)] . Image to be filtered. Must be 2D or 3D. required kernel np . ndarray int [kernel_szY x kernel_szX (x kernel_szZ)] . Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. required coords np . ndarray int [n_points x image.ndims] . Coordinates where result of filtering is desired. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. 'conv' - Performs multidimensional filtering using convolution. 'corr' Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def imfilter_coords ( image : np . ndarray , kernel : np . ndarray , coords : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Only finds result of filtering at specific locations but still filters entire image. !!! note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Args: image: `int [image_szY x image_szX (x image_szZ)]`. Image to be filtered. Must be 2D or 3D. kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. coords: `int [n_points x image.ndims]`. Coordinates where result of filtering is desired. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. - `'conv'` - Performs multidimensional filtering using convolution. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" im_filt = imfilter ( image . astype ( int ), kernel , padding , corr_or_conv , oa = False ) return im_filt [ tuple ([ coords [:, j ] for j in range ( im_filt . ndim )])]","title":"imfilter_coords()"},{"location":"code/utils/morphology/#filter-optimised","text":"","title":"Filter Optimised"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.get_shifts_from_kernel","text":"Returns where kernel is positive as shifts in y, x and z. I.e. kernel=jnp.ones((3,3,3)) would return y_shifts = x_shifts = z_shifts = -1, 0, 1 . Parameters: Name Type Description Default kernel jnp . ndarray int [kernel_szY x kernel_szX x kernel_szY] required Returns: Type Description jnp . ndarray int [n_shifts] . y_shifts. jnp . ndarray int [n_shifts] . x_shifts. jnp . ndarray int [n_shifts] . z_shifts. Source code in coppafish/utils/morphology/filter_optimised.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def get_shifts_from_kernel ( kernel : jnp . ndarray ) -> Tuple [ jnp . ndarray , jnp . ndarray , jnp . ndarray ]: \"\"\" Returns where kernel is positive as shifts in y, x and z. I.e. `kernel=jnp.ones((3,3,3))` would return `y_shifts = x_shifts = z_shifts = -1, 0, 1`. Args: kernel: int [kernel_szY x kernel_szX x kernel_szY] Returns: - `int [n_shifts]`. y_shifts. - `int [n_shifts]`. x_shifts. - `int [n_shifts]`. z_shifts. \"\"\" shifts = list ( jnp . where ( kernel > 0 )) for i in range ( kernel . ndim ): shifts [ i ] = ( shifts [ i ] - ( kernel . shape [ i ] - 1 ) / 2 ) . astype ( int ) return tuple ( shifts )","title":"get_shifts_from_kernel()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.imfilter_coords","text":"Copy of MATLAB imfilter function with 'output_size' equal to 'same' . Only finds result of filtering at specific locations. Note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Parameters: Name Type Description Default image np . ndarray int [image_szY x image_szX (x image_szZ)] . Image to be filtered. Must be 2D or 3D. required kernel np . ndarray int [kernel_szY x kernel_szX (x kernel_szZ)] . Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. required coords np . ndarray int [n_points x image.ndims] . Coordinates where result of filtering is desired. required padding Union [ float , str ] One of the following, indicated which padding to be used. numeric scalar - Input array values outside the bounds of the array are assigned the value X . When no padding option is specified, the default is 0 . \u2018symmetric\u2019 - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. \u2018edge\u2019 - Input array values outside the bounds of the array are assumed to equal the nearest array border value. 'wrap' - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. 0 corr_or_conv str 'corr' - Performs multidimensional filtering using correlation. This is the default when no option specified. 'conv' - Performs multidimensional filtering using convolution. 'corr' Returns: Type Description Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]] int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter_optimised.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def imfilter_coords ( image : np . ndarray , kernel : np . ndarray , coords : np . ndarray , padding : Union [ float , str ] = 0 , corr_or_conv : str = 'corr' ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ]]: \"\"\" Copy of MATLAB `imfilter` function with `'output_size'` equal to `'same'`. Only finds result of filtering at specific locations. !!! note image and image2 need to be np.int8 and kernel needs to be int otherwise will get cython error. Args: image: `int [image_szY x image_szX (x image_szZ)]`. Image to be filtered. Must be 2D or 3D. kernel: `int [kernel_szY x kernel_szX (x kernel_szZ)]`. Multidimensional filter, expected to be binary i.e. only contains 0 and/or 1. coords: `int [n_points x image.ndims]`. Coordinates where result of filtering is desired. padding: One of the following, indicated which padding to be used. - numeric scalar - Input array values outside the bounds of the array are assigned the value `X`. When no padding option is specified, the default is `0`. - `\u2018symmetric\u2019` - Input array values outside the bounds of the array are computed by mirror-reflecting the array across the array border. - `\u2018edge\u2019`- Input array values outside the bounds of the array are assumed to equal the nearest array border value. - `'wrap'` - Input array values outside the bounds of the array are computed by implicitly assuming the input array is periodic. corr_or_conv: - `'corr'` - Performs multidimensional filtering using correlation. This is the default when no option specified. - `'conv'` - Performs multidimensional filtering using convolution. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" if corr_or_conv == 'corr' : kernel = np . flip ( kernel ) elif corr_or_conv != 'conv' : raise ValueError ( f \"corr_or_conv should be either 'corr' or 'conv' but given value is { corr_or_conv } \" ) kernel = ensure_odd_kernel ( kernel , 'end' ) # Ensure shape of image and kernel correct if image . ndim != coords . shape [ 1 ]: raise ValueError ( f \"Image has { image . ndim } dimensions but coords only have { coords . shape [ 1 ] } dimensions.\" ) if image . ndim == 2 : image = np . expand_dims ( image , 2 ) elif image . ndim != 3 : raise ValueError ( f \"image must have 2 or 3 dimensions but given image has { image . ndim } .\" ) if kernel . ndim == 2 : kernel = np . expand_dims ( kernel , 2 ) elif kernel . ndim != 3 : raise ValueError ( f \"kernel must have 2 or 3 dimensions but given image has { image . ndim } .\" ) if kernel . max () > 1 : raise ValueError ( f 'kernel is expected to be binary, only containing 0 or 1 but kernel.max = { kernel . max () } ' ) if coords . shape [ 1 ] == 2 : # set all z coordinates to 0 if 2D. coords = np . append ( coords , np . zeros (( coords . shape [ 0 ], 1 ), dtype = int ), axis = 1 ) if ( coords . max ( axis = 0 ) >= np . array ( image . shape )) . any (): raise ValueError ( f \"Max yxz coordinates provided are { coords . max ( axis = 0 ) } but image has shape { image . shape } .\" ) pad_size = [( int (( ax_size - 1 ) / 2 ),) * 2 for ax_size in kernel . shape ] pad_coords = jnp . asarray ( coords ) + jnp . array ([ val [ 0 ] for val in pad_size ]) if isinstance ( padding , numbers . Number ): image_pad = jnp . pad ( jnp . asarray ( image ), pad_size , 'constant' , constant_values = padding ) . astype ( int ) else : image_pad = jnp . pad ( jnp . asarray ( image ), pad_size , padding ) . astype ( int ) y_shifts , x_shifts , z_shifts = get_shifts_from_kernel ( jnp . asarray ( np . flip ( kernel ))) return np . asarray ( manual_convolve ( image_pad , y_shifts , x_shifts , z_shifts , pad_coords ))","title":"imfilter_coords()"},{"location":"code/utils/morphology/#coppafish.utils.morphology.filter_optimised.manual_convolve","text":"Finds result of convolution at specific locations indicated by coords with binary kernel. I.e. instead of convolving whole image , just find result at these points . Note image needs to be padded before this function is called otherwise get an error when go out of bounds. Parameters: Name Type Description Default image jnp . ndarray int [image_szY x image_szX x image_szZ] . Image to be filtered. Must be 3D. required y_kernel_shifts jnp . ndarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required x_kernel_shifts jnp . asarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required z_kernel_shifts jnp . ndarray int [n_nonzero_kernel] Shifts indicating where kernel equals 1. I.e. if kernel = np.ones((3,3)) then y_shift = x_shift = z_shift = [-1, 0, 1] . required coords jnp . ndarray int [n_points x 3] . yxz coordinates where result of filtering is desired. required Returns: Type Description jnp . ndarray int [n_points] . Result of filtering of image at each point in coords . Source code in coppafish/utils/morphology/filter_optimised.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 @jax . jit def manual_convolve ( image : jnp . ndarray , y_kernel_shifts : jnp . ndarray , x_kernel_shifts : jnp . asarray , z_kernel_shifts : jnp . ndarray , coords : jnp . ndarray ) -> jnp . ndarray : \"\"\" Finds result of convolution at specific locations indicated by `coords` with binary kernel. I.e. instead of convolving whole `image`, just find result at these `points`. !!! note image needs to be padded before this function is called otherwise get an error when go out of bounds. Args: image: `int [image_szY x image_szX x image_szZ]`. Image to be filtered. Must be 3D. y_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. x_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. z_kernel_shifts: `int [n_nonzero_kernel]` Shifts indicating where kernel equals 1. I.e. if `kernel = np.ones((3,3))` then `y_shift = x_shift = z_shift = [-1, 0, 1]`. coords: `int [n_points x 3]`. yxz coordinates where result of filtering is desired. Returns: `int [n_points]`. Result of filtering of `image` at each point in `coords`. \"\"\" return jax . vmap ( manual_convolve_single , in_axes = ( None , None , None , None , 0 ), out_axes = 0 )( image , y_kernel_shifts , x_kernel_shifts , z_kernel_shifts , coords )","title":"manual_convolve()"},{"location":"code/utils/nd2/","text":"get_image ( images , fov , channel , use_z = None ) Using dask array from nd2 file, this loads the image of the desired fov and channel. Parameters: Name Type Description Default images np . ndarray Dask array with fov , channel , y, x, z as index order. required fov int fov index of desired image required channel int channel of desired image required use_z Optional [ List [ int ]] int [n_use_z] . Which z-planes of image to load. If None , will load all z-planes. None Returns: Type Description np . ndarray uint16 [im_sz_y x im_sz_x x n_use_z] . Image of the desired fov and channel . Source code in coppafish/utils/nd2.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_image ( images : np . ndarray , fov : int , channel : int , use_z : Optional [ List [ int ]] = None ) -> np . ndarray : \"\"\" Using dask array from nd2 file, this loads the image of the desired fov and channel. Args: images: Dask array with `fov`, `channel`, y, x, z as index order. fov: `fov` index of desired image channel: `channel` of desired image use_z: `int [n_use_z]`. Which z-planes of image to load. If `None`, will load all z-planes. Returns: `uint16 [im_sz_y x im_sz_x x n_use_z]`. Image of the desired `fov` and `channel`. \"\"\" if use_z is None : use_z = np . arange ( images . shape [ - 1 ]) return np . asarray ( images [ fov , channel , :, :, use_z ]) get_metadata ( file_path ) Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of tiles/channels/z-planes. Parameters: Name Type Description Default file_path str Path to desired nd2 file. required Returns: Type Description dict Dictionary containing - dict xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. dict pixel_microns - float . xy pixel size in microns. dict pixel_microns_z - float . z pixel size in microns. dict sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Source code in coppafish/utils/nd2.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def get_metadata ( file_path : str ) -> dict : \"\"\" Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of tiles/channels/z-planes. Args: file_path: Path to desired nd2 file. Returns: Dictionary containing - - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels. - `pixel_microns` - `float`. xy pixel size in microns. - `pixel_microns_z` - `float`. z pixel size in microns. - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions. \"\"\" if not os . path . isfile ( file_path ): raise errors . NoFileError ( file_path ) images = nd2 . ND2File ( file_path ) metadata = { 'sizes' : { 't' : images . sizes [ 'P' ], 'c' : images . sizes [ 'C' ], 'y' : images . sizes [ 'Y' ], 'x' : images . sizes [ 'X' ], 'z' : images . sizes [ 'Z' ]}, 'pixel_microns' : images . metadata . channels [ 0 ] . volume . axesCalibration [ 0 ], 'pixel_microns_z' : images . metadata . channels [ 0 ] . volume . axesCalibration [ 2 ]} xy_pos = np . array ([ images . experiment [ 0 ] . parameters . points [ i ] . stagePositionUm [: 2 ] for i in range ( images . sizes [ 'P' ])]) metadata [ 'xy_pos' ] = ( xy_pos - np . min ( xy_pos , 0 )) / metadata [ 'pixel_microns' ] metadata [ 'xy_pos' ] = metadata [ 'xy_pos' ] . tolist () return metadata get_nd2_tile_ind ( tile_ind_npy , tile_pos_yx_nd2 , tile_pos_yx_npy ) Gets index of tiles in nd2 file from tile index of npy file. Parameters: Name Type Description Default tile_ind_npy Union [ int , List [ int ]] Indices of tiles in npy file required tile_pos_yx_nd2 np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with nd2 index i . Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . required tile_pos_yx_npy np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with npy index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . required Returns: Type Description Union [ int , List [ int ]] Corresponding indices in nd2 file Source code in coppafish/utils/nd2.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_nd2_tile_ind ( tile_ind_npy : Union [ int , List [ int ]], tile_pos_yx_nd2 : np . ndarray , tile_pos_yx_npy : np . ndarray ) -> Union [ int , List [ int ]]: \"\"\" Gets index of tiles in nd2 file from tile index of npy file. Args: tile_ind_npy: Indices of tiles in npy file tile_pos_yx_nd2: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with nd2 index ```i```. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. tile_pos_yx_npy: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with npy index ```i```. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. Returns: Corresponding indices in nd2 file \"\"\" if isinstance ( tile_ind_npy , numbers . Number ): tile_ind_npy = [ tile_ind_npy ] nd2_index = numpy_indexed . indices ( tile_pos_yx_nd2 , tile_pos_yx_npy [ tile_ind_npy ]) . tolist () if len ( nd2_index ) == 1 : return nd2_index [ 0 ] else : return nd2_index load ( file_path ) Returns dask array with indices in order fov , channel , y , x , z . Parameters: Name Type Description Default file_path str Path to desired nd2 file. required Returns: Type Description np . ndarray Dask array indices in order fov , channel , y , x , z . Source code in coppafish/utils/nd2.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def load ( file_path : str ) -> np . ndarray : \"\"\" Returns dask array with indices in order `fov`, `channel`, `y`, `x`, `z`. Args: file_path: Path to desired nd2 file. Returns: Dask array indices in order `fov`, `channel`, `y`, `x`, `z`. \"\"\" if not os . path . isfile ( file_path ): raise errors . NoFileError ( file_path ) images = nd2 . ND2File ( file_path ) images = images . to_dask () # images = nd2.imread(file_name, dask=True) # get python crashing with this in get_image for some reason images = np . moveaxis ( images , 1 , - 1 ) # put z index to end return images save_metadata ( json_file , nd2_file , use_channels = None ) Saves the required metadata as a json file which will contain xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. pixel_microns - float . xy pixel size in microns. pixel_microns_z - float . z pixel size in microns. sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Parameters: Name Type Description Default json_file str Where to save json file required nd2_file str Path to nd2 file required use_channels Optional [ List ] The channels which have been extracted from the nd2 file. If None , assume all channels in nd2 file used None Source code in coppafish/utils/nd2.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def save_metadata ( json_file : str , nd2_file : str , use_channels : Optional [ List ] = None ): \"\"\" Saves the required metadata as a json file which will contain - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels. - `pixel_microns` - `float`. xy pixel size in microns. - `pixel_microns_z` - `float`. z pixel size in microns. - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions. Args: json_file: Where to save json file nd2_file: Path to nd2 file use_channels: The channels which have been extracted from the nd2 file. If `None`, assume all channels in nd2 file used \"\"\" metadata = get_metadata ( nd2_file ) if use_channels is not None : if len ( use_channels ) > metadata [ 'sizes' ][ 'c' ]: raise ValueError ( f \"use_channels contains { len ( use_channels ) } channels but there \" f \"are only { metadata [ 'sizes' ][ 'c' ] } channels in the nd2 metadata.\" ) metadata [ 'sizes' ][ 'c' ] = len ( use_channels ) metadata [ 'use_channels' ] = use_channels # channels extracted from nd2 file json . dump ( metadata , open ( json_file , 'w' ))","title":"nd2"},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_image","text":"Using dask array from nd2 file, this loads the image of the desired fov and channel. Parameters: Name Type Description Default images np . ndarray Dask array with fov , channel , y, x, z as index order. required fov int fov index of desired image required channel int channel of desired image required use_z Optional [ List [ int ]] int [n_use_z] . Which z-planes of image to load. If None , will load all z-planes. None Returns: Type Description np . ndarray uint16 [im_sz_y x im_sz_x x n_use_z] . Image of the desired fov and channel . Source code in coppafish/utils/nd2.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def get_image ( images : np . ndarray , fov : int , channel : int , use_z : Optional [ List [ int ]] = None ) -> np . ndarray : \"\"\" Using dask array from nd2 file, this loads the image of the desired fov and channel. Args: images: Dask array with `fov`, `channel`, y, x, z as index order. fov: `fov` index of desired image channel: `channel` of desired image use_z: `int [n_use_z]`. Which z-planes of image to load. If `None`, will load all z-planes. Returns: `uint16 [im_sz_y x im_sz_x x n_use_z]`. Image of the desired `fov` and `channel`. \"\"\" if use_z is None : use_z = np . arange ( images . shape [ - 1 ]) return np . asarray ( images [ fov , channel , :, :, use_z ])","title":"get_image()"},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_metadata","text":"Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of tiles/channels/z-planes. Parameters: Name Type Description Default file_path str Path to desired nd2 file. required Returns: Type Description dict Dictionary containing - dict xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. dict pixel_microns - float . xy pixel size in microns. dict pixel_microns_z - float . z pixel size in microns. dict sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Source code in coppafish/utils/nd2.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def get_metadata ( file_path : str ) -> dict : \"\"\" Gets metadata containing information from nd2 data about pixel sizes, position of tiles and numbers of tiles/channels/z-planes. Args: file_path: Path to desired nd2 file. Returns: Dictionary containing - - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels. - `pixel_microns` - `float`. xy pixel size in microns. - `pixel_microns_z` - `float`. z pixel size in microns. - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions. \"\"\" if not os . path . isfile ( file_path ): raise errors . NoFileError ( file_path ) images = nd2 . ND2File ( file_path ) metadata = { 'sizes' : { 't' : images . sizes [ 'P' ], 'c' : images . sizes [ 'C' ], 'y' : images . sizes [ 'Y' ], 'x' : images . sizes [ 'X' ], 'z' : images . sizes [ 'Z' ]}, 'pixel_microns' : images . metadata . channels [ 0 ] . volume . axesCalibration [ 0 ], 'pixel_microns_z' : images . metadata . channels [ 0 ] . volume . axesCalibration [ 2 ]} xy_pos = np . array ([ images . experiment [ 0 ] . parameters . points [ i ] . stagePositionUm [: 2 ] for i in range ( images . sizes [ 'P' ])]) metadata [ 'xy_pos' ] = ( xy_pos - np . min ( xy_pos , 0 )) / metadata [ 'pixel_microns' ] metadata [ 'xy_pos' ] = metadata [ 'xy_pos' ] . tolist () return metadata","title":"get_metadata()"},{"location":"code/utils/nd2/#coppafish.utils.nd2.get_nd2_tile_ind","text":"Gets index of tiles in nd2 file from tile index of npy file. Parameters: Name Type Description Default tile_ind_npy Union [ int , List [ int ]] Indices of tiles in npy file required tile_pos_yx_nd2 np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with nd2 index i . Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . required tile_pos_yx_npy np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with npy index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . required Returns: Type Description Union [ int , List [ int ]] Corresponding indices in nd2 file Source code in coppafish/utils/nd2.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def get_nd2_tile_ind ( tile_ind_npy : Union [ int , List [ int ]], tile_pos_yx_nd2 : np . ndarray , tile_pos_yx_npy : np . ndarray ) -> Union [ int , List [ int ]]: \"\"\" Gets index of tiles in nd2 file from tile index of npy file. Args: tile_ind_npy: Indices of tiles in npy file tile_pos_yx_nd2: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with nd2 index ```i```. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. tile_pos_yx_npy: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with npy index ```i```. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. Returns: Corresponding indices in nd2 file \"\"\" if isinstance ( tile_ind_npy , numbers . Number ): tile_ind_npy = [ tile_ind_npy ] nd2_index = numpy_indexed . indices ( tile_pos_yx_nd2 , tile_pos_yx_npy [ tile_ind_npy ]) . tolist () if len ( nd2_index ) == 1 : return nd2_index [ 0 ] else : return nd2_index","title":"get_nd2_tile_ind()"},{"location":"code/utils/nd2/#coppafish.utils.nd2.load","text":"Returns dask array with indices in order fov , channel , y , x , z . Parameters: Name Type Description Default file_path str Path to desired nd2 file. required Returns: Type Description np . ndarray Dask array indices in order fov , channel , y , x , z . Source code in coppafish/utils/nd2.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def load ( file_path : str ) -> np . ndarray : \"\"\" Returns dask array with indices in order `fov`, `channel`, `y`, `x`, `z`. Args: file_path: Path to desired nd2 file. Returns: Dask array indices in order `fov`, `channel`, `y`, `x`, `z`. \"\"\" if not os . path . isfile ( file_path ): raise errors . NoFileError ( file_path ) images = nd2 . ND2File ( file_path ) images = images . to_dask () # images = nd2.imread(file_name, dask=True) # get python crashing with this in get_image for some reason images = np . moveaxis ( images , 1 , - 1 ) # put z index to end return images","title":"load()"},{"location":"code/utils/nd2/#coppafish.utils.nd2.save_metadata","text":"Saves the required metadata as a json file which will contain xy_pos - List [n_tiles x 2] . xy position of tiles in pixels. pixel_microns - float . xy pixel size in microns. pixel_microns_z - float . z pixel size in microns. sizes - dict with fov ( t ), channels ( c ), y, x, z-planes ( z ) dimensions. Parameters: Name Type Description Default json_file str Where to save json file required nd2_file str Path to nd2 file required use_channels Optional [ List ] The channels which have been extracted from the nd2 file. If None , assume all channels in nd2 file used None Source code in coppafish/utils/nd2.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def save_metadata ( json_file : str , nd2_file : str , use_channels : Optional [ List ] = None ): \"\"\" Saves the required metadata as a json file which will contain - `xy_pos` - `List [n_tiles x 2]`. xy position of tiles in pixels. - `pixel_microns` - `float`. xy pixel size in microns. - `pixel_microns_z` - `float`. z pixel size in microns. - `sizes` - dict with fov (`t`), channels (`c`), y, x, z-planes (`z`) dimensions. Args: json_file: Where to save json file nd2_file: Path to nd2 file use_channels: The channels which have been extracted from the nd2 file. If `None`, assume all channels in nd2 file used \"\"\" metadata = get_metadata ( nd2_file ) if use_channels is not None : if len ( use_channels ) > metadata [ 'sizes' ][ 'c' ]: raise ValueError ( f \"use_channels contains { len ( use_channels ) } channels but there \" f \"are only { metadata [ 'sizes' ][ 'c' ] } channels in the nd2 metadata.\" ) metadata [ 'sizes' ][ 'c' ] = len ( use_channels ) metadata [ 'use_channels' ] = use_channels # channels extracted from nd2 file json . dump ( metadata , open ( json_file , 'w' ))","title":"save_metadata()"},{"location":"code/utils/npy/","text":"get_npy_tile_ind ( tile_ind_nd2 , tile_pos_yx_nd2 , tile_pos_yx_npy ) Gets index of tile in npy file from tile index of nd2 file. Parameters: Name Type Description Default tile_ind_nd2 Union [ int , List [ int ]] Index of tile in nd2 file required tile_pos_yx_nd2 np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with nd2 index i . Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . required tile_pos_yx_npy np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with npy index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . required Returns: Type Description Union [ int , List [ int ]] Corresponding indices in npy file Source code in coppafish/utils/npy.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_npy_tile_ind ( tile_ind_nd2 : Union [ int , List [ int ]], tile_pos_yx_nd2 : np . ndarray , tile_pos_yx_npy : np . ndarray ) -> Union [ int , List [ int ]]: \"\"\" Gets index of tile in npy file from tile index of nd2 file. Args: tile_ind_nd2: Index of tile in nd2 file tile_pos_yx_nd2: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with nd2 index ```i```. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. tile_pos_yx_npy: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with npy index ```i```. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. Returns: Corresponding indices in npy file \"\"\" if isinstance ( tile_ind_nd2 , numbers . Number ): tile_ind_nd2 = [ tile_ind_nd2 ] npy_index = numpy_indexed . indices ( tile_pos_yx_npy , tile_pos_yx_nd2 [ tile_ind_nd2 ]) . tolist () if len ( npy_index ) == 1 : return npy_index [ 0 ] else : return npy_index load_tile ( nbp_file , nbp_basic , t , r , c , yxz = None , apply_shift = True ) Loads in image corresponding to desired tile, round and channel from the relavent npy file. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required t int npy tile index considering required r int Round considering required c int Channel considering required yxz Optional [ Union [ List , Tuple , np . ndarray , jnp . ndarray ]] If None , whole image is loaded otherwise there are two choices: int [2 or 3] . List containing y,x,z coordinates of sub image to load in. E.g. if yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])] returned image will have shape [1 x 3 x 2] . if yxz = [None, None, z_planes] , all pixels on given z_planes will be returned i.e. shape of image will be [tile_sz x tile_sz x n_z_planes] . int [n_pixels x (2 or 3)] . Array containing yxz coordinates for which the pixel value is desired. E.g. if yxz = np.ones((10,3)) , returned image will have shape [10,] with all values indicating the pixel value at [1,1,1] . None apply_shift bool If True , dtype will be int32 otherwise dtype will be uint16 with the pixels values shifted by +nbp_basic.tile_pixel_value_shift . May want to disable apply_shift to save memory and/or make loading quicker as there will be no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift. True Returns: Type Description np . ndarray int32 [ny x nx (x nz)] or int32 [n_pixels x (2 or 3)] Loaded image. Source code in coppafish/utils/npy.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def load_tile ( nbp_file : NotebookPage , nbp_basic : NotebookPage , t : int , r : int , c : int , yxz : Optional [ Union [ List , Tuple , np . ndarray , jnp . ndarray ]] = None , apply_shift : bool = True ) -> np . ndarray : \"\"\" Loads in image corresponding to desired tile, round and channel from the relavent npy file. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page t: npy tile index considering r: Round considering c: Channel considering yxz: If `None`, whole image is loaded otherwise there are two choices: - `int [2 or 3]`. List containing y,x,z coordinates of sub image to load in. E.g. if `yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])]` returned `image` will have shape `[1 x 3 x 2]`. if `yxz = [None, None, z_planes]`, all pixels on given z_planes will be returned i.e. shape of image will be `[tile_sz x tile_sz x n_z_planes]`. - `int [n_pixels x (2 or 3)]`. Array containing yxz coordinates for which the pixel value is desired. E.g. if `yxz = np.ones((10,3))`, returned `image` will have shape `[10,]` with all values indicating the pixel value at `[1,1,1]`. apply_shift: If `True`, dtype will be `int32` otherwise dtype will be `uint16` with the pixels values shifted by `+nbp_basic.tile_pixel_value_shift`. May want to disable `apply_shift` to save memory and/or make loading quicker as there will be no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift. Returns: `int32 [ny x nx (x nz)]` or `int32 [n_pixels x (2 or 3)]` Loaded image. \"\"\" if yxz is not None : # Use mmap when only loading in part of image if isinstance ( yxz , ( list , tuple )): if nbp_basic . is_3d : if len ( yxz ) != 3 : raise ValueError ( f 'Loading in a 3D tile but dimension of coordinates given is { len ( yxz ) } .' ) if yxz [ 0 ] is None and yxz [ 1 ] is None : image = np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' )[ yxz [ 2 ]] if image . ndim == 3 : image = np . moveaxis ( image , 0 , 2 ) else : coord_index = np . ix_ ( yxz [ 0 ], yxz [ 1 ], yxz [ 2 ]) image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ), 0 , 2 )[ coord_index ] else : if len ( yxz ) != 2 : raise ValueError ( f 'Loading in a 2D tile but dimension of coordinates given is { len ( yxz ) } .' ) coord_index = np . ix_ ( np . array ([ c ]), yxz [ 0 ], yxz [ 1 ]) # add channel as first coordinate in 2D. # [0] below is to remove channel index of length 1. image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ coord_index ][ 0 ] elif isinstance ( yxz , ( np . ndarray , jnp . ndarray )): if nbp_basic . is_3d : if yxz . shape [ 1 ] != 3 : raise ValueError ( f 'Loading in a 3D tile but dimension of coordinates given is { yxz . shape [ 1 ] } .' ) coord_index = tuple ( np . asarray ( yxz [:, i ]) for i in range ( 3 )) image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ), 0 , 2 )[ coord_index ] else : if yxz . shape [ 1 ] != 2 : raise ValueError ( f 'Loading in a 2D tile but dimension of coordinates given is { yxz . shape [ 1 ] } .' ) coord_index = tuple ( np . asarray ( yxz [:, i ]) for i in range ( 2 )) coord_index = ( np . full ( yxz . shape [ 0 ], c , int ),) + coord_index # add channel as first coordinate in 2D. image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ coord_index ] else : raise ValueError ( f 'yxz should either be an [n_spots x n_dim] array to return an n_spots array indicating ' f 'the value of the image at these coordinates or \\n ' f 'a list containing { 2 + int ( nbp_basic . is_3d ) } arrays indicating the sub image to load.' ) else : if nbp_basic . is_3d : # Don't use mmap when loading in whole image image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ]), 0 , 2 ) else : # Use mmap when only loading in part of image image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ c ] if apply_shift and not ( r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel ): image = image . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift return image save_stitched ( im_file , nbp_file , nbp_basic , tile_origin , r , c , from_raw = False , zero_thresh = 0 ) Stitches together all tiles from round r , channel c and saves the resultant compressed npz at im_file . Saved image will be uint16 if from nd2 or from DAPI filtered npy files. Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range. Parameters: Name Type Description Default im_file Optional [ str ] Path to save file. If None , stitched image is returned (with z axis last) instead of saved. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required tile_origin np . ndarray float [n_tiles x 3] . yxz origin of each tile on round r . required r int save_stitched will save stitched image of all tiles of round r , channel c . required c int save_stitched will save stitched image of all tiles of round r , channel c . required from_raw bool If False , will stitch together tiles from saved npy files, otherwise will load in raw un-filtered images from nd2/npy file. False zero_thresh int All pixels with absolute value less than or equal to zero_thresh will be set to 0. The larger it is, the smaller the compressed file will be. save: If True, saves image as im_file, otherwise returns image 0 Source code in coppafish/utils/npy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def save_stitched ( im_file : Optional [ str ], nbp_file : NotebookPage , nbp_basic : NotebookPage , tile_origin : np . ndarray , r : int , c : int , from_raw : bool = False , zero_thresh : int = 0 ): \"\"\" Stitches together all tiles from round `r`, channel `c` and saves the resultant compressed npz at `im_file`. Saved image will be uint16 if from nd2 or from DAPI filtered npy files. Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range. Args: im_file: Path to save file. If `None`, stitched `image` is returned (with z axis last) instead of saved. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page tile_origin: `float [n_tiles x 3]`. yxz origin of each tile on round `r`. r: save_stitched will save stitched image of all tiles of round `r`, channel `c`. c: save_stitched will save stitched image of all tiles of round `r`, channel `c`. from_raw: If `False`, will stitch together tiles from saved npy files, otherwise will load in raw un-filtered images from nd2/npy file. zero_thresh: All pixels with absolute value less than or equal to `zero_thresh` will be set to 0. The larger it is, the smaller the compressed file will be.\\ save: If True, saves image as im_file, otherwise returns image \"\"\" yx_origin = np . round ( tile_origin [:, : 2 ]) . astype ( int ) z_origin = np . round ( tile_origin [:, 2 ]) . astype ( int ) . flatten () yx_size = np . max ( yx_origin , axis = 0 ) + nbp_basic . tile_sz if nbp_basic . is_3d : z_size = z_origin . max () + nbp_basic . nz stitched_image = np . zeros ( np . append ( z_size , yx_size ), dtype = np . uint16 ) else : z_size = 1 stitched_image = np . zeros ( yx_size , dtype = np . uint16 ) if from_raw : round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) shift = 0 # if from nd2 file, data type is already un-shifted uint16 else : if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : shift = 0 # if filtered dapi, data type is already un-shifted uint16 else : # if from filtered npy files, data type is shifted uint16, want to save stitched as un-shifted int16. shift = nbp_basic . tile_pixel_value_shift if shift != 0 : # change dtype to accommodate negative values and set base value to be zero in the shifted image. stitched_image = stitched_image . astype ( np . int32 ) + shift with tqdm ( total = z_size * len ( nbp_basic . use_tiles )) as pbar : for t in nbp_basic . use_tiles : if from_raw : image_t = utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , c , nbp_basic . use_z ) # replicate non-filtering procedure in extract_and_filter if not nbp_basic . is_3d : image_t = extract . focus_stack ( image_t ) image_t , bad_columns = extract . strip_hack ( image_t ) # find faulty columns image_t [:, bad_columns ] = 0 if nbp_basic . is_3d : image_t = np . moveaxis ( image_t , 2 , 0 ) # put z-axis back to the start else : if nbp_basic . is_3d : image_t = np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ) else : image_t = load_tile ( nbp_file , nbp_basic , t , r , c , apply_shift = False ) for z in range ( z_size ): # any tiles not used will be kept as 0. pbar . set_postfix ({ 'tile' : t , 'z' : z }) if nbp_basic . is_3d : file_z = z - z_origin [ t ] if file_z < 0 or file_z >= nbp_basic . nz : # Set tile to 0 if currently outside its area local_image = np . zeros (( nbp_basic . tile_sz , nbp_basic . tile_sz )) else : local_image = image_t [ file_z ] stitched_image [ z , yx_origin [ t , 0 ]: yx_origin [ t , 0 ] + nbp_basic . tile_sz , yx_origin [ t , 1 ]: yx_origin [ t , 1 ] + nbp_basic . tile_sz ] = local_image else : stitched_image [ yx_origin [ t , 0 ]: yx_origin [ t , 0 ] + nbp_basic . tile_sz , yx_origin [ t , 1 ]: yx_origin [ t , 1 ] + nbp_basic . tile_sz ] = image_t pbar . update ( 1 ) pbar . close () if shift != 0 : # remove shift and re-scale so fits the whole int16 range stitched_image = stitched_image - shift stitched_image = stitched_image * np . iinfo ( np . int16 ) . max / np . abs ( stitched_image ) . max () stitched_image = np . rint ( stitched_image , np . zeros_like ( stitched_image , dtype = np . int16 ), casting = 'unsafe' ) if zero_thresh > 0 : stitched_image [ np . abs ( stitched_image ) <= zero_thresh ] = 0 if im_file is None : if z_size > 1 : stitched_image = np . moveaxis ( stitched_image , 0 , - 1 ) return stitched_image else : np . savez_compressed ( im_file , stitched_image ) save_tile ( nbp_file , nbp_basic , image , t , r , c = None ) Wrapper function to save tiles as npy files with correct shift. Moves z-axis to start before saving as it is quicker to load in this order. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required image np . ndarray int32 [ny x nx x nz] or int32 [n_channels x ny x nx] . Image to save. required t int npy tile index considering required r int Round considering required c Optional [ int ] Channel considering None Source code in coppafish/utils/npy.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def save_tile ( nbp_file : NotebookPage , nbp_basic : NotebookPage , image : np . ndarray , t : int , r : int , c : Optional [ int ] = None ): \"\"\" Wrapper function to save tiles as npy files with correct shift. Moves z-axis to start before saving as it is quicker to load in this order. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page image: `int32 [ny x nx x nz]` or `int32 [n_channels x ny x nx]`. Image to save. t: npy tile index considering r: Round considering c: Channel considering \"\"\" if nbp_basic . is_3d : if c is None : raise ValueError ( '3d image but channel not given.' ) if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : # If dapi is given then image should already by uint16 so no clipping image = image . astype ( np . uint16 ) else : # need to shift and clip image so fits into uint16 dtype. # clip at 1 not 0 because 0 (or -tile_pixel_value_shift) # will be used as an invalid value when reading in spot_colors. image = np . clip ( image + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , np . zeros_like ( image , dtype = np . uint16 ), casting = \"unsafe\" ) # In 3D, cannot possibly save any un-used channel hence no exception for this case. expected_shape = ( nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ) if not utils . errors . check_shape ( image , expected_shape ): raise utils . errors . ShapeError ( \"tile to be saved\" , image . shape , expected_shape ) np . save ( nbp_file . tile [ t ][ r ][ c ], np . moveaxis ( image , 2 , 0 )) else : if r == nbp_basic . anchor_round : if nbp_basic . anchor_channel is not None : # If anchor round, only shift and clip anchor channel, leave DAPI and un-used channels alone. image [ nbp_basic . anchor_channel ] = \\ np . clip ( image [ nbp_basic . anchor_channel ] + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , image [ nbp_basic . anchor_channel ]) image = image . astype ( np . uint16 ) use_channels = [ val for val in [ nbp_basic . dapi_channel , nbp_basic . anchor_channel ] if val is not None ] else : image = np . clip ( image + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , np . zeros_like ( image , dtype = np . uint16 ), casting = \"unsafe\" ) use_channels = nbp_basic . use_channels # set un-used channels to be 0, not clipped to 1. image [ np . setdiff1d ( np . arange ( nbp_basic . n_channels ), use_channels )] = 0 expected_shape = ( nbp_basic . n_channels , nbp_basic . tile_sz , nbp_basic . tile_sz ) if not utils . errors . check_shape ( image , expected_shape ): raise utils . errors . ShapeError ( \"tile to be saved\" , image . shape , expected_shape ) np . save ( nbp_file . tile [ t ][ r ], image )","title":"npy"},{"location":"code/utils/npy/#coppafish.utils.npy.get_npy_tile_ind","text":"Gets index of tile in npy file from tile index of nd2 file. Parameters: Name Type Description Default tile_ind_nd2 Union [ int , List [ int ]] Index of tile in nd2 file required tile_pos_yx_nd2 np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with nd2 index i . Index 0 refers to YX = [0, 0] . Index 1 refers to YX = [0, 1] if MaxX > 0 . required tile_pos_yx_npy np . ndarray int [n_tiles x 2] . [i,:] contains YX position of tile with npy index i . Index 0 refers to YX = [MaxY, MaxX] . Index 1 refers to YX = [MaxY, MaxX - 1] if MaxX > 0 . required Returns: Type Description Union [ int , List [ int ]] Corresponding indices in npy file Source code in coppafish/utils/npy.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_npy_tile_ind ( tile_ind_nd2 : Union [ int , List [ int ]], tile_pos_yx_nd2 : np . ndarray , tile_pos_yx_npy : np . ndarray ) -> Union [ int , List [ int ]]: \"\"\" Gets index of tile in npy file from tile index of nd2 file. Args: tile_ind_nd2: Index of tile in nd2 file tile_pos_yx_nd2: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with nd2 index ```i```. Index 0 refers to ```YX = [0, 0]```. Index 1 refers to ```YX = [0, 1] if MaxX > 0```. tile_pos_yx_npy: ```int [n_tiles x 2]```. ```[i,:]``` contains YX position of tile with npy index ```i```. Index 0 refers to ```YX = [MaxY, MaxX]```. Index 1 refers to ```YX = [MaxY, MaxX - 1] if MaxX > 0```. Returns: Corresponding indices in npy file \"\"\" if isinstance ( tile_ind_nd2 , numbers . Number ): tile_ind_nd2 = [ tile_ind_nd2 ] npy_index = numpy_indexed . indices ( tile_pos_yx_npy , tile_pos_yx_nd2 [ tile_ind_nd2 ]) . tolist () if len ( npy_index ) == 1 : return npy_index [ 0 ] else : return npy_index","title":"get_npy_tile_ind()"},{"location":"code/utils/npy/#coppafish.utils.npy.load_tile","text":"Loads in image corresponding to desired tile, round and channel from the relavent npy file. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required t int npy tile index considering required r int Round considering required c int Channel considering required yxz Optional [ Union [ List , Tuple , np . ndarray , jnp . ndarray ]] If None , whole image is loaded otherwise there are two choices: int [2 or 3] . List containing y,x,z coordinates of sub image to load in. E.g. if yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])] returned image will have shape [1 x 3 x 2] . if yxz = [None, None, z_planes] , all pixels on given z_planes will be returned i.e. shape of image will be [tile_sz x tile_sz x n_z_planes] . int [n_pixels x (2 or 3)] . Array containing yxz coordinates for which the pixel value is desired. E.g. if yxz = np.ones((10,3)) , returned image will have shape [10,] with all values indicating the pixel value at [1,1,1] . None apply_shift bool If True , dtype will be int32 otherwise dtype will be uint16 with the pixels values shifted by +nbp_basic.tile_pixel_value_shift . May want to disable apply_shift to save memory and/or make loading quicker as there will be no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift. True Returns: Type Description np . ndarray int32 [ny x nx (x nz)] or int32 [n_pixels x (2 or 3)] Loaded image. Source code in coppafish/utils/npy.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def load_tile ( nbp_file : NotebookPage , nbp_basic : NotebookPage , t : int , r : int , c : int , yxz : Optional [ Union [ List , Tuple , np . ndarray , jnp . ndarray ]] = None , apply_shift : bool = True ) -> np . ndarray : \"\"\" Loads in image corresponding to desired tile, round and channel from the relavent npy file. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page t: npy tile index considering r: Round considering c: Channel considering yxz: If `None`, whole image is loaded otherwise there are two choices: - `int [2 or 3]`. List containing y,x,z coordinates of sub image to load in. E.g. if `yxz = [np.array([5]), np.array([10,11,12]), np.array([8,9])]` returned `image` will have shape `[1 x 3 x 2]`. if `yxz = [None, None, z_planes]`, all pixels on given z_planes will be returned i.e. shape of image will be `[tile_sz x tile_sz x n_z_planes]`. - `int [n_pixels x (2 or 3)]`. Array containing yxz coordinates for which the pixel value is desired. E.g. if `yxz = np.ones((10,3))`, returned `image` will have shape `[10,]` with all values indicating the pixel value at `[1,1,1]`. apply_shift: If `True`, dtype will be `int32` otherwise dtype will be `uint16` with the pixels values shifted by `+nbp_basic.tile_pixel_value_shift`. May want to disable `apply_shift` to save memory and/or make loading quicker as there will be no dtype conversion. If loading in DAPI, dtype always uint16 as is no shift. Returns: `int32 [ny x nx (x nz)]` or `int32 [n_pixels x (2 or 3)]` Loaded image. \"\"\" if yxz is not None : # Use mmap when only loading in part of image if isinstance ( yxz , ( list , tuple )): if nbp_basic . is_3d : if len ( yxz ) != 3 : raise ValueError ( f 'Loading in a 3D tile but dimension of coordinates given is { len ( yxz ) } .' ) if yxz [ 0 ] is None and yxz [ 1 ] is None : image = np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' )[ yxz [ 2 ]] if image . ndim == 3 : image = np . moveaxis ( image , 0 , 2 ) else : coord_index = np . ix_ ( yxz [ 0 ], yxz [ 1 ], yxz [ 2 ]) image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ), 0 , 2 )[ coord_index ] else : if len ( yxz ) != 2 : raise ValueError ( f 'Loading in a 2D tile but dimension of coordinates given is { len ( yxz ) } .' ) coord_index = np . ix_ ( np . array ([ c ]), yxz [ 0 ], yxz [ 1 ]) # add channel as first coordinate in 2D. # [0] below is to remove channel index of length 1. image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ coord_index ][ 0 ] elif isinstance ( yxz , ( np . ndarray , jnp . ndarray )): if nbp_basic . is_3d : if yxz . shape [ 1 ] != 3 : raise ValueError ( f 'Loading in a 3D tile but dimension of coordinates given is { yxz . shape [ 1 ] } .' ) coord_index = tuple ( np . asarray ( yxz [:, i ]) for i in range ( 3 )) image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ), 0 , 2 )[ coord_index ] else : if yxz . shape [ 1 ] != 2 : raise ValueError ( f 'Loading in a 2D tile but dimension of coordinates given is { yxz . shape [ 1 ] } .' ) coord_index = tuple ( np . asarray ( yxz [:, i ]) for i in range ( 2 )) coord_index = ( np . full ( yxz . shape [ 0 ], c , int ),) + coord_index # add channel as first coordinate in 2D. image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ coord_index ] else : raise ValueError ( f 'yxz should either be an [n_spots x n_dim] array to return an n_spots array indicating ' f 'the value of the image at these coordinates or \\n ' f 'a list containing { 2 + int ( nbp_basic . is_3d ) } arrays indicating the sub image to load.' ) else : if nbp_basic . is_3d : # Don't use mmap when loading in whole image image = np . moveaxis ( np . load ( nbp_file . tile [ t ][ r ][ c ]), 0 , 2 ) else : # Use mmap when only loading in part of image image = np . load ( nbp_file . tile [ t ][ r ], mmap_mode = 'r' )[ c ] if apply_shift and not ( r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel ): image = image . astype ( np . int32 ) - nbp_basic . tile_pixel_value_shift return image","title":"load_tile()"},{"location":"code/utils/npy/#coppafish.utils.npy.save_stitched","text":"Stitches together all tiles from round r , channel c and saves the resultant compressed npz at im_file . Saved image will be uint16 if from nd2 or from DAPI filtered npy files. Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range. Parameters: Name Type Description Default im_file Optional [ str ] Path to save file. If None , stitched image is returned (with z axis last) instead of saved. required nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required tile_origin np . ndarray float [n_tiles x 3] . yxz origin of each tile on round r . required r int save_stitched will save stitched image of all tiles of round r , channel c . required c int save_stitched will save stitched image of all tiles of round r , channel c . required from_raw bool If False , will stitch together tiles from saved npy files, otherwise will load in raw un-filtered images from nd2/npy file. False zero_thresh int All pixels with absolute value less than or equal to zero_thresh will be set to 0. The larger it is, the smaller the compressed file will be. save: If True, saves image as im_file, otherwise returns image 0 Source code in coppafish/utils/npy.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def save_stitched ( im_file : Optional [ str ], nbp_file : NotebookPage , nbp_basic : NotebookPage , tile_origin : np . ndarray , r : int , c : int , from_raw : bool = False , zero_thresh : int = 0 ): \"\"\" Stitches together all tiles from round `r`, channel `c` and saves the resultant compressed npz at `im_file`. Saved image will be uint16 if from nd2 or from DAPI filtered npy files. Otherwise, if from filtered npy files, will remove shift and re-scale to fill int16 range. Args: im_file: Path to save file. If `None`, stitched `image` is returned (with z axis last) instead of saved. nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page tile_origin: `float [n_tiles x 3]`. yxz origin of each tile on round `r`. r: save_stitched will save stitched image of all tiles of round `r`, channel `c`. c: save_stitched will save stitched image of all tiles of round `r`, channel `c`. from_raw: If `False`, will stitch together tiles from saved npy files, otherwise will load in raw un-filtered images from nd2/npy file. zero_thresh: All pixels with absolute value less than or equal to `zero_thresh` will be set to 0. The larger it is, the smaller the compressed file will be.\\ save: If True, saves image as im_file, otherwise returns image \"\"\" yx_origin = np . round ( tile_origin [:, : 2 ]) . astype ( int ) z_origin = np . round ( tile_origin [:, 2 ]) . astype ( int ) . flatten () yx_size = np . max ( yx_origin , axis = 0 ) + nbp_basic . tile_sz if nbp_basic . is_3d : z_size = z_origin . max () + nbp_basic . nz stitched_image = np . zeros ( np . append ( z_size , yx_size ), dtype = np . uint16 ) else : z_size = 1 stitched_image = np . zeros ( yx_size , dtype = np . uint16 ) if from_raw : round_dask_array = utils . raw . load ( nbp_file , nbp_basic , r = r ) shift = 0 # if from nd2 file, data type is already un-shifted uint16 else : if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : shift = 0 # if filtered dapi, data type is already un-shifted uint16 else : # if from filtered npy files, data type is shifted uint16, want to save stitched as un-shifted int16. shift = nbp_basic . tile_pixel_value_shift if shift != 0 : # change dtype to accommodate negative values and set base value to be zero in the shifted image. stitched_image = stitched_image . astype ( np . int32 ) + shift with tqdm ( total = z_size * len ( nbp_basic . use_tiles )) as pbar : for t in nbp_basic . use_tiles : if from_raw : image_t = utils . raw . load ( nbp_file , nbp_basic , round_dask_array , r , t , c , nbp_basic . use_z ) # replicate non-filtering procedure in extract_and_filter if not nbp_basic . is_3d : image_t = extract . focus_stack ( image_t ) image_t , bad_columns = extract . strip_hack ( image_t ) # find faulty columns image_t [:, bad_columns ] = 0 if nbp_basic . is_3d : image_t = np . moveaxis ( image_t , 2 , 0 ) # put z-axis back to the start else : if nbp_basic . is_3d : image_t = np . load ( nbp_file . tile [ t ][ r ][ c ], mmap_mode = 'r' ) else : image_t = load_tile ( nbp_file , nbp_basic , t , r , c , apply_shift = False ) for z in range ( z_size ): # any tiles not used will be kept as 0. pbar . set_postfix ({ 'tile' : t , 'z' : z }) if nbp_basic . is_3d : file_z = z - z_origin [ t ] if file_z < 0 or file_z >= nbp_basic . nz : # Set tile to 0 if currently outside its area local_image = np . zeros (( nbp_basic . tile_sz , nbp_basic . tile_sz )) else : local_image = image_t [ file_z ] stitched_image [ z , yx_origin [ t , 0 ]: yx_origin [ t , 0 ] + nbp_basic . tile_sz , yx_origin [ t , 1 ]: yx_origin [ t , 1 ] + nbp_basic . tile_sz ] = local_image else : stitched_image [ yx_origin [ t , 0 ]: yx_origin [ t , 0 ] + nbp_basic . tile_sz , yx_origin [ t , 1 ]: yx_origin [ t , 1 ] + nbp_basic . tile_sz ] = image_t pbar . update ( 1 ) pbar . close () if shift != 0 : # remove shift and re-scale so fits the whole int16 range stitched_image = stitched_image - shift stitched_image = stitched_image * np . iinfo ( np . int16 ) . max / np . abs ( stitched_image ) . max () stitched_image = np . rint ( stitched_image , np . zeros_like ( stitched_image , dtype = np . int16 ), casting = 'unsafe' ) if zero_thresh > 0 : stitched_image [ np . abs ( stitched_image ) <= zero_thresh ] = 0 if im_file is None : if z_size > 1 : stitched_image = np . moveaxis ( stitched_image , 0 , - 1 ) return stitched_image else : np . savez_compressed ( im_file , stitched_image )","title":"save_stitched()"},{"location":"code/utils/npy/#coppafish.utils.npy.save_tile","text":"Wrapper function to save tiles as npy files with correct shift. Moves z-axis to start before saving as it is quicker to load in this order. Parameters: Name Type Description Default nbp_file NotebookPage file_names notebook page required nbp_basic NotebookPage basic_info notebook page required image np . ndarray int32 [ny x nx x nz] or int32 [n_channels x ny x nx] . Image to save. required t int npy tile index considering required r int Round considering required c Optional [ int ] Channel considering None Source code in coppafish/utils/npy.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def save_tile ( nbp_file : NotebookPage , nbp_basic : NotebookPage , image : np . ndarray , t : int , r : int , c : Optional [ int ] = None ): \"\"\" Wrapper function to save tiles as npy files with correct shift. Moves z-axis to start before saving as it is quicker to load in this order. Args: nbp_file: `file_names` notebook page nbp_basic: `basic_info` notebook page image: `int32 [ny x nx x nz]` or `int32 [n_channels x ny x nx]`. Image to save. t: npy tile index considering r: Round considering c: Channel considering \"\"\" if nbp_basic . is_3d : if c is None : raise ValueError ( '3d image but channel not given.' ) if r == nbp_basic . anchor_round and c == nbp_basic . dapi_channel : # If dapi is given then image should already by uint16 so no clipping image = image . astype ( np . uint16 ) else : # need to shift and clip image so fits into uint16 dtype. # clip at 1 not 0 because 0 (or -tile_pixel_value_shift) # will be used as an invalid value when reading in spot_colors. image = np . clip ( image + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , np . zeros_like ( image , dtype = np . uint16 ), casting = \"unsafe\" ) # In 3D, cannot possibly save any un-used channel hence no exception for this case. expected_shape = ( nbp_basic . tile_sz , nbp_basic . tile_sz , nbp_basic . nz ) if not utils . errors . check_shape ( image , expected_shape ): raise utils . errors . ShapeError ( \"tile to be saved\" , image . shape , expected_shape ) np . save ( nbp_file . tile [ t ][ r ][ c ], np . moveaxis ( image , 2 , 0 )) else : if r == nbp_basic . anchor_round : if nbp_basic . anchor_channel is not None : # If anchor round, only shift and clip anchor channel, leave DAPI and un-used channels alone. image [ nbp_basic . anchor_channel ] = \\ np . clip ( image [ nbp_basic . anchor_channel ] + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , image [ nbp_basic . anchor_channel ]) image = image . astype ( np . uint16 ) use_channels = [ val for val in [ nbp_basic . dapi_channel , nbp_basic . anchor_channel ] if val is not None ] else : image = np . clip ( image + nbp_basic . tile_pixel_value_shift , 1 , np . iinfo ( np . uint16 ) . max , np . zeros_like ( image , dtype = np . uint16 ), casting = \"unsafe\" ) use_channels = nbp_basic . use_channels # set un-used channels to be 0, not clipped to 1. image [ np . setdiff1d ( np . arange ( nbp_basic . n_channels ), use_channels )] = 0 expected_shape = ( nbp_basic . n_channels , nbp_basic . tile_sz , nbp_basic . tile_sz ) if not utils . errors . check_shape ( image , expected_shape ): raise utils . errors . ShapeError ( \"tile to be saved\" , image . shape , expected_shape ) np . save ( nbp_file . tile [ t ][ r ], image )","title":"save_tile()"},{"location":"code/utils/pciseq/","text":"export_to_pciseq ( nb ) This saves .csv files containing plotting information for pciseq- y - y coordinate of each spot in stitched coordinate system. x - x coordinate of each spot in stitched coordinate system. z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). Gene - Name of gene each spot was assigned to. Only spots which pass quality_threshold are saved. This depends on parameters given in config['thresholds'] . One .csv file is saved for each method: omp and ref_spots if the notebook contains both pages. Also adds the thresholds page to the notebook and re-saves. This is so the thresholds section in the config file cannot be further changed. Parameters: Name Type Description Default nb Notebook Notebook for the experiment containing at least the ref_spots page. required Source code in coppafish/utils/pciseq.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def export_to_pciseq ( nb : Notebook ): \"\"\" This saves .csv files containing plotting information for pciseq- - y - y coordinate of each spot in stitched coordinate system. - x - x coordinate of each spot in stitched coordinate system. - z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). - Gene - Name of gene each spot was assigned to. Only spots which pass `quality_threshold` are saved. This depends on parameters given in `config['thresholds']`. One .csv file is saved for each method: *omp* and *ref_spots* if the notebook contains both pages. Also adds the *thresholds* page to the notebook and re-saves. This is so the *thresholds* section in the config file cannot be further changed. Args: nb: Notebook for the experiment containing at least the *ref_spots* page. \"\"\" page_names = [ 'omp' , 'ref_spots' ] method = [ 'omp' , 'anchor' ] # for calling qual_ok files_saved = 0 for i in range ( 2 ): if not nb . has_page ( page_names [ i ]): warnings . warn ( f 'No file saved for method { method [ i ] } as notebook does not have a { page_names [ i ] } page.' ) continue if os . path . isfile ( nb . file_names . pciseq [ i ]): warnings . warn ( f \"File { nb . file_names . pciseq [ i ] } already exists\" ) continue qual_ok = quality_threshold ( nb , method [ i ]) # only keep spots which pass quality thresholding # get coordinates in stitched image global_spot_yxz = nb . __getattribute__ ( page_names [ i ]) . local_yxz + \\ nb . stitch . tile_origin [ nb . __getattribute__ ( page_names [ i ]) . tile ] spot_gene = nb . call_spots . gene_names [ nb . __getattribute__ ( page_names [ i ]) . gene_no [ qual_ok ]] global_spot_yxz = global_spot_yxz [ qual_ok ] df_to_export = pd . DataFrame ( data = global_spot_yxz , index = spot_gene , columns = [ 'y' , 'x' , 'z_stack' ]) df_to_export [ 'Gene' ] = df_to_export . index df_to_export . to_csv ( nb . file_names . pciseq [ i ], index = False ) print ( f 'pciSeq file saved for method = { method [ i ] } : ' + nb . file_names . pciseq [ i ]) files_saved += 1 if files_saved > 0 : # If saved any files, add thresholds page to notebook so cannot make any further changes to # config - will trigger save if not nb . has_page ( 'thresholds' ): nbp_thresholds = get_thresholds_page ( nb ) nb += nbp_thresholds else : warnings . warn ( 'thresholds' , utils . warnings . NotebookPageWarning ) else : warnings . warn ( f \"No files saved\" ) get_thresholds_page ( nb ) Makes notebook page from thresholds section of config file. Parameters: Name Type Description Default nb Notebook Notebook containing all experiment information. required Returns: Type Description NotebookPage thresholds NotebookPage. Source code in coppafish/utils/pciseq.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_thresholds_page ( nb : Notebook ) -> NotebookPage : \"\"\" Makes notebook page from thresholds section of config file. Args: nb: Notebook containing all experiment information. Returns: thresholds NotebookPage. \"\"\" config = nb . get_config ()[ 'thresholds' ] if config [ 'intensity' ] is None : config [ 'intensity' ] = nb . call_spots . gene_efficiency_intensity_thresh nbp = NotebookPage ( 'thresholds' ) nbp . intensity = config [ 'intensity' ] nbp . score_ref = config [ 'score_ref' ] nbp . score_omp = config [ 'score_omp' ] nbp . score_omp_multiplier = config [ 'score_omp_multiplier' ] return nbp","title":"pciSeq"},{"location":"code/utils/pciseq/#coppafish.utils.pciseq.export_to_pciseq","text":"This saves .csv files containing plotting information for pciseq- y - y coordinate of each spot in stitched coordinate system. x - x coordinate of each spot in stitched coordinate system. z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). Gene - Name of gene each spot was assigned to. Only spots which pass quality_threshold are saved. This depends on parameters given in config['thresholds'] . One .csv file is saved for each method: omp and ref_spots if the notebook contains both pages. Also adds the thresholds page to the notebook and re-saves. This is so the thresholds section in the config file cannot be further changed. Parameters: Name Type Description Default nb Notebook Notebook for the experiment containing at least the ref_spots page. required Source code in coppafish/utils/pciseq.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def export_to_pciseq ( nb : Notebook ): \"\"\" This saves .csv files containing plotting information for pciseq- - y - y coordinate of each spot in stitched coordinate system. - x - x coordinate of each spot in stitched coordinate system. - z_stack - z coordinate of each spot in stitched coordinate system (in units of z-pixels). - Gene - Name of gene each spot was assigned to. Only spots which pass `quality_threshold` are saved. This depends on parameters given in `config['thresholds']`. One .csv file is saved for each method: *omp* and *ref_spots* if the notebook contains both pages. Also adds the *thresholds* page to the notebook and re-saves. This is so the *thresholds* section in the config file cannot be further changed. Args: nb: Notebook for the experiment containing at least the *ref_spots* page. \"\"\" page_names = [ 'omp' , 'ref_spots' ] method = [ 'omp' , 'anchor' ] # for calling qual_ok files_saved = 0 for i in range ( 2 ): if not nb . has_page ( page_names [ i ]): warnings . warn ( f 'No file saved for method { method [ i ] } as notebook does not have a { page_names [ i ] } page.' ) continue if os . path . isfile ( nb . file_names . pciseq [ i ]): warnings . warn ( f \"File { nb . file_names . pciseq [ i ] } already exists\" ) continue qual_ok = quality_threshold ( nb , method [ i ]) # only keep spots which pass quality thresholding # get coordinates in stitched image global_spot_yxz = nb . __getattribute__ ( page_names [ i ]) . local_yxz + \\ nb . stitch . tile_origin [ nb . __getattribute__ ( page_names [ i ]) . tile ] spot_gene = nb . call_spots . gene_names [ nb . __getattribute__ ( page_names [ i ]) . gene_no [ qual_ok ]] global_spot_yxz = global_spot_yxz [ qual_ok ] df_to_export = pd . DataFrame ( data = global_spot_yxz , index = spot_gene , columns = [ 'y' , 'x' , 'z_stack' ]) df_to_export [ 'Gene' ] = df_to_export . index df_to_export . to_csv ( nb . file_names . pciseq [ i ], index = False ) print ( f 'pciSeq file saved for method = { method [ i ] } : ' + nb . file_names . pciseq [ i ]) files_saved += 1 if files_saved > 0 : # If saved any files, add thresholds page to notebook so cannot make any further changes to # config - will trigger save if not nb . has_page ( 'thresholds' ): nbp_thresholds = get_thresholds_page ( nb ) nb += nbp_thresholds else : warnings . warn ( 'thresholds' , utils . warnings . NotebookPageWarning ) else : warnings . warn ( f \"No files saved\" )","title":"export_to_pciseq()"},{"location":"code/utils/pciseq/#coppafish.utils.pciseq.get_thresholds_page","text":"Makes notebook page from thresholds section of config file. Parameters: Name Type Description Default nb Notebook Notebook containing all experiment information. required Returns: Type Description NotebookPage thresholds NotebookPage. Source code in coppafish/utils/pciseq.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def get_thresholds_page ( nb : Notebook ) -> NotebookPage : \"\"\" Makes notebook page from thresholds section of config file. Args: nb: Notebook containing all experiment information. Returns: thresholds NotebookPage. \"\"\" config = nb . get_config ()[ 'thresholds' ] if config [ 'intensity' ] is None : config [ 'intensity' ] = nb . call_spots . gene_efficiency_intensity_thresh nbp = NotebookPage ( 'thresholds' ) nbp . intensity = config [ 'intensity' ] nbp . score_ref = config [ 'score_ref' ] nbp . score_omp = config [ 'score_omp' ] nbp . score_omp_multiplier = config [ 'score_omp_multiplier' ] return nbp","title":"get_thresholds_page()"},{"location":"code/utils/spot_images/","text":"get_average_spot_image ( spot_images , av_type = 'mean' , symmetry = None , annulus_width = 1.0 ) Given an array of spot images, this returns the average spot image. Parameters: Name Type Description Default spot_images np . ndarray float [n_peaks x y_shape x x_shape (x z_shape)] . spot_images[s] is the small image surrounding spot s . Any nan values will be ignored when computing the average spot image. required av_type str Optional, one of the following indicating which average to use: 'mean' 'median' 'mean' symmetry Optional [ str ] Optional, one of the following: None - Just finds mean at every pixel. 'quadrant_2d' - Assumes each quadrant of each z-plane expected to look the same so concatenates these. 'annulus_2d' - assumes each z-plane is circularly symmetric about central pixel. I.e. only finds only pixel value from all pixels a certain distance from centre. 'annulus_3d' - Same as 'annulus_2d' , except now z-planes are symmetric about the mid-plane. I.e. av_image[:,:,mid-i] = av_image[:,:,mid+i] for all i . None annulus_width float If symmetry = 'annulus' , this specifies how big an annulus to use, within which we expect all pixel values to be the same. 1.0 Returns: Type Description np . ndarray float [y_shape x x_shape (x z_shape)] . Average small image about a spot. Source code in coppafish/utils/spot_images.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_average_spot_image ( spot_images : np . ndarray , av_type : str = 'mean' , symmetry : Optional [ str ] = None , annulus_width : float = 1.0 ) -> np . ndarray : \"\"\" Given an array of spot images, this returns the average spot image. Args: spot_images: ```float [n_peaks x y_shape x x_shape (x z_shape)]```. ```spot_images[s]``` is the small image surrounding spot ```s```. Any nan values will be ignored when computing the average spot image. av_type: Optional, one of the following indicating which average to use: - ```'mean'``` - ```'median'``` symmetry: Optional, one of the following: - ```None``` - Just finds mean at every pixel. - ```'quadrant_2d'``` - Assumes each quadrant of each z-plane expected to look the same so concatenates these. - ```'annulus_2d'``` - assumes each z-plane is circularly symmetric about central pixel. I.e. only finds only pixel value from all pixels a certain distance from centre. - ```'annulus_3d'``` - Same as ```'annulus_2d'```, except now z-planes are symmetric about the mid-plane. I.e. `av_image[:,:,mid-i] = av_image[:,:,mid+i]` for all `i`. annulus_width: If ```symmetry = 'annulus'```, this specifies how big an annulus to use, within which we expect all pixel values to be the same. Returns: ```float [y_shape x x_shape (x z_shape)]```. Average small image about a spot. \"\"\" # avoid nan in average because some spot_images may have nans because the image ran out of bounds of the tile. if av_type == 'mean' : av_func = lambda x , axis : np . nanmean ( x , axis ) elif av_type == 'median' : av_func = lambda x , axis : np . nanmedian ( x , axis ) else : raise ValueError ( f \"av_type must be 'mean' or 'median' but value given was { av_type } \" ) mid_index = np . ceil ( np . array ( spot_images . shape [ 1 :]) / 2 ) . astype ( int ) - 1 if symmetry is None : av_image = av_func ( spot_images , 0 ) elif symmetry == \"quadrant_2d\" : # rotate all quadrants so spot is at bottom right corner quad1 = spot_images [:, 0 : mid_index [ 0 ] + 1 , 0 : mid_index [ 1 ] + 1 ] quad2 = np . rot90 ( spot_images [:, 0 : mid_index [ 0 ] + 1 , mid_index [ 1 ]:], 1 , axes = ( 1 , 2 )) quad3 = np . rot90 ( spot_images [:, mid_index [ 0 ]:, mid_index [ 1 ]:], 2 , axes = ( 1 , 2 )) quad4 = np . rot90 ( spot_images [:, mid_index [ 0 ]:, 0 : mid_index [ 1 ] + 1 ], 3 , axes = ( 1 , 2 )) all_quads = np . concatenate (( quad1 , quad2 , quad3 , quad4 )) av_quad = av_func ( all_quads , 0 ) if spot_images . ndim == 4 : av_image = np . pad ( av_quad , [[ 0 , mid_index [ 0 ] + 1 ], [ 0 , mid_index [ 1 ] + 1 ], [ 0 , 0 ]], 'symmetric' ) else : av_image = np . pad ( av_quad , [[ 0 , mid_index [ 0 ] + 1 ], [ 0 , mid_index [ 1 ] + 1 ]], 'symmetric' ) # remove repeated central column and row av_image = np . delete ( av_image , mid_index [ 0 ] + 1 , axis = 0 ) av_image = np . delete ( av_image , mid_index [ 1 ] + 1 , axis = 1 ) elif symmetry == \"annulus_2d\" or symmetry == \"annulus_3d\" : X , Y = np . meshgrid ( np . arange ( spot_images . shape [ 1 ]) - mid_index [ 0 ], np . arange ( spot_images . shape [ 2 ]) - mid_index [ 1 ]) d = np . sqrt ( X ** 2 + Y ** 2 ) annulus_bins = np . arange ( 0 , d . max (), annulus_width ) # find which bin each pixel should contribute to. bin_index = np . abs ( np . expand_dims ( d , 2 ) - annulus_bins ) . argmin ( axis = 2 ) av_image = np . zeros_like ( spot_images [ 0 ]) if symmetry == \"annulus_3d\" : if spot_images . ndim != 4 : raise ValueError ( \"Must give 3D images with symmetry = 'annulus_3d'\" ) n_z = spot_images . shape [ 3 ] if n_z % 2 == 0 : raise ValueError ( \"Must have odd number of z-planes with symmetry = 'annulus_3d'\" ) # ensure each z-plane has unique set of indices so can average each separately. bin_index = np . tile ( np . expand_dims ( bin_index , 2 ), [ 1 , 1 , n_z ]) for i in range ( mid_index [ 2 ]): current_max_index = bin_index [:, :, mid_index [ 2 ] - i ] . max () bin_index [:, :, mid_index [ 2 ] - i - 1 ] = bin_index [:, :, mid_index [ 2 ]] + current_max_index + 1 bin_index [:, :, mid_index [ 2 ] + i + 1 ] = bin_index [:, :, mid_index [ 2 ] - i - 1 ] for i in np . unique ( bin_index ): current_bin = bin_index == i av_image [ current_bin ] = av_func ( spot_images [:, current_bin ], ( 0 , 1 )) else : raise ValueError ( f \"symmetry must be None, 'quadrant_2d', 'annulus_2d' or 'annulus_3d' but value given was \" f \" { symmetry } \" ) if symmetry is not None : is_odd = ( np . array ( spot_images . shape [ 1 : 3 ]) % 2 ) . astype ( bool ) if not is_odd . all (): warnings . warn ( f \"spot_images shape is { av_image . shape } which is even in some dimensions.\" f \" \\n This means centre of symmetry will be off-centre.\" ) return av_image get_spot_images ( image , spot_yxz , shape ) Builds an image around each spot of size given by shape and returns array containing all of these. Parameters: Name Type Description Default image np . ndarray float [nY x nX (x nZ)] . Image that spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. required shape Union [ np . ndarray , List [ int ]] int [image.ndim] [y_shape, x_shape, (z_shape)] : Desired size of image for each spot in each direction. required Returns: Type Description np . ndarray float [n_peaks x y_shape x x_shape (x z_shape)] . [s] is the small image surrounding spot s . Source code in coppafish/utils/spot_images.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get_spot_images ( image : np . ndarray , spot_yxz : np . ndarray , shape : Union [ np . ndarray , List [ int ]]) -> np . ndarray : \"\"\" Builds an image around each spot of size given by shape and returns array containing all of these. Args: image: ```float [nY x nX (x nZ)]```. Image that spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. shape: ```int [image.ndim]``` ```[y_shape, x_shape, (z_shape)]```: Desired size of image for each spot in each direction. Returns: ```float [n_peaks x y_shape x x_shape (x z_shape)]```. ```[s]``` is the small image surrounding spot ```s```. \"\"\" if min ( np . array ( shape ) % 2 ) == 0 : raise ValueError ( f \"Require shape to be odd in each dimension but given shape was { shape } .\" ) mid_index = np . ceil ( np . array ( shape ) / 2 ) . astype ( int ) - 1 # index in spot_images where max intensity is for each spot. spot_images = np . empty (( spot_yxz . shape [ 0 ], * shape )) spot_images [:] = np . nan # set to nan if spot image goes out of bounds of image. max_image_index = np . array ( image . shape ) n_spots = spot_yxz . shape [ 0 ] no_verbose = n_spots < 6000 / len ( shape ) # show progress bar with lots of pixels. with tqdm ( total = n_spots , disable = no_verbose ) as pbar : pbar . set_description ( \"Loading in spot images from tiff files\" ) for s in range ( n_spots ): min_pos = np . clip (( spot_yxz [ s ] - mid_index ), 0 , max_image_index ) max_pos = np . clip (( spot_yxz [ s ] + mid_index + 1 ), 0 , max_image_index ) spot_images_min_index = mid_index - ( spot_yxz [ s ] - min_pos ) spot_images_max_index = mid_index + ( max_pos - spot_yxz [ s ]) if len ( shape ) == 2 : small_im = image [ min_pos [ 0 ]: max_pos [ 0 ], min_pos [ 1 ]: max_pos [ 1 ]] spot_images [ s , spot_images_min_index [ 0 ]: spot_images_max_index [ 0 ], spot_images_min_index [ 1 ]: spot_images_max_index [ 1 ]] = small_im elif len ( shape ) == 3 : small_im = image [ min_pos [ 0 ]: max_pos [ 0 ], min_pos [ 1 ]: max_pos [ 1 ], min_pos [ 2 ]: max_pos [ 2 ]] spot_images [ s , spot_images_min_index [ 0 ]: spot_images_max_index [ 0 ], spot_images_min_index [ 1 ]: spot_images_max_index [ 1 ], spot_images_min_index [ 2 ]: spot_images_max_index [ 2 ]] = small_im pbar . update ( 1 ) pbar . close () return spot_images","title":"Spot images"},{"location":"code/utils/spot_images/#coppafish.utils.spot_images.get_average_spot_image","text":"Given an array of spot images, this returns the average spot image. Parameters: Name Type Description Default spot_images np . ndarray float [n_peaks x y_shape x x_shape (x z_shape)] . spot_images[s] is the small image surrounding spot s . Any nan values will be ignored when computing the average spot image. required av_type str Optional, one of the following indicating which average to use: 'mean' 'median' 'mean' symmetry Optional [ str ] Optional, one of the following: None - Just finds mean at every pixel. 'quadrant_2d' - Assumes each quadrant of each z-plane expected to look the same so concatenates these. 'annulus_2d' - assumes each z-plane is circularly symmetric about central pixel. I.e. only finds only pixel value from all pixels a certain distance from centre. 'annulus_3d' - Same as 'annulus_2d' , except now z-planes are symmetric about the mid-plane. I.e. av_image[:,:,mid-i] = av_image[:,:,mid+i] for all i . None annulus_width float If symmetry = 'annulus' , this specifies how big an annulus to use, within which we expect all pixel values to be the same. 1.0 Returns: Type Description np . ndarray float [y_shape x x_shape (x z_shape)] . Average small image about a spot. Source code in coppafish/utils/spot_images.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def get_average_spot_image ( spot_images : np . ndarray , av_type : str = 'mean' , symmetry : Optional [ str ] = None , annulus_width : float = 1.0 ) -> np . ndarray : \"\"\" Given an array of spot images, this returns the average spot image. Args: spot_images: ```float [n_peaks x y_shape x x_shape (x z_shape)]```. ```spot_images[s]``` is the small image surrounding spot ```s```. Any nan values will be ignored when computing the average spot image. av_type: Optional, one of the following indicating which average to use: - ```'mean'``` - ```'median'``` symmetry: Optional, one of the following: - ```None``` - Just finds mean at every pixel. - ```'quadrant_2d'``` - Assumes each quadrant of each z-plane expected to look the same so concatenates these. - ```'annulus_2d'``` - assumes each z-plane is circularly symmetric about central pixel. I.e. only finds only pixel value from all pixels a certain distance from centre. - ```'annulus_3d'``` - Same as ```'annulus_2d'```, except now z-planes are symmetric about the mid-plane. I.e. `av_image[:,:,mid-i] = av_image[:,:,mid+i]` for all `i`. annulus_width: If ```symmetry = 'annulus'```, this specifies how big an annulus to use, within which we expect all pixel values to be the same. Returns: ```float [y_shape x x_shape (x z_shape)]```. Average small image about a spot. \"\"\" # avoid nan in average because some spot_images may have nans because the image ran out of bounds of the tile. if av_type == 'mean' : av_func = lambda x , axis : np . nanmean ( x , axis ) elif av_type == 'median' : av_func = lambda x , axis : np . nanmedian ( x , axis ) else : raise ValueError ( f \"av_type must be 'mean' or 'median' but value given was { av_type } \" ) mid_index = np . ceil ( np . array ( spot_images . shape [ 1 :]) / 2 ) . astype ( int ) - 1 if symmetry is None : av_image = av_func ( spot_images , 0 ) elif symmetry == \"quadrant_2d\" : # rotate all quadrants so spot is at bottom right corner quad1 = spot_images [:, 0 : mid_index [ 0 ] + 1 , 0 : mid_index [ 1 ] + 1 ] quad2 = np . rot90 ( spot_images [:, 0 : mid_index [ 0 ] + 1 , mid_index [ 1 ]:], 1 , axes = ( 1 , 2 )) quad3 = np . rot90 ( spot_images [:, mid_index [ 0 ]:, mid_index [ 1 ]:], 2 , axes = ( 1 , 2 )) quad4 = np . rot90 ( spot_images [:, mid_index [ 0 ]:, 0 : mid_index [ 1 ] + 1 ], 3 , axes = ( 1 , 2 )) all_quads = np . concatenate (( quad1 , quad2 , quad3 , quad4 )) av_quad = av_func ( all_quads , 0 ) if spot_images . ndim == 4 : av_image = np . pad ( av_quad , [[ 0 , mid_index [ 0 ] + 1 ], [ 0 , mid_index [ 1 ] + 1 ], [ 0 , 0 ]], 'symmetric' ) else : av_image = np . pad ( av_quad , [[ 0 , mid_index [ 0 ] + 1 ], [ 0 , mid_index [ 1 ] + 1 ]], 'symmetric' ) # remove repeated central column and row av_image = np . delete ( av_image , mid_index [ 0 ] + 1 , axis = 0 ) av_image = np . delete ( av_image , mid_index [ 1 ] + 1 , axis = 1 ) elif symmetry == \"annulus_2d\" or symmetry == \"annulus_3d\" : X , Y = np . meshgrid ( np . arange ( spot_images . shape [ 1 ]) - mid_index [ 0 ], np . arange ( spot_images . shape [ 2 ]) - mid_index [ 1 ]) d = np . sqrt ( X ** 2 + Y ** 2 ) annulus_bins = np . arange ( 0 , d . max (), annulus_width ) # find which bin each pixel should contribute to. bin_index = np . abs ( np . expand_dims ( d , 2 ) - annulus_bins ) . argmin ( axis = 2 ) av_image = np . zeros_like ( spot_images [ 0 ]) if symmetry == \"annulus_3d\" : if spot_images . ndim != 4 : raise ValueError ( \"Must give 3D images with symmetry = 'annulus_3d'\" ) n_z = spot_images . shape [ 3 ] if n_z % 2 == 0 : raise ValueError ( \"Must have odd number of z-planes with symmetry = 'annulus_3d'\" ) # ensure each z-plane has unique set of indices so can average each separately. bin_index = np . tile ( np . expand_dims ( bin_index , 2 ), [ 1 , 1 , n_z ]) for i in range ( mid_index [ 2 ]): current_max_index = bin_index [:, :, mid_index [ 2 ] - i ] . max () bin_index [:, :, mid_index [ 2 ] - i - 1 ] = bin_index [:, :, mid_index [ 2 ]] + current_max_index + 1 bin_index [:, :, mid_index [ 2 ] + i + 1 ] = bin_index [:, :, mid_index [ 2 ] - i - 1 ] for i in np . unique ( bin_index ): current_bin = bin_index == i av_image [ current_bin ] = av_func ( spot_images [:, current_bin ], ( 0 , 1 )) else : raise ValueError ( f \"symmetry must be None, 'quadrant_2d', 'annulus_2d' or 'annulus_3d' but value given was \" f \" { symmetry } \" ) if symmetry is not None : is_odd = ( np . array ( spot_images . shape [ 1 : 3 ]) % 2 ) . astype ( bool ) if not is_odd . all (): warnings . warn ( f \"spot_images shape is { av_image . shape } which is even in some dimensions.\" f \" \\n This means centre of symmetry will be off-centre.\" ) return av_image","title":"get_average_spot_image()"},{"location":"code/utils/spot_images/#coppafish.utils.spot_images.get_spot_images","text":"Builds an image around each spot of size given by shape and returns array containing all of these. Parameters: Name Type Description Default image np . ndarray float [nY x nX (x nZ)] . Image that spots were found on. required spot_yxz np . ndarray int [n_peaks x image.ndim] . yx or yxz location of spots found. required shape Union [ np . ndarray , List [ int ]] int [image.ndim] [y_shape, x_shape, (z_shape)] : Desired size of image for each spot in each direction. required Returns: Type Description np . ndarray float [n_peaks x y_shape x x_shape (x z_shape)] . [s] is the small image surrounding spot s . Source code in coppafish/utils/spot_images.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def get_spot_images ( image : np . ndarray , spot_yxz : np . ndarray , shape : Union [ np . ndarray , List [ int ]]) -> np . ndarray : \"\"\" Builds an image around each spot of size given by shape and returns array containing all of these. Args: image: ```float [nY x nX (x nZ)]```. Image that spots were found on. spot_yxz: ```int [n_peaks x image.ndim]```. yx or yxz location of spots found. shape: ```int [image.ndim]``` ```[y_shape, x_shape, (z_shape)]```: Desired size of image for each spot in each direction. Returns: ```float [n_peaks x y_shape x x_shape (x z_shape)]```. ```[s]``` is the small image surrounding spot ```s```. \"\"\" if min ( np . array ( shape ) % 2 ) == 0 : raise ValueError ( f \"Require shape to be odd in each dimension but given shape was { shape } .\" ) mid_index = np . ceil ( np . array ( shape ) / 2 ) . astype ( int ) - 1 # index in spot_images where max intensity is for each spot. spot_images = np . empty (( spot_yxz . shape [ 0 ], * shape )) spot_images [:] = np . nan # set to nan if spot image goes out of bounds of image. max_image_index = np . array ( image . shape ) n_spots = spot_yxz . shape [ 0 ] no_verbose = n_spots < 6000 / len ( shape ) # show progress bar with lots of pixels. with tqdm ( total = n_spots , disable = no_verbose ) as pbar : pbar . set_description ( \"Loading in spot images from tiff files\" ) for s in range ( n_spots ): min_pos = np . clip (( spot_yxz [ s ] - mid_index ), 0 , max_image_index ) max_pos = np . clip (( spot_yxz [ s ] + mid_index + 1 ), 0 , max_image_index ) spot_images_min_index = mid_index - ( spot_yxz [ s ] - min_pos ) spot_images_max_index = mid_index + ( max_pos - spot_yxz [ s ]) if len ( shape ) == 2 : small_im = image [ min_pos [ 0 ]: max_pos [ 0 ], min_pos [ 1 ]: max_pos [ 1 ]] spot_images [ s , spot_images_min_index [ 0 ]: spot_images_max_index [ 0 ], spot_images_min_index [ 1 ]: spot_images_max_index [ 1 ]] = small_im elif len ( shape ) == 3 : small_im = image [ min_pos [ 0 ]: max_pos [ 0 ], min_pos [ 1 ]: max_pos [ 1 ], min_pos [ 2 ]: max_pos [ 2 ]] spot_images [ s , spot_images_min_index [ 0 ]: spot_images_max_index [ 0 ], spot_images_min_index [ 1 ]: spot_images_max_index [ 1 ], spot_images_min_index [ 2 ]: spot_images_max_index [ 2 ]] = small_im pbar . update ( 1 ) pbar . close () return spot_images","title":"get_spot_images()"},{"location":"code/utils/strel/","text":"annulus ( r0 , r_xy , r_z = None ) Gets structuring element used to assess if spot isolated. Parameters: Name Type Description Default r0 float Inner radius within which values are all zero. required r_xy float Outer radius in xy direction. Can be float not integer because all values with radius < r_xy1 and > r0 will be set to 1 . required r_z Optional [ float ] Outer radius in z direction. Size in z-pixels. None means 2D annulus returned. None Returns: Type Description np . ndarray int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1] . Structuring element with each element either 0 or 1 . Source code in coppafish/utils/strel.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def annulus ( r0 : float , r_xy : float , r_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Gets structuring element used to assess if spot isolated. Args: r0: Inner radius within which values are all zero. r_xy: Outer radius in xy direction. Can be float not integer because all values with `radius < r_xy1` and `> r0` will be set to `1`. r_z: Outer radius in z direction. Size in z-pixels. None means 2D annulus returned. Returns: `int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1]`. Structuring element with each element either `0` or `1`. \"\"\" r_xy1_int = floor ( r_xy ) if r_z is None : y , x = np . meshgrid ( np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_xy1_int , r_xy1_int + 1 )) m = x ** 2 + y ** 2 else : r_z1_int = floor ( r_z ) y , x , z = np . meshgrid ( np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_z1_int , r_z1_int + 1 )) m = x ** 2 + y ** 2 + z ** 2 # only use upper radius in xy direction as z direction has different pixel size. annulus = r_xy ** 2 >= m annulus = np . logical_and ( annulus , m > r0 ** 2 ) return annulus . astype ( int ) disk ( r , n = 4 ) Creates a flat disk-shaped structuring element with the specified radius, r . r must be a nonnegative integer. n must be 0, 4, 6, or 8 . When n is greater than 0 , the disk-shaped structuring element is approximated by a sequence of n (or sometimes n+2 ) periodic-line structuring elements. When n is 0 , no approximation is used, and the structuring element members comprise all pixels whose centers are no greater than r away from the origin. n can be omitted, in which case its default value is 4 . Note Morphological operations using disk approximations ( n>0 ) run much faster than when n=0 . Also, the structuring elements resulting from choosing n>0 are suitable for computing granulometries, which is not the case for vn=0 . Sometimes it is necessary for STREL to use two extra line structuring elements in the approximation, in which case the number of decomposed structuring elements used is n+2 . Copy of MATLAB strel('disk') . Source code in coppafish/utils/strel.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def disk ( r : int , n : int = 4 ) -> np . ndarray : \"\"\" Creates a flat disk-shaped structuring element with the specified radius, `r`. `r` must be a nonnegative integer. `n` must be `0, 4, 6, or 8`. When `n` is greater than `0`, the disk-shaped structuring element is approximated by a sequence of `n` (or sometimes `n+2`) periodic-line structuring elements. When `n` is `0`, no approximation is used, and the structuring element members comprise all pixels whose centers are no greater than `r` away from the origin. `n` can be omitted, in which case its default value is `4`. !!! note Morphological operations using disk approximations (`n>0`) run much faster than when `n=0`. Also, the structuring elements resulting from choosing `n>0` are suitable for computing granulometries, which is not the case for `vn=0`. Sometimes it is necessary for STREL to use two extra line structuring elements in the approximation, in which case the number of decomposed structuring elements used is `n+2`. Copy of MATLAB `strel('disk')`. \"\"\" if r < 3 : # Radius is too small to use decomposition, so force n=0. n = 0 if n == 0 : xx , yy = np . meshgrid ( np . arange ( - r , r + 1 ), np . arange ( - r , r + 1 )) nhood = xx ** 2 + yy ** 2 <= r ** 2 else : # Reference for radial decomposition of disks: Rolf Adams, \"Radial # Decomposition of Discs and Spheres,\" CVGIP: Graphical Models and # Image Processing, vol. 55, no. 5, September 1993, pp. 325-332. # # The specific decomposition technique used here is radial # decomposition using periodic lines. The reference is: Ronald # Jones and Pierre Soille, \"Periodic lines: Definition, cascades, and # application to granulometries,\" Pattern Recognition Letters, # vol. 17, 1996, pp. 1057-1063. # # Determine the set of \"basis\" vectors to be used for the # decomposition. The rows of v will be used as offset vectors for # periodic line strels. if n == 4 : v = np . array ([[ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ], [ - 1 , 1 ]]) elif n == 6 : v = np . array ([[ 1 , 0 ], [ 1 , 2 ], [ 2 , 1 ], [ 0 , 1 ], [ - 1 , 2 ], [ - 2 , 1 ]]) elif n == 8 : v = np . array ([[ 1 , 0 ], [ 2 , 1 ], [ 1 , 1 ], [ 1 , 2 ], [ 0 , 1 ], [ - 1 , 2 ], [ - 1 , 1 ], [ - 2 , 1 ]]) else : raise ValueError ( f 'Value of n provided ( { n } ) is not 0, 4, 6 or 8.' ) # Determine k, which is the desired radial extent of the periodic # line strels. For the origin of this formula, see the second # paragraph on page 328 of the Rolf Adams paper. theta = np . pi / ( 2 * n ) k = 2 * r / ( 1 / np . tan ( theta ) + 1 / np . sin ( theta )) # For each periodic line strel, determine the repetition parameter, # rp. The use of floor() in the computation means that the resulting # strel will be a little small, but we will compensate for this # below. nhood = np . ones (( 2 * r - 1 , 2 * r - 1 ), np . uint8 ) * - np . inf nhood [ int (( nhood . shape [ 0 ] - 1 ) / 2 ), int (( nhood . shape [ 0 ] - 1 ) / 2 )] = 1 for q in range ( n ): rp = int ( np . floor ( k / np . linalg . norm ( v [ q , :]))) decomposition = periodic_line ( rp , v [ q , :]) nhood = dilate ( nhood , decomposition ) nhood = nhood > 0 # Now we are going to add additional vertical and horizontal line # strels to compensate for the fact that the strel resulting from the # above decomposition tends to be smaller than the desired size. extra_strel_size = int ( sum ( np . sum ( nhood , axis = 1 ) == 0 ) + 1 ) if extra_strel_size > 0 : # Update the computed neighborhood to reflect the additional strels in # the decomposition. nhood = cv2 . dilate ( nhood . astype ( np . uint8 ), np . ones (( 1 , extra_strel_size ), dtype = np . uint8 )) nhood = cv2 . dilate ( nhood , np . ones (( extra_strel_size , 1 ), dtype = np . uint8 )) nhood = nhood > 0 return nhood . astype ( int ) disk_3d ( r_xy , r_z ) Gets structuring element used to find spots when dilated with 3d image. Parameters: Name Type Description Default r_xy float Radius in xy direction. required r_z float Radius in z direction. required Returns: Type Description np . ndarray int [2*r_xy+1, 2*r_xy+1, 2*r_z+1] . Structuring element with each element either 0 or 1 . Source code in coppafish/utils/strel.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def disk_3d ( r_xy : float , r_z : float ) -> np . ndarray : \"\"\" Gets structuring element used to find spots when dilated with 3d image. Args: r_xy: Radius in xy direction. r_z: Radius in z direction. Returns: `int [2*r_xy+1, 2*r_xy+1, 2*r_z+1]`. Structuring element with each element either `0` or `1`. \"\"\" y , x , z = np . meshgrid ( np . arange ( - np . ceil ( r_xy ), np . ceil ( r_xy ) + 1 ), np . arange ( - np . ceil ( r_xy ), np . ceil ( r_xy ) + 1 ), np . arange ( - np . ceil ( r_z ), np . ceil ( r_z ) + 1 )) se = x ** 2 + y ** 2 + z ** 2 <= r_xy ** 2 # Crop se to remove zeros at extremities se = se [:, :, ~ np . all ( se == 0 , axis = ( 0 , 1 ))] se = se [:, ~ np . all ( se == 0 , axis = ( 0 , 2 )), :] se = se [ ~ np . all ( se == 0 , axis = ( 1 , 2 )), :, :] return se . astype ( int ) fspecial ( r_y , r_x = None , r_z = None ) Creates an ellipsoidal 3D filter kernel if r_y , r_x and r_z given. Copy of MATlAB fspecial3('ellipsoid') . Creates a disk 2D filter kernel if just r_y given. Copy of MATlAB fspecial('disk') . Parameters: Name Type Description Default r_y float Radius in y direction or radius of disk if only parameter provided. required r_x Optional [ float ] Radius in x direction. None r_z Optional [ float ] Radius in z direction. None Returns: Type Description np . ndarray float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1] . Filtering kernel. Source code in coppafish/utils/strel.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def fspecial ( r_y : float , r_x : Optional [ float ] = None , r_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Creates an ellipsoidal 3D filter kernel if `r_y`, `r_x` and `r_z` given. Copy of MATlAB `fspecial3('ellipsoid')`. Creates a disk 2D filter kernel if just `r_y` given. Copy of MATlAB `fspecial('disk')`. Args: r_y: Radius in y direction or radius of disk if only parameter provided. r_x: Radius in x direction. r_z: Radius in z direction. Returns: `float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1]`. Filtering kernel. \"\"\" if r_x is None and r_z is None : r = r_y crad = ceil ( r - 0.5 ) x , y = np . meshgrid ( np . arange ( - crad , crad + 1 ), np . arange ( - crad , crad + 1 )) max_xy = np . maximum ( np . abs ( x ), np . abs ( y )) min_xy = np . minimum ( np . abs ( x ), np . abs ( y )) m1 = ( r ** 2 < ( max_xy + 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 ) * ( min_xy - 0.5 ) + \\ ( r ** 2 >= ( max_xy + 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 ) * np . sqrt ( r ** 2 - ( max_xy + 0.5 ) ** 2 , dtype = np . complex_ ) m1 = np . real ( m1 ) m2 = ( r ** 2 > ( max_xy - 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) * ( min_xy + 0.5 ) + \\ ( r ** 2 <= ( max_xy - 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) * np . sqrt ( r ** 2 - ( max_xy - 0.5 ) ** 2 , dtype = np . complex_ ) m2 = np . real ( m2 ) sgrid = ( r ** 2 * ( 0.5 * ( np . arcsin ( m2 / r ) - np . arcsin ( m1 / r )) + 0.25 * ( np . sin ( 2 * np . arcsin ( m2 / r )) - np . sin ( 2 * np . arcsin ( m1 / r )))) - ( max_xy - 0.5 ) * ( m2 - m1 ) + ( m1 - min_xy + 0.5 )) * (((( r ** 2 < ( max_xy + 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) & ( r ** 2 > ( max_xy - 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 )) | (( min_xy == 0 ) & ( max_xy - 0.5 < r ) & ( max_xy + 0.5 >= r )))) sgrid = sgrid + (( max_xy + 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 < r ** 2 ) sgrid [ crad , crad ] = min ( np . pi * r ** 2 , np . pi / 2 ) if crad > 0.0 and r > crad - 0.5 and r ** 2 < ( crad - 0.5 ) ** 2 + 0.25 : m1 = np . sqrt ( r ** 2 - ( crad - 0.5 ) ** 2 ) m1n = m1 / r sg0 = 2 * ( r ** 2 * ( 0.5 * np . arcsin ( m1n ) + 0.25 * np . sin ( 2 * np . arcsin ( m1n ))) - m1 * ( crad - 0.5 )) sgrid [ 2 * crad , crad ] = sg0 sgrid [ crad , 2 * crad ] = sg0 sgrid [ crad , 0 ] = sg0 sgrid [ 0 , crad ] = sg0 sgrid [ 2 * crad - 1 , crad ] = sgrid [ 2 * crad - 1 , crad ] - sg0 sgrid [ crad , 2 * crad - 1 ] = sgrid [ crad , 2 * crad - 1 ] - sg0 sgrid [ crad , 1 ] = sgrid [ crad , 1 ] - sg0 sgrid [ 1 , crad ] = sgrid [ 1 , crad + 1 ] - sg0 sgrid [ crad , crad ] = min ( sgrid [ crad , crad ], 1 ) h = sgrid / np . sum ( sgrid ) else : x , y , z = np . meshgrid ( np . arange ( - ceil ( r_x ), ceil ( r_x ) + 1 ), np . arange ( - ceil ( r_y ), ceil ( r_y ) + 1 ), np . arange ( - ceil ( r_z ), ceil ( r_z ) + 1 )) h = ( 1 - x ** 2 / r_x ** 2 - y ** 2 / r_y ** 2 - z ** 2 / r_z ** 2 ) >= 0 h = h / np . sum ( h ) return h periodic_line ( p , v ) Creates a flat structuring element containing 2*p+1 members. v is a two-element vector containing integer-valued row and column offsets. One structuring element member is located at the origin. The other members are located at 1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v . Copy of MATLAB strel('periodicline') . Source code in coppafish/utils/strel.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def periodic_line ( p : int , v : np . ndarray ) -> np . ndarray : \"\"\" Creates a flat structuring element containing `2*p+1` members. `v` is a two-element vector containing integer-valued row and column offsets. One structuring element member is located at the origin. The other members are located at `1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v`. Copy of MATLAB `strel('periodicline')`. \"\"\" pp = np . repeat ( np . arange ( - p , p + 1 ) . reshape ( - 1 , 1 ), 2 , axis = 1 ) rc = pp * v r = rc [:, 0 ] c = rc [:, 1 ] M = 2 * np . abs ( r ) . max () + 1 N = 2 * np . abs ( c ) . max () + 1 nhood = np . zeros (( M , N ), dtype = bool ) # idx = np.ravel_multi_index([r + np.abs(r).max(), c + np.abs(c).max()], (M, N)) nhood [ r + np . abs ( r ) . max (), c + np . abs ( c ) . max ()] = True return nhood . astype ( np . uint8 )","title":"Strel"},{"location":"code/utils/strel/#coppafish.utils.strel.annulus","text":"Gets structuring element used to assess if spot isolated. Parameters: Name Type Description Default r0 float Inner radius within which values are all zero. required r_xy float Outer radius in xy direction. Can be float not integer because all values with radius < r_xy1 and > r0 will be set to 1 . required r_z Optional [ float ] Outer radius in z direction. Size in z-pixels. None means 2D annulus returned. None Returns: Type Description np . ndarray int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1] . Structuring element with each element either 0 or 1 . Source code in coppafish/utils/strel.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def annulus ( r0 : float , r_xy : float , r_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Gets structuring element used to assess if spot isolated. Args: r0: Inner radius within which values are all zero. r_xy: Outer radius in xy direction. Can be float not integer because all values with `radius < r_xy1` and `> r0` will be set to `1`. r_z: Outer radius in z direction. Size in z-pixels. None means 2D annulus returned. Returns: `int [2*floor(r_xy1)+1, 2*floor(r_xy1)+1, 2*floor(r_z1)+1]`. Structuring element with each element either `0` or `1`. \"\"\" r_xy1_int = floor ( r_xy ) if r_z is None : y , x = np . meshgrid ( np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_xy1_int , r_xy1_int + 1 )) m = x ** 2 + y ** 2 else : r_z1_int = floor ( r_z ) y , x , z = np . meshgrid ( np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_xy1_int , r_xy1_int + 1 ), np . arange ( - r_z1_int , r_z1_int + 1 )) m = x ** 2 + y ** 2 + z ** 2 # only use upper radius in xy direction as z direction has different pixel size. annulus = r_xy ** 2 >= m annulus = np . logical_and ( annulus , m > r0 ** 2 ) return annulus . astype ( int )","title":"annulus()"},{"location":"code/utils/strel/#coppafish.utils.strel.disk","text":"Creates a flat disk-shaped structuring element with the specified radius, r . r must be a nonnegative integer. n must be 0, 4, 6, or 8 . When n is greater than 0 , the disk-shaped structuring element is approximated by a sequence of n (or sometimes n+2 ) periodic-line structuring elements. When n is 0 , no approximation is used, and the structuring element members comprise all pixels whose centers are no greater than r away from the origin. n can be omitted, in which case its default value is 4 . Note Morphological operations using disk approximations ( n>0 ) run much faster than when n=0 . Also, the structuring elements resulting from choosing n>0 are suitable for computing granulometries, which is not the case for vn=0 . Sometimes it is necessary for STREL to use two extra line structuring elements in the approximation, in which case the number of decomposed structuring elements used is n+2 . Copy of MATLAB strel('disk') . Source code in coppafish/utils/strel.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def disk ( r : int , n : int = 4 ) -> np . ndarray : \"\"\" Creates a flat disk-shaped structuring element with the specified radius, `r`. `r` must be a nonnegative integer. `n` must be `0, 4, 6, or 8`. When `n` is greater than `0`, the disk-shaped structuring element is approximated by a sequence of `n` (or sometimes `n+2`) periodic-line structuring elements. When `n` is `0`, no approximation is used, and the structuring element members comprise all pixels whose centers are no greater than `r` away from the origin. `n` can be omitted, in which case its default value is `4`. !!! note Morphological operations using disk approximations (`n>0`) run much faster than when `n=0`. Also, the structuring elements resulting from choosing `n>0` are suitable for computing granulometries, which is not the case for `vn=0`. Sometimes it is necessary for STREL to use two extra line structuring elements in the approximation, in which case the number of decomposed structuring elements used is `n+2`. Copy of MATLAB `strel('disk')`. \"\"\" if r < 3 : # Radius is too small to use decomposition, so force n=0. n = 0 if n == 0 : xx , yy = np . meshgrid ( np . arange ( - r , r + 1 ), np . arange ( - r , r + 1 )) nhood = xx ** 2 + yy ** 2 <= r ** 2 else : # Reference for radial decomposition of disks: Rolf Adams, \"Radial # Decomposition of Discs and Spheres,\" CVGIP: Graphical Models and # Image Processing, vol. 55, no. 5, September 1993, pp. 325-332. # # The specific decomposition technique used here is radial # decomposition using periodic lines. The reference is: Ronald # Jones and Pierre Soille, \"Periodic lines: Definition, cascades, and # application to granulometries,\" Pattern Recognition Letters, # vol. 17, 1996, pp. 1057-1063. # # Determine the set of \"basis\" vectors to be used for the # decomposition. The rows of v will be used as offset vectors for # periodic line strels. if n == 4 : v = np . array ([[ 1 , 0 ], [ 1 , 1 ], [ 0 , 1 ], [ - 1 , 1 ]]) elif n == 6 : v = np . array ([[ 1 , 0 ], [ 1 , 2 ], [ 2 , 1 ], [ 0 , 1 ], [ - 1 , 2 ], [ - 2 , 1 ]]) elif n == 8 : v = np . array ([[ 1 , 0 ], [ 2 , 1 ], [ 1 , 1 ], [ 1 , 2 ], [ 0 , 1 ], [ - 1 , 2 ], [ - 1 , 1 ], [ - 2 , 1 ]]) else : raise ValueError ( f 'Value of n provided ( { n } ) is not 0, 4, 6 or 8.' ) # Determine k, which is the desired radial extent of the periodic # line strels. For the origin of this formula, see the second # paragraph on page 328 of the Rolf Adams paper. theta = np . pi / ( 2 * n ) k = 2 * r / ( 1 / np . tan ( theta ) + 1 / np . sin ( theta )) # For each periodic line strel, determine the repetition parameter, # rp. The use of floor() in the computation means that the resulting # strel will be a little small, but we will compensate for this # below. nhood = np . ones (( 2 * r - 1 , 2 * r - 1 ), np . uint8 ) * - np . inf nhood [ int (( nhood . shape [ 0 ] - 1 ) / 2 ), int (( nhood . shape [ 0 ] - 1 ) / 2 )] = 1 for q in range ( n ): rp = int ( np . floor ( k / np . linalg . norm ( v [ q , :]))) decomposition = periodic_line ( rp , v [ q , :]) nhood = dilate ( nhood , decomposition ) nhood = nhood > 0 # Now we are going to add additional vertical and horizontal line # strels to compensate for the fact that the strel resulting from the # above decomposition tends to be smaller than the desired size. extra_strel_size = int ( sum ( np . sum ( nhood , axis = 1 ) == 0 ) + 1 ) if extra_strel_size > 0 : # Update the computed neighborhood to reflect the additional strels in # the decomposition. nhood = cv2 . dilate ( nhood . astype ( np . uint8 ), np . ones (( 1 , extra_strel_size ), dtype = np . uint8 )) nhood = cv2 . dilate ( nhood , np . ones (( extra_strel_size , 1 ), dtype = np . uint8 )) nhood = nhood > 0 return nhood . astype ( int )","title":"disk()"},{"location":"code/utils/strel/#coppafish.utils.strel.disk_3d","text":"Gets structuring element used to find spots when dilated with 3d image. Parameters: Name Type Description Default r_xy float Radius in xy direction. required r_z float Radius in z direction. required Returns: Type Description np . ndarray int [2*r_xy+1, 2*r_xy+1, 2*r_z+1] . Structuring element with each element either 0 or 1 . Source code in coppafish/utils/strel.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def disk_3d ( r_xy : float , r_z : float ) -> np . ndarray : \"\"\" Gets structuring element used to find spots when dilated with 3d image. Args: r_xy: Radius in xy direction. r_z: Radius in z direction. Returns: `int [2*r_xy+1, 2*r_xy+1, 2*r_z+1]`. Structuring element with each element either `0` or `1`. \"\"\" y , x , z = np . meshgrid ( np . arange ( - np . ceil ( r_xy ), np . ceil ( r_xy ) + 1 ), np . arange ( - np . ceil ( r_xy ), np . ceil ( r_xy ) + 1 ), np . arange ( - np . ceil ( r_z ), np . ceil ( r_z ) + 1 )) se = x ** 2 + y ** 2 + z ** 2 <= r_xy ** 2 # Crop se to remove zeros at extremities se = se [:, :, ~ np . all ( se == 0 , axis = ( 0 , 1 ))] se = se [:, ~ np . all ( se == 0 , axis = ( 0 , 2 )), :] se = se [ ~ np . all ( se == 0 , axis = ( 1 , 2 )), :, :] return se . astype ( int )","title":"disk_3d()"},{"location":"code/utils/strel/#coppafish.utils.strel.fspecial","text":"Creates an ellipsoidal 3D filter kernel if r_y , r_x and r_z given. Copy of MATlAB fspecial3('ellipsoid') . Creates a disk 2D filter kernel if just r_y given. Copy of MATlAB fspecial('disk') . Parameters: Name Type Description Default r_y float Radius in y direction or radius of disk if only parameter provided. required r_x Optional [ float ] Radius in x direction. None r_z Optional [ float ] Radius in z direction. None Returns: Type Description np . ndarray float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1] . Filtering kernel. Source code in coppafish/utils/strel.py 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def fspecial ( r_y : float , r_x : Optional [ float ] = None , r_z : Optional [ float ] = None ) -> np . ndarray : \"\"\" Creates an ellipsoidal 3D filter kernel if `r_y`, `r_x` and `r_z` given. Copy of MATlAB `fspecial3('ellipsoid')`. Creates a disk 2D filter kernel if just `r_y` given. Copy of MATlAB `fspecial('disk')`. Args: r_y: Radius in y direction or radius of disk if only parameter provided. r_x: Radius in x direction. r_z: Radius in z direction. Returns: `float [2*ceil(r_y)+1, 2*ceil(r_x)+1, 2*ceil(r_z)+1]`. Filtering kernel. \"\"\" if r_x is None and r_z is None : r = r_y crad = ceil ( r - 0.5 ) x , y = np . meshgrid ( np . arange ( - crad , crad + 1 ), np . arange ( - crad , crad + 1 )) max_xy = np . maximum ( np . abs ( x ), np . abs ( y )) min_xy = np . minimum ( np . abs ( x ), np . abs ( y )) m1 = ( r ** 2 < ( max_xy + 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 ) * ( min_xy - 0.5 ) + \\ ( r ** 2 >= ( max_xy + 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 ) * np . sqrt ( r ** 2 - ( max_xy + 0.5 ) ** 2 , dtype = np . complex_ ) m1 = np . real ( m1 ) m2 = ( r ** 2 > ( max_xy - 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) * ( min_xy + 0.5 ) + \\ ( r ** 2 <= ( max_xy - 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) * np . sqrt ( r ** 2 - ( max_xy - 0.5 ) ** 2 , dtype = np . complex_ ) m2 = np . real ( m2 ) sgrid = ( r ** 2 * ( 0.5 * ( np . arcsin ( m2 / r ) - np . arcsin ( m1 / r )) + 0.25 * ( np . sin ( 2 * np . arcsin ( m2 / r )) - np . sin ( 2 * np . arcsin ( m1 / r )))) - ( max_xy - 0.5 ) * ( m2 - m1 ) + ( m1 - min_xy + 0.5 )) * (((( r ** 2 < ( max_xy + 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 ) & ( r ** 2 > ( max_xy - 0.5 ) ** 2 + ( min_xy - 0.5 ) ** 2 )) | (( min_xy == 0 ) & ( max_xy - 0.5 < r ) & ( max_xy + 0.5 >= r )))) sgrid = sgrid + (( max_xy + 0.5 ) ** 2 + ( min_xy + 0.5 ) ** 2 < r ** 2 ) sgrid [ crad , crad ] = min ( np . pi * r ** 2 , np . pi / 2 ) if crad > 0.0 and r > crad - 0.5 and r ** 2 < ( crad - 0.5 ) ** 2 + 0.25 : m1 = np . sqrt ( r ** 2 - ( crad - 0.5 ) ** 2 ) m1n = m1 / r sg0 = 2 * ( r ** 2 * ( 0.5 * np . arcsin ( m1n ) + 0.25 * np . sin ( 2 * np . arcsin ( m1n ))) - m1 * ( crad - 0.5 )) sgrid [ 2 * crad , crad ] = sg0 sgrid [ crad , 2 * crad ] = sg0 sgrid [ crad , 0 ] = sg0 sgrid [ 0 , crad ] = sg0 sgrid [ 2 * crad - 1 , crad ] = sgrid [ 2 * crad - 1 , crad ] - sg0 sgrid [ crad , 2 * crad - 1 ] = sgrid [ crad , 2 * crad - 1 ] - sg0 sgrid [ crad , 1 ] = sgrid [ crad , 1 ] - sg0 sgrid [ 1 , crad ] = sgrid [ 1 , crad + 1 ] - sg0 sgrid [ crad , crad ] = min ( sgrid [ crad , crad ], 1 ) h = sgrid / np . sum ( sgrid ) else : x , y , z = np . meshgrid ( np . arange ( - ceil ( r_x ), ceil ( r_x ) + 1 ), np . arange ( - ceil ( r_y ), ceil ( r_y ) + 1 ), np . arange ( - ceil ( r_z ), ceil ( r_z ) + 1 )) h = ( 1 - x ** 2 / r_x ** 2 - y ** 2 / r_y ** 2 - z ** 2 / r_z ** 2 ) >= 0 h = h / np . sum ( h ) return h","title":"fspecial()"},{"location":"code/utils/strel/#coppafish.utils.strel.periodic_line","text":"Creates a flat structuring element containing 2*p+1 members. v is a two-element vector containing integer-valued row and column offsets. One structuring element member is located at the origin. The other members are located at 1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v . Copy of MATLAB strel('periodicline') . Source code in coppafish/utils/strel.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def periodic_line ( p : int , v : np . ndarray ) -> np . ndarray : \"\"\" Creates a flat structuring element containing `2*p+1` members. `v` is a two-element vector containing integer-valued row and column offsets. One structuring element member is located at the origin. The other members are located at `1*v, -1*v, 2*v, -2*v, ..., p*v, -p*v`. Copy of MATLAB `strel('periodicline')`. \"\"\" pp = np . repeat ( np . arange ( - p , p + 1 ) . reshape ( - 1 , 1 ), 2 , axis = 1 ) rc = pp * v r = rc [:, 0 ] c = rc [:, 1 ] M = 2 * np . abs ( r ) . max () + 1 N = 2 * np . abs ( c ) . max () + 1 nhood = np . zeros (( M , N ), dtype = bool ) # idx = np.ravel_multi_index([r + np.abs(r).max(), c + np.abs(c).max()], (M, N)) nhood [ r + np . abs ( r ) . max (), c + np . abs ( c ) . max ()] = True return nhood . astype ( np . uint8 )","title":"periodic_line()"},{"location":"pipeline/call_reference_spots/","text":"Call Reference Spots The call reference spots step of the pipeline uses the \\(n_{rounds} \\times n_{channels}\\) color obtained for each reference spot (detected on the reference round/reference channel, \\(r_{ref}\\) / \\(c_{ref}\\) ) in the get reference spots step of the pipeline to compute the bleed_matrix , accounting for crosstalk between color channels. We then compute the gene_efficiency which allows for varying round strengths for each gene, before assigning each reference spot to a gene. These gene assignments are saved in the ref_spots NotebookPage , while the bleed_matrix and expected bled_code for each gene are saved in the call_spots NotebookPage . The distribution of the genes can be seen using coppafish.Viewer once these pages have been added. Note: config in this section, with no section specified, means config['call_spots'] Re-run call_reference_spots To re-run the call reference spots step of the pipeline of the pipeline with different parameters in the configuration file, the call_spots page must be deleted and then the run_reference_spots function must be called with overwrite_ref_spots = True . This is so the variables gene_no , score , score_diff , intensity in the ref_spots page will be updated. Example The code below illustrates how you can re-run the call reference spots step of the pipeline step of the pipeline without weighting or gene_efficiency . Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini import numpy as np from coppafish import Notebook from coppafish.pipeline.run import run_reference_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) config = nb . get_config ()[ 'call_spots' ] print ( 'Using config file saved to notebook:' ) print ( f \"alpha: { config [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config [ 'gene_efficiency_n_iter' ] } \" ) print ( f \"First 5 gene assignments: { nb . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb . ref_spots . score [: 5 ], 3 ) } \" ) print ( f \"Bled Code of Gene 0, Round 0: { np . around ( nb . call_spots . bled_codes_ge [ 0 , 0 ], 2 ) } \" ) # Change call_spots del nb . call_spots # delete old call_spots nb . save ( nb_file_new ) # save Notebook with no call_spots page to new file # so does not overwrite old Notebook # Load in new notebook with new config file nb_new = Notebook ( nb_file_new , ini_file_new ) config_new = nb_new . get_config ()[ 'call_spots' ] # Show that config params have changed print ( f ' \\n Using new config file but before re-running:' ) print ( f \"alpha: { config_new [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config_new [ 'gene_efficiency_n_iter' ] } \" ) # Show that ref_spots page variables are still the same print ( f \"First 5 gene assignments: { nb_new . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb_new . ref_spots . score [: 5 ], 3 ) } \" ) # Get new call_spots page run_reference_spots ( nb_new , overwrite_ref_spots = True ) print ( f ' \\n Using new config file after re-running:' ) print ( f \"alpha: { config_new [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config_new [ 'gene_efficiency_n_iter' ] } \" ) # Show that ref_spots and call_spots page variables have been updated print ( f \"First 5 gene assignments: { nb_new . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb_new . ref_spots . score [: 5 ], 3 ) } \" ) print ( f \"Bled Code of Gene 0, Round 0: { np . around ( nb_new . call_spots . bled_codes_ge [ 0 , 0 ], 2 ) } \" ) Using config file saved to notebook: alpha: 120.0 gene_efficiency_n_iter: 10 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.645 0.806 0.702 0.692 0.796] Bled Code of Gene 0, Round 0: [ 0.04 0. 0. -0. 0. -0. -0. ] Using new config file but before re-running: alpha: 0.0 gene_efficiency_n_iter: 0 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.645 0.806 0.702 0.692 0.796] Using new config file after re-running: alpha: 0.0 gene_efficiency_n_iter: 0 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.65 0.776 0.744 0.65 0.797] Bled Code of Gene 0, Round 0: [ 0.25 0. 0.03 -0. 0.01 -0. -0. ] [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [call_spots] gene_efficiency_n_iter = 0 alpha = 0 Color Normalisation We assign a spot \\(s\\) to a gene \\(g\\) , based on its \\(n_{rounds} \\times n_{channels}\\) color , \\(\\pmb{\\acute{\\zeta}_s}\\) , indicating the intensity in each round and channel. However, we first need to equalize color channels, so that no one color channel dominates the others when it comes to gene assignment. The normalised spot_colors , \\(\\pmb{\\zeta}\\) , are obtained by dividing the saved spot_colors , \\(\\pmb{\\acute{\\zeta}}\\) , by a \\(n_{rounds} \\times n_{channels}\\) normalisation factor, nb.call_spots.color_norm_factor . Why do we need to normalise color channels? The gene assignment of the spot below indicates why we need to normalise color channels. With no normalisation, the gene assignment is overly influenced by channel 4, which is one of the most intense channels (see color_norm_factor in example box below). Thus it matches to Reln which appears in channel 4 in rounds 0, 1, 5 and 6. It also doesn't care at all about channel 2 because it is the weakest channel and Reln does not appear in channel 2 in any rounds. With normalisation, the most obvious effect is channel 2 gets boosted and now has an influence. Given that Id2 appears in channel 2 in rounds 1, 3, 5 and 6, if channel 2 was never considered, it would always make the score to Id2 low. When we include channel 2 though, we get a high score even though we are not matching the round 0, channel 4 intensity which contributed to the Reln assignment without normalisation. No Normalisation With Normalisation This is obtained using the parameters config['color_norm_intensities'] and config['color_norm_probs'] such that for each round, \\(r\\) , and channel \\(c\\) , the probability of \\(\\zeta_{rc}\\) being larger than config['color_norm_intensities'][i] is less than config['color_norm_probs'][i] for each \\(i\\) . The probabilities come from the histograms produced in the extract and filter step i.e. if config['color_norm_intensities'] = 0.5 and config['color_norm_probs'] = 0.01 , then in each round and channel, only 1% of pixels on the mid z-plane across all tiles would have \\(\\zeta_{rc} > 0.5\\) . If config[bleed_matrix_method] = 'single' , then we combine all rounds for each channel so that nb.call_spots.color_norm_factor[r, c] is the same for all \\(r\\) of a particular \\(c\\) . Example With config['color_norm_intensities'] = 0.5, 1, 5 and config['color_norm_probs'] = 0.01, 5e-4, 1e-5 and the histograms shown in the extract and filter step, the two methods of config[bleed_matrix_method] produce the following color_norm_factor : Single Separate The normalised histogram shown was normalised using the Single color_norm_factor and you can see that for each round and channel, there is a similar area under the curve (probability) beyond \\(\\zeta_{rc}=0.5\\) , as expected from config['color_norm_intensities'] . Background After we have the normalised spot colors, \\(\\pmb{\\zeta}\\) , we remove some background genes from them. There is one background gene for each channel, \\(\\pmb{B}_C\\) . The background gene for channel \\(C\\) is defined by: \\(\\pmb{B}_{C_{rc}}=1\\) if \\(c = C\\) and 0 otherwise i.e. it is just a strip in channel \\(C\\) : It is also normalised to have an L2 norm of 1. These are saved as nb.call_spots.background_codes . Why do we need to fit background genes ? We fit the background genes because it is fairly common for \\(\\pmb{\\zeta}_s\\) to have high intensity across all rounds of a channel as shown for the example view_codes plot below: No Background Removal With Background Removal No gene in the codebook looks that much like a background gene but if the background genes have not been fit, as with the first image above, spots like this will match to the gene which has the most rounds in the relavent channel/s. Here, Thsd7a has intensity in channel 2 in all rounds apart from round 2. This problem will be exacerbated in the omp step, because at each iteration of the OMP algorithm, it will just try and fit more and more genes to explain the intense channel. If we do remove background though, as with the second image, the gene assignment will be based on the other channels where not all rounds were intense. In this case, we get a match to Sst due to round 2 and 3 in channel 0. If we look at histogram_score for Thsd7a , we see that without background removal (purple), the peak in score is significantly larger: This indicates that without background removal, we would end up with a lot more spots assigned to genes like Thsd7a which have high intensity in most rounds of a single channel. The coefficient, \\(\\mu_{sC}\\) , for the channel \\(C\\) background gene , \\(\\pmb{B}_C\\) , fit to the spot \\(s\\) color, \\(\\pmb{\\zeta}_s\\) is: \\[ \\mu_{sC} = \\frac{\\sum_rw^2_{rC}\\zeta_{s_{rC}}B_{C_{rC}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}}; w_{rC} = \\frac{1}{|\\zeta_{s_{rC}}| + \\lambda_b} \\] Where, \\(\\lambda_b\\) is config['background_weight_shift'] and the sum is over all rounds used. Value of \\(\\lambda_b\\) If config['background_weight_shift'] is left blank, it is set to the median of the intensity computed from the absolute colors, \\(\\tilde{\\chi}\\) , of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). This is because it gives an estimate of what \\(|\\zeta_{s_{rC}}|\\) would be for an average non-spot pixel. If we set \\(\\lambda_b\\) to be equal to this, it is saying that if a spot had a round and channel with very low intensity, then that round and channel would have as much influence on the final coefficient as an average pixel. If \\(\\lambda_b = 0\\) , then \\(w_{rc}\\) would go to \\(\\infty\\) for low intensity rounds and channels, so the value of \\(\\lambda_b\\) chosen also provides an upper bound on the contribution of low intensity rounds and channels to the final coefficient. If the weighting, \\(\\pmb{w}\\) , was a constant across rounds and channels ( \\(\\lambda_b = \\infty\\) ), this would just be the least squares solution. After we have found the coefficients, we remove the background contribution from the spot colors to give \\(\\pmb{\\zeta}_{{s0}}\\) which we use from now on. \\[ \\zeta_{{s0}_{rC}} = \\zeta_{{s}_{rC}} - \\mu_{sC}B_{C_{rC}} \\] Why do we need a weighting? We find \\(\\mu_{sC}\\) via weighted least squares, because it limits the influence of outliers. The example below shows that with \\(\\lambda_b = \\infty\\) (normal least squares), it really tries to remove the outlier in round 4, channel 0. The result of this is that all the other rounds of channel 0 become very negative. \\(\\pmb{\\zeta}_s\\) \\(\\pmb{\\zeta}_{s0} (\\lambda_b = 0.08)\\) \\(\\pmb{\\zeta}_{s0} (\\lambda_b = \\infty)\\) This spot should be Slc6a1 as shown from the \\(\\lambda_b = 0.08\\) image, but Slc6a1 is expected to be in 4 of the 6 rounds that were set to negative by the background fitting with \\(\\lambda_b = \\infty\\) . Thus, this spot can no longer be assigned to the correct gene after \\(\\lambda_b = \\infty\\) background fitting. So basically, the job of the background fitting is to remove intensity in a particular channel if that channel is intense across all rounds. This is because there are no actual genes which can explain this. We do not want it to tone down outlier rounds and channels because outliers are usually due to actual genes. view_background The background coefficient calculation can be visualised by using the view_background function: For each plot, each row corresponds to a different background gene coefficient calculation i.e. a different channel. There is no overlap between the background codes hence we can view all the calculations at the same time. Round \\(r\\) , channel \\(c\\) in the Weighted Dot Product plot refers to the value of \\(\\frac{w^2_{rc}\\zeta_{s_{rc}}B_{C_{rc}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}}\\) . The Dot Product plot is the same as the Weighted Dot Product plot except \\(\\lambda_b = \\infty\\) . The Weighted Coef plot thus shows the coefficient computed for the current value of \\(\\lambda_b\\) (0.08 here, but can be specified in the textbox). The Coef plot shows the coefficient that would be computed with \\(\\lambda_b = \\infty\\) . The main difference between the two in this case is that the channel 0 coefficient is much larger for the \\(\\lambda_b = \\infty\\) case. This is because the Weight Squared , \\(\\Omega^2_{s_{rc}}\\) term acts to increase the contribution of the weak round 1, channel 0 and decrease the contribution of the strong rounds: 0, 2, 3 and 6. Bleed Matrix Crosstalk can occur between color channels. Some crosstalk may occur due to optical bleedthrough; additional crosstalk can occur due to chemical cross-reactivity of probes. The precise degree of crosstalk does not seem to vary much between sequencing rounds. It is therefore possible to largely compensate for this crosstalk by learning the precise amount of crosstalk between each pair of color channels. To estimate the crosstalk , we use the spot colors, \\(\\pmb{\\zeta}_{s0}\\) , of all well isolated spots. We reshape these, so we have a set of \\(n_{isolated} \\times n_{rounds}\\) vectors, each of dimension \\(n_{channels}\\) , \\(\\pmb{v}_i\\) ( \\(v_{0_c} = \\zeta_{{s=0,0}_{r=0,c}}\\) ). Only well-isolated spots are used to ensure that crosstalk estimation is not affected by spatial overlap of spots corresponding to different genes. Crosstalk is then estimated by running a scaled k-means algorithm on these vectors, which finds a set of \\(n_{dyes}\\) vectors, \\(\\pmb{c}_d\\) , such that the error function: \\[ \\sum_i\\min_{\\lambda_i, d(i)}|\\pmb{v}_i - \\lambda_i\\pmb{c}_{d(i)}|^2 \\] is minimized. In other words, it finds the \\(n_{dyes}\\) intensity vectors, \\(\\pmb{c}_d\\) , each of dimension \\(n_{channels}\\) , such that the spot color of each well isolated spot on every round is close to a scaled version of one of them. The \\(n_{dyes} \\times n_{channels}\\) array of dye vectors is termed the bleed matrix and is saved as nb.call_spots.bleed_matrix (a bleed_matrix is saved for each round, but if config['bleed_matrix_method'] = 'single' , it will be the same for each round). The view_bleed_matrix function can be used to show it: Single Bleed Matrix Separate Bleed Matrix For Each Round As shown in the second plot, if config['bleed_matrix_method'] = 'separate' , we compute a different bleed_matrix for each round - i.e. we loosen the assumption that crosstalk does not vary between sequencing rounds. Initial Bleed Matrix To estimate the dye intensity vectors, \\(\\pmb{c}_d\\) , the scaled_k_means algorithm needs to know the number of dyes and a starting guess for what each dye vector looks like. This is specified in the basic_info section of the configuration file as explained here . Scaled K Means The pseudocode for the scaled_k_means algorithm to obtain the dye intensity vectors, \\(\\pmb{c}_d\\) , is given below: v: Single round intensity vectors of well isolated spots [n_vectors x n_channels] c: Initial Guess of Bleed Matrix [n_dyes x n_channels] dye_ind_old: [n_vectors] array of zeros. v_norm = v normalised so each vector has an L2 norm of 1. Normalise c so that each dye vector has an L2 norm of 1. i = 0 while i < n_iter: score = dot product between each vector in v_norm and each dye in c. [n_vectors x n_dyes] top_score = highest score across dyes for each vector in v_norm. [n_vectors]. dye_ind = dye corresponding to top_score for each vector in v_norm. [n_vectors]. if dye_ind == dye_ind_old: Stop iteration because we have reached convergence i.e. i = n_iter dye_ind_old = dye_ind for d in range(n_dyes): v_use = all vectors in v with dye_ind = d and top_score > score_thresh. Use un-normalised as to avoid overweighting weak points. [n_use x n_channels] if n_use < min_size: c[d] = 0 else: Update c[d] to be top svd component of v_use i.e. v_matrix = v_use.transpose() @ v_use / n_use [n_channels x n_channels] c[d] = np.linalg.svd(v_matrix)[0][:, 0] [n_channels] i = i + 1 return c There are a few parameters in the config file which are used: bleed_matrix_n_iter : This is n_iter in the above code. bleed_matrix_min_cluster_size : This is min_size in the above code. bleed_matrix_score_thresh : This is score_thresh in the above code. bleed_matrix_anneal : If this is True , the scaled_k_means algorithm will be run twice. The second time will use \\(\\pmb{c}_d\\) returned from the first run as the starting point, and it will have a different score_thresh for each dye. score_thresh for dye \\(d\\) will be equal to the median of top_score[dye_ind == d] in the last iteration of the first run. The idea is that for the second run, we only use vectors which have a large score, to get a more accurate estimate. view_scaled_k_means The scaled_k_means algorithm can be visualised using the view_scaled_k_means function: In each column, in the top row, boxplot \\(d\\) is for top_score[dye_ind == d] (only showing scores above score_thresh - 0 for the first two columns). The dye vectors, \\(\\pmb{c}_d\\) , are indicated in the second row. The number of vectors assigned to each dye is indicated by the number within each boxplot. The first column is for the first iteration i.e. with the initial guess of the bleed matrix. The second column is after the first scaled_k_means algorithm has finished. The third column is after the second scaled_k_means algorithm has finished (only shown if bleed_matrix_anneal=True ). The bottom whisker of the boxplots in the third column indicate the score_thresh used for each dye. This is useful for debugging the bleed_matrix computation, as you want the boxplots to show high scores and for those scores to increase from left to right as the algorithm is run. Gene Bled Codes Once the bleed_matrix has been computed, the expected code for each gene can be obtained . Each gene appears with a single dye in each imaging round as indicated by the codebook and saved as nb.call_spots.gene_codes . bled_code[g, r, c] for gene \\(g\\) in round \\(r\\) , channel \\(c\\) is then given by bleed_matrix[r, c, gene_codes[g, r]] . If gene_codes[g, r] is outside nb.basic_info.use_dyes , then bled_code[g, r] will be set to 0. Each bled_code is also normalised to have an L2 norm of 1. They are saved as nb.call_spots.bled_codes . Example Using the Single Bleed Matrix shown as an example earlier , the bled_code for Kcnk2 with gene_code = 6304152 is: Dot Product Score To assign spot \\(s\\) , with spot color, \\(\\pmb{\\zeta}_{{si}}\\) , to a gene, we compute a dot product score, \\(\\Delta_{sig}\\) to each gene, \\(g\\) , with bled_code \\(\\pmb{b}_g\\) . This is defined to be: \\[ \\Delta_{sig} = \\sum_{r=0}^{n_r-1}\\sum_{c=0}^{n_c-1}\\omega^2_{{si}_{rc}}\\tilde{\\zeta}_{{si}_{rc}}b_{g_{rc}} \\] Where: \\[ \\tilde{\\zeta}_{{si}_{rc}} = \\frac{\\zeta_{{si}_{rc}}} {\\sqrt{\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\zeta^2_{{si}_{\\mathscr{rc}}}} + \\lambda_d} \\] \\[ \\sigma^2_{{si}_{rc}} = \\beta^2 + \\alpha\\sum_g\\mu^2_{sig}b^2_{g_{rc}} \\] \\[ \\omega^2_{{si}_{rc}} = n_rn_c\\frac{\\sigma^{-2}_{{si}_{rc}}} {\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\sigma^{-2}_{{si}_{\\mathscr{rc}}}} \\] \\(n_{r}\\) is the number of rounds. \\(n_{c}\\) is the number of channels. \\(\\lambda_d\\) is config['dp_norm_shift'] * sqrt(n_rounds) (typical config['dp_norm_shift'] is 0.1). \\(\\alpha\\) is config['alpha'] ( default is 120 ). \\(\\beta\\) is config['beta'] ( default is 1 ). The sum over genes, \\(\\sum_g\\) , is over all real and background genes i.e. \\(\\sum_{g=0}^{n_g+n_c-1}\\) . \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i\\) and \\(\\pmb{b}_{g=n_g+C} = \\pmb{B}_C\\) where \\(\\mu_{sC}\\) and \\(\\pmb{B}_C\\) were introduced in the background section. \\(i\\) refers to the number of actual genes fit prior to this iteration of OMP . Here, because we are only fitting one gene, \\(i=0\\) , meaning only background has been fit ( \\(\\sum_{g=0}^{n_g-1}\\mu^2_{sig}b^2_{g_{rc}}=0\\) ). So if \\(\\lambda_d = 0\\) and \\(\\pmb{\\omega}^2_{si}=1\\) for each round and channel (achieved through \\(\\alpha=0\\) ), then this would just be the normal dot product between two vectors with L2 norm of one. The min value is 0 and max value is 1. The purpose of the weighting, \\(\\pmb{\\omega}^2_{{si}}\\) , is to decrease the contribution of rounds/channels which already have a gene in. It is really more relevant to the OMP algorithm . It can be turned off in this part of the pipeline by setting config['alpha'] = 0 . The \\(n_rn_c\\) term at the start of the \\(\\omega^2_{{si}_{rc}}\\) equation is a normalisation term such that the max possible value of \\(\\Delta_{sig}\\) is approximately 1 (it can be more though). Value of \\(\\lambda_d\\) If in the \\(\\Delta_{sig}\\) equation, we used \\(\\pmb{\\zeta}_{{si}}\\) instead of \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\) , then the max value would no longer have an upper limit and a high score could be achieved by having large values of \\(\\pmb{\\zeta}_{{si}}\\) in some rounds and channels as well as by having \\(\\pmb{\\zeta}_{{si}}\\) being similar to \\(\\pmb{b}_g\\) . We use the intensity value to indicate the strength of the spot, so for \\(\\Delta_{sig}\\) , we really just want a variable which indicates how alike the spot color is to the gene, indepenent of strength. Hence, we use \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\) . If \\(\\lambda_d = 0\\) , it would mean that even background pixels with very small intensity could get a high score. So, we use a non-zero value of \\(\\lambda_d\\) to prevent very weak spots getting a large \\(\\Delta_{sig}\\) . If config['dp_norm_shift'] is not specified, it is set to the median of the L2 norm in a single round computed from the colors of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). The idea behind this, is that the L2 norm of an average background pixel would be config['dp_norm_shift'] * sqrt(n_rounds) . So if \\(\\lambda_d\\) was set to this, it is giving a penalty to any spot which is less intense than the average background pixel. This is desirable since any pixels of such low intensity are unlikely to be spots. The spot \\(s\\) , is assigned to the gene \\(g\\) , for which \\(\\Delta_{sig}\\) is the largest. view_score How the various parameters in the dot product score calculation affect the final value can be investigated, for a single spot, through the function view_score : view_score view_weight The top left plot shows the spot color prior to any removal of genes or background. The bottom left plot shows the spot color after all genes fit prior to the current iteration have been removed (just background for iteration 0). The top right plot shows the dot product score obtained without weighting ( \\(\\alpha=0\\) ). The bottom right plot shows the actual score obtained with the current weighting parameters. To get to the gene with the highest dot product score for the current iteration, you can enter an impossible value in the Gene textbox. As well as typing the index of the gene, you can also type in the gene name to look at the calculation for a specific gene. Clicking on the Weight Squared plot shows the view_weight plot indicating how it is calculated (second image above). This is more useful for iterations other than 0 . Looking at the view_weight image and the far right plots of the view_score image, we see that the effect of the weighting is to down-weight color channel 0 because this is where the background coefficient is the largest. The channels with a smaller background coefficient (1, 5 and 6) then have a weighting greater than 1. Thus, the weighted score is greater than the non-weighted one because channel 6, where this spot is particularly strong has a greater contribution. I.e. because the intensity in channel 6 cannot be explained by background genes, but it can be explained by Plp1, we boost the score. For most spots, the background coefficients are very small and so the weighting has little effect. Using the histogram_score function, we see that the effect of weighting (blue) is to add a tail of scores greater than 1 for spots where an increased contribution is given to the rounds/channels where they are most intense. The mode score does not change though: Gene Efficiency Once we have a score and gene assigned to each spot, we can update the bled_codes for each gene, \\(\\pmb{b}_g\\) based on all the spot colors assigned to them, \\(\\pmb{\\zeta}_{{s0}}\\) . We do this by determining nb.call_spots.gene_efficiency . gene_efficiency[g, r] gives the expected intensity of gene \\(g\\) in round \\(r\\) , as determined by the spots assigned to it, compared to that expected by the bleed_matrix . The pseudocode below explains how it is computed . spot_colors: Intensity of each spot in each channel [n_spots x n_rounds x n_channels] spot_gene_no: Gene each spot was assigned to. [n_spots] bm: Bleed Matrix, indicates expected intensity of each dye in each round and channel. [n_rounds x n_channels x n_dyes] gene_codes: Indicates dye each gene should appear with in each round. [n_genes x n_rounds] for g in range(n_genes): Only use spots assigned to current gene. use = spot_gene_no == g for r in use_rounds: Get bleed matrix prediction for strength of gene g in round r. bm_pred = bm[r, :, gene_codes[g, r]] [n_channels] Get spot colors for this round. spot_colors_r = spot_colors[use, r] [n_use x n_channels] For each spot, s, find the least squares coefficient, coef, such that spot_colors_r[s] = coef * bm_pred Store coef for each spot and round as spot_round_strength [n_use x n_rounds] for r in use_rounds: av_round_strength[r] = median(spot_round_strength[:, r]) Find av_round which is the round such that av_round_strength[av_round] is the closest to median(av_round_strength). Update spot_round_strength to only use spots with positive strength in av_round. keep = spot_round_strength[:, av_round] > 0 spot_round_strength = spot_round_strength[keep] [n_use2 x n_rounds] For each spot, determine the strength of each round relative to av_round. for s in range(n_use2): for r in use_rounds: relative_round_strength[s, r] = spot_round_strength[s, r] / spot_round_strength[s, av_round] Update relative_round_strength based on maximum value. max_round_strength is max of relative_round_strength for each spot across rounds [n_use2]. keep = max_round_strength < max_thresh relative_round_strength = relative_round_strength[keep] [n_use3 x n_rounds] Update relative_round_strength based on low values. Count number of rounds for each spot below min_thresh. for s in range(n_use3): n_min[s] = sum(relative_round_strength[s] < min_thresh) keep = n_min <= n_min_thresh relative_round_strength = relative_round_strength[keep] [n_use4 x n_rounds] for r in use_rounds: if n_use4 > min_spots: gene_efficiency[g, r] = median(relative_round_strength[:, r]) else: Not enought spots to compute gene efficiency so just set to 1 in every round. gene_efficiency[g, r] = 1 Clip negative gene efficiency at 0. gene_efficiency[gene_efficiency < 0] = 0 return gene_efficiency There are a few parameters in the configuration file which are used: gene_efficiency_max : This is max_thresh in the above code. gene_efficiency_min : This is min_thresh in the above code. gene_efficiency_min_factor : n_min_thresh in the above code is set to ceil(gene_efficiency_min_factor * n_rounds) . gene_efficiency_min_spots : This is min_spots in the above code. In the gene_efficiency calculation, we computed the strength of each spot relative to av_round because, as with the bleed_matrix calculation, we expect each spot color to be a scaled version of one of the bled_codes . So we are trying to find out, once a spot color has been normalised such that its strength in av_round is 1, what is the corresponding strength in the other rounds. We do this normalisation relative to the average round so that half the gene_efficiency values will be more than 1 and half less than 1 for each gene. For gene \\(g\\) , one value of gene_efficiency[g] will be 1, corresponding to av_round but this round will be different for each gene. Why do we need gene_efficiency ? We need gene_efficiency because there is a high variance in the strength with which each gene appears in each round. For example, in the the bled_code plot below, we see that the effect of incorporating gene_efficiency is to reduce the strength of rounds 0, 5 and 6 while boosting rounds 2 and 3. Note In the example below, it seems that rounds corresponding to the same dye (0 and 5; 1 and 4; 2 and 3) have similar strengths, so it may be that different dyes (instead of rounds) have different strengths for different genes. bled_code histogram The histogram plot above then shows that when gene efficiency is included (blue line), the score distribution is shifted considerably. This indicates that gene efficiency is required to truly capture what spot colors corresponding to Serpini1 look like. The histogram_score plot combining all genes, also shows a shift in the peak of the distribution towards higher scores when gene efficiency is included: Spots used Because we use the gene_efficiency to update the bled_codes , we only want to use spots, which we are fairly certain have been assigned to the correct gene. Thus, only spots which satisfy all the following are used in the gene_efficiency calculation: Like with the scaled_k_means calculation, only spots identified as isolated in the find spots step of the pipeline are used. The dot product score to the best gene, \\(g_0\\) , \\(\\Delta_{s0g_0}\\) must exceed config['gene_efficiency_score_thresh'] . The difference between the dot product score to the best gene, \\(g_0\\) , and the second best gene, \\(g_1\\) : \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) must exceed config['gene_efficiency_score_diff_thresh'] . The intensity , \\(\\chi_s\\) , must exceed config['gene_efficiency_intensity_thresh'] . Value of config['gene_efficiency_intensity_thresh'] If config['gene_efficiency_intensity_thresh'] is not specified, it is set to the percentile indicated by config['gene_efficiency_intensity_thresh_percentile'] of the intensity computed from the colors of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). It is then clipped to be between config[gene_efficiency_intensity_thresh_min] and config[gene_efficiency_intensity_thresh_max] . The idea is that this is quite a low threshold (default percentile is 37), just ensuring that the intensity is not amongst the weakest background pixels. If the intensity threshold was too high, we would end up losing spots which look a lot like genes just because they are weak. But if it was too low, we would identify some background pixels as genes. Updating bled_codes Once the gene_efficiency has been computed, the bled_codes can be updated: bled_codes: Those computed from the bleed_matrix [n_genes x n_rounds x n_channels]. gene_efficiency: [n_genes x n_rounds] for g in range (n_genes): for r in use_rounds: for c in use_channels: bled_codes[g, r, c] = bled_codes[g, r, c] * gene_efficiency[g, r] Normalise bled_codes[g] so it has an L2 norm of 1. We then re-compute the dot product score and gene assignment for each spot with the new bled_codes . We continue this process of computing the gene efficiency, updating the dot product score until the same spots have been used to compute the gene efficiency in two subsequent iterations or until config[gene_efficiency_n_iter] iterations have been run. The bled_codes computed from the final iteration will be saved as nb.call_spots.bled_codes_ge . This will be the same as nb.call_spots.bled_codes if config[gene_efficiency_n_iter] = 0 . These are the ones used to compute dot product score to the best gene, \\(g_0\\) , \\(\\Delta_{s0g_0}\\) . These are saved as nb.ref_spots.gene_no and nb.ref_spots.score respectively. The difference between the dot product score to the best gene, \\(g_0\\) , and the second best gene, \\(g_1\\) : \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) is saved as nb.ref_spots.score_diff . Intensity As well as a variable indicating how closely a spot matches a gene ( nb.ref_spots.score ), we also save a variable indicating the overall fluorescence of a spot, independent of which gene it belongs to. This intensity, \\(\\chi\\) , is saved as nb.ref_spots.intensity and for a spot \\(s\\) , it is defined by: \\[ \\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}}) \\] I.e. for each round, we take the max color across channels to give a set of \\(n_{rounds}\\) values. We then take the median of these. The logic behind this is that if the spot is actually a gene, then there should be at least one channel in every round which is intense, because the relevant dye shows up in it. If the spot was not actually a gene though, you would expect all channels in any given round to be similarly weakly intense and thus the max over channels would give a low value. view_intensity The intensity calculation can be visualised with the view_intensity function: \\(\\chi_s = 0.542\\) for this example spot, which is the median of all the values shown with a green border. Diagnostics As well as view_background , view_scaled_k_means , view_score and view_intensity , there are a few other functions using matplotlib which may help to debug this section of the pipeline. histogram_score This shows the histogram of the dot product score , \\(\\Delta_s\\) , assigned to every reference spot: Dot Product Score All Plots This is useful for checking how well the gene assignment worked. The higher the score where the distribution peaks, the better. Certainly, if the peak is around 0.8, as with this example, then it probably worked well. The Dot Product Score image above is showing the histogram of nb.ref_spots.score , but there are 4 other plots which can be selected, as shown in the All Plots image above: No Weighting : This is the score that would be computed if \\(\\alpha=0\\) in the dot product score calculation . The max possible score in this case is 1. No Background : This is the score that would be computed if the background genes were not removed before determining the score. This also has no weighting because in the dot product calculation, \\(\\omega^2_{si_{rc}} = 1\\) if no background has been fitted. Hence, the max score is 1 as with No Weighting . No Gene Efficiency : This is the score that would be computed if the nb.call_spots.bled_codes were used instead of nb.call_spots.bled_codes_ge . \\(\\omega^2_{si_{rc}} \\neq 1\\) here so the max score is over 1. No Background / No Gene Efficiency : This is the score that would be computed if the background genes were not removed before determining the score and if nb.call_spots.bled_codes were used instead of nb.call_spots.bled_codes_ge . The max score is 1 in this case. The Gene textbox can also be used to view the histogram of a single gene. Either the index of the gene or the gene name can be entered. To go back to viewing all genes, type in all into the textbox. The Histogram Spacing textbox can be used to change the bin size of the histogram. gene_counts This plot indicates the number of spots assigned to each gene which also have nb.ref_spots.score > score_thresh and nb.ref_spots.intensity > intensity_thresh . The default score_thresh and intensity_thresh are config['thresholds']['score_ref'] and config['thresholds']['intensity'] respectively. They can be changed with the textboxes though. This thresholding is the same that is done in the results Viewer and when exporting to pciSeq . Gene Counts Gene Counts with Fake Genes There is also a second Ref Spots - Fake Genes plot which can be shown in yellow. This shows the results of the gene assignment if we added some fake bled_codes as well as the ones corresponding to genes. The idea is to choose fake bled_codes which are well separated from the actual bled_codes . If spots then match to these fake genes, then it probably means the initial gene assignment is not reliable. The fake bled_codes can be specified, but by default there is one fake bled_code added for each round, \\(r\\) , and channel \\(c\\) , which is 1 in round \\(r\\) , channel \\(c\\) and 0 everywhere else. In the second image above, we see that there is not much change in the gene counts when we add the fake genes, indicating the initial assignment is probably reliable. Example Dataset with lots of Fake Genes The example below indicates a case where the fake genes functionality may be useful. When we open coppafish.Viewer , we see that there seems to be too many spots assigned to Penk and Vip . coppafish.Viewer gene_counts Penk Vip If we then look at the gene_counts , we see that when we include fake genes, the number of spots assigned to Penk and Vip decreases drastically because they have been assigned to the \\(r0c18\\) fake gene. When we look at the Penk and Vip bled_codes , we see that they are very intense in round 0, channel 18. So most spots seem to only have been assigned to these genes on the basis of this one round and channel. view_bleed_matrix This function is useful for seeing if the dye vectors in the bleed_matrix are easily distinguished. view_bled_codes This function is useful for seeing how the gene_efficiency affected the bled_codes . view_codes This function is useful for seeing how a particular spot matches the gene it was assigned to. view_spot This function is useful for seeing if the neighbourhood of a particular spot has high intensity in all rounds/channels where the gene it was assigned, expects it to. Psuedocode This is the pseudocode outlining the basics of this step of the pipeline . There is more detailed pseudocode about how the bleed_matrix and gene_efficiency are found. Determine color_norm_factor from nb.extract.hist_counts [n_rounds x n_channels] Load in pixel colors of all pixel of middle z-plane of central tile. Use these to determine the following if not provided in the config file: - nb.call_spots.background_weight_shift - nb.call_spots.dp_norm_shift - nb.call_spots.gene_efficiency_intensity_thresh - nb.call_spots.abs_intensity_percentile Normalise reference spot colors spot_colors = nb.ref_spots.colors / color_norm_factor [n_spots x n_rounds x n_channels] Compute Spot Intensity (nb.ref_spots.intensity) Remove Background from spot_colors Compute Bleed Matrix (nb.call_spots.bleed_matrix) Compute Bled Codes (nb.call_spots.bled_codes) use_ge_last = array of length n_spots where all values are False. i = 0 while i < gene_efficiency_n_iter: Determine all_scores, the dot product score of each spot to each bled_code. [n_spots x n_genes] Determine gene_no, the gene for which all_scores is the greatest for each spot. [n_spots] Determine score, the score in all_scores, corresponding to gene_no for each spot. [n_spots] Determine score_diff, the difference between score and the second largest value in all_scores for each spot. [n_spots] Determine whether each spot was used for gene efficiency calculation. use_ge = score > ge_score_thresh and score_diff > ge_score_diff_thresh and intensity > ge_intensity_thresh and nb.ref_spots.isolated. [n_spots] Compute gene_efficiency with spots indicated by use_ge Update bled_codes based on gene_efficiency If use_ge == use_ge_last: End iteration i.e. i = gene_efficiency_n_iter use_ge_last = use_ge i += 1 Save final bled_codes as nb.call_spots.bled_codes_ge Save final gene_efficiency as nb.call_spots.gene_efficiency Save final gene_no as nb.ref_spots.gene_no Save final score as nb.ref_spots.score Save final score_diff as nb.ref_spots.score_diff","title":"Call Reference Spots"},{"location":"pipeline/call_reference_spots/#call-reference-spots","text":"The call reference spots step of the pipeline uses the \\(n_{rounds} \\times n_{channels}\\) color obtained for each reference spot (detected on the reference round/reference channel, \\(r_{ref}\\) / \\(c_{ref}\\) ) in the get reference spots step of the pipeline to compute the bleed_matrix , accounting for crosstalk between color channels. We then compute the gene_efficiency which allows for varying round strengths for each gene, before assigning each reference spot to a gene. These gene assignments are saved in the ref_spots NotebookPage , while the bleed_matrix and expected bled_code for each gene are saved in the call_spots NotebookPage . The distribution of the genes can be seen using coppafish.Viewer once these pages have been added. Note: config in this section, with no section specified, means config['call_spots']","title":"Call Reference Spots"},{"location":"pipeline/call_reference_spots/#re-run-call_reference_spots","text":"To re-run the call reference spots step of the pipeline of the pipeline with different parameters in the configuration file, the call_spots page must be deleted and then the run_reference_spots function must be called with overwrite_ref_spots = True . This is so the variables gene_no , score , score_diff , intensity in the ref_spots page will be updated. Example The code below illustrates how you can re-run the call reference spots step of the pipeline step of the pipeline without weighting or gene_efficiency . Code Output nb._config (saved to Notebook ) /Users/user/coppafish/experiment/settings_new.ini import numpy as np from coppafish import Notebook from coppafish.pipeline.run import run_reference_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' # Save new notebook with different name so it does not overwrite old notebook # Make sure notebook_name is specified in [file_names] section # of settings_new.ini file to be same as name given here. nb_file_new = '/Users/user/coppafish/experiment/notebook_new.npz' ini_file_new = '/Users/user/coppafish/experiment/settings_new.ini' # config_file not given so will use last one saved to Notebook nb = Notebook ( nb_file ) config = nb . get_config ()[ 'call_spots' ] print ( 'Using config file saved to notebook:' ) print ( f \"alpha: { config [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config [ 'gene_efficiency_n_iter' ] } \" ) print ( f \"First 5 gene assignments: { nb . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb . ref_spots . score [: 5 ], 3 ) } \" ) print ( f \"Bled Code of Gene 0, Round 0: { np . around ( nb . call_spots . bled_codes_ge [ 0 , 0 ], 2 ) } \" ) # Change call_spots del nb . call_spots # delete old call_spots nb . save ( nb_file_new ) # save Notebook with no call_spots page to new file # so does not overwrite old Notebook # Load in new notebook with new config file nb_new = Notebook ( nb_file_new , ini_file_new ) config_new = nb_new . get_config ()[ 'call_spots' ] # Show that config params have changed print ( f ' \\n Using new config file but before re-running:' ) print ( f \"alpha: { config_new [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config_new [ 'gene_efficiency_n_iter' ] } \" ) # Show that ref_spots page variables are still the same print ( f \"First 5 gene assignments: { nb_new . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb_new . ref_spots . score [: 5 ], 3 ) } \" ) # Get new call_spots page run_reference_spots ( nb_new , overwrite_ref_spots = True ) print ( f ' \\n Using new config file after re-running:' ) print ( f \"alpha: { config_new [ 'alpha' ] } \" ) print ( f \"gene_efficiency_n_iter: { config_new [ 'gene_efficiency_n_iter' ] } \" ) # Show that ref_spots and call_spots page variables have been updated print ( f \"First 5 gene assignments: { nb_new . ref_spots . gene_no [: 5 ] } \" ) print ( f \"First 5 spot scores: { np . around ( nb_new . ref_spots . score [: 5 ], 3 ) } \" ) print ( f \"Bled Code of Gene 0, Round 0: { np . around ( nb_new . call_spots . bled_codes_ge [ 0 , 0 ], 2 ) } \" ) Using config file saved to notebook: alpha: 120.0 gene_efficiency_n_iter: 10 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.645 0.806 0.702 0.692 0.796] Bled Code of Gene 0, Round 0: [ 0.04 0. 0. -0. 0. -0. -0. ] Using new config file but before re-running: alpha: 0.0 gene_efficiency_n_iter: 0 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.645 0.806 0.702 0.692 0.796] Using new config file after re-running: alpha: 0.0 gene_efficiency_n_iter: 0 First 5 gene assignments: [17 17 17 13 17] First 5 spot scores: [0.65 0.776 0.744 0.65 0.797] Bled Code of Gene 0, Round 0: [ 0.25 0. 0.03 -0. 0.01 -0. -0. ] [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [file_names] input_dir = /Users/user/coppafish/experiment1/raw output_dir = /Users/user/coppafish/experiment1/output tile_dir = /Users/user/coppafish/experiment1/tiles round = Exp1_r0, Exp1_r1, Exp1_r2, Exp1_r3, Exp1_r4, Exp1_r5, Exp1_r6 anchor = Exp1_anchor code_book = /Users/user/coppafish/experiment1/codebook.txt notebook_name = notebook_new [basic_info] is_3d = True anchor_channel = 4 dapi_channel = 0 [call_spots] gene_efficiency_n_iter = 0 alpha = 0","title":"Re-run call_reference_spots"},{"location":"pipeline/call_reference_spots/#color-normalisation","text":"We assign a spot \\(s\\) to a gene \\(g\\) , based on its \\(n_{rounds} \\times n_{channels}\\) color , \\(\\pmb{\\acute{\\zeta}_s}\\) , indicating the intensity in each round and channel. However, we first need to equalize color channels, so that no one color channel dominates the others when it comes to gene assignment. The normalised spot_colors , \\(\\pmb{\\zeta}\\) , are obtained by dividing the saved spot_colors , \\(\\pmb{\\acute{\\zeta}}\\) , by a \\(n_{rounds} \\times n_{channels}\\) normalisation factor, nb.call_spots.color_norm_factor . Why do we need to normalise color channels? The gene assignment of the spot below indicates why we need to normalise color channels. With no normalisation, the gene assignment is overly influenced by channel 4, which is one of the most intense channels (see color_norm_factor in example box below). Thus it matches to Reln which appears in channel 4 in rounds 0, 1, 5 and 6. It also doesn't care at all about channel 2 because it is the weakest channel and Reln does not appear in channel 2 in any rounds. With normalisation, the most obvious effect is channel 2 gets boosted and now has an influence. Given that Id2 appears in channel 2 in rounds 1, 3, 5 and 6, if channel 2 was never considered, it would always make the score to Id2 low. When we include channel 2 though, we get a high score even though we are not matching the round 0, channel 4 intensity which contributed to the Reln assignment without normalisation. No Normalisation With Normalisation This is obtained using the parameters config['color_norm_intensities'] and config['color_norm_probs'] such that for each round, \\(r\\) , and channel \\(c\\) , the probability of \\(\\zeta_{rc}\\) being larger than config['color_norm_intensities'][i] is less than config['color_norm_probs'][i] for each \\(i\\) . The probabilities come from the histograms produced in the extract and filter step i.e. if config['color_norm_intensities'] = 0.5 and config['color_norm_probs'] = 0.01 , then in each round and channel, only 1% of pixels on the mid z-plane across all tiles would have \\(\\zeta_{rc} > 0.5\\) . If config[bleed_matrix_method] = 'single' , then we combine all rounds for each channel so that nb.call_spots.color_norm_factor[r, c] is the same for all \\(r\\) of a particular \\(c\\) . Example With config['color_norm_intensities'] = 0.5, 1, 5 and config['color_norm_probs'] = 0.01, 5e-4, 1e-5 and the histograms shown in the extract and filter step, the two methods of config[bleed_matrix_method] produce the following color_norm_factor : Single Separate The normalised histogram shown was normalised using the Single color_norm_factor and you can see that for each round and channel, there is a similar area under the curve (probability) beyond \\(\\zeta_{rc}=0.5\\) , as expected from config['color_norm_intensities'] .","title":"Color Normalisation"},{"location":"pipeline/call_reference_spots/#background","text":"After we have the normalised spot colors, \\(\\pmb{\\zeta}\\) , we remove some background genes from them. There is one background gene for each channel, \\(\\pmb{B}_C\\) . The background gene for channel \\(C\\) is defined by: \\(\\pmb{B}_{C_{rc}}=1\\) if \\(c = C\\) and 0 otherwise i.e. it is just a strip in channel \\(C\\) : It is also normalised to have an L2 norm of 1. These are saved as nb.call_spots.background_codes . Why do we need to fit background genes ? We fit the background genes because it is fairly common for \\(\\pmb{\\zeta}_s\\) to have high intensity across all rounds of a channel as shown for the example view_codes plot below: No Background Removal With Background Removal No gene in the codebook looks that much like a background gene but if the background genes have not been fit, as with the first image above, spots like this will match to the gene which has the most rounds in the relavent channel/s. Here, Thsd7a has intensity in channel 2 in all rounds apart from round 2. This problem will be exacerbated in the omp step, because at each iteration of the OMP algorithm, it will just try and fit more and more genes to explain the intense channel. If we do remove background though, as with the second image, the gene assignment will be based on the other channels where not all rounds were intense. In this case, we get a match to Sst due to round 2 and 3 in channel 0. If we look at histogram_score for Thsd7a , we see that without background removal (purple), the peak in score is significantly larger: This indicates that without background removal, we would end up with a lot more spots assigned to genes like Thsd7a which have high intensity in most rounds of a single channel. The coefficient, \\(\\mu_{sC}\\) , for the channel \\(C\\) background gene , \\(\\pmb{B}_C\\) , fit to the spot \\(s\\) color, \\(\\pmb{\\zeta}_s\\) is: \\[ \\mu_{sC} = \\frac{\\sum_rw^2_{rC}\\zeta_{s_{rC}}B_{C_{rC}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}}; w_{rC} = \\frac{1}{|\\zeta_{s_{rC}}| + \\lambda_b} \\] Where, \\(\\lambda_b\\) is config['background_weight_shift'] and the sum is over all rounds used. Value of \\(\\lambda_b\\) If config['background_weight_shift'] is left blank, it is set to the median of the intensity computed from the absolute colors, \\(\\tilde{\\chi}\\) , of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). This is because it gives an estimate of what \\(|\\zeta_{s_{rC}}|\\) would be for an average non-spot pixel. If we set \\(\\lambda_b\\) to be equal to this, it is saying that if a spot had a round and channel with very low intensity, then that round and channel would have as much influence on the final coefficient as an average pixel. If \\(\\lambda_b = 0\\) , then \\(w_{rc}\\) would go to \\(\\infty\\) for low intensity rounds and channels, so the value of \\(\\lambda_b\\) chosen also provides an upper bound on the contribution of low intensity rounds and channels to the final coefficient. If the weighting, \\(\\pmb{w}\\) , was a constant across rounds and channels ( \\(\\lambda_b = \\infty\\) ), this would just be the least squares solution. After we have found the coefficients, we remove the background contribution from the spot colors to give \\(\\pmb{\\zeta}_{{s0}}\\) which we use from now on. \\[ \\zeta_{{s0}_{rC}} = \\zeta_{{s}_{rC}} - \\mu_{sC}B_{C_{rC}} \\] Why do we need a weighting? We find \\(\\mu_{sC}\\) via weighted least squares, because it limits the influence of outliers. The example below shows that with \\(\\lambda_b = \\infty\\) (normal least squares), it really tries to remove the outlier in round 4, channel 0. The result of this is that all the other rounds of channel 0 become very negative. \\(\\pmb{\\zeta}_s\\) \\(\\pmb{\\zeta}_{s0} (\\lambda_b = 0.08)\\) \\(\\pmb{\\zeta}_{s0} (\\lambda_b = \\infty)\\) This spot should be Slc6a1 as shown from the \\(\\lambda_b = 0.08\\) image, but Slc6a1 is expected to be in 4 of the 6 rounds that were set to negative by the background fitting with \\(\\lambda_b = \\infty\\) . Thus, this spot can no longer be assigned to the correct gene after \\(\\lambda_b = \\infty\\) background fitting. So basically, the job of the background fitting is to remove intensity in a particular channel if that channel is intense across all rounds. This is because there are no actual genes which can explain this. We do not want it to tone down outlier rounds and channels because outliers are usually due to actual genes.","title":"Background"},{"location":"pipeline/call_reference_spots/#view_background","text":"The background coefficient calculation can be visualised by using the view_background function: For each plot, each row corresponds to a different background gene coefficient calculation i.e. a different channel. There is no overlap between the background codes hence we can view all the calculations at the same time. Round \\(r\\) , channel \\(c\\) in the Weighted Dot Product plot refers to the value of \\(\\frac{w^2_{rc}\\zeta_{s_{rc}}B_{C_{rc}}}{\\sum_rw^2_{rC}B^2_{C_{rC}}}\\) . The Dot Product plot is the same as the Weighted Dot Product plot except \\(\\lambda_b = \\infty\\) . The Weighted Coef plot thus shows the coefficient computed for the current value of \\(\\lambda_b\\) (0.08 here, but can be specified in the textbox). The Coef plot shows the coefficient that would be computed with \\(\\lambda_b = \\infty\\) . The main difference between the two in this case is that the channel 0 coefficient is much larger for the \\(\\lambda_b = \\infty\\) case. This is because the Weight Squared , \\(\\Omega^2_{s_{rc}}\\) term acts to increase the contribution of the weak round 1, channel 0 and decrease the contribution of the strong rounds: 0, 2, 3 and 6.","title":"view_background"},{"location":"pipeline/call_reference_spots/#bleed-matrix","text":"Crosstalk can occur between color channels. Some crosstalk may occur due to optical bleedthrough; additional crosstalk can occur due to chemical cross-reactivity of probes. The precise degree of crosstalk does not seem to vary much between sequencing rounds. It is therefore possible to largely compensate for this crosstalk by learning the precise amount of crosstalk between each pair of color channels. To estimate the crosstalk , we use the spot colors, \\(\\pmb{\\zeta}_{s0}\\) , of all well isolated spots. We reshape these, so we have a set of \\(n_{isolated} \\times n_{rounds}\\) vectors, each of dimension \\(n_{channels}\\) , \\(\\pmb{v}_i\\) ( \\(v_{0_c} = \\zeta_{{s=0,0}_{r=0,c}}\\) ). Only well-isolated spots are used to ensure that crosstalk estimation is not affected by spatial overlap of spots corresponding to different genes. Crosstalk is then estimated by running a scaled k-means algorithm on these vectors, which finds a set of \\(n_{dyes}\\) vectors, \\(\\pmb{c}_d\\) , such that the error function: \\[ \\sum_i\\min_{\\lambda_i, d(i)}|\\pmb{v}_i - \\lambda_i\\pmb{c}_{d(i)}|^2 \\] is minimized. In other words, it finds the \\(n_{dyes}\\) intensity vectors, \\(\\pmb{c}_d\\) , each of dimension \\(n_{channels}\\) , such that the spot color of each well isolated spot on every round is close to a scaled version of one of them. The \\(n_{dyes} \\times n_{channels}\\) array of dye vectors is termed the bleed matrix and is saved as nb.call_spots.bleed_matrix (a bleed_matrix is saved for each round, but if config['bleed_matrix_method'] = 'single' , it will be the same for each round). The view_bleed_matrix function can be used to show it: Single Bleed Matrix Separate Bleed Matrix For Each Round As shown in the second plot, if config['bleed_matrix_method'] = 'separate' , we compute a different bleed_matrix for each round - i.e. we loosen the assumption that crosstalk does not vary between sequencing rounds.","title":"Bleed Matrix"},{"location":"pipeline/call_reference_spots/#initial-bleed-matrix","text":"To estimate the dye intensity vectors, \\(\\pmb{c}_d\\) , the scaled_k_means algorithm needs to know the number of dyes and a starting guess for what each dye vector looks like. This is specified in the basic_info section of the configuration file as explained here .","title":"Initial Bleed Matrix"},{"location":"pipeline/call_reference_spots/#scaled-k-means","text":"The pseudocode for the scaled_k_means algorithm to obtain the dye intensity vectors, \\(\\pmb{c}_d\\) , is given below: v: Single round intensity vectors of well isolated spots [n_vectors x n_channels] c: Initial Guess of Bleed Matrix [n_dyes x n_channels] dye_ind_old: [n_vectors] array of zeros. v_norm = v normalised so each vector has an L2 norm of 1. Normalise c so that each dye vector has an L2 norm of 1. i = 0 while i < n_iter: score = dot product between each vector in v_norm and each dye in c. [n_vectors x n_dyes] top_score = highest score across dyes for each vector in v_norm. [n_vectors]. dye_ind = dye corresponding to top_score for each vector in v_norm. [n_vectors]. if dye_ind == dye_ind_old: Stop iteration because we have reached convergence i.e. i = n_iter dye_ind_old = dye_ind for d in range(n_dyes): v_use = all vectors in v with dye_ind = d and top_score > score_thresh. Use un-normalised as to avoid overweighting weak points. [n_use x n_channels] if n_use < min_size: c[d] = 0 else: Update c[d] to be top svd component of v_use i.e. v_matrix = v_use.transpose() @ v_use / n_use [n_channels x n_channels] c[d] = np.linalg.svd(v_matrix)[0][:, 0] [n_channels] i = i + 1 return c There are a few parameters in the config file which are used: bleed_matrix_n_iter : This is n_iter in the above code. bleed_matrix_min_cluster_size : This is min_size in the above code. bleed_matrix_score_thresh : This is score_thresh in the above code. bleed_matrix_anneal : If this is True , the scaled_k_means algorithm will be run twice. The second time will use \\(\\pmb{c}_d\\) returned from the first run as the starting point, and it will have a different score_thresh for each dye. score_thresh for dye \\(d\\) will be equal to the median of top_score[dye_ind == d] in the last iteration of the first run. The idea is that for the second run, we only use vectors which have a large score, to get a more accurate estimate.","title":"Scaled K Means"},{"location":"pipeline/call_reference_spots/#view_scaled_k_means","text":"The scaled_k_means algorithm can be visualised using the view_scaled_k_means function: In each column, in the top row, boxplot \\(d\\) is for top_score[dye_ind == d] (only showing scores above score_thresh - 0 for the first two columns). The dye vectors, \\(\\pmb{c}_d\\) , are indicated in the second row. The number of vectors assigned to each dye is indicated by the number within each boxplot. The first column is for the first iteration i.e. with the initial guess of the bleed matrix. The second column is after the first scaled_k_means algorithm has finished. The third column is after the second scaled_k_means algorithm has finished (only shown if bleed_matrix_anneal=True ). The bottom whisker of the boxplots in the third column indicate the score_thresh used for each dye. This is useful for debugging the bleed_matrix computation, as you want the boxplots to show high scores and for those scores to increase from left to right as the algorithm is run.","title":"view_scaled_k_means"},{"location":"pipeline/call_reference_spots/#gene-bled-codes","text":"Once the bleed_matrix has been computed, the expected code for each gene can be obtained . Each gene appears with a single dye in each imaging round as indicated by the codebook and saved as nb.call_spots.gene_codes . bled_code[g, r, c] for gene \\(g\\) in round \\(r\\) , channel \\(c\\) is then given by bleed_matrix[r, c, gene_codes[g, r]] . If gene_codes[g, r] is outside nb.basic_info.use_dyes , then bled_code[g, r] will be set to 0. Each bled_code is also normalised to have an L2 norm of 1. They are saved as nb.call_spots.bled_codes . Example Using the Single Bleed Matrix shown as an example earlier , the bled_code for Kcnk2 with gene_code = 6304152 is:","title":"Gene Bled Codes"},{"location":"pipeline/call_reference_spots/#dot-product-score","text":"To assign spot \\(s\\) , with spot color, \\(\\pmb{\\zeta}_{{si}}\\) , to a gene, we compute a dot product score, \\(\\Delta_{sig}\\) to each gene, \\(g\\) , with bled_code \\(\\pmb{b}_g\\) . This is defined to be: \\[ \\Delta_{sig} = \\sum_{r=0}^{n_r-1}\\sum_{c=0}^{n_c-1}\\omega^2_{{si}_{rc}}\\tilde{\\zeta}_{{si}_{rc}}b_{g_{rc}} \\] Where: \\[ \\tilde{\\zeta}_{{si}_{rc}} = \\frac{\\zeta_{{si}_{rc}}} {\\sqrt{\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\zeta^2_{{si}_{\\mathscr{rc}}}} + \\lambda_d} \\] \\[ \\sigma^2_{{si}_{rc}} = \\beta^2 + \\alpha\\sum_g\\mu^2_{sig}b^2_{g_{rc}} \\] \\[ \\omega^2_{{si}_{rc}} = n_rn_c\\frac{\\sigma^{-2}_{{si}_{rc}}} {\\sum_{\\mathscr{r}=0}^{n_r-1}\\sum_{\\mathscr{c}=0}^{n_c-1}\\sigma^{-2}_{{si}_{\\mathscr{rc}}}} \\] \\(n_{r}\\) is the number of rounds. \\(n_{c}\\) is the number of channels. \\(\\lambda_d\\) is config['dp_norm_shift'] * sqrt(n_rounds) (typical config['dp_norm_shift'] is 0.1). \\(\\alpha\\) is config['alpha'] ( default is 120 ). \\(\\beta\\) is config['beta'] ( default is 1 ). The sum over genes, \\(\\sum_g\\) , is over all real and background genes i.e. \\(\\sum_{g=0}^{n_g+n_c-1}\\) . \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i\\) and \\(\\pmb{b}_{g=n_g+C} = \\pmb{B}_C\\) where \\(\\mu_{sC}\\) and \\(\\pmb{B}_C\\) were introduced in the background section. \\(i\\) refers to the number of actual genes fit prior to this iteration of OMP . Here, because we are only fitting one gene, \\(i=0\\) , meaning only background has been fit ( \\(\\sum_{g=0}^{n_g-1}\\mu^2_{sig}b^2_{g_{rc}}=0\\) ). So if \\(\\lambda_d = 0\\) and \\(\\pmb{\\omega}^2_{si}=1\\) for each round and channel (achieved through \\(\\alpha=0\\) ), then this would just be the normal dot product between two vectors with L2 norm of one. The min value is 0 and max value is 1. The purpose of the weighting, \\(\\pmb{\\omega}^2_{{si}}\\) , is to decrease the contribution of rounds/channels which already have a gene in. It is really more relevant to the OMP algorithm . It can be turned off in this part of the pipeline by setting config['alpha'] = 0 . The \\(n_rn_c\\) term at the start of the \\(\\omega^2_{{si}_{rc}}\\) equation is a normalisation term such that the max possible value of \\(\\Delta_{sig}\\) is approximately 1 (it can be more though). Value of \\(\\lambda_d\\) If in the \\(\\Delta_{sig}\\) equation, we used \\(\\pmb{\\zeta}_{{si}}\\) instead of \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\) , then the max value would no longer have an upper limit and a high score could be achieved by having large values of \\(\\pmb{\\zeta}_{{si}}\\) in some rounds and channels as well as by having \\(\\pmb{\\zeta}_{{si}}\\) being similar to \\(\\pmb{b}_g\\) . We use the intensity value to indicate the strength of the spot, so for \\(\\Delta_{sig}\\) , we really just want a variable which indicates how alike the spot color is to the gene, indepenent of strength. Hence, we use \\(\\pmb{\\tilde{\\zeta}}_{{si}}\\) . If \\(\\lambda_d = 0\\) , it would mean that even background pixels with very small intensity could get a high score. So, we use a non-zero value of \\(\\lambda_d\\) to prevent very weak spots getting a large \\(\\Delta_{sig}\\) . If config['dp_norm_shift'] is not specified, it is set to the median of the L2 norm in a single round computed from the colors of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). The idea behind this, is that the L2 norm of an average background pixel would be config['dp_norm_shift'] * sqrt(n_rounds) . So if \\(\\lambda_d\\) was set to this, it is giving a penalty to any spot which is less intense than the average background pixel. This is desirable since any pixels of such low intensity are unlikely to be spots. The spot \\(s\\) , is assigned to the gene \\(g\\) , for which \\(\\Delta_{sig}\\) is the largest.","title":"Dot Product Score"},{"location":"pipeline/call_reference_spots/#view_score","text":"How the various parameters in the dot product score calculation affect the final value can be investigated, for a single spot, through the function view_score : view_score view_weight The top left plot shows the spot color prior to any removal of genes or background. The bottom left plot shows the spot color after all genes fit prior to the current iteration have been removed (just background for iteration 0). The top right plot shows the dot product score obtained without weighting ( \\(\\alpha=0\\) ). The bottom right plot shows the actual score obtained with the current weighting parameters. To get to the gene with the highest dot product score for the current iteration, you can enter an impossible value in the Gene textbox. As well as typing the index of the gene, you can also type in the gene name to look at the calculation for a specific gene. Clicking on the Weight Squared plot shows the view_weight plot indicating how it is calculated (second image above). This is more useful for iterations other than 0 . Looking at the view_weight image and the far right plots of the view_score image, we see that the effect of the weighting is to down-weight color channel 0 because this is where the background coefficient is the largest. The channels with a smaller background coefficient (1, 5 and 6) then have a weighting greater than 1. Thus, the weighted score is greater than the non-weighted one because channel 6, where this spot is particularly strong has a greater contribution. I.e. because the intensity in channel 6 cannot be explained by background genes, but it can be explained by Plp1, we boost the score. For most spots, the background coefficients are very small and so the weighting has little effect. Using the histogram_score function, we see that the effect of weighting (blue) is to add a tail of scores greater than 1 for spots where an increased contribution is given to the rounds/channels where they are most intense. The mode score does not change though:","title":"view_score"},{"location":"pipeline/call_reference_spots/#gene-efficiency","text":"Once we have a score and gene assigned to each spot, we can update the bled_codes for each gene, \\(\\pmb{b}_g\\) based on all the spot colors assigned to them, \\(\\pmb{\\zeta}_{{s0}}\\) . We do this by determining nb.call_spots.gene_efficiency . gene_efficiency[g, r] gives the expected intensity of gene \\(g\\) in round \\(r\\) , as determined by the spots assigned to it, compared to that expected by the bleed_matrix . The pseudocode below explains how it is computed . spot_colors: Intensity of each spot in each channel [n_spots x n_rounds x n_channels] spot_gene_no: Gene each spot was assigned to. [n_spots] bm: Bleed Matrix, indicates expected intensity of each dye in each round and channel. [n_rounds x n_channels x n_dyes] gene_codes: Indicates dye each gene should appear with in each round. [n_genes x n_rounds] for g in range(n_genes): Only use spots assigned to current gene. use = spot_gene_no == g for r in use_rounds: Get bleed matrix prediction for strength of gene g in round r. bm_pred = bm[r, :, gene_codes[g, r]] [n_channels] Get spot colors for this round. spot_colors_r = spot_colors[use, r] [n_use x n_channels] For each spot, s, find the least squares coefficient, coef, such that spot_colors_r[s] = coef * bm_pred Store coef for each spot and round as spot_round_strength [n_use x n_rounds] for r in use_rounds: av_round_strength[r] = median(spot_round_strength[:, r]) Find av_round which is the round such that av_round_strength[av_round] is the closest to median(av_round_strength). Update spot_round_strength to only use spots with positive strength in av_round. keep = spot_round_strength[:, av_round] > 0 spot_round_strength = spot_round_strength[keep] [n_use2 x n_rounds] For each spot, determine the strength of each round relative to av_round. for s in range(n_use2): for r in use_rounds: relative_round_strength[s, r] = spot_round_strength[s, r] / spot_round_strength[s, av_round] Update relative_round_strength based on maximum value. max_round_strength is max of relative_round_strength for each spot across rounds [n_use2]. keep = max_round_strength < max_thresh relative_round_strength = relative_round_strength[keep] [n_use3 x n_rounds] Update relative_round_strength based on low values. Count number of rounds for each spot below min_thresh. for s in range(n_use3): n_min[s] = sum(relative_round_strength[s] < min_thresh) keep = n_min <= n_min_thresh relative_round_strength = relative_round_strength[keep] [n_use4 x n_rounds] for r in use_rounds: if n_use4 > min_spots: gene_efficiency[g, r] = median(relative_round_strength[:, r]) else: Not enought spots to compute gene efficiency so just set to 1 in every round. gene_efficiency[g, r] = 1 Clip negative gene efficiency at 0. gene_efficiency[gene_efficiency < 0] = 0 return gene_efficiency There are a few parameters in the configuration file which are used: gene_efficiency_max : This is max_thresh in the above code. gene_efficiency_min : This is min_thresh in the above code. gene_efficiency_min_factor : n_min_thresh in the above code is set to ceil(gene_efficiency_min_factor * n_rounds) . gene_efficiency_min_spots : This is min_spots in the above code. In the gene_efficiency calculation, we computed the strength of each spot relative to av_round because, as with the bleed_matrix calculation, we expect each spot color to be a scaled version of one of the bled_codes . So we are trying to find out, once a spot color has been normalised such that its strength in av_round is 1, what is the corresponding strength in the other rounds. We do this normalisation relative to the average round so that half the gene_efficiency values will be more than 1 and half less than 1 for each gene. For gene \\(g\\) , one value of gene_efficiency[g] will be 1, corresponding to av_round but this round will be different for each gene. Why do we need gene_efficiency ? We need gene_efficiency because there is a high variance in the strength with which each gene appears in each round. For example, in the the bled_code plot below, we see that the effect of incorporating gene_efficiency is to reduce the strength of rounds 0, 5 and 6 while boosting rounds 2 and 3. Note In the example below, it seems that rounds corresponding to the same dye (0 and 5; 1 and 4; 2 and 3) have similar strengths, so it may be that different dyes (instead of rounds) have different strengths for different genes. bled_code histogram The histogram plot above then shows that when gene efficiency is included (blue line), the score distribution is shifted considerably. This indicates that gene efficiency is required to truly capture what spot colors corresponding to Serpini1 look like. The histogram_score plot combining all genes, also shows a shift in the peak of the distribution towards higher scores when gene efficiency is included:","title":"Gene Efficiency"},{"location":"pipeline/call_reference_spots/#spots-used","text":"Because we use the gene_efficiency to update the bled_codes , we only want to use spots, which we are fairly certain have been assigned to the correct gene. Thus, only spots which satisfy all the following are used in the gene_efficiency calculation: Like with the scaled_k_means calculation, only spots identified as isolated in the find spots step of the pipeline are used. The dot product score to the best gene, \\(g_0\\) , \\(\\Delta_{s0g_0}\\) must exceed config['gene_efficiency_score_thresh'] . The difference between the dot product score to the best gene, \\(g_0\\) , and the second best gene, \\(g_1\\) : \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) must exceed config['gene_efficiency_score_diff_thresh'] . The intensity , \\(\\chi_s\\) , must exceed config['gene_efficiency_intensity_thresh'] . Value of config['gene_efficiency_intensity_thresh'] If config['gene_efficiency_intensity_thresh'] is not specified, it is set to the percentile indicated by config['gene_efficiency_intensity_thresh_percentile'] of the intensity computed from the colors of all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). It is then clipped to be between config[gene_efficiency_intensity_thresh_min] and config[gene_efficiency_intensity_thresh_max] . The idea is that this is quite a low threshold (default percentile is 37), just ensuring that the intensity is not amongst the weakest background pixels. If the intensity threshold was too high, we would end up losing spots which look a lot like genes just because they are weak. But if it was too low, we would identify some background pixels as genes.","title":"Spots used"},{"location":"pipeline/call_reference_spots/#updating-bled_codes","text":"Once the gene_efficiency has been computed, the bled_codes can be updated: bled_codes: Those computed from the bleed_matrix [n_genes x n_rounds x n_channels]. gene_efficiency: [n_genes x n_rounds] for g in range (n_genes): for r in use_rounds: for c in use_channels: bled_codes[g, r, c] = bled_codes[g, r, c] * gene_efficiency[g, r] Normalise bled_codes[g] so it has an L2 norm of 1. We then re-compute the dot product score and gene assignment for each spot with the new bled_codes . We continue this process of computing the gene efficiency, updating the dot product score until the same spots have been used to compute the gene efficiency in two subsequent iterations or until config[gene_efficiency_n_iter] iterations have been run. The bled_codes computed from the final iteration will be saved as nb.call_spots.bled_codes_ge . This will be the same as nb.call_spots.bled_codes if config[gene_efficiency_n_iter] = 0 . These are the ones used to compute dot product score to the best gene, \\(g_0\\) , \\(\\Delta_{s0g_0}\\) . These are saved as nb.ref_spots.gene_no and nb.ref_spots.score respectively. The difference between the dot product score to the best gene, \\(g_0\\) , and the second best gene, \\(g_1\\) : \\(\\Delta_{s0g_0}-\\Delta_{s0g_1}\\) is saved as nb.ref_spots.score_diff .","title":"Updating bled_codes"},{"location":"pipeline/call_reference_spots/#intensity","text":"As well as a variable indicating how closely a spot matches a gene ( nb.ref_spots.score ), we also save a variable indicating the overall fluorescence of a spot, independent of which gene it belongs to. This intensity, \\(\\chi\\) , is saved as nb.ref_spots.intensity and for a spot \\(s\\) , it is defined by: \\[ \\chi_s = \\underset{r}{\\mathrm{median}}(\\max_c\\zeta_{s_{rc}}) \\] I.e. for each round, we take the max color across channels to give a set of \\(n_{rounds}\\) values. We then take the median of these. The logic behind this is that if the spot is actually a gene, then there should be at least one channel in every round which is intense, because the relevant dye shows up in it. If the spot was not actually a gene though, you would expect all channels in any given round to be similarly weakly intense and thus the max over channels would give a low value.","title":"Intensity"},{"location":"pipeline/call_reference_spots/#view_intensity","text":"The intensity calculation can be visualised with the view_intensity function: \\(\\chi_s = 0.542\\) for this example spot, which is the median of all the values shown with a green border.","title":"view_intensity"},{"location":"pipeline/call_reference_spots/#diagnostics","text":"As well as view_background , view_scaled_k_means , view_score and view_intensity , there are a few other functions using matplotlib which may help to debug this section of the pipeline.","title":"Diagnostics"},{"location":"pipeline/call_reference_spots/#histogram_score","text":"This shows the histogram of the dot product score , \\(\\Delta_s\\) , assigned to every reference spot: Dot Product Score All Plots This is useful for checking how well the gene assignment worked. The higher the score where the distribution peaks, the better. Certainly, if the peak is around 0.8, as with this example, then it probably worked well. The Dot Product Score image above is showing the histogram of nb.ref_spots.score , but there are 4 other plots which can be selected, as shown in the All Plots image above: No Weighting : This is the score that would be computed if \\(\\alpha=0\\) in the dot product score calculation . The max possible score in this case is 1. No Background : This is the score that would be computed if the background genes were not removed before determining the score. This also has no weighting because in the dot product calculation, \\(\\omega^2_{si_{rc}} = 1\\) if no background has been fitted. Hence, the max score is 1 as with No Weighting . No Gene Efficiency : This is the score that would be computed if the nb.call_spots.bled_codes were used instead of nb.call_spots.bled_codes_ge . \\(\\omega^2_{si_{rc}} \\neq 1\\) here so the max score is over 1. No Background / No Gene Efficiency : This is the score that would be computed if the background genes were not removed before determining the score and if nb.call_spots.bled_codes were used instead of nb.call_spots.bled_codes_ge . The max score is 1 in this case. The Gene textbox can also be used to view the histogram of a single gene. Either the index of the gene or the gene name can be entered. To go back to viewing all genes, type in all into the textbox. The Histogram Spacing textbox can be used to change the bin size of the histogram.","title":"histogram_score"},{"location":"pipeline/call_reference_spots/#gene_counts","text":"This plot indicates the number of spots assigned to each gene which also have nb.ref_spots.score > score_thresh and nb.ref_spots.intensity > intensity_thresh . The default score_thresh and intensity_thresh are config['thresholds']['score_ref'] and config['thresholds']['intensity'] respectively. They can be changed with the textboxes though. This thresholding is the same that is done in the results Viewer and when exporting to pciSeq . Gene Counts Gene Counts with Fake Genes There is also a second Ref Spots - Fake Genes plot which can be shown in yellow. This shows the results of the gene assignment if we added some fake bled_codes as well as the ones corresponding to genes. The idea is to choose fake bled_codes which are well separated from the actual bled_codes . If spots then match to these fake genes, then it probably means the initial gene assignment is not reliable. The fake bled_codes can be specified, but by default there is one fake bled_code added for each round, \\(r\\) , and channel \\(c\\) , which is 1 in round \\(r\\) , channel \\(c\\) and 0 everywhere else. In the second image above, we see that there is not much change in the gene counts when we add the fake genes, indicating the initial assignment is probably reliable. Example Dataset with lots of Fake Genes The example below indicates a case where the fake genes functionality may be useful. When we open coppafish.Viewer , we see that there seems to be too many spots assigned to Penk and Vip . coppafish.Viewer gene_counts Penk Vip If we then look at the gene_counts , we see that when we include fake genes, the number of spots assigned to Penk and Vip decreases drastically because they have been assigned to the \\(r0c18\\) fake gene. When we look at the Penk and Vip bled_codes , we see that they are very intense in round 0, channel 18. So most spots seem to only have been assigned to these genes on the basis of this one round and channel.","title":"gene_counts"},{"location":"pipeline/call_reference_spots/#view_bleed_matrix","text":"This function is useful for seeing if the dye vectors in the bleed_matrix are easily distinguished.","title":"view_bleed_matrix"},{"location":"pipeline/call_reference_spots/#view_bled_codes","text":"This function is useful for seeing how the gene_efficiency affected the bled_codes .","title":"view_bled_codes"},{"location":"pipeline/call_reference_spots/#view_codes","text":"This function is useful for seeing how a particular spot matches the gene it was assigned to.","title":"view_codes"},{"location":"pipeline/call_reference_spots/#view_spot","text":"This function is useful for seeing if the neighbourhood of a particular spot has high intensity in all rounds/channels where the gene it was assigned, expects it to.","title":"view_spot"},{"location":"pipeline/call_reference_spots/#psuedocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . There is more detailed pseudocode about how the bleed_matrix and gene_efficiency are found. Determine color_norm_factor from nb.extract.hist_counts [n_rounds x n_channels] Load in pixel colors of all pixel of middle z-plane of central tile. Use these to determine the following if not provided in the config file: - nb.call_spots.background_weight_shift - nb.call_spots.dp_norm_shift - nb.call_spots.gene_efficiency_intensity_thresh - nb.call_spots.abs_intensity_percentile Normalise reference spot colors spot_colors = nb.ref_spots.colors / color_norm_factor [n_spots x n_rounds x n_channels] Compute Spot Intensity (nb.ref_spots.intensity) Remove Background from spot_colors Compute Bleed Matrix (nb.call_spots.bleed_matrix) Compute Bled Codes (nb.call_spots.bled_codes) use_ge_last = array of length n_spots where all values are False. i = 0 while i < gene_efficiency_n_iter: Determine all_scores, the dot product score of each spot to each bled_code. [n_spots x n_genes] Determine gene_no, the gene for which all_scores is the greatest for each spot. [n_spots] Determine score, the score in all_scores, corresponding to gene_no for each spot. [n_spots] Determine score_diff, the difference between score and the second largest value in all_scores for each spot. [n_spots] Determine whether each spot was used for gene efficiency calculation. use_ge = score > ge_score_thresh and score_diff > ge_score_diff_thresh and intensity > ge_intensity_thresh and nb.ref_spots.isolated. [n_spots] Compute gene_efficiency with spots indicated by use_ge Update bled_codes based on gene_efficiency If use_ge == use_ge_last: End iteration i.e. i = gene_efficiency_n_iter use_ge_last = use_ge i += 1 Save final bled_codes as nb.call_spots.bled_codes_ge Save final gene_efficiency as nb.call_spots.gene_efficiency Save final gene_no as nb.ref_spots.gene_no Save final score as nb.ref_spots.score Save final score_diff as nb.ref_spots.score_diff","title":"Psuedocode"},{"location":"pipeline/extract/","text":"Extract and Filter The extract and filter step of the pipeline loads in the raw images, filters them and saves the resultant filtered images for each tile/round/channel combination as npy files . It also adds the extract and extract_debug NotebookPages to the Notebook . If the extract and filter step of the pipeline bugs out halfway through for some reason, it can be re-run without needing to remake all the tiles already saved to the tile directory . It will just start with the first tile yet to be saved. The scale values must not be changed when re-running though. Variables in extract page auto_thresh The extract NotebookPage contains the variable auto_thresh . auto_thresh[t, r, c] is the threshold spot intensity for tile \\(t\\) , round \\(r\\) , channel \\(c\\) used for spot detection in the find spots step of the pipeline. auto_thresh[t, r, c] = config['extract']['auto_thresh_multiplier'] * median(abs(image)) where image is the mid z-plane ( nb.extract_debug.z_info ) of the image saved to tile_dir for tile \\(t\\) , round \\(r\\) , channel \\(c\\) during the extract step of the pipeline. This is just saying that we expect median(abs(image)) to be the characteristic intensity of background pixels and spot pixels should be much more intense that this. These values can be viewed with the function thresh_box_plots : For each round and channel, this shows a box plot combining all tiles (i.e. a box plot of auto_thresh[:, r, c] ). For the anchor_round , only one channel ( anchor_channel ) is shown as it is the only one used on this round. In this plot, we expect for a given channel, auto_thresh should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any outlier tiles (white crosses, +) not far from the boxplot). hist_counts The extract NotebookPage also contains the variable hist_counts . hist_counts[i, r, c] is the number of pixels across the mid z-plane ( nb.extract_debug.z_info ) of all tiles in round \\(r\\) , channel \\(c\\) which had the value nb.extract.hist_values[i] . It is used for normalisation (see Norm Button box here ) between channels in the call reference spots step. The histograms can be viewed using histogram_plots . Initially, this will show the hist_counts[:, r, c] vs hist_values for each round and channel. There is also a Norm Button which equalises the channels according to config['call_spots']['color_norm_intensities'] and config['call_spots']['color_norm_probs'] . In the normalised histograms, most values will be between \u00b11. Un-normalised Histograms Normalised Histograms In the normalised histograms, we want to see a sharp peak at 0 accounting for the background pixels with a long tail to larger values accounting for the spot pixels and a tail to negative values accounting for the pixels in annuli surrounding spots . So in this example, channel 2 will likely prove the most problematic because the peak centered on 0 is much wider than for any other channel. This indicates that there is quite a lot of variance in the background pixels, making it harder to distinguish the spots from the background. Also, from the un-normalised histograms we can see that the peak centered on 0 is widest for channel 0. Thus, the median of absolute values will be largest for this channel. This explains why auto_thresh is significantly larger for channel 0 than any other channel. Raw data The raw data can be viewed using view_raw . It can either be called for an experiment which already has a Notebook , or for one for which no code has been run yet, but the config_file has been made: With Notebook Without Notebook from coppafish import Notebook from coppafish.plot import view_raw nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) tiles = [ 0 , 1 ] # tiles to view rounds = [ 3 , 5 ] # rounds to view channels = [ 1 , 6 ] # channels to view view_raw ( nb , tiles , rounds , channels ) from coppafish.plot import view_raw ini_file = '/Users/user/coppafish/experiment/settings.ini' tiles = [ 0 , 1 ] # tiles to view rounds = [ 3 , 5 ] # rounds to view channels = [ 1 , 6 ] # channels to view view_raw ( None , tiles , rounds , channels , config_file = ini_file ) This will open a napari viewer with up to 4 scrollbars to change tile, round, channel and z-plane. When any of these scrollbars are used, the status in the bottom left corner will indicate the current tile, round, channel and z-plane being shown (e.g. below, the round at index 0 is 3 and the channel at index 1 is 6). Using this tool before running the pipeline may be useful for deciding which z-planes to use. For example, if say the first 10 z-planes don't show any clear spots, then removing them with use_z in the basic_info section of the configuration file would make the pipeline run much quicker, especially in the omp and extract and filter sections. Filtering Once the raw images are loaded in, they are convolved with a 2D difference of hanning kernel . Difference with 2D pipeline If config['basic_info']['is_3d'] == False , before the convolution with the difference of hanning kernel, the 3D raw data will be focus stacked so that it becomes 2D . Difference of hanning kernel The difference of hanning kernel is made up by adding together a positive hanning window (yellow below) of radius \\(r_1\\) and an outer negative hanning window (cyan below) of radius \\(r_2\\) (typically twice \\(r_1\\) ). It is normalised such that the sum of the difference of hanning kernel is 0. An example for a 1D version of the kernel with \\(r_1 = 3\\) and \\(r_2 = 6\\) is shown below: Conversion to 2D The 1D kernel shown in purple above is converted to the 2D kernel shown on the right via the ftrans2 function. In the pipeline, the value of \\(r_1\\) is set to config['extract']['r1'] and \\(r_2\\) is set to config['extract']['r2'] . If config['extract']['r1'] is not specified, it is converted to units of pixels from the micron value config['extract']['r1_auto_microns'] (0.5 \\(\\mu\\) m typically gives \\(r_1=3\\) ). If config['extract']['r2'] is not specified, \\(r_2\\) is set to twice \\(r_1\\) . In general, \\(r_1\\) should be the typical radius of a spot in the raw image and \\(r_2\\) should be twice this. Smoothing After the convolution with the difference of hanning kernel, there is an option to smooth the image by applying a correlation with an averaging kernel. This can be included by setting the config['extract']['r_smooth'] parameter. DAPI For the dapi_channel of the anchor_round , convolution with the difference of hanning kernel is not appropriate as the features that need extracting do not look like spots. Instead, tophat filtering can be performed by setting config['extract']['r_dapi'] . No smoothing is permitted. Viewer The purpose of filtering the raw images is to make the spots appear much more prominently compared to the background i.e. extract the spots. We can see this effect and how the various parameters affect things with view_filter . This can be called in a similar way to view_raw : With Notebook Without Notebook from coppafish import Notebook from coppafish.plot import view_filter nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_filter ( nb , t , r , c ) from coppafish.plot import view_filter ini_file = '/Users/user/coppafish/experiment/settings.ini' t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_filter ( None , t , r , c , config_file = ini_file ) This will open a napari viewer with up to 2 scrollbars. One to change z-plane and another to change the filter method. The filter method scrollbar can change between the raw image, the result of convolution with difference of hanning kernel and the result with smoothing in addition to this. There are also up to 3 of the following sliders in the bottom left: Difference of Hanning Radius : This is the value of config['extract']['r1'] . Whenever this is changed, config['extract']['r2'] will be set to twice the new value. Tophat kernel radius : If r == anchor_round and c == dapi_channel , this slider will appear and refers to the value of config['extract']['r_dapi'] . Smooth Radius YX : This is the value of config['extract']['r_smooth'][0] and config['extract']['r_smooth'][1] . Both will be set to the same value. Smooth Radius Z : This is the value of config['extract']['r_smooth'][2] . When both this slider and the Smooth Radius YX slider are set to 1, no smoothing will be performed and the last two images in the filter method scrollbar will be identical. Whenever any of these are changed, the filtering will be redone using the new values of the parameters and thus the last two images of the filter method scrollbar will be updated. The time taken will be printed to the console. The 1D version of the current difference of hanning kernel can be seen at any time by pressing the h key. Effect of Filtering The images below show the effect of filtering with config['extract']['r1'] = 3 , config['extract']['r2'] = 6 and config['extract']['r_smooth'] = 1, 1, 2 : Raw Difference of Hanning Convolution Smoothing From this, we see that away from the spots, the raw image has a non-zero intensity value (around 300). After convolution with the difference of hanning kernel though, these regions become a lot darker (approximately 0). This is because the sum of the difference of hanning kernel is 0 so its effect on a background region with a uniform non-zero value is to set it to 0. Looking at the spots, we see that the convolution helps isolate them from the background and separate spots which are close together. There is also a very dark (negative) region surrounding the spots. It is a feature of convolution with the difference of hanning kernel that it produces a negative annulus about spots. Why negative annulus is expected The convolution in the annulus of a spot is like the sum of the multiplication of the spot line (yellow) with the kernel line (cyan). This multiplication produces the purple line, the sum of which is negative. The smoothing in this example is only in the z direction (averaging over 3 z-planes: the one shown, 1 above and 1 below) and seems to emphasise the spots and the negative annulus even more. This is because on one of the neighbouring z-planes, the spot has a larger intensity than on the z-plane shown so averaging increases the absolute intensity. Varying difference of hanning kernel radius The plots below show the results of the convolution with the difference of hanning kernel for four different values of config['extract']['r1'] . In each case, config['extract']['r2'] is twice this value. 2 3 4 6 From this, we see that with \\(r_1 = 2\\) , the background regions away from the spots appear less uniform than with \\(r_1 = 3\\) , with quite a few patches of negative values. Also, the shape of the second spot from the left appears distorted. These both indicate that the kernel is wanting to extract features smaller than those of interest. As \\(r_1\\) increases, we see that the negative annulus around the spots becomes larger and eventually at \\(r_1=6\\) , the spots start merging together, indicating the kernel is wanting to extract features larger than those of interest. Varying smoothing radius The plots below show the results of the convolution with the difference of hanning kernel followed by smoothing for four different values of config['extract']['r_smooth'] . In each case, config['extract']['r1'] = 3 and config['extract']['r2'] = 6 . 1, 1, 2 1, 1, 3 1, 1, 5 2, 2, 2 4, 4, 1 From this, we see that smoothing in the z direction makes spots which appear most prominantly on other z-planes appear much more intense in the z-plane shown. For example, the feature towards the bottom just to right of centre is barely visible with r_smooth = 1, 1, 2 but is clear with r_smooth = 1, 1, 5 . We also see that the difference between the r_smooth = 1, 1, 2 and r_smooth = 2, 2, 2 plots is barely perceivable. This suggests that the z averaging is more important, which makes sense, seen as the convolution with the difference of hanning kernel is done in 2D , so treats each z-plane independently. In the r_smooth = 4, 4, 1 image with no z-averaging, we see that the spots have more of a gradual increase in intensity instead of a sharp peak. Scale The filtered images produced are of float data type with negatives, but they are saved to config['file_names']['tile_dir'] in uint16 format. To do this conversion, the images are first multiplied by a scale factor so that they fill most of the uint16 range (between 0 and 65535) to keep the maximum amount of information. There are two different scale factors, scale which is applied to all tiles and channels of the imaging rounds ( config['file_names']['round'] ) and scale_anchor which is applied to all tiles of the anchor_channel of the anchor_round . Potential error if scale changed It is important that the value of scale used does not vary between tiles/rounds/channels as it would affect the assignment of spots to genes. For example, if the value of config['extract']['scale'] was larger for round 2, channel 3, then spots will be more likely to be assigned to genes which appear here according to their barcode in the code_book ( scale_anchor is allowed to differ from scale because the anchor_round is not used in gene assignment). To stop this possibility, the values of scale and scale_anchor used are saved to the config['file_names']['tile_dir'] in a text file ( config['file_names']['scale'] ). Then if these differ from config['extract']['scale'] and config['extract']['scale_anchor'] , an error will occur. scale can be specified through config['extract']['scale'] but if this is empty, it will be set to: scale = config['extract']['scale_norm']/max(scale_image) scale_image is the nb.basic_info.tile_sz x nb.basic_info.tile_sz raw image belonging to the channel and z-plane containing the pixel with maximum intensity of the central tile (saved as scale_channel , scale_z , scale_tile in nb.extract_debug ) in round 0. It is then filtered/smoothed according to the parameters in config['extract'] before being used in the scale calculation. scale_anchor can be specified through config['extract']['scale_anchor'] . If it is left empty, it is computed in the same way as scale (the channel used is anchor_channel and the tile and z-plane used are saved as scale_ancor_tile and scale_anchor_z in nb.extract_debug ). After the tiles are multiplied by the scale factor, they still contain negative values, so when they are saved , a shift ( config['basic_info']['tile_pixel_value_shift'] ) in intensity is added to each pixel. This shift is then subtracted when the tiles are loaded . Error - clipped pixels Because scale is computed from one tile and round, there is a possibility during the course of the extract step of the pipeline that a much more intense tile/round will be encountered such that the pixel values will have to be clipped after scaling, to be kept within the uint16 range. The number of pixels for which this happens on tile \\(t\\) , round \\(r\\) , channel \\(c\\) is saved as nb.extract_debug.n_clip_pixels[t, r, c] . Clipped pixels can cause more spots to be detected in the find spots section of the pipeline, as shown below, so are best avoided: Spot detection with no clipped pixels Spot detection with clipped pixels If more than config['extract']['n_clip_error'] (will be set to 1% of pixels on single z-plane if not specified) pixels have been clipped for config['extract']['n_clip_error_images_thresh'] images, an error will be raised stopping the extract section of the pipeline. When this error occurs, a Notebook called notebook_extract_error.npz will be saved to the output directory with the pages extract_fail and extract_debug_fail . nb.extract_fail.fail_trc records the tile, round, channel where it terminated. Solution If the failed round, nb.extract_fail.fail_trc[1] is not the anchor_round , then delete everything in the tile directory including the scale.txt file. Then set config['extract']['scale'] to new_scale and re-run: scale_clip = nb . extract_debug_fail . clip_extract_scale new_scale = scale_clip [ scale_clip > 0 ] . min () This is the scale such that all tiles saved so far will not have any clipped pixels. If the failed round, nb.extract_fail.fail_trc[1] is the anchor_round , then delete all .npy files belonging to the anchor round in the tile directory as well as the scale.txt file. Then set config['extract']['scale_anchor'] to new_anchor_scale and re-run: anchor_scale_clip = \\ nb . extract_debug_fail . clip_extract_scale [:, anchor_round , anchor_channel ] new_anchor_scale = anchor_scale_clip [ anchor_scale_clip > 0 ] . min () This is the scale such that all anchor tiles saved so far will not have any clipped pixels. Psuedocode This is the pseudocode outlining the basics of this step of the pipeline . for r in use_rounds: for t in use_tiles: for c in use_channels: im = load image from raw data in input_dir if 2D: im = focus_stack(im) if r is anchor_round and c is dapi_channel: im = tophat_filter(im, dapi_kernel) else: im = convolve(im, diff_hanning_kernel) im = im * scale if smooth: im = correlate(im, smooth_kernel) Compute auto_thresh[t, r, c] and hist_counts[t, r, c] from mid z-plane of im. Save im to tile directory. Add information needed for later stages of pipeline to extract NotebookPage Add useful debugging info to extract_debug NotebookPage. Return both.","title":"Extract and Filter"},{"location":"pipeline/extract/#extract-and-filter","text":"The extract and filter step of the pipeline loads in the raw images, filters them and saves the resultant filtered images for each tile/round/channel combination as npy files . It also adds the extract and extract_debug NotebookPages to the Notebook . If the extract and filter step of the pipeline bugs out halfway through for some reason, it can be re-run without needing to remake all the tiles already saved to the tile directory . It will just start with the first tile yet to be saved. The scale values must not be changed when re-running though.","title":"Extract and Filter"},{"location":"pipeline/extract/#variables-in-extract-page","text":"","title":"Variables in extract page"},{"location":"pipeline/extract/#auto_thresh","text":"The extract NotebookPage contains the variable auto_thresh . auto_thresh[t, r, c] is the threshold spot intensity for tile \\(t\\) , round \\(r\\) , channel \\(c\\) used for spot detection in the find spots step of the pipeline. auto_thresh[t, r, c] = config['extract']['auto_thresh_multiplier'] * median(abs(image)) where image is the mid z-plane ( nb.extract_debug.z_info ) of the image saved to tile_dir for tile \\(t\\) , round \\(r\\) , channel \\(c\\) during the extract step of the pipeline. This is just saying that we expect median(abs(image)) to be the characteristic intensity of background pixels and spot pixels should be much more intense that this. These values can be viewed with the function thresh_box_plots : For each round and channel, this shows a box plot combining all tiles (i.e. a box plot of auto_thresh[:, r, c] ). For the anchor_round , only one channel ( anchor_channel ) is shown as it is the only one used on this round. In this plot, we expect for a given channel, auto_thresh should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any outlier tiles (white crosses, +) not far from the boxplot).","title":"auto_thresh"},{"location":"pipeline/extract/#hist_counts","text":"The extract NotebookPage also contains the variable hist_counts . hist_counts[i, r, c] is the number of pixels across the mid z-plane ( nb.extract_debug.z_info ) of all tiles in round \\(r\\) , channel \\(c\\) which had the value nb.extract.hist_values[i] . It is used for normalisation (see Norm Button box here ) between channels in the call reference spots step. The histograms can be viewed using histogram_plots . Initially, this will show the hist_counts[:, r, c] vs hist_values for each round and channel. There is also a Norm Button which equalises the channels according to config['call_spots']['color_norm_intensities'] and config['call_spots']['color_norm_probs'] . In the normalised histograms, most values will be between \u00b11. Un-normalised Histograms Normalised Histograms In the normalised histograms, we want to see a sharp peak at 0 accounting for the background pixels with a long tail to larger values accounting for the spot pixels and a tail to negative values accounting for the pixels in annuli surrounding spots . So in this example, channel 2 will likely prove the most problematic because the peak centered on 0 is much wider than for any other channel. This indicates that there is quite a lot of variance in the background pixels, making it harder to distinguish the spots from the background. Also, from the un-normalised histograms we can see that the peak centered on 0 is widest for channel 0. Thus, the median of absolute values will be largest for this channel. This explains why auto_thresh is significantly larger for channel 0 than any other channel.","title":"hist_counts"},{"location":"pipeline/extract/#raw-data","text":"The raw data can be viewed using view_raw . It can either be called for an experiment which already has a Notebook , or for one for which no code has been run yet, but the config_file has been made: With Notebook Without Notebook from coppafish import Notebook from coppafish.plot import view_raw nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) tiles = [ 0 , 1 ] # tiles to view rounds = [ 3 , 5 ] # rounds to view channels = [ 1 , 6 ] # channels to view view_raw ( nb , tiles , rounds , channels ) from coppafish.plot import view_raw ini_file = '/Users/user/coppafish/experiment/settings.ini' tiles = [ 0 , 1 ] # tiles to view rounds = [ 3 , 5 ] # rounds to view channels = [ 1 , 6 ] # channels to view view_raw ( None , tiles , rounds , channels , config_file = ini_file ) This will open a napari viewer with up to 4 scrollbars to change tile, round, channel and z-plane. When any of these scrollbars are used, the status in the bottom left corner will indicate the current tile, round, channel and z-plane being shown (e.g. below, the round at index 0 is 3 and the channel at index 1 is 6). Using this tool before running the pipeline may be useful for deciding which z-planes to use. For example, if say the first 10 z-planes don't show any clear spots, then removing them with use_z in the basic_info section of the configuration file would make the pipeline run much quicker, especially in the omp and extract and filter sections.","title":"Raw data"},{"location":"pipeline/extract/#filtering","text":"Once the raw images are loaded in, they are convolved with a 2D difference of hanning kernel . Difference with 2D pipeline If config['basic_info']['is_3d'] == False , before the convolution with the difference of hanning kernel, the 3D raw data will be focus stacked so that it becomes 2D .","title":"Filtering"},{"location":"pipeline/extract/#difference-of-hanning-kernel","text":"The difference of hanning kernel is made up by adding together a positive hanning window (yellow below) of radius \\(r_1\\) and an outer negative hanning window (cyan below) of radius \\(r_2\\) (typically twice \\(r_1\\) ). It is normalised such that the sum of the difference of hanning kernel is 0. An example for a 1D version of the kernel with \\(r_1 = 3\\) and \\(r_2 = 6\\) is shown below: Conversion to 2D The 1D kernel shown in purple above is converted to the 2D kernel shown on the right via the ftrans2 function. In the pipeline, the value of \\(r_1\\) is set to config['extract']['r1'] and \\(r_2\\) is set to config['extract']['r2'] . If config['extract']['r1'] is not specified, it is converted to units of pixels from the micron value config['extract']['r1_auto_microns'] (0.5 \\(\\mu\\) m typically gives \\(r_1=3\\) ). If config['extract']['r2'] is not specified, \\(r_2\\) is set to twice \\(r_1\\) . In general, \\(r_1\\) should be the typical radius of a spot in the raw image and \\(r_2\\) should be twice this.","title":"Difference of hanning kernel"},{"location":"pipeline/extract/#smoothing","text":"After the convolution with the difference of hanning kernel, there is an option to smooth the image by applying a correlation with an averaging kernel. This can be included by setting the config['extract']['r_smooth'] parameter.","title":"Smoothing"},{"location":"pipeline/extract/#dapi","text":"For the dapi_channel of the anchor_round , convolution with the difference of hanning kernel is not appropriate as the features that need extracting do not look like spots. Instead, tophat filtering can be performed by setting config['extract']['r_dapi'] . No smoothing is permitted.","title":"DAPI"},{"location":"pipeline/extract/#viewer","text":"The purpose of filtering the raw images is to make the spots appear much more prominently compared to the background i.e. extract the spots. We can see this effect and how the various parameters affect things with view_filter . This can be called in a similar way to view_raw : With Notebook Without Notebook from coppafish import Notebook from coppafish.plot import view_filter nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_filter ( nb , t , r , c ) from coppafish.plot import view_filter ini_file = '/Users/user/coppafish/experiment/settings.ini' t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_filter ( None , t , r , c , config_file = ini_file ) This will open a napari viewer with up to 2 scrollbars. One to change z-plane and another to change the filter method. The filter method scrollbar can change between the raw image, the result of convolution with difference of hanning kernel and the result with smoothing in addition to this. There are also up to 3 of the following sliders in the bottom left: Difference of Hanning Radius : This is the value of config['extract']['r1'] . Whenever this is changed, config['extract']['r2'] will be set to twice the new value. Tophat kernel radius : If r == anchor_round and c == dapi_channel , this slider will appear and refers to the value of config['extract']['r_dapi'] . Smooth Radius YX : This is the value of config['extract']['r_smooth'][0] and config['extract']['r_smooth'][1] . Both will be set to the same value. Smooth Radius Z : This is the value of config['extract']['r_smooth'][2] . When both this slider and the Smooth Radius YX slider are set to 1, no smoothing will be performed and the last two images in the filter method scrollbar will be identical. Whenever any of these are changed, the filtering will be redone using the new values of the parameters and thus the last two images of the filter method scrollbar will be updated. The time taken will be printed to the console. The 1D version of the current difference of hanning kernel can be seen at any time by pressing the h key.","title":"Viewer"},{"location":"pipeline/extract/#effect-of-filtering","text":"The images below show the effect of filtering with config['extract']['r1'] = 3 , config['extract']['r2'] = 6 and config['extract']['r_smooth'] = 1, 1, 2 : Raw Difference of Hanning Convolution Smoothing From this, we see that away from the spots, the raw image has a non-zero intensity value (around 300). After convolution with the difference of hanning kernel though, these regions become a lot darker (approximately 0). This is because the sum of the difference of hanning kernel is 0 so its effect on a background region with a uniform non-zero value is to set it to 0. Looking at the spots, we see that the convolution helps isolate them from the background and separate spots which are close together. There is also a very dark (negative) region surrounding the spots. It is a feature of convolution with the difference of hanning kernel that it produces a negative annulus about spots. Why negative annulus is expected The convolution in the annulus of a spot is like the sum of the multiplication of the spot line (yellow) with the kernel line (cyan). This multiplication produces the purple line, the sum of which is negative. The smoothing in this example is only in the z direction (averaging over 3 z-planes: the one shown, 1 above and 1 below) and seems to emphasise the spots and the negative annulus even more. This is because on one of the neighbouring z-planes, the spot has a larger intensity than on the z-plane shown so averaging increases the absolute intensity.","title":"Effect of Filtering"},{"location":"pipeline/extract/#varying-difference-of-hanning-kernel-radius","text":"The plots below show the results of the convolution with the difference of hanning kernel for four different values of config['extract']['r1'] . In each case, config['extract']['r2'] is twice this value. 2 3 4 6 From this, we see that with \\(r_1 = 2\\) , the background regions away from the spots appear less uniform than with \\(r_1 = 3\\) , with quite a few patches of negative values. Also, the shape of the second spot from the left appears distorted. These both indicate that the kernel is wanting to extract features smaller than those of interest. As \\(r_1\\) increases, we see that the negative annulus around the spots becomes larger and eventually at \\(r_1=6\\) , the spots start merging together, indicating the kernel is wanting to extract features larger than those of interest.","title":"Varying difference of hanning kernel radius"},{"location":"pipeline/extract/#varying-smoothing-radius","text":"The plots below show the results of the convolution with the difference of hanning kernel followed by smoothing for four different values of config['extract']['r_smooth'] . In each case, config['extract']['r1'] = 3 and config['extract']['r2'] = 6 . 1, 1, 2 1, 1, 3 1, 1, 5 2, 2, 2 4, 4, 1 From this, we see that smoothing in the z direction makes spots which appear most prominantly on other z-planes appear much more intense in the z-plane shown. For example, the feature towards the bottom just to right of centre is barely visible with r_smooth = 1, 1, 2 but is clear with r_smooth = 1, 1, 5 . We also see that the difference between the r_smooth = 1, 1, 2 and r_smooth = 2, 2, 2 plots is barely perceivable. This suggests that the z averaging is more important, which makes sense, seen as the convolution with the difference of hanning kernel is done in 2D , so treats each z-plane independently. In the r_smooth = 4, 4, 1 image with no z-averaging, we see that the spots have more of a gradual increase in intensity instead of a sharp peak.","title":"Varying smoothing radius"},{"location":"pipeline/extract/#scale","text":"The filtered images produced are of float data type with negatives, but they are saved to config['file_names']['tile_dir'] in uint16 format. To do this conversion, the images are first multiplied by a scale factor so that they fill most of the uint16 range (between 0 and 65535) to keep the maximum amount of information. There are two different scale factors, scale which is applied to all tiles and channels of the imaging rounds ( config['file_names']['round'] ) and scale_anchor which is applied to all tiles of the anchor_channel of the anchor_round . Potential error if scale changed It is important that the value of scale used does not vary between tiles/rounds/channels as it would affect the assignment of spots to genes. For example, if the value of config['extract']['scale'] was larger for round 2, channel 3, then spots will be more likely to be assigned to genes which appear here according to their barcode in the code_book ( scale_anchor is allowed to differ from scale because the anchor_round is not used in gene assignment). To stop this possibility, the values of scale and scale_anchor used are saved to the config['file_names']['tile_dir'] in a text file ( config['file_names']['scale'] ). Then if these differ from config['extract']['scale'] and config['extract']['scale_anchor'] , an error will occur. scale can be specified through config['extract']['scale'] but if this is empty, it will be set to: scale = config['extract']['scale_norm']/max(scale_image) scale_image is the nb.basic_info.tile_sz x nb.basic_info.tile_sz raw image belonging to the channel and z-plane containing the pixel with maximum intensity of the central tile (saved as scale_channel , scale_z , scale_tile in nb.extract_debug ) in round 0. It is then filtered/smoothed according to the parameters in config['extract'] before being used in the scale calculation. scale_anchor can be specified through config['extract']['scale_anchor'] . If it is left empty, it is computed in the same way as scale (the channel used is anchor_channel and the tile and z-plane used are saved as scale_ancor_tile and scale_anchor_z in nb.extract_debug ). After the tiles are multiplied by the scale factor, they still contain negative values, so when they are saved , a shift ( config['basic_info']['tile_pixel_value_shift'] ) in intensity is added to each pixel. This shift is then subtracted when the tiles are loaded .","title":"Scale"},{"location":"pipeline/extract/#error-clipped-pixels","text":"Because scale is computed from one tile and round, there is a possibility during the course of the extract step of the pipeline that a much more intense tile/round will be encountered such that the pixel values will have to be clipped after scaling, to be kept within the uint16 range. The number of pixels for which this happens on tile \\(t\\) , round \\(r\\) , channel \\(c\\) is saved as nb.extract_debug.n_clip_pixels[t, r, c] . Clipped pixels can cause more spots to be detected in the find spots section of the pipeline, as shown below, so are best avoided: Spot detection with no clipped pixels Spot detection with clipped pixels If more than config['extract']['n_clip_error'] (will be set to 1% of pixels on single z-plane if not specified) pixels have been clipped for config['extract']['n_clip_error_images_thresh'] images, an error will be raised stopping the extract section of the pipeline. When this error occurs, a Notebook called notebook_extract_error.npz will be saved to the output directory with the pages extract_fail and extract_debug_fail . nb.extract_fail.fail_trc records the tile, round, channel where it terminated.","title":"Error - clipped pixels"},{"location":"pipeline/extract/#solution","text":"If the failed round, nb.extract_fail.fail_trc[1] is not the anchor_round , then delete everything in the tile directory including the scale.txt file. Then set config['extract']['scale'] to new_scale and re-run: scale_clip = nb . extract_debug_fail . clip_extract_scale new_scale = scale_clip [ scale_clip > 0 ] . min () This is the scale such that all tiles saved so far will not have any clipped pixels. If the failed round, nb.extract_fail.fail_trc[1] is the anchor_round , then delete all .npy files belonging to the anchor round in the tile directory as well as the scale.txt file. Then set config['extract']['scale_anchor'] to new_anchor_scale and re-run: anchor_scale_clip = \\ nb . extract_debug_fail . clip_extract_scale [:, anchor_round , anchor_channel ] new_anchor_scale = anchor_scale_clip [ anchor_scale_clip > 0 ] . min () This is the scale such that all anchor tiles saved so far will not have any clipped pixels.","title":"Solution"},{"location":"pipeline/extract/#psuedocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . for r in use_rounds: for t in use_tiles: for c in use_channels: im = load image from raw data in input_dir if 2D: im = focus_stack(im) if r is anchor_round and c is dapi_channel: im = tophat_filter(im, dapi_kernel) else: im = convolve(im, diff_hanning_kernel) im = im * scale if smooth: im = correlate(im, smooth_kernel) Compute auto_thresh[t, r, c] and hist_counts[t, r, c] from mid z-plane of im. Save im to tile directory. Add information needed for later stages of pipeline to extract NotebookPage Add useful debugging info to extract_debug NotebookPage. Return both.","title":"Psuedocode"},{"location":"pipeline/find_spots/","text":"Find Spots The find spots step of the pipeline loads in the filtered images for each tile, round, channel saved during the extract step and detects spots on them. We obtain a point cloud from the images because in the stitch and register sections of the pipeline, it is quicker to use point clouds than the full images. The find_spots NotebookPage is added to the Notebook after this stage is finished. Spot detection The spots on tile \\(t\\) , round \\(r\\) , channel \\(c\\) are the local maxima in the filtered image (loaded in through load_tile(nb.file_names, nb.basic_info, t, r, c) ) with an intensity greater than auto_thresh[t, r, c] . Local maxima means pixel with the largest intensity in a neighbourhood defined by config['find_spots']['radius_xy'] and config['find_spots']['radius_z'] : kernel = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 )) The position of the local maxima is found to be where the dilation of the image with the kernel is equal to the image. The spot detection process can be visualised with view_find_spots . Optimised spot detection The dilation method is quite slow, so if jax is installed, a different spot detection method is used. In this method, we look at all pixels with intensity greater than auto_thresh[t, r, c] . For each of these, we say that the pixel is a spot if it has a greater intensity than all of its neighbouring pixels, where the neighbourhood is determined by the kernel . The larger auto_thresh[t, r, c] and the smaller the kernel , the faster this method is, whereas the value of auto_thresh[t, r, c] makes no difference to the speed of the dilation method. In our case, auto_thresh[t, r, c] is pretty large as the whole point is that all background pixels (the vast majority) have intensity less than it. For images of size \\(2048 \\times 2048 \\times 50\\) , the optimised method is around 9 times faster. Dealing with duplicates If there are two neighbouring pixels which have the same intensity, which is the local maxima intensity, by default both pixels will be declared to be local maxima. However, if remove_duplicates == True in detect_spots , only one will be deemed a local maxima. This is achieved by adding a random shift to the intensity of each pixel. The max possible shift is 0.2 so it will not change the integer version of the image but it will ensure each pixel has a different intensity to its neighbour. Imaging spots For non reference spots (all round/channel combinations apart from ref_round / ref_channel ), we only use the spots for registration to the reference spots, so the quantity of spots is not important. In fact, registration tends to work better if there are fewer but more reliable spots, as this means there is a lesser chance of matching up spots by chance. To exploit this, for each imaging tile, round, channel, the point cloud is made up of the max_spots most intense spots on each z-plane. In 2D , max_spots is config['find_spots']['max_spots_2d'] and in 3D , it is config['find_spots']['max_spots_3d'] . If there are fewer than max_spots spots detected on a particular z-plane, all the spots will be kept. Reference spots We want to assign a gene to each reference spot ( ref_round / ref_channel ), as well as use it for registration, so it is beneficial to maximise the number of reference spots. As such, we do not do the max_spots thresholding for reference spots. However, we want to know which reference spots are isolated because when it comes to the bleed_matrix calculation, we do not want to use overlapping spots. Isolated spots We deem a spot to be isolated if it has a prominent negative annulus , because if there was an overlapping spot, you would expect positive intensity in the annulus around the spot. We find the intensity of the annulus by computing the correlation of the image with an annulus kernel obtained from annulus(r0, r_xy, r_z) where: r0 = config['find_spots']['isolation_radius_inner'] r_xy = config['find_spots']['isolation_radius_xy'] r_z = config['find_spots']['isolation_radius_z'] . If the value of this correlation at the location of a spot is less than config['find_spots']['isolation_thresh'] , then we deem the spot to be isolated. If config['find_spots']['isolation_thresh'] is not given, it is set to: config [ 'find_spots' ][ 'auto_isolation_thresh_multiplier' ] * auto_thresh [ t , r , c ] The final isolation thresholds used for each tile are saved as nb.find_spots.isolation_thresh . The process of obtaining isolated spots can be visualised with view_find_spots . Annulus kernel The annulus kernel should be equal to 1 over the pixels in the neighbourhood of an isolated spot which are usually negative. The example images below show a typical spot (left) and the annulus kernel used for this data (right) with r0 = 4 , r_xy = 14 and r_z = 1 . The dimensions of each image is 29 x 29 pixels, red is positive and blue is negative. z = -1 z = 0 z = 1 z = -1 z = 0 z = 1 Error - too few spots After the find_spots NotebookPage has been added to the Notebook , check_n_spots will be run. This will produce a warning for any tile, round, channel for which fewer than n_spots_warn = config [ 'find_spots' ][ 'n_spots_warn_fraction' ] * max_spots * nb . basic_info . nz spots were detected, where max_spots is config['find_spots']['max_spots_2d'] if 2D and config['find_spots']['max_spots_3d'] if 3D . An error will be raised if any of the following is satisfied: For any given channel, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/rounds. The faulty channels should then be removed from use_channels . For any given tile, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of rounds/channels. The faulty tiles should then be removed from use_tiles . For any given round, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/channels. The faulty rounds should then be removed from use_rounds . Example The following is the \\(n_{tiles}\\) (3) x \\(n_{rounds}\\) (5) array of number of spots found for a given channel: spot_no = array ([[ 1295 , 1016 , 869 , 719 , 829 ], [ 1055 , 888 , 687 , 556 , 824 ], [ 5901 , 4208 , 5160 , 4069 , 4006 ]]) The value of n_spots_warn for this experiment is 3500 so a warning will be raised for the 10 tiles/rounds for which spot_no[t, r] < n_spots_warn : array ([[ 0 , 0 ], [ 0 , 1 ], [ 0 , 2 ], [ 0 , 3 ], [ 0 , 4 ], [ 1 , 0 ], [ 1 , 1 ], [ 1 , 2 ], [ 1 , 3 ], [ 1 , 4 ]]) The value of n_spots_error_fraction for this experiment is 0.5 so the threshold number of failed tiles/rounds to give an error is \\(0.5 \\times n_{tiles} \\times n_{rounds} = 7.5\\) . We have 10 failed tiles/rounds so an error would be raised in this case. The use_tiles , use_rounds and use_channels parameters can be changed without having to re-run the find_spots section of the pipeline as explained here . If tiles/rounds/channels are added instead of removed though, it will need re-running, as will the extract step. n_spots_grid The n_spots_grid function is useful to visualise the number of spots detected on each tile, round and channel: Good Bad In the good example, you can see from the minimum on the colorbar that lots of spots have been detected on every image. You can also see that tile 13 seems to have significantly fewer spots than the other tiles and that round 0 often has fewer spots than the other rounds. In the bad example, all tiles, rounds and channels where n_spots < n_spots_warn are highlighted by a red border. Clearly, channels 0 and 3 did not work for this experiment and channel 1 probably didn't. Also, tile 1 appears to have fewer spots than the other tiles. Viewer We can see how the various parameters affect which spots are detected using view_find_spots . This can be called as follows (in the Without Notebook case, the raw images will be loaded and then filtered according to parameters in config['extract'] ). With Notebook Without Notebook Reference round/channel showing isolated spots from coppafish import Notebook from coppafish.plot import view_find_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = 3 # round to view c = 6 # channels to view view_find_spots ( nb , t , r , c ) from coppafish.plot import view_filter ini_file = '/Users/user/coppafish/experiment/settings.ini' t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_find_spots ( None , t , r , c , config_file = ini_file ) from coppafish import Notebook from coppafish.plot import view_find_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = nb . basic_info . ref_round # round to view c = nb . basic_info . ref_channel # channel to view view_find_spots ( nb , t , r , c , show_isolated = True ) This will open a napari viewer with up to 5 sliders in the bottom left: Detection Radius YX : This is the value of config['find_spots']['r_xy'] . Detection Radius Z : This is the value of config['find_spots']['r_z'] . Intensity Threshold : This is the value of nb.extract.auto_thresh[t, r, c] . Isolation Threshold : This is the value of nb.find_spots.isolation_thresh[t] . It will only appear if show_isolated == True , r = nb.basic_info.ref_round and c = nb.basic_info.ref_channel . Z Thickness : Spots detected on the current z-plane and this many z-planes either side of it will be shown. Initially, this will be set to 1 so spots from the current z-plane and 1 either side of it will be shown. Whenever the first two are changed, the dilation will be redone using the new values of the radii and the time taken will be printed to the console. The correlation calculation required to determine which spots are isolated is slow hence, by default show_isolated == False . Z thickness The images below show the effect of changing the z-thickness. The size of the spots is related to the z-plane they were detected on. The closer to the current z-plane, the larger they appear: 0 1 2 The blue spot has a neighbouring pixel with negative intensity and is not kept in the final point cloud. Detection radius YX The images below show the effect of using the slider to change config['find_spots']['r_xy'] with config['find_spots']['r_z'] fixed at 2 and z thickness = 2: 2 3 10 Clearly, as the radius increases the spots have to be separated by a larger distance, resulting in less spots found. Z The images below show the effect of using the slider to change config['find_spots']['r_z'] with config['find_spots']['r_yx'] fixed at 2 and z thickness = 2: 2 6 12 Again we see the number of spots reducing as this increases. However, it is less clear as to why, because the minimum separation in z is what is changing but, we are only seeing spots from 5 z-planes imposed on a single z-plane. Intensity threshold The images below show the effect of using the slider to change nb.extract.auto_thresh[t, r, c] . This is for a 2D experiment with config['find_spots']['r_yx'] = 2 . 447 2234 4757 This is useful to see what a suitable intensity threshold should be. For example, the 447 plot identifies spots which do not look real while the 4757 plot misses some obvious spots. The value of nb.extract.auto_thresh[t, r, c] obtained for this data is approximately 2234. The green spots are those which are identified as isolated. Changing auto_thresh If after playing with this slider, it is decided that the find_spots part of the pipeline should be run (or re-run) with the updated intensity threshold, new_thresh , for tile \\(t\\) , round \\(r\\) , channel \\(c\\) , this can be achieved by setting nb.extract.auto_thresh[t, r, c] = new_thresh . Note, this is an abuse of the rules of the Notebook as it is changing a variable after it has been added and thus should not be allowed, but it does work. To keep the new_thresh value, the Notebook will need saving after nb.extract.auto_thresh has been updated. It should be saved to a different location, so it does not overwrite the initial Notebook i.e. nb.save('/Users/user/experiment1/output/notebook_new_auto_thresh.npz') . Isolation threshold The images below show the effect of using the slider to change nb.find_spots.isolation_thresh[t] . This is for a 2D experiment with: config['find_spots']['r_yx'] = 2 , config['find_spots']['isolation_radius_inner'] = 2 config['find_spots']['isolation_radius_xy'] = 2 . There is no slider to change the isolation radii because the dilation calculation is quite slow and has to be re-done everytime the radii change. -200 -438 -494 Here, we see that as the absolute threshold increases, the spots need a darker (more negative) annulus to be considered isolated (green). The value of -438 is approximately the value of nb.find_spots.isolation_thresh[t] used for this data (i.e. config['find_spots']['auto_isolation_thresh_multiplier'] = -0.2 and auto_thresh[t, r, c] = 2234 gives -447 which is almost the same). Pseudocode This is the pseudocode outlining the basics of this step of the pipeline . for r in use_rounds: for t in use_tiles: for c in use_channels: if r is anchor_round and c is not anchor_channel: Skip to next channel as no spots need detecting on this channel. im = load image from npy file in tile directory spots_trc = detect spots(im) Remove spots with negative neighbouring pixel from spots_trc if r is ref_round and c is ref_channel: Determine which spots are isolated. else: Keep only most intense spots on each z-plane. Set spot_no[t, r, c] to be the number of spots found. For spot s, record in spot_details[s]: - tile: tile found on - round: round found on - channel: channel found on - isolated: whether spot isolated (if not ref_round/ref_channel, this will be False) - y: y coordinate of spot in tile - x: x coordinate of spot in tile - z: z coordinate of spot in tile Add isolation_threshold, spot_no and spot_details to find_spots NotebookPage Return find_spots NotebookPage","title":"Find Spots"},{"location":"pipeline/find_spots/#find-spots","text":"The find spots step of the pipeline loads in the filtered images for each tile, round, channel saved during the extract step and detects spots on them. We obtain a point cloud from the images because in the stitch and register sections of the pipeline, it is quicker to use point clouds than the full images. The find_spots NotebookPage is added to the Notebook after this stage is finished.","title":"Find Spots"},{"location":"pipeline/find_spots/#spot-detection","text":"The spots on tile \\(t\\) , round \\(r\\) , channel \\(c\\) are the local maxima in the filtered image (loaded in through load_tile(nb.file_names, nb.basic_info, t, r, c) ) with an intensity greater than auto_thresh[t, r, c] . Local maxima means pixel with the largest intensity in a neighbourhood defined by config['find_spots']['radius_xy'] and config['find_spots']['radius_z'] : kernel = np . ones (( 2 * radius_xy - 1 , 2 * radius_xy - 1 , 2 * radius_z - 1 )) The position of the local maxima is found to be where the dilation of the image with the kernel is equal to the image. The spot detection process can be visualised with view_find_spots . Optimised spot detection The dilation method is quite slow, so if jax is installed, a different spot detection method is used. In this method, we look at all pixels with intensity greater than auto_thresh[t, r, c] . For each of these, we say that the pixel is a spot if it has a greater intensity than all of its neighbouring pixels, where the neighbourhood is determined by the kernel . The larger auto_thresh[t, r, c] and the smaller the kernel , the faster this method is, whereas the value of auto_thresh[t, r, c] makes no difference to the speed of the dilation method. In our case, auto_thresh[t, r, c] is pretty large as the whole point is that all background pixels (the vast majority) have intensity less than it. For images of size \\(2048 \\times 2048 \\times 50\\) , the optimised method is around 9 times faster. Dealing with duplicates If there are two neighbouring pixels which have the same intensity, which is the local maxima intensity, by default both pixels will be declared to be local maxima. However, if remove_duplicates == True in detect_spots , only one will be deemed a local maxima. This is achieved by adding a random shift to the intensity of each pixel. The max possible shift is 0.2 so it will not change the integer version of the image but it will ensure each pixel has a different intensity to its neighbour.","title":"Spot detection"},{"location":"pipeline/find_spots/#imaging-spots","text":"For non reference spots (all round/channel combinations apart from ref_round / ref_channel ), we only use the spots for registration to the reference spots, so the quantity of spots is not important. In fact, registration tends to work better if there are fewer but more reliable spots, as this means there is a lesser chance of matching up spots by chance. To exploit this, for each imaging tile, round, channel, the point cloud is made up of the max_spots most intense spots on each z-plane. In 2D , max_spots is config['find_spots']['max_spots_2d'] and in 3D , it is config['find_spots']['max_spots_3d'] . If there are fewer than max_spots spots detected on a particular z-plane, all the spots will be kept.","title":"Imaging spots"},{"location":"pipeline/find_spots/#reference-spots","text":"We want to assign a gene to each reference spot ( ref_round / ref_channel ), as well as use it for registration, so it is beneficial to maximise the number of reference spots. As such, we do not do the max_spots thresholding for reference spots. However, we want to know which reference spots are isolated because when it comes to the bleed_matrix calculation, we do not want to use overlapping spots.","title":"Reference spots"},{"location":"pipeline/find_spots/#isolated-spots","text":"We deem a spot to be isolated if it has a prominent negative annulus , because if there was an overlapping spot, you would expect positive intensity in the annulus around the spot. We find the intensity of the annulus by computing the correlation of the image with an annulus kernel obtained from annulus(r0, r_xy, r_z) where: r0 = config['find_spots']['isolation_radius_inner'] r_xy = config['find_spots']['isolation_radius_xy'] r_z = config['find_spots']['isolation_radius_z'] . If the value of this correlation at the location of a spot is less than config['find_spots']['isolation_thresh'] , then we deem the spot to be isolated. If config['find_spots']['isolation_thresh'] is not given, it is set to: config [ 'find_spots' ][ 'auto_isolation_thresh_multiplier' ] * auto_thresh [ t , r , c ] The final isolation thresholds used for each tile are saved as nb.find_spots.isolation_thresh . The process of obtaining isolated spots can be visualised with view_find_spots . Annulus kernel The annulus kernel should be equal to 1 over the pixels in the neighbourhood of an isolated spot which are usually negative. The example images below show a typical spot (left) and the annulus kernel used for this data (right) with r0 = 4 , r_xy = 14 and r_z = 1 . The dimensions of each image is 29 x 29 pixels, red is positive and blue is negative. z = -1 z = 0 z = 1 z = -1 z = 0 z = 1","title":"Isolated spots"},{"location":"pipeline/find_spots/#error-too-few-spots","text":"After the find_spots NotebookPage has been added to the Notebook , check_n_spots will be run. This will produce a warning for any tile, round, channel for which fewer than n_spots_warn = config [ 'find_spots' ][ 'n_spots_warn_fraction' ] * max_spots * nb . basic_info . nz spots were detected, where max_spots is config['find_spots']['max_spots_2d'] if 2D and config['find_spots']['max_spots_3d'] if 3D . An error will be raised if any of the following is satisfied: For any given channel, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/rounds. The faulty channels should then be removed from use_channels . For any given tile, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of rounds/channels. The faulty tiles should then be removed from use_tiles . For any given round, the number of spots found was less than n_spots_warn for at least the fraction n_spots_error_fraction of tiles/channels. The faulty rounds should then be removed from use_rounds . Example The following is the \\(n_{tiles}\\) (3) x \\(n_{rounds}\\) (5) array of number of spots found for a given channel: spot_no = array ([[ 1295 , 1016 , 869 , 719 , 829 ], [ 1055 , 888 , 687 , 556 , 824 ], [ 5901 , 4208 , 5160 , 4069 , 4006 ]]) The value of n_spots_warn for this experiment is 3500 so a warning will be raised for the 10 tiles/rounds for which spot_no[t, r] < n_spots_warn : array ([[ 0 , 0 ], [ 0 , 1 ], [ 0 , 2 ], [ 0 , 3 ], [ 0 , 4 ], [ 1 , 0 ], [ 1 , 1 ], [ 1 , 2 ], [ 1 , 3 ], [ 1 , 4 ]]) The value of n_spots_error_fraction for this experiment is 0.5 so the threshold number of failed tiles/rounds to give an error is \\(0.5 \\times n_{tiles} \\times n_{rounds} = 7.5\\) . We have 10 failed tiles/rounds so an error would be raised in this case. The use_tiles , use_rounds and use_channels parameters can be changed without having to re-run the find_spots section of the pipeline as explained here . If tiles/rounds/channels are added instead of removed though, it will need re-running, as will the extract step.","title":"Error - too few spots"},{"location":"pipeline/find_spots/#n_spots_grid","text":"The n_spots_grid function is useful to visualise the number of spots detected on each tile, round and channel: Good Bad In the good example, you can see from the minimum on the colorbar that lots of spots have been detected on every image. You can also see that tile 13 seems to have significantly fewer spots than the other tiles and that round 0 often has fewer spots than the other rounds. In the bad example, all tiles, rounds and channels where n_spots < n_spots_warn are highlighted by a red border. Clearly, channels 0 and 3 did not work for this experiment and channel 1 probably didn't. Also, tile 1 appears to have fewer spots than the other tiles.","title":"n_spots_grid"},{"location":"pipeline/find_spots/#viewer","text":"We can see how the various parameters affect which spots are detected using view_find_spots . This can be called as follows (in the Without Notebook case, the raw images will be loaded and then filtered according to parameters in config['extract'] ). With Notebook Without Notebook Reference round/channel showing isolated spots from coppafish import Notebook from coppafish.plot import view_find_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = 3 # round to view c = 6 # channels to view view_find_spots ( nb , t , r , c ) from coppafish.plot import view_filter ini_file = '/Users/user/coppafish/experiment/settings.ini' t = 1 # tile to view r = 3 # round to view c = 6 # channel to view view_find_spots ( None , t , r , c , config_file = ini_file ) from coppafish import Notebook from coppafish.plot import view_find_spots nb_file = '/Users/user/coppafish/experiment/notebook.npz' nb = Notebook ( nb_file ) t = 1 # tile to view r = nb . basic_info . ref_round # round to view c = nb . basic_info . ref_channel # channel to view view_find_spots ( nb , t , r , c , show_isolated = True ) This will open a napari viewer with up to 5 sliders in the bottom left: Detection Radius YX : This is the value of config['find_spots']['r_xy'] . Detection Radius Z : This is the value of config['find_spots']['r_z'] . Intensity Threshold : This is the value of nb.extract.auto_thresh[t, r, c] . Isolation Threshold : This is the value of nb.find_spots.isolation_thresh[t] . It will only appear if show_isolated == True , r = nb.basic_info.ref_round and c = nb.basic_info.ref_channel . Z Thickness : Spots detected on the current z-plane and this many z-planes either side of it will be shown. Initially, this will be set to 1 so spots from the current z-plane and 1 either side of it will be shown. Whenever the first two are changed, the dilation will be redone using the new values of the radii and the time taken will be printed to the console. The correlation calculation required to determine which spots are isolated is slow hence, by default show_isolated == False .","title":"Viewer"},{"location":"pipeline/find_spots/#z-thickness","text":"The images below show the effect of changing the z-thickness. The size of the spots is related to the z-plane they were detected on. The closer to the current z-plane, the larger they appear: 0 1 2 The blue spot has a neighbouring pixel with negative intensity and is not kept in the final point cloud.","title":"Z thickness"},{"location":"pipeline/find_spots/#detection-radius","text":"","title":"Detection radius"},{"location":"pipeline/find_spots/#yx","text":"The images below show the effect of using the slider to change config['find_spots']['r_xy'] with config['find_spots']['r_z'] fixed at 2 and z thickness = 2: 2 3 10 Clearly, as the radius increases the spots have to be separated by a larger distance, resulting in less spots found.","title":"YX"},{"location":"pipeline/find_spots/#z","text":"The images below show the effect of using the slider to change config['find_spots']['r_z'] with config['find_spots']['r_yx'] fixed at 2 and z thickness = 2: 2 6 12 Again we see the number of spots reducing as this increases. However, it is less clear as to why, because the minimum separation in z is what is changing but, we are only seeing spots from 5 z-planes imposed on a single z-plane.","title":"Z"},{"location":"pipeline/find_spots/#intensity-threshold","text":"The images below show the effect of using the slider to change nb.extract.auto_thresh[t, r, c] . This is for a 2D experiment with config['find_spots']['r_yx'] = 2 . 447 2234 4757 This is useful to see what a suitable intensity threshold should be. For example, the 447 plot identifies spots which do not look real while the 4757 plot misses some obvious spots. The value of nb.extract.auto_thresh[t, r, c] obtained for this data is approximately 2234. The green spots are those which are identified as isolated.","title":"Intensity threshold"},{"location":"pipeline/find_spots/#changing-auto_thresh","text":"If after playing with this slider, it is decided that the find_spots part of the pipeline should be run (or re-run) with the updated intensity threshold, new_thresh , for tile \\(t\\) , round \\(r\\) , channel \\(c\\) , this can be achieved by setting nb.extract.auto_thresh[t, r, c] = new_thresh . Note, this is an abuse of the rules of the Notebook as it is changing a variable after it has been added and thus should not be allowed, but it does work. To keep the new_thresh value, the Notebook will need saving after nb.extract.auto_thresh has been updated. It should be saved to a different location, so it does not overwrite the initial Notebook i.e. nb.save('/Users/user/experiment1/output/notebook_new_auto_thresh.npz') .","title":"Changing auto_thresh"},{"location":"pipeline/find_spots/#isolation-threshold","text":"The images below show the effect of using the slider to change nb.find_spots.isolation_thresh[t] . This is for a 2D experiment with: config['find_spots']['r_yx'] = 2 , config['find_spots']['isolation_radius_inner'] = 2 config['find_spots']['isolation_radius_xy'] = 2 . There is no slider to change the isolation radii because the dilation calculation is quite slow and has to be re-done everytime the radii change. -200 -438 -494 Here, we see that as the absolute threshold increases, the spots need a darker (more negative) annulus to be considered isolated (green). The value of -438 is approximately the value of nb.find_spots.isolation_thresh[t] used for this data (i.e. config['find_spots']['auto_isolation_thresh_multiplier'] = -0.2 and auto_thresh[t, r, c] = 2234 gives -447 which is almost the same).","title":"Isolation threshold"},{"location":"pipeline/find_spots/#pseudocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . for r in use_rounds: for t in use_tiles: for c in use_channels: if r is anchor_round and c is not anchor_channel: Skip to next channel as no spots need detecting on this channel. im = load image from npy file in tile directory spots_trc = detect spots(im) Remove spots with negative neighbouring pixel from spots_trc if r is ref_round and c is ref_channel: Determine which spots are isolated. else: Keep only most intense spots on each z-plane. Set spot_no[t, r, c] to be the number of spots found. For spot s, record in spot_details[s]: - tile: tile found on - round: round found on - channel: channel found on - isolated: whether spot isolated (if not ref_round/ref_channel, this will be False) - y: y coordinate of spot in tile - x: x coordinate of spot in tile - z: z coordinate of spot in tile Add isolation_threshold, spot_no and spot_details to find_spots NotebookPage Return find_spots NotebookPage","title":"Pseudocode"},{"location":"pipeline/get_reference_spots/","text":"Get Reference Spots The get reference spots step of the pipeline uses the affine transforms found in the register step of the pipeline to compute the corresponding coordinate of each reference spot (detected on the reference round/reference channel ( \\(r_{ref}\\) / \\(c_{ref}\\) ) in find spots step ) in each imaging round and channel. By reading off the intensity values at these coordinates, an \\(n_{rounds} \\times n_{channels}\\) intensity vector or spot_color can be found for each reference spot. These intensity vectors are saved as colors in the ref_spots NotebookPage which is added to the Notebook after this stage is finished and are used for assigning each spot to a gene in the call reference spots step . Variables in ref_spots NotebookPage All variables in the ref_spots NotebookPage are arrays where the size of the first axis is \\(n_{spots}\\) i.e. each variable has info for each reference spot. The variables local_yxz , isolated and tile are just copied from nb.find_spots.spot_details . The variables gene_no , score , score_diff , intensity are not computed until the call reference spots step but because each is an array of size \\(n_{spots}\\) , they are saved in the ref_spots page instead of the call_spots page. Before the call reference spots step though, their values will be set to None . This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run. Spot Colors Duplicates We don't find the spot_color for every reference spot because there will be duplicates - the same spot detected on more than 1 tile. This is because there is an overlap between the tiles. To remove these duplicates , we only keep spots which were detected on a tile (saved in nb.find_spots.spot_details during the find spots step ) which is also the tile whose centre they are closest to in the global coordinate system ( nb.stitch.tile_origin + nb.basic_info.tile_centre ). The view_stitch function shows the duplicate spots in blue. Applying transform To determine the aligned coordinate of each reference spot detected on tile \\(t\\) in round \\(r\\) , channel \\(c\\) , we must apply the affine transform found for tile \\(t\\) , round \\(r\\) , channel \\(c\\) : nb.register.transform[t, r, c] to the \\(yxz\\) coordinates of the spots. First the \\(yxz\\) coordinates must be centered (subtract nb.basic_info.tile_centre ) and the z-coordinate must be converted into units of yx-pixels (multiply by z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy ). The \\(n_{spots} \\times 3\\) array must then be padded with ones to form an \\(n_{spots} \\times 4\\) array, so it can be multiplied by the \\(4 \\times 3\\) transform. The coordinates are prepared in this way because they must be in the same form as was used to compute the transform (see Preparing point clouds and Padding ref_spot_yxz notes here ). Once the \\(n_{spots} \\times 4\\) spot coordinate array is multiplied by the \\(4 \\times 3\\) transform, a \\(n_{spots} \\times 3\\) array is obtained, and after the z-scaling and centering are removed, this gives the corresponding \\(yxz\\) coordinates in round \\(r\\) , channel \\(c\\) . Reading off intensity After the \\(yxz\\) coordinates in tile \\(t\\) , round \\(r\\) , channel \\(c\\) , are found, the intensity values at these coordinates are obtained by supplying the \\(n_{spots} \\times 3\\) array as the parameter yxz in the function load_tile . After doing this for all tiles, rounds and channels, we obtain the spot_color where spot_color[s, r, c] is the intensity value found for spot s in round r , channel c . Invalid Values For some spots, the corresponding coordinates in round \\(r\\) , channel \\(c\\) , will be outside the bounds of the tile and thus the intensity cannot be read off. We therefore only save to the Notebook spots which remain in the tile bounds across all rounds and channels, allowing the full spot_color array to be computed. Once the Notebook has the ref_spots NotebookPage , when view_stitch is run, there will a button called No Spot Color which shows in blue all spots removed for this reason: spot_color outside rounds/channels used nb.ref_spots.colors is an n_spots x nb.basic_info.n_rounds x nb.basic_info.n_channels array. nb.ref_spots.colors[s, r, c] will be set to -nb.basic_info.tile_pixel_value_shift for all spots, s , if either r is not in nb.basic_info.use_rounds or c is not in nb.basic_info.use_channels . This is because it is impossible for an actual pixel to have this intensity, due to clipping done in the extract step when saving the tiles . So basically, this is an integer version of nan . Pseudocode This is the pseudocode outlining the basics of this step of the pipeline . r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. transform[t, r, c] = affine transform between tile t, round r_ref, channel c_ref and round r, channel c. [n_tiles x n_rounds x n_channels x 4 x 3] Remove duplicate spots from spot_yxz[:, r_ref, c_ref]. Center reference point cloud: spot_yxz[:, r_ref, c_ref] = spot_yxz[:, r_ref, c_ref] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[:, r_ref, c_ref][:, 2] = spot_yxz[:, r_ref, c_ref][:, 2] * z_scale Pad reference point cloud with ones: spot_yxz[:, r_ref, c_ref][:, 3] = 1 for t in use_tiles: spot_colors_t: [n_spots_t x n_rounds x n_channels] for r in use_rounds: for c in use_channels: Apply transform to get new coordinates in round r, channel c: spot_yxz_rc = spot_yxz[t, r_ref, c_ref] @ transform[t, r, c] Convert z coordinate back to z-pixels: spot_yxz_rc[:, 2] = spot_yxz_rc[:, 2] / z_scale Remove centering: spot_yxz_rc = spot_yxz_rc + tile_centre For spots where spot_yxz_rc is outside the tile bounds, cannot read off intensity: spot_colors_t[oob_spots, r, c] = nan For all other spots, we read off the intensity at the coordinates we found image_trc = load in image for tile t, round r, channel c from npy file in tile directory spot_colors_t[good_spots, r, c] = image_trc[spot_yxz_rc[good_spots]] Concatenate all spot_colors_t together so have one large [n_spots x n_rounds x n_channels] array giving colors for all spots across all tiles. Get rid of any spot for which at least one round and channel has the nan value i.e. they were out of bounds on at least one round/channel. Add colors to ref_spots NotebookPage Add local_yxz, isolated and tile variables by reshaping information in nb.find_spots.spot_details. Add gene_no, score, score_diff, intensity to ref_spots NotebookPage all with the value None.","title":"Get Reference Spots"},{"location":"pipeline/get_reference_spots/#get-reference-spots","text":"The get reference spots step of the pipeline uses the affine transforms found in the register step of the pipeline to compute the corresponding coordinate of each reference spot (detected on the reference round/reference channel ( \\(r_{ref}\\) / \\(c_{ref}\\) ) in find spots step ) in each imaging round and channel. By reading off the intensity values at these coordinates, an \\(n_{rounds} \\times n_{channels}\\) intensity vector or spot_color can be found for each reference spot. These intensity vectors are saved as colors in the ref_spots NotebookPage which is added to the Notebook after this stage is finished and are used for assigning each spot to a gene in the call reference spots step . Variables in ref_spots NotebookPage All variables in the ref_spots NotebookPage are arrays where the size of the first axis is \\(n_{spots}\\) i.e. each variable has info for each reference spot. The variables local_yxz , isolated and tile are just copied from nb.find_spots.spot_details . The variables gene_no , score , score_diff , intensity are not computed until the call reference spots step but because each is an array of size \\(n_{spots}\\) , they are saved in the ref_spots page instead of the call_spots page. Before the call reference spots step though, their values will be set to None . This is so if there is an error in call_reference_spots , get_reference_spots won't have to be re-run.","title":"Get Reference Spots"},{"location":"pipeline/get_reference_spots/#spot-colors","text":"","title":"Spot Colors"},{"location":"pipeline/get_reference_spots/#duplicates","text":"We don't find the spot_color for every reference spot because there will be duplicates - the same spot detected on more than 1 tile. This is because there is an overlap between the tiles. To remove these duplicates , we only keep spots which were detected on a tile (saved in nb.find_spots.spot_details during the find spots step ) which is also the tile whose centre they are closest to in the global coordinate system ( nb.stitch.tile_origin + nb.basic_info.tile_centre ). The view_stitch function shows the duplicate spots in blue.","title":"Duplicates"},{"location":"pipeline/get_reference_spots/#applying-transform","text":"To determine the aligned coordinate of each reference spot detected on tile \\(t\\) in round \\(r\\) , channel \\(c\\) , we must apply the affine transform found for tile \\(t\\) , round \\(r\\) , channel \\(c\\) : nb.register.transform[t, r, c] to the \\(yxz\\) coordinates of the spots. First the \\(yxz\\) coordinates must be centered (subtract nb.basic_info.tile_centre ) and the z-coordinate must be converted into units of yx-pixels (multiply by z_scale = nb.basic_info.pixel_size_z / nb.basic_info.pixel_size_xy ). The \\(n_{spots} \\times 3\\) array must then be padded with ones to form an \\(n_{spots} \\times 4\\) array, so it can be multiplied by the \\(4 \\times 3\\) transform. The coordinates are prepared in this way because they must be in the same form as was used to compute the transform (see Preparing point clouds and Padding ref_spot_yxz notes here ). Once the \\(n_{spots} \\times 4\\) spot coordinate array is multiplied by the \\(4 \\times 3\\) transform, a \\(n_{spots} \\times 3\\) array is obtained, and after the z-scaling and centering are removed, this gives the corresponding \\(yxz\\) coordinates in round \\(r\\) , channel \\(c\\) .","title":"Applying transform"},{"location":"pipeline/get_reference_spots/#reading-off-intensity","text":"After the \\(yxz\\) coordinates in tile \\(t\\) , round \\(r\\) , channel \\(c\\) , are found, the intensity values at these coordinates are obtained by supplying the \\(n_{spots} \\times 3\\) array as the parameter yxz in the function load_tile . After doing this for all tiles, rounds and channels, we obtain the spot_color where spot_color[s, r, c] is the intensity value found for spot s in round r , channel c .","title":"Reading off intensity"},{"location":"pipeline/get_reference_spots/#invalid-values","text":"For some spots, the corresponding coordinates in round \\(r\\) , channel \\(c\\) , will be outside the bounds of the tile and thus the intensity cannot be read off. We therefore only save to the Notebook spots which remain in the tile bounds across all rounds and channels, allowing the full spot_color array to be computed. Once the Notebook has the ref_spots NotebookPage , when view_stitch is run, there will a button called No Spot Color which shows in blue all spots removed for this reason: spot_color outside rounds/channels used nb.ref_spots.colors is an n_spots x nb.basic_info.n_rounds x nb.basic_info.n_channels array. nb.ref_spots.colors[s, r, c] will be set to -nb.basic_info.tile_pixel_value_shift for all spots, s , if either r is not in nb.basic_info.use_rounds or c is not in nb.basic_info.use_channels . This is because it is impossible for an actual pixel to have this intensity, due to clipping done in the extract step when saving the tiles . So basically, this is an integer version of nan .","title":"Invalid Values"},{"location":"pipeline/get_reference_spots/#pseudocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. transform[t, r, c] = affine transform between tile t, round r_ref, channel c_ref and round r, channel c. [n_tiles x n_rounds x n_channels x 4 x 3] Remove duplicate spots from spot_yxz[:, r_ref, c_ref]. Center reference point cloud: spot_yxz[:, r_ref, c_ref] = spot_yxz[:, r_ref, c_ref] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[:, r_ref, c_ref][:, 2] = spot_yxz[:, r_ref, c_ref][:, 2] * z_scale Pad reference point cloud with ones: spot_yxz[:, r_ref, c_ref][:, 3] = 1 for t in use_tiles: spot_colors_t: [n_spots_t x n_rounds x n_channels] for r in use_rounds: for c in use_channels: Apply transform to get new coordinates in round r, channel c: spot_yxz_rc = spot_yxz[t, r_ref, c_ref] @ transform[t, r, c] Convert z coordinate back to z-pixels: spot_yxz_rc[:, 2] = spot_yxz_rc[:, 2] / z_scale Remove centering: spot_yxz_rc = spot_yxz_rc + tile_centre For spots where spot_yxz_rc is outside the tile bounds, cannot read off intensity: spot_colors_t[oob_spots, r, c] = nan For all other spots, we read off the intensity at the coordinates we found image_trc = load in image for tile t, round r, channel c from npy file in tile directory spot_colors_t[good_spots, r, c] = image_trc[spot_yxz_rc[good_spots]] Concatenate all spot_colors_t together so have one large [n_spots x n_rounds x n_channels] array giving colors for all spots across all tiles. Get rid of any spot for which at least one round and channel has the nan value i.e. they were out of bounds on at least one round/channel. Add colors to ref_spots NotebookPage Add local_yxz, isolated and tile variables by reshaping information in nb.find_spots.spot_details. Add gene_no, score, score_diff, intensity to ref_spots NotebookPage all with the value None.","title":"Pseudocode"},{"location":"pipeline/omp/","text":"OMP The OMP step of the pipeline runs an Orthogonal Matching Pursuit ( OMP ) algorithm on every pixel, adding multiple gene bled_codes to explain the \\(n_{rounds} \\times n_{channels}\\) intensity vector at each pixel. This gives us a coefficient, \\(\\mu_{sg}\\) for every pixel, \\(s\\) , and gene, \\(g\\) , and thus we can create a coefficient image for every gene. By doing a local maxima search on these images, we can obtain another estimate of the distribution of genes, which we can compare to that obtained from the reference spots . The number and location of spots found by the OMP method will be different though. There are some variables obtained for each spot ( local_yxz , tile , colors , gene_no and intensity ) saved in the omp NotebookPage which are equivalent to the same variables saved in the ref_spots NotebookPage . There are also some other variables added to the omp NotebookPage which relate to the typical shape of a spot in the gene coefficient images. The OMP section takes quite a long time, so config['omp']['use_z'] can be used to only run OMP on a subset of z-planes. Note: config in this section, with no section specified, means config['omp'] Re-run call_spots_omp To re-run the OMP section, the files generated during the OMP step which were saved at the paths indicated by: nb.file_names.omp_spot_shape nb.file_names.omp_spot_info nb.file_names.omp_spot_coef need to be deleted or re-named (alternatively the omp_spot_shape , omp_spot_info and omp_spot_coef parameters in the file names section of the configuration file can be changed). This is so the old data is not loaded in when the OMP part of the pipeline is run. Other than that, the usual instructions can be followed. Why bother with OMP ? There are two main reasons to use the OMP method for finding the distribution of genes instead of reference spots . The first is that OMP fits multiple coefficients to every pixel and thus allows for overlapping spots: view_omp_fit view_omp In the view_omp_fit plot, it shows an example of a spot color, \\(\\pmb{\\zeta}_s\\) , which requires the bled_code of both Plp1 and Aldoc to explain it. The view_omp plot then shows that spots for both these genes are detected with the OMP method. The reference spots method though, can only fit one gene to each pixel, so here it only detects the Plp1 spot. The second reason is that the reference spots method can only assign genes to spots detected on the reference round / reference channel images. It is thus restricted, because there may be genes present at pixels other than the finite amount considered. Also, the config['find_spots']['radius_xy'] and config['find_spots']['radius_z'] parameters in the spot detection necessitates a minimum distance between any two genes. The OMP method says that spots are local maxima in the gene coefficient images, and thus it can find spots at locations other than the location of the reference spots . Also, because it does a separate local maxima search for each gene, the config['radius_xy'] and config['radius_z'] parameters in spot detection only necessitates a minimum distance between two spots of the same gene. The consequence of this, is that the spots detected by the OMP method tend to be in more dense clusters, as shown by the coppafish.Viewer images below. This is then more useful for cell typing Reference Spots OMP Initial Intensity Threshold To produce the gene coefficient images, as shown by the view_omp_fit function, we need to run OMP on every pixel in the image. However, for a single \\(2048\\times2048\\times50\\) tile, there are \\(2.1\\times10^8\\) pixels. Thus, we do an initial thresholding so as not to consider all of them. We only consider pixel \\(s\\) , with pixel color \\(\\pmb{\\zeta}_s\\) , if it has an intensity computed from its absolute pixel color, \\(\\tilde{\\chi}_s\\) , greater than config['initial_intensity_thresh'] . I.e. \\[ \\tilde{\\chi}_s = \\underset{r}{\\mathrm{median}}(\\max_c|\\zeta_{s_{rc}}|) \\] Why is absolute pixel color is used? We use \\(|\\pmb{\\zeta}_s|\\) because we are also interested in whether a pixel has a negative gene coefficient. We expect a negative coefficient in an annulus around a spot, as shown in the view_omp plots, as a result of the difference of hanning kernel used in the initial filtering. If a gene has a negative coefficient annulus around the local maxima, then it boosts our confidence that it is legitimate. If config['initial_intensity_thresh'] is not specified, it is set to the percentile indicated by config['initial_intensity_thresh_percentile'] of \\(\\tilde{\\chi}\\) computed for all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). I.e., it is set to: nb . call_spots . abs_intensity_percentile [ config [ 'initial_intensity_thresh_percentile' ]] So this is saying that for a tile and z-plane which we expect to be among the most fluorescent, we are getting rid of a quarter of the pixels (with the default value of config['initial_intensity_thresh'] = 25 ) which are the least intense. On other z-planes, we will get rid of more. OMP Algorithm For every pixel that passes the initial intensity threshold, \\(s\\) , we run an Orthogonal Matching Pursuit ( OMP ) algorithm to find a coefficient, \\(\\mu_{sg}\\) , for every gene \\(g\\) . The pseudocode for how this is done for each pixel is given below: color: Intensity read from .npy files in tile directory for each round and channel. [n_rounds x n_channels] color_norm_factor: nb.call_spots.color_norm_factor [n_rounds x n_channels] bled_codes: nb.call_spots.bled_codes_ge [n_genes x n_rounds x n_channels] background_codes: nb.call_spots.background_codes [n_channels x n_rounds x n_channels] 1. Normalise color color = color / color_norm_factor 2. Compute background_coefs. Remove background from color - for c in range(n_channels): color = color - background_coefs[c] * background_codes[c] 3. Initialize variables for iterating. residual = color added_genes = [] Append background_codes to bled_codes so has shape [(n_genes+n_channels) x n_rounds x n_channels] Initialize coefs with background_coefs which will not change - coefs = zeros(n_genes+n_channels) coefs[n_genes:] = background_coefs i = 0 while i < n_iter: 4. Find best_gene to add based on dot product score between residual and bled_codes. If score < score_thresh or best_gene is background or best_gene already added: Stop - go to step 7. Append best_gene to added_genes. 5. Obtain added_coefs [i+1] for how the bled_codes of all genes in added_genes can be combined to best explain color. Update coefs - for g_ind in range(i+1): coefs[added_genes[g_ind]] = added_coefs[g_ind] 6. Update residual - residual = color for g in added_genes: residual = residual - coefs[g] * bled_codes[g] i += 1 7. return coefs There are a few parameters in the configuration file which are used: max_genes : This is n_iter in the above code. dp_thresh : This is score_thresh in the above code. Pre-Iteration Procedure Prior to Step 1 , color is \\(\\pmb{\\acute{\\zeta}}_s\\) found through get_spot_colors by obtaining the aligned coordinate of pixel \\(s\\) in each round and channel and then reading off the corresponding intensity . Step 1 is then just converting color from \\(\\pmb{\\acute{\\zeta}}_s\\) to \\(\\pmb{\\zeta}_s\\) so the color channels are equalised, as is done in call_reference_spots . Step 2 is just finding a coefficient \\(\\mu_{sC}\\) for each background gene , \\(\\pmb{B}_C\\) as is done in call_reference_spots . The value of \\(\\lambda_b\\) in the \\(w_{rC}\\) equation is set to nb.call_spots.background_weight_shift so the same value is used for the reference spots and OMP methods. After this step, color is \\(\\pmb{\\zeta}_{s0}\\) and will always remain so. I.e. once background is fit once, it is never updated. Step 3 is adding the artificial background genes to our actual genes. After this step, we have a set of bled_codes , \\(\\pmb{b}\\) such that \\(\\pmb{b}_{g=n_{g}+C} = \\pmb{B}_C\\) . We also have a set of coefficients, \\(\\pmb{\\mu}_{s0}\\) such that \\(\\mu_{s0g=n_g+C} = \\mu_{sC}\\) . The set of coefficients after \\(i\\) actual genes have been fit is \\(\\pmb{\\mu}_{si}\\) ( \\(i=0\\) means just background has been fit) but \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i,C\\) as the background coefficients are never updated. Finding Best Gene Step 4 is finding the gene, \\(g_i\\) , with the bled_code , \\(b_{g_i}\\) which best represents the residual after \\(i\\) actual genes have been added, \\(\\pmb{\\zeta}_{si}\\) . We determine \\(g_i\\) to be the gene for which \\(|\\Delta_{sig}|\\) is the largest, where \\(\\Delta_{sig}\\) is exactly the same dot product score used in call_reference_spots . We use the absolute score here because negative gene coefficients also provide useful information as explained earlier . Dot Product Score Parameters In the configuration file, alpha and beta are specified in both the call_spots and omp sections, so different values of these parameters can be used for each method. The value of \\(\\lambda_d\\) in the \\(\\tilde{\\zeta}_{{si}_{rc}}\\) equation is set to nb.call_spots.dp_norm_shift * sqrt(n_rounds) so the same value is used for the reference spots and OMP methods. After we have found \\(g_i\\) , we stop the algorithm if any of the following are satisfied: \\(|\\Delta_{si}| = |\\Delta_{sig_i}|\\) < config['dp_thresh'] . \\(g_i \\geq n_g\\) i.e. \\(g_i\\) is a background gene . \\(g_i\\) was found to be the best gene on a previous iteration. This second condition occurred for the third gene added in the view_omp_fit example shown earlier . This case is quite rare though, since the background genes have already been fit. We include it because if there is an artificial gene being identified as the best one, then we are also likely to erroneously assign actual genes to the pixel which are not actually there. But stopping the algorithm prevents this from happening. It may be more appropriate to recompute the background coefficient, for the background gene identified if this happens though. The third condition is just to stop getting in a loop where we are always fitting the same gene, but it is even rarer than the second. Finding Gene Coefficients Once we have decided that a gene is acceptable, in Step 5 we find the coefficient of that gene as well as updating the coefficients of all genes previously fit. On iteration \\(i\\) , there will be \\(i+1\\) genes to find the coefficient of. The coefficients are found through normal least squares: \\[ \\tilde{\\pmb{\\mu}}_{si} = (\\pmb{G}_i^T\\pmb{G}_i)^{-1}\\pmb{G}_i^T\\pmb{\\zeta}_{s0} \\] Where: \\(\\tilde{\\pmb{\\mu}}_{si}\\) is a vector of \\(i+1\\) values such that \\(\\mu_{sig_j} = \\tilde{\\mu}_{si_j}\\) where \\(\\mu_{sig_j}\\) is the coefficient found for gene \\(g_j\\) on iteration \\(i\\) for pixel \\(s\\) . \\(\\pmb{G}_i\\) is a matrix of shape \\([n_{rounds}n_{channels} \\times (i+1)]\\) such that column \\(j\\) is the flattened bled_code , \\(\\pmb{b}_{g_j}\\) , for gene \\(g_j\\) , added on iteration \\(j\\) . \\(\\pmb{\\zeta}_{s0}\\) is the color for pixel \\(s\\) after background removal flattened, so it has shape \\([n_{rounds}n_{channels} \\times 1]\\) . Weighted Least Squares If config['weight_coef_fit'] = True , then the coefficients are found through weighted least squares . In this case, both \\(\\pmb{\\zeta}_{s0}\\) and every column of \\(\\pmb{G}_i\\) are multiplied by \\(\\pmb{\\omega}_{si}\\) where \\(\\pmb{\\omega}^2_{si}\\) is defined in the dot product score calculation . The idea behind this is that for normal least squares, the coefficients will be overly influenced by outliers. In the Least Squares view_omp_fit example below, when fitting Plp1 , it is so concerned about getting rid of the very intense values in channel 4 that it makes lots of channel 2 and 6 values negative. Least Squares Weighted Least Squares Weight - Iteration 1 Weight - Iteration 2 In the Weighted Least Squares , we see that after Aldoc has been fit, the channel 4 intensity is much larger than in the Least Squares case and the channel 6 intensities are much less negative. If we look at the Weight - Iteration 1 view_weight plot, we see that this occurs because the contribution of the channel 4 rounds is very small. A problem with this though, is that the weight is re-computed at each iteration which can cause the coefficient of each gene to change drastically. For example, after BG2 has been fit, we see that channel 6 has become very negative again. Looking at the Weight - Iteration 2 plot, we see that this is because the weight of round 4, channel 4 has increased. The coefficient for Plp1 in the Weighted Least Squares case changes from 0.84 to 0.93 after BG2 has been fit but in the Least Squares case, it changes from 1.00 to 1.01. After we have found the coefficients on iteration \\(i\\) , we compute the residual for the next iteration in Step 6 : \\[ \\pmb{\\zeta}_{si+1} = \\pmb{\\zeta}_{s0} - \\sum_{g=0}^{n_g-1}\\mu_{sig}\\pmb{b}_g = \\pmb{\\zeta}_s - \\sum_{g=0}^{n_g+n_c-1}\\mu_{sig}\\pmb{b}_g \\] Where \\(n_c\\) is the number of channels and \\(n_g\\) is the number of genes. The second equation includes the combination from the background genes . In the first equation, \\(\\mu_{sig}\\) will only be non-zero for \\(i+1\\) genes (in the second equation, it will be for \\(i+1+n_c\\) genes, because we include the \\(n_c\\) background genes ). We continue iterating between Step 4 and Step 6 until any of the stopping criteria are met, or we fit config['max_genes'] to the pixel. Weighting in Dot Product Score The difference between this algorithm and the standard Orthogonal Matching Pursuit algorithm is the weight factor, \\(\\pmb{\\omega}^2_{si}\\) , used when computing \\(\\Delta_{sig}\\) . Normal OMP would have \\(\\alpha = 0\\) so \\(\\pmb{\\omega}^2_{si}=1\\) . By using this weighting, we are trying to say that if a gene has already been fit with high intensity in round \\(r\\) , channel \\(c\\) , then the remaining intensity in round \\(r\\) , channel \\(c\\) , after it has been fit is probably because the coefficient of that gene was not fit correctly, rather than because another gene is present. As the example below shows, without it, genes are fit to really try and get rid of any anomalously intense rounds/channels even if a gene has already been fit there. view_omp_fit - \\(\\alpha=120\\) view_omp_fit - \\(\\alpha=0\\) view_weight - \\(\\alpha=120\\) , Iteration 4 view_score - \\(\\alpha=120\\) , Iteration 4 If we compare the view_omp_fit plots, we see that in the \\(\\alpha=0\\) case, Rgs4 has been fit to explain the anomalously negative round 6, channel 5 and Aldoc has been fit to explain the anomalously positive round 2, channel 1. The view_weight image shows the calculation of the weight factor for \\(i=4\\) (all 4 of the actual genes shown have been added). This shows that round 2, channel 1 has a low weighting because Lhx6 has already been fit and this gene has high intensity in round 2, channel 1. If we look the view_score image, we see that in the top right, with \\(\\alpha=0\\) , we get a large score for Aldoc solely because of the contribution from round 2, channel 1. If we look at the bottom right though, we see that with \\(\\alpha=120\\) , the score for Aldoc is very small, as we would expect it should be, when comparing the 2 plots in the second column of the view_score image. Basically, we need the weighting because we know that the least squares coefficient fitting will produce some anomalously intense rounds and channels. When selecting the best gene , we need to be robust to this. Finding Spots After we have run the OMP algorithm on every pixel of a tile, we can produce an image for each gene based on the \\(n_{pixels}\\times n_{genes}\\) array of coefficients, \\(\\pmb{\\mu}\\) . A \\(200\\times 200\\) pixel section of such an image for three genes is shown on the right. Clearly, most values in each image are zero because each pixel only has a non-zero coefficient for a very small fraction of genes. We then take each gene in turn and find spots , which are the local maxima in the coefficient image. These local maxima are found in exactly the same way as in the find spots part of the pipeline. But here, the threshold intensity is 0 and the neighbourhood (kernel for dilation) is defined by the parameters config['radius_xy'] and config['radius_z'] . Spot Shape In the gene coefficient images above, the spots can clearly be seen as red (positive) circles surrounded by a blue (negative) annulus. So to decide whether a particular spot is legitimate, we want to compare its shape to the average spot shape. This average spot shape can be specified in advance (e.g. if you have already run an experiment, which is expected to be similar to the current one, and you want the same shape to be used) with a npy file in the output directory with the name given by config['file_names']['omp_spot_shape'] . This file must contain an image (axis in the order z-y-x) indicating the expected sign of a coefficient (only values are 1, 0, -1) in the neighbourhood of a spot. If the file indicated by config['file_names']['omp_spot_shape'] does not exist, then it will be computed from the spots found on a specific tile. The tile used is the one for which the most spots were found with the reference spots method, and it is saved as nb.omp.shape_tile . The psuedocode for obtaining the spot shape is given below: spot_yxz: yxz coordinates of all spots found on the tile [n_spots x 3] gene_no: gene_no[s] is the gene whose coefficient image spot s was found on. [n_spots]. tile_sz: nb.basic_info.tile_sz nz: nb.basic_info.nz gene_coef_im: gene_coef_im[g] is the gene coefficient image for gene g, which can be made after running OMP on each pixel. [n_genes x tile_sz x tile_sz x nz] for s in range(n_spots): 1. use = True if number of pixels neighbouring spot_yxz[s] in gene_coef_im[gene_no[s]] with a positive value equals use_thresh. 2. If use, then obtain the [shape_ny x shape_nx x shape_nz] image centered on spot_yxz[s] in gene_coef_im[gene_no[s]]. 3. We now have spot_images which is a [n_use x shape_ny x shape_nx x shape_nz] array. We update this by only keeping images from isolated spots. [n_use2 x shape_ny x shape_nx x shape_nz] 4. Compute spot_sign_images by taking the sign of every value in spot_images so the only values are 1, 0, -1. Next, we compute av_spot_sign_image which is the average of spot_sign_images across spots. Where abs(av_spot_sign_image) < sign_thresh, set it to 0. Take sign of av_spot_sign_image so it only contains 1, 0, -1. [shape_ny x shape_nx x shape_nz] There are a few parameters in the configuration file which are used: shape_pos_neighbour_thresh : In 2D , this is use_thresh in the above code. In 3D , this is use_thresh - 2 . shape_max_size : This is [shape_ny, shape_nx, shape_nz] in the above code. shape_sign_thresh : This is sign_thresh in the above code. Step 1 is a thresholding procedure, so we only use spots we are quite confident are real for computing the average shape. For a spot to be used in computing the shape, on the z-plane of the gene coefficient image that it was found on, within the \\(n_{pos_{use}}\\times n_{pos_{use}}\\) neighbourhood with the local maxima in the centre, all n_use = \\(n_{pos_{use}}^2\\) coefficients must be positive. In 3D , the coefficient at the same \\(yx\\) coordinate as the local maxima but on 1 z-plane either side must be also positive (now require n_use = \\(n_{pos_{use}}^2 + 2\\) positive coefficients in the neighbourhood of the spot). The value of \\(n_{pos_{use}}^2\\) is given by config['shape_pos_neighbour_thresh'] , the default value is 9 meaning all pixels in a \\(3\\times 3\\) grid centered on the local maxima must be positive. Example The first image below shows a spot that would be used to compute the shape with config['shape_pos_neighbour_thresh'] = 3 because all 9 pixels in the central z-plane have a positive coefficient, as do those on 1 z plane either side but in the middle yx pixel. The second image shows a spot that would not be used. \u2705 \u274c In Step 2 , we just get the cropped gene coefficient image in the neighbourhood of the local maxima. 3 examples are shown as Spot 1 , Spot 2 and Spot 3 below. In Step 3 , we are saying that most spots are probably not overlapping with any other genes so to get an estimate of what an average spot looks like, let us only use spots which are quite well isolated. Our definition of isolated here, is that the distance between each spot used in Step 2 and any other spot used in Step 2 must exceed config[shape_isolation_dist] . Spot 1 Spot 2 Spot 3 Average Sign Spot Shape In Step 4 , we first take the sign of the spot images and then compute the average of these using get_average_spot_image . We set av_type = 'mean' and symmetry = 'annulus_3d' . We use this symmetry because we assume that the spot should be circular within a z-plane and symmetric in z. This procedure produces the Average Sign image shown above. This is saved to the Notebook as nb.omp.spot_shape_float . After we have this av_sign_image , we get our final spot_shape via: import numpy as np av_sign_image [ np . abs ( av_sign_image )] < config [ 'omp' ][ 'shape_sign_thresh' ]] = 0 spot_shape = np . sign ( av_sign_image ) This is just saying that if the absolute value of the mean sign across all these spots was less than config['omp']['shape_sign_thresh'] , then we are not sure what sign these pixels should be, so we won't assign them a sign in our final spot_shape . Otherwise, we are fairly confident what sign they should be, so we do assign the sign to our final shape. The Spot Shape image above shows what this looks like. Why does the spot shape indicate the expected sign of the coefficient? The spot_shape indicates the expected sign of a coefficient, not the expected value. This is because, through the OMP algorithm , for a pixel to have a non-zero coefficient for any gene, it has already gone through a thresholding procedure which has decided that the gene bled_code is required to explain the pixel color. The idea behind counting the number of coefficients with the correct sign is that if there are lots of pixels in the neighbourhood of the spot which required a certain gene, then that gives us confidence that the gene is actually present. If we instead convolved the actual coefficent image (e.g. Spot 1 image shown above) with the expected coefficient kernel (e.g. mean of spot_images in Step 4 of the pseudocode), it would boost the score of intense pixels which we don't want to do. The final spot_shape is saved to the Notebook as nb.omp.spot_shape and is also saved as a npy file in the output directory with the name indicated by config['file_names']['omp_spot_shape'] . The coordinates and corresponding gene of spots used to compute nb.omp.spot_shape are saved as nb.shape_spot_local_yxz and nb.shape_spot_gene_no respectively. Error - No negative values in nb.omp.spot_shape If config['shape_sign_thresh'] is too large, nb.omp.spot_shape will not have any -1 values. This will then raise an error because nb.omp_n_neighbours_neg cannot be computed. To get past this error, the OMP step needs to be re-run with either config['shape_sign_thresh'] set to a lower value or nb.omp.spot_shape specified through config['file_names']['omp_spot_shape'] . Example The following error was hit when running with config['shape_sign_thresh'] = 0.7 : The error saves the spot shape, nb.omp.spot_shape_float , which is converted to nb.omp.spot_shape through thresholding with config['shape_sign_thresh'] and then taking the sign. If we then load this file, we can get an idea of a suitable value of config['shape_sign_thresh'] : Code napari Viewer import numpy as np import napari im_file = \"/Users/joshduffield/Documents/UCL/ISS/Python/play/B8S5_Slice001_npy/omp_spot_shape_float_ERROR.npy\" im = np . load ( im_file ) napari . view_image ( im , contrast_limits = [ - 1 , 1 ], colormap = \"PiYG\" ) Say we decide after looking at the image in napari , that we want values more negative than at the blue cross to be included in the final shape. Then we set config['shape_sign_thresh'] to be just below the absolute value at this cross (0.336 as indicated in bottom left). The value of config['shape_sign_thresh'] indicated in the error is the maximum allowed value such that nb.omp.spot_shape will contain at least one pixel with the value of -1. If the saved nb.omp.spot_shape_float image has no negative values, then config['file_names']['omp_spot_shape'] must be used to specify nb.omp.spot_shape . The saved image can still be used as a guide as to where nb.omp.spot_shape=1 though. n_neighbours Once we have found the spot_shape , we want to quantify how much each spot resembles it. To do this, if spot \\(s\\) , was found on the gene \\(g\\) coefficient image, we first obtain coef_im_s which is the gene \\(g\\) coefficient image centered on the coordinates of spot \\(s\\) , with the same size as spot_shape . We then count the number of pixels for which coef_im_s and spot_shape both have positive coefficients. We also count the number of pixels for which coef_im_s and spot_shape both have negative coefficients. Once this has been done for all spots, these are saved as nb.omp.n_neighbours_pos and nb.omp.n_neighbours_neg respectively. To reduce the memory of the Notebook file, we only save spots to the Notebook if n_neighbours_pos exceeds config['initial_pos_neighbour_thresh'] . If this is left blank, it will be set to the fraction config['initial_pos_neighbour_thresh_param'] of the maximum possible value (i.e. sum(spot_shape>0) ). It will also be clipped between config['initial_pos_neighbour_thresh_min'] and config['initial_pos_neighbour_thresh_max'] . view_omp_score The way nb.omp.n_neighbours_pos and nb.omp.n_neighbours_neg are obtained, can be visualised for a specific spot with the function view_omp_score : \\(\\rho = 0.95\\) \\(\\rho = 0.1\\) \\(\\rho = 20\\) Here, the top plot shows nb.omp.spot_shape and the bottom plot shows the coefficient image for Snca in the neighbourhood of spot \\(371046\\) which has \\(yxz\\) coordinates in the global coordinate system of \\([3970, 509, 25]\\) . The green hatching in the top plot indicates where both plots have the same sign. So nb.omp.n_neighbours_pos[371046] is the number of red pixels with green hatching (316) and nb.omp.n_neighbours_neg[371046] is the number of blue pixels with green hatching (192). OMP Score The final score , \\(\\gamma\\) , used for thresholding OMP spots in coppafish.Viewer and when exporting to pciSeq is: \\[ \\gamma_s = \\frac{n_{neg_s} + \\rho n_{pos_s}}{n_{neg_{max}} + \\rho n_{pos_{max}}} \\] Where: \\(n_{neg_s}\\) is nb.omp.n_neighbours_neg[s] \\(n_{neg_s}\\) is nb.omp.n_neighbours_pos[s] \\(n_{neg_{max}}\\) is sum(nb.omp.spot_shape<0) \\(n_{pos_{max}}\\) is sum(nb.omp.spot_shape>0) \\(\\rho\\) is config['thresholds']['score_omp_multiplier'] So if \\(\\rho = 1\\) , this would just be the fraction of pixels in the neighbourhood of spot \\(s\\) with the correct sign. The larger \\(\\rho\\) , the greater the contribution of the positive pixels to \\(\\gamma_s\\) . With the view_omp_score function, the effect of varying \\(\\rho\\) on the score, can be seen by using the textbox. The top row of plots are also normalised so \\(\\gamma_s\\) is equal to the sum of the absolute value of the pixels with the green hatching. I.e. in the \\(\\rho=0.1\\) image, the negative pixels are much larger than the positive and in the \\(\\rho=20\\) image, the positive pixels are much larger than the negative. Saving OMP results Because the OMP section of the pipeline takes a long time, we want to save the results as we go along, so we don't have to restart the whole thing if it crashes for some reason e.g. a memory error. Thus, after we have found the spots on a tile, we save the information for all of them in a npy file in the output directory with a name indicated by config['file_names']['omp_spot_info'] . For each spot, \\(s\\) , this file contains 7 integer values. The first 3 are the local \\(yxz\\) coordinates (z-coordinate in units of z-pixels) on the tile it was found on. The fourth is the gene it was assigned to. The fifth is n_neighbours_pos and the sixth is n_neighbours_neg . The seventh is the tile it was found on. We also save the coefficients of each spot \\(s\\) for each gene, \\(\\pmb{\\mu}_s\\) , found via the OMP algorithm . This \\(n_{spots}\\times n_{genes}\\) sparse array is saved as a npz file in the output directory with a name indicated by config['file_names']['omp_spot_coef'] . This is not actually used anymore but may be useful for debugging purposes. If these files already exist, the call_spots_omp function will simply skip over all tiles for which spots have been saved and then load them all in after spots have been found on all the other tiles. After spots have been found on all tiles, duplicate spots are removed in the same way as in the get reference spots step of the pipeline. We then save details of each spot to the OMP NotebookPage i.e. local_yxz , tile , colors , gene_no , intensity , n_neighbours_neg and n_neighbours_pos . Diagnostics As well as view_omp_score , there are a few other functions using matplotlib which may help to debug this section of the pipeline. The view_codes , view_spot and view_intensity functions can also be used for OMP spots if the argument method has the value 'omp' when calling them. histogram_score When histogram_score is run with method = 'omp' , the histogram of the OMP score , \\(\\gamma_s\\) , will be plotted: OMP Score All Plots The Score Multiplier textbox can then be used to see how the value of \\(\\rho\\) affects the distribution. There is also the option to show the 5 histograms available when histogram_score is run with method = 'anchor' . These show the distribution of the dot product score, \\(\\Delta_{s0g_s}\\) , of each spot, \\(s\\) , with gene \\(g_s=\\) nb.omp.gene_no[s] , if gene \\(g_s\\) was added on the first iteration. I.e. this is the score the spot would have had if it was a reference spot instead of an OMP spot. Clearly, we see that \\(\\Delta_{s0g_s}\\) tends to be larger than \\(\\gamma_s\\) . histogram_2d_score The histogram_2d_score function shows the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score, \\(\\Delta_{s0g_s}\\) (between the orange \\(\\gamma_s\\) and cyan Dot Product Score line in the above plot): \\(\\rho=0.95\\) \\(\\rho=10\\) Plp1 Like with the histogram_score plot, you can use the textboxes to change the bin spacing, see how \\(\\rho\\) affects the distribution (compare first and second plots to see how a larger \\(\\rho\\) creates two clusters of spots, one at \\(\\Delta=0.4, \\gamma=0.1\\) and one at \\(\\Delta=0.9, \\gamma=0.8\\) ) and see the distribution of a single gene (see Plp1 image above). gene_counts If the gene_counts function is run and the Notebook contains the OMP page, then the number of OMP spots with \\(\\gamma_s >\\) omp_score_thresh and nb.omp.intensity > intensity_thresh assigned to each gene will also be shown: Default Score Thresholds Score Thresholds = 0 The default omp_score_thresh , omp_score_multiplier and intensity_thresh are config['thresholds']['score_omp'] , config['thresholds']['score_omp_multiplier'] and config['thresholds']['intensity'] respectively. Clearly, when all the score thresholds are set to 0, there are many more OMP spots because we can detect multiple spots at the same location if they belong to different genes. The Fake Genes plot has nothing to do with the OMP spots. view_omp This function is useful for seeing all gene coefficients fit in the neighbourhood of a spot. view_omp_fit This function is useful for seeing how the OMP algorithm proceeded on the single pixel at the location of the spot. If you right-click on a column, it will run the view_score function to indicate how the dot product was calculated for that gene at that iteration. For example, right cicking the third column in the view_omp_fit image below, produces the view_omp_score image indicating how the dot product score for Trp53i11 was calculated on iteration 1. view_omp_fit view_score view_weight view_background view_score The view_score is the same as described for reference spots method . However, for the OMP method, \\(\\Delta_s\\) is computed for each gene on each iteration. The calculation of all these different \\(\\Delta_s\\) values can be viewed by changing the gene/iteration in the textboxes. For example, the view_score image above shows the calculation for Trp53i11 on iteration 1, but if you wanted to know why Pde1a was not fit on iteration 1, you can type in Pde1a or 42 in the Gene textbox: To get back to the best gene on any iteration, just type in any invalid number/letter into the Gene textbox. After iteration 0, once actual genes have been fit, the role of the weighting becomes much more important . If the Weight Squared plot is clicked on, it will run the view_weight function for the current iteration, as shown in the view_weight image above . view_weight The view_weight function produces a plot to indicate how the weight squared value for spot \\(s\\) , \\(\\pmb{\\omega}^2_{si}\\) , is calculated from the genes fit prior to iteration \\(i\\) . The view_weight image above shows how the background and Snca gene combine to produce \\(\\pmb{\\omega}^2_{si}\\) on iteration 1. Clearly, the rounds/channels where Snca is strong have an extremely low weight, as does channel 0 where the strongest background has been fit. The \\(\\alpha\\) and \\(\\beta\\) textboxes can be used to see how these parameters affect the final weighting. The iteration textbox can be used to see how the weighting and gene coefficients ( \\(\\mu\\) values in titles of bottom row plots) change once more or less genes have been added. If the Background plot is clicked on, it will run the view_background function, as shown in the view_background image above . view_background The background calculation and view_background function are exactly the same as described for reference spots method . Psuedocode This is the pseudocode outlining the basics of this step of the pipeline . There is more detailed pseudocode about how OMP was run on each pixel and how the spot_shape was found . color_norm_factor: nb.call_spots.color_norm_factor [n_rounds x n_channels] t_most_spots: tile with most spots found on it in the call_reference_spots function. tile_sz: nb.basic_info.tile_sz nz: nb.basic_info.nz Alter use_tiles so t_most_spots will be the first tile. for t in use_tiles: for z in use_z: Load in pixel_colors of all pixels on tile t, z-plane z. [n_pixels_z x n_rounds x n_channels] Normalise pixel_colors pixel_colors = pixel_colors / color_norm_factor Compute pixel_intensity from abs(pixel_colors) [n_pixels_z] Only keep pixels with intensity above threshold pixel_colors = pixel_colors[pixel_intensity > initial_intensity_thresh [n_keep x n_rounds x n_channels] for s in range(n_keep): Remove background from pixel_colors[s] Run OMP algorithm to obtain coefficient of each gene. Save coefficient for spot s, gene g as pixel_coefs_z[s, g] [n_keep x n_genes] Concatenate all pixel_coefs_z arrays into one big pixel_coefs array containing all pixels across all z-planes. [n_pixels x n_genes] If t == use_tiles[0]: Compute spot_shape from pixel_coefs on first tile if not provided. Save spot_shape to output directory. for g in range(n_genes): Convert pixel_coefs[:, g] into coef_image [tile_sz x tile_sz x nz] Find yxz coordinates of local maxima in coef_image [n_spots_g x 3] Using spot_shape, find n_neighbours_pos [n_spots_g] Using spot_shape, find n_neighbours_neg [n_spots_g] Combine spots on all genes to create spot_info_t [n_spots_t x 7] Where: spot_info_t[s, :3] are the yxz coordinates of spot s. spot_info_t[s, 3] is the gene assigned to spot s. spot_info_t[s, 4] is n_neighbours_pos for spot s. spot_info_t[s, 5] is n_neighbours_neg for spot s. spot_info_t[:, 6] = t Threshold based on n_neighbours_pos so only keep spots for which n_neighbours_pos > n_pos_thresh: spot_info_t = spot_info_t[spot_info_t[s, 4]>n_pos_thresh, :] [n_save_t x 7] Save spot_info_t to output directory Load in spot_info_t from all tiles and combine into large spot_info array [n_spots x 7] Remove duplicate spots from spot_info [n_non_duplicate x 7] Load in spot_colors corresponding to the spots with yxz coordinates given by spot_info[:, :3] [n_non_duplicate x n_rounds x n_channels] Save spot_info to Notebook: nb.omp.local_yxz = spot_info[:, :3] nb.omp.gene_no = spot_info[:, 3] nb.omp.n_neighbours_pos = spot_info[:, 4] nb.omp.n_neighbours_neg = spot_info[:, 5] nb.omp.tile = spot_info[:, 6] nb.omp.colors = spot_colors","title":"OMP"},{"location":"pipeline/omp/#omp","text":"The OMP step of the pipeline runs an Orthogonal Matching Pursuit ( OMP ) algorithm on every pixel, adding multiple gene bled_codes to explain the \\(n_{rounds} \\times n_{channels}\\) intensity vector at each pixel. This gives us a coefficient, \\(\\mu_{sg}\\) for every pixel, \\(s\\) , and gene, \\(g\\) , and thus we can create a coefficient image for every gene. By doing a local maxima search on these images, we can obtain another estimate of the distribution of genes, which we can compare to that obtained from the reference spots . The number and location of spots found by the OMP method will be different though. There are some variables obtained for each spot ( local_yxz , tile , colors , gene_no and intensity ) saved in the omp NotebookPage which are equivalent to the same variables saved in the ref_spots NotebookPage . There are also some other variables added to the omp NotebookPage which relate to the typical shape of a spot in the gene coefficient images. The OMP section takes quite a long time, so config['omp']['use_z'] can be used to only run OMP on a subset of z-planes. Note: config in this section, with no section specified, means config['omp']","title":"OMP"},{"location":"pipeline/omp/#re-run-call_spots_omp","text":"To re-run the OMP section, the files generated during the OMP step which were saved at the paths indicated by: nb.file_names.omp_spot_shape nb.file_names.omp_spot_info nb.file_names.omp_spot_coef need to be deleted or re-named (alternatively the omp_spot_shape , omp_spot_info and omp_spot_coef parameters in the file names section of the configuration file can be changed). This is so the old data is not loaded in when the OMP part of the pipeline is run. Other than that, the usual instructions can be followed.","title":"Re-run call_spots_omp"},{"location":"pipeline/omp/#why-bother-with-omp","text":"There are two main reasons to use the OMP method for finding the distribution of genes instead of reference spots . The first is that OMP fits multiple coefficients to every pixel and thus allows for overlapping spots: view_omp_fit view_omp In the view_omp_fit plot, it shows an example of a spot color, \\(\\pmb{\\zeta}_s\\) , which requires the bled_code of both Plp1 and Aldoc to explain it. The view_omp plot then shows that spots for both these genes are detected with the OMP method. The reference spots method though, can only fit one gene to each pixel, so here it only detects the Plp1 spot. The second reason is that the reference spots method can only assign genes to spots detected on the reference round / reference channel images. It is thus restricted, because there may be genes present at pixels other than the finite amount considered. Also, the config['find_spots']['radius_xy'] and config['find_spots']['radius_z'] parameters in the spot detection necessitates a minimum distance between any two genes. The OMP method says that spots are local maxima in the gene coefficient images, and thus it can find spots at locations other than the location of the reference spots . Also, because it does a separate local maxima search for each gene, the config['radius_xy'] and config['radius_z'] parameters in spot detection only necessitates a minimum distance between two spots of the same gene. The consequence of this, is that the spots detected by the OMP method tend to be in more dense clusters, as shown by the coppafish.Viewer images below. This is then more useful for cell typing Reference Spots OMP","title":"Why bother with OMP?"},{"location":"pipeline/omp/#initial-intensity-threshold","text":"To produce the gene coefficient images, as shown by the view_omp_fit function, we need to run OMP on every pixel in the image. However, for a single \\(2048\\times2048\\times50\\) tile, there are \\(2.1\\times10^8\\) pixels. Thus, we do an initial thresholding so as not to consider all of them. We only consider pixel \\(s\\) , with pixel color \\(\\pmb{\\zeta}_s\\) , if it has an intensity computed from its absolute pixel color, \\(\\tilde{\\chi}_s\\) , greater than config['initial_intensity_thresh'] . I.e. \\[ \\tilde{\\chi}_s = \\underset{r}{\\mathrm{median}}(\\max_c|\\zeta_{s_{rc}}|) \\] Why is absolute pixel color is used? We use \\(|\\pmb{\\zeta}_s|\\) because we are also interested in whether a pixel has a negative gene coefficient. We expect a negative coefficient in an annulus around a spot, as shown in the view_omp plots, as a result of the difference of hanning kernel used in the initial filtering. If a gene has a negative coefficient annulus around the local maxima, then it boosts our confidence that it is legitimate. If config['initial_intensity_thresh'] is not specified, it is set to the percentile indicated by config['initial_intensity_thresh_percentile'] of \\(\\tilde{\\chi}\\) computed for all pixels in the middle z-plane ( nb.call_spots.norm_shift_tile ) of the central tile ( nb.call_spots.norm_shift_z ). I.e., it is set to: nb . call_spots . abs_intensity_percentile [ config [ 'initial_intensity_thresh_percentile' ]] So this is saying that for a tile and z-plane which we expect to be among the most fluorescent, we are getting rid of a quarter of the pixels (with the default value of config['initial_intensity_thresh'] = 25 ) which are the least intense. On other z-planes, we will get rid of more.","title":"Initial Intensity Threshold"},{"location":"pipeline/omp/#omp-algorithm","text":"For every pixel that passes the initial intensity threshold, \\(s\\) , we run an Orthogonal Matching Pursuit ( OMP ) algorithm to find a coefficient, \\(\\mu_{sg}\\) , for every gene \\(g\\) . The pseudocode for how this is done for each pixel is given below: color: Intensity read from .npy files in tile directory for each round and channel. [n_rounds x n_channels] color_norm_factor: nb.call_spots.color_norm_factor [n_rounds x n_channels] bled_codes: nb.call_spots.bled_codes_ge [n_genes x n_rounds x n_channels] background_codes: nb.call_spots.background_codes [n_channels x n_rounds x n_channels] 1. Normalise color color = color / color_norm_factor 2. Compute background_coefs. Remove background from color - for c in range(n_channels): color = color - background_coefs[c] * background_codes[c] 3. Initialize variables for iterating. residual = color added_genes = [] Append background_codes to bled_codes so has shape [(n_genes+n_channels) x n_rounds x n_channels] Initialize coefs with background_coefs which will not change - coefs = zeros(n_genes+n_channels) coefs[n_genes:] = background_coefs i = 0 while i < n_iter: 4. Find best_gene to add based on dot product score between residual and bled_codes. If score < score_thresh or best_gene is background or best_gene already added: Stop - go to step 7. Append best_gene to added_genes. 5. Obtain added_coefs [i+1] for how the bled_codes of all genes in added_genes can be combined to best explain color. Update coefs - for g_ind in range(i+1): coefs[added_genes[g_ind]] = added_coefs[g_ind] 6. Update residual - residual = color for g in added_genes: residual = residual - coefs[g] * bled_codes[g] i += 1 7. return coefs There are a few parameters in the configuration file which are used: max_genes : This is n_iter in the above code. dp_thresh : This is score_thresh in the above code.","title":"OMP Algorithm"},{"location":"pipeline/omp/#pre-iteration-procedure","text":"Prior to Step 1 , color is \\(\\pmb{\\acute{\\zeta}}_s\\) found through get_spot_colors by obtaining the aligned coordinate of pixel \\(s\\) in each round and channel and then reading off the corresponding intensity . Step 1 is then just converting color from \\(\\pmb{\\acute{\\zeta}}_s\\) to \\(\\pmb{\\zeta}_s\\) so the color channels are equalised, as is done in call_reference_spots . Step 2 is just finding a coefficient \\(\\mu_{sC}\\) for each background gene , \\(\\pmb{B}_C\\) as is done in call_reference_spots . The value of \\(\\lambda_b\\) in the \\(w_{rC}\\) equation is set to nb.call_spots.background_weight_shift so the same value is used for the reference spots and OMP methods. After this step, color is \\(\\pmb{\\zeta}_{s0}\\) and will always remain so. I.e. once background is fit once, it is never updated. Step 3 is adding the artificial background genes to our actual genes. After this step, we have a set of bled_codes , \\(\\pmb{b}\\) such that \\(\\pmb{b}_{g=n_{g}+C} = \\pmb{B}_C\\) . We also have a set of coefficients, \\(\\pmb{\\mu}_{s0}\\) such that \\(\\mu_{s0g=n_g+C} = \\mu_{sC}\\) . The set of coefficients after \\(i\\) actual genes have been fit is \\(\\pmb{\\mu}_{si}\\) ( \\(i=0\\) means just background has been fit) but \\(\\mu_{sig=n_g+C} = \\mu_{sC} \\forall i,C\\) as the background coefficients are never updated.","title":"Pre-Iteration Procedure"},{"location":"pipeline/omp/#finding-best-gene","text":"Step 4 is finding the gene, \\(g_i\\) , with the bled_code , \\(b_{g_i}\\) which best represents the residual after \\(i\\) actual genes have been added, \\(\\pmb{\\zeta}_{si}\\) . We determine \\(g_i\\) to be the gene for which \\(|\\Delta_{sig}|\\) is the largest, where \\(\\Delta_{sig}\\) is exactly the same dot product score used in call_reference_spots . We use the absolute score here because negative gene coefficients also provide useful information as explained earlier . Dot Product Score Parameters In the configuration file, alpha and beta are specified in both the call_spots and omp sections, so different values of these parameters can be used for each method. The value of \\(\\lambda_d\\) in the \\(\\tilde{\\zeta}_{{si}_{rc}}\\) equation is set to nb.call_spots.dp_norm_shift * sqrt(n_rounds) so the same value is used for the reference spots and OMP methods. After we have found \\(g_i\\) , we stop the algorithm if any of the following are satisfied: \\(|\\Delta_{si}| = |\\Delta_{sig_i}|\\) < config['dp_thresh'] . \\(g_i \\geq n_g\\) i.e. \\(g_i\\) is a background gene . \\(g_i\\) was found to be the best gene on a previous iteration. This second condition occurred for the third gene added in the view_omp_fit example shown earlier . This case is quite rare though, since the background genes have already been fit. We include it because if there is an artificial gene being identified as the best one, then we are also likely to erroneously assign actual genes to the pixel which are not actually there. But stopping the algorithm prevents this from happening. It may be more appropriate to recompute the background coefficient, for the background gene identified if this happens though. The third condition is just to stop getting in a loop where we are always fitting the same gene, but it is even rarer than the second.","title":"Finding Best Gene"},{"location":"pipeline/omp/#finding-gene-coefficients","text":"Once we have decided that a gene is acceptable, in Step 5 we find the coefficient of that gene as well as updating the coefficients of all genes previously fit. On iteration \\(i\\) , there will be \\(i+1\\) genes to find the coefficient of. The coefficients are found through normal least squares: \\[ \\tilde{\\pmb{\\mu}}_{si} = (\\pmb{G}_i^T\\pmb{G}_i)^{-1}\\pmb{G}_i^T\\pmb{\\zeta}_{s0} \\] Where: \\(\\tilde{\\pmb{\\mu}}_{si}\\) is a vector of \\(i+1\\) values such that \\(\\mu_{sig_j} = \\tilde{\\mu}_{si_j}\\) where \\(\\mu_{sig_j}\\) is the coefficient found for gene \\(g_j\\) on iteration \\(i\\) for pixel \\(s\\) . \\(\\pmb{G}_i\\) is a matrix of shape \\([n_{rounds}n_{channels} \\times (i+1)]\\) such that column \\(j\\) is the flattened bled_code , \\(\\pmb{b}_{g_j}\\) , for gene \\(g_j\\) , added on iteration \\(j\\) . \\(\\pmb{\\zeta}_{s0}\\) is the color for pixel \\(s\\) after background removal flattened, so it has shape \\([n_{rounds}n_{channels} \\times 1]\\) . Weighted Least Squares If config['weight_coef_fit'] = True , then the coefficients are found through weighted least squares . In this case, both \\(\\pmb{\\zeta}_{s0}\\) and every column of \\(\\pmb{G}_i\\) are multiplied by \\(\\pmb{\\omega}_{si}\\) where \\(\\pmb{\\omega}^2_{si}\\) is defined in the dot product score calculation . The idea behind this is that for normal least squares, the coefficients will be overly influenced by outliers. In the Least Squares view_omp_fit example below, when fitting Plp1 , it is so concerned about getting rid of the very intense values in channel 4 that it makes lots of channel 2 and 6 values negative. Least Squares Weighted Least Squares Weight - Iteration 1 Weight - Iteration 2 In the Weighted Least Squares , we see that after Aldoc has been fit, the channel 4 intensity is much larger than in the Least Squares case and the channel 6 intensities are much less negative. If we look at the Weight - Iteration 1 view_weight plot, we see that this occurs because the contribution of the channel 4 rounds is very small. A problem with this though, is that the weight is re-computed at each iteration which can cause the coefficient of each gene to change drastically. For example, after BG2 has been fit, we see that channel 6 has become very negative again. Looking at the Weight - Iteration 2 plot, we see that this is because the weight of round 4, channel 4 has increased. The coefficient for Plp1 in the Weighted Least Squares case changes from 0.84 to 0.93 after BG2 has been fit but in the Least Squares case, it changes from 1.00 to 1.01. After we have found the coefficients on iteration \\(i\\) , we compute the residual for the next iteration in Step 6 : \\[ \\pmb{\\zeta}_{si+1} = \\pmb{\\zeta}_{s0} - \\sum_{g=0}^{n_g-1}\\mu_{sig}\\pmb{b}_g = \\pmb{\\zeta}_s - \\sum_{g=0}^{n_g+n_c-1}\\mu_{sig}\\pmb{b}_g \\] Where \\(n_c\\) is the number of channels and \\(n_g\\) is the number of genes. The second equation includes the combination from the background genes . In the first equation, \\(\\mu_{sig}\\) will only be non-zero for \\(i+1\\) genes (in the second equation, it will be for \\(i+1+n_c\\) genes, because we include the \\(n_c\\) background genes ). We continue iterating between Step 4 and Step 6 until any of the stopping criteria are met, or we fit config['max_genes'] to the pixel.","title":"Finding Gene Coefficients"},{"location":"pipeline/omp/#weighting-in-dot-product-score","text":"The difference between this algorithm and the standard Orthogonal Matching Pursuit algorithm is the weight factor, \\(\\pmb{\\omega}^2_{si}\\) , used when computing \\(\\Delta_{sig}\\) . Normal OMP would have \\(\\alpha = 0\\) so \\(\\pmb{\\omega}^2_{si}=1\\) . By using this weighting, we are trying to say that if a gene has already been fit with high intensity in round \\(r\\) , channel \\(c\\) , then the remaining intensity in round \\(r\\) , channel \\(c\\) , after it has been fit is probably because the coefficient of that gene was not fit correctly, rather than because another gene is present. As the example below shows, without it, genes are fit to really try and get rid of any anomalously intense rounds/channels even if a gene has already been fit there. view_omp_fit - \\(\\alpha=120\\) view_omp_fit - \\(\\alpha=0\\) view_weight - \\(\\alpha=120\\) , Iteration 4 view_score - \\(\\alpha=120\\) , Iteration 4 If we compare the view_omp_fit plots, we see that in the \\(\\alpha=0\\) case, Rgs4 has been fit to explain the anomalously negative round 6, channel 5 and Aldoc has been fit to explain the anomalously positive round 2, channel 1. The view_weight image shows the calculation of the weight factor for \\(i=4\\) (all 4 of the actual genes shown have been added). This shows that round 2, channel 1 has a low weighting because Lhx6 has already been fit and this gene has high intensity in round 2, channel 1. If we look the view_score image, we see that in the top right, with \\(\\alpha=0\\) , we get a large score for Aldoc solely because of the contribution from round 2, channel 1. If we look at the bottom right though, we see that with \\(\\alpha=120\\) , the score for Aldoc is very small, as we would expect it should be, when comparing the 2 plots in the second column of the view_score image. Basically, we need the weighting because we know that the least squares coefficient fitting will produce some anomalously intense rounds and channels. When selecting the best gene , we need to be robust to this.","title":"Weighting in Dot Product Score"},{"location":"pipeline/omp/#finding-spots","text":"After we have run the OMP algorithm on every pixel of a tile, we can produce an image for each gene based on the \\(n_{pixels}\\times n_{genes}\\) array of coefficients, \\(\\pmb{\\mu}\\) . A \\(200\\times 200\\) pixel section of such an image for three genes is shown on the right. Clearly, most values in each image are zero because each pixel only has a non-zero coefficient for a very small fraction of genes. We then take each gene in turn and find spots , which are the local maxima in the coefficient image. These local maxima are found in exactly the same way as in the find spots part of the pipeline. But here, the threshold intensity is 0 and the neighbourhood (kernel for dilation) is defined by the parameters config['radius_xy'] and config['radius_z'] .","title":"Finding Spots"},{"location":"pipeline/omp/#spot-shape","text":"In the gene coefficient images above, the spots can clearly be seen as red (positive) circles surrounded by a blue (negative) annulus. So to decide whether a particular spot is legitimate, we want to compare its shape to the average spot shape. This average spot shape can be specified in advance (e.g. if you have already run an experiment, which is expected to be similar to the current one, and you want the same shape to be used) with a npy file in the output directory with the name given by config['file_names']['omp_spot_shape'] . This file must contain an image (axis in the order z-y-x) indicating the expected sign of a coefficient (only values are 1, 0, -1) in the neighbourhood of a spot. If the file indicated by config['file_names']['omp_spot_shape'] does not exist, then it will be computed from the spots found on a specific tile. The tile used is the one for which the most spots were found with the reference spots method, and it is saved as nb.omp.shape_tile . The psuedocode for obtaining the spot shape is given below: spot_yxz: yxz coordinates of all spots found on the tile [n_spots x 3] gene_no: gene_no[s] is the gene whose coefficient image spot s was found on. [n_spots]. tile_sz: nb.basic_info.tile_sz nz: nb.basic_info.nz gene_coef_im: gene_coef_im[g] is the gene coefficient image for gene g, which can be made after running OMP on each pixel. [n_genes x tile_sz x tile_sz x nz] for s in range(n_spots): 1. use = True if number of pixels neighbouring spot_yxz[s] in gene_coef_im[gene_no[s]] with a positive value equals use_thresh. 2. If use, then obtain the [shape_ny x shape_nx x shape_nz] image centered on spot_yxz[s] in gene_coef_im[gene_no[s]]. 3. We now have spot_images which is a [n_use x shape_ny x shape_nx x shape_nz] array. We update this by only keeping images from isolated spots. [n_use2 x shape_ny x shape_nx x shape_nz] 4. Compute spot_sign_images by taking the sign of every value in spot_images so the only values are 1, 0, -1. Next, we compute av_spot_sign_image which is the average of spot_sign_images across spots. Where abs(av_spot_sign_image) < sign_thresh, set it to 0. Take sign of av_spot_sign_image so it only contains 1, 0, -1. [shape_ny x shape_nx x shape_nz] There are a few parameters in the configuration file which are used: shape_pos_neighbour_thresh : In 2D , this is use_thresh in the above code. In 3D , this is use_thresh - 2 . shape_max_size : This is [shape_ny, shape_nx, shape_nz] in the above code. shape_sign_thresh : This is sign_thresh in the above code. Step 1 is a thresholding procedure, so we only use spots we are quite confident are real for computing the average shape. For a spot to be used in computing the shape, on the z-plane of the gene coefficient image that it was found on, within the \\(n_{pos_{use}}\\times n_{pos_{use}}\\) neighbourhood with the local maxima in the centre, all n_use = \\(n_{pos_{use}}^2\\) coefficients must be positive. In 3D , the coefficient at the same \\(yx\\) coordinate as the local maxima but on 1 z-plane either side must be also positive (now require n_use = \\(n_{pos_{use}}^2 + 2\\) positive coefficients in the neighbourhood of the spot). The value of \\(n_{pos_{use}}^2\\) is given by config['shape_pos_neighbour_thresh'] , the default value is 9 meaning all pixels in a \\(3\\times 3\\) grid centered on the local maxima must be positive. Example The first image below shows a spot that would be used to compute the shape with config['shape_pos_neighbour_thresh'] = 3 because all 9 pixels in the central z-plane have a positive coefficient, as do those on 1 z plane either side but in the middle yx pixel. The second image shows a spot that would not be used. \u2705 \u274c In Step 2 , we just get the cropped gene coefficient image in the neighbourhood of the local maxima. 3 examples are shown as Spot 1 , Spot 2 and Spot 3 below. In Step 3 , we are saying that most spots are probably not overlapping with any other genes so to get an estimate of what an average spot looks like, let us only use spots which are quite well isolated. Our definition of isolated here, is that the distance between each spot used in Step 2 and any other spot used in Step 2 must exceed config[shape_isolation_dist] . Spot 1 Spot 2 Spot 3 Average Sign Spot Shape In Step 4 , we first take the sign of the spot images and then compute the average of these using get_average_spot_image . We set av_type = 'mean' and symmetry = 'annulus_3d' . We use this symmetry because we assume that the spot should be circular within a z-plane and symmetric in z. This procedure produces the Average Sign image shown above. This is saved to the Notebook as nb.omp.spot_shape_float . After we have this av_sign_image , we get our final spot_shape via: import numpy as np av_sign_image [ np . abs ( av_sign_image )] < config [ 'omp' ][ 'shape_sign_thresh' ]] = 0 spot_shape = np . sign ( av_sign_image ) This is just saying that if the absolute value of the mean sign across all these spots was less than config['omp']['shape_sign_thresh'] , then we are not sure what sign these pixels should be, so we won't assign them a sign in our final spot_shape . Otherwise, we are fairly confident what sign they should be, so we do assign the sign to our final shape. The Spot Shape image above shows what this looks like. Why does the spot shape indicate the expected sign of the coefficient? The spot_shape indicates the expected sign of a coefficient, not the expected value. This is because, through the OMP algorithm , for a pixel to have a non-zero coefficient for any gene, it has already gone through a thresholding procedure which has decided that the gene bled_code is required to explain the pixel color. The idea behind counting the number of coefficients with the correct sign is that if there are lots of pixels in the neighbourhood of the spot which required a certain gene, then that gives us confidence that the gene is actually present. If we instead convolved the actual coefficent image (e.g. Spot 1 image shown above) with the expected coefficient kernel (e.g. mean of spot_images in Step 4 of the pseudocode), it would boost the score of intense pixels which we don't want to do. The final spot_shape is saved to the Notebook as nb.omp.spot_shape and is also saved as a npy file in the output directory with the name indicated by config['file_names']['omp_spot_shape'] . The coordinates and corresponding gene of spots used to compute nb.omp.spot_shape are saved as nb.shape_spot_local_yxz and nb.shape_spot_gene_no respectively.","title":"Spot Shape"},{"location":"pipeline/omp/#error-no-negative-values-in-nbompspot_shape","text":"If config['shape_sign_thresh'] is too large, nb.omp.spot_shape will not have any -1 values. This will then raise an error because nb.omp_n_neighbours_neg cannot be computed. To get past this error, the OMP step needs to be re-run with either config['shape_sign_thresh'] set to a lower value or nb.omp.spot_shape specified through config['file_names']['omp_spot_shape'] . Example The following error was hit when running with config['shape_sign_thresh'] = 0.7 : The error saves the spot shape, nb.omp.spot_shape_float , which is converted to nb.omp.spot_shape through thresholding with config['shape_sign_thresh'] and then taking the sign. If we then load this file, we can get an idea of a suitable value of config['shape_sign_thresh'] : Code napari Viewer import numpy as np import napari im_file = \"/Users/joshduffield/Documents/UCL/ISS/Python/play/B8S5_Slice001_npy/omp_spot_shape_float_ERROR.npy\" im = np . load ( im_file ) napari . view_image ( im , contrast_limits = [ - 1 , 1 ], colormap = \"PiYG\" ) Say we decide after looking at the image in napari , that we want values more negative than at the blue cross to be included in the final shape. Then we set config['shape_sign_thresh'] to be just below the absolute value at this cross (0.336 as indicated in bottom left). The value of config['shape_sign_thresh'] indicated in the error is the maximum allowed value such that nb.omp.spot_shape will contain at least one pixel with the value of -1. If the saved nb.omp.spot_shape_float image has no negative values, then config['file_names']['omp_spot_shape'] must be used to specify nb.omp.spot_shape . The saved image can still be used as a guide as to where nb.omp.spot_shape=1 though.","title":"Error - No negative values in nb.omp.spot_shape"},{"location":"pipeline/omp/#n_neighbours","text":"Once we have found the spot_shape , we want to quantify how much each spot resembles it. To do this, if spot \\(s\\) , was found on the gene \\(g\\) coefficient image, we first obtain coef_im_s which is the gene \\(g\\) coefficient image centered on the coordinates of spot \\(s\\) , with the same size as spot_shape . We then count the number of pixels for which coef_im_s and spot_shape both have positive coefficients. We also count the number of pixels for which coef_im_s and spot_shape both have negative coefficients. Once this has been done for all spots, these are saved as nb.omp.n_neighbours_pos and nb.omp.n_neighbours_neg respectively. To reduce the memory of the Notebook file, we only save spots to the Notebook if n_neighbours_pos exceeds config['initial_pos_neighbour_thresh'] . If this is left blank, it will be set to the fraction config['initial_pos_neighbour_thresh_param'] of the maximum possible value (i.e. sum(spot_shape>0) ). It will also be clipped between config['initial_pos_neighbour_thresh_min'] and config['initial_pos_neighbour_thresh_max'] .","title":"n_neighbours"},{"location":"pipeline/omp/#view_omp_score","text":"The way nb.omp.n_neighbours_pos and nb.omp.n_neighbours_neg are obtained, can be visualised for a specific spot with the function view_omp_score : \\(\\rho = 0.95\\) \\(\\rho = 0.1\\) \\(\\rho = 20\\) Here, the top plot shows nb.omp.spot_shape and the bottom plot shows the coefficient image for Snca in the neighbourhood of spot \\(371046\\) which has \\(yxz\\) coordinates in the global coordinate system of \\([3970, 509, 25]\\) . The green hatching in the top plot indicates where both plots have the same sign. So nb.omp.n_neighbours_pos[371046] is the number of red pixels with green hatching (316) and nb.omp.n_neighbours_neg[371046] is the number of blue pixels with green hatching (192).","title":"view_omp_score"},{"location":"pipeline/omp/#omp-score","text":"The final score , \\(\\gamma\\) , used for thresholding OMP spots in coppafish.Viewer and when exporting to pciSeq is: \\[ \\gamma_s = \\frac{n_{neg_s} + \\rho n_{pos_s}}{n_{neg_{max}} + \\rho n_{pos_{max}}} \\] Where: \\(n_{neg_s}\\) is nb.omp.n_neighbours_neg[s] \\(n_{neg_s}\\) is nb.omp.n_neighbours_pos[s] \\(n_{neg_{max}}\\) is sum(nb.omp.spot_shape<0) \\(n_{pos_{max}}\\) is sum(nb.omp.spot_shape>0) \\(\\rho\\) is config['thresholds']['score_omp_multiplier'] So if \\(\\rho = 1\\) , this would just be the fraction of pixels in the neighbourhood of spot \\(s\\) with the correct sign. The larger \\(\\rho\\) , the greater the contribution of the positive pixels to \\(\\gamma_s\\) . With the view_omp_score function, the effect of varying \\(\\rho\\) on the score, can be seen by using the textbox. The top row of plots are also normalised so \\(\\gamma_s\\) is equal to the sum of the absolute value of the pixels with the green hatching. I.e. in the \\(\\rho=0.1\\) image, the negative pixels are much larger than the positive and in the \\(\\rho=20\\) image, the positive pixels are much larger than the negative.","title":"OMP Score"},{"location":"pipeline/omp/#saving-omp-results","text":"Because the OMP section of the pipeline takes a long time, we want to save the results as we go along, so we don't have to restart the whole thing if it crashes for some reason e.g. a memory error. Thus, after we have found the spots on a tile, we save the information for all of them in a npy file in the output directory with a name indicated by config['file_names']['omp_spot_info'] . For each spot, \\(s\\) , this file contains 7 integer values. The first 3 are the local \\(yxz\\) coordinates (z-coordinate in units of z-pixels) on the tile it was found on. The fourth is the gene it was assigned to. The fifth is n_neighbours_pos and the sixth is n_neighbours_neg . The seventh is the tile it was found on. We also save the coefficients of each spot \\(s\\) for each gene, \\(\\pmb{\\mu}_s\\) , found via the OMP algorithm . This \\(n_{spots}\\times n_{genes}\\) sparse array is saved as a npz file in the output directory with a name indicated by config['file_names']['omp_spot_coef'] . This is not actually used anymore but may be useful for debugging purposes. If these files already exist, the call_spots_omp function will simply skip over all tiles for which spots have been saved and then load them all in after spots have been found on all the other tiles. After spots have been found on all tiles, duplicate spots are removed in the same way as in the get reference spots step of the pipeline. We then save details of each spot to the OMP NotebookPage i.e. local_yxz , tile , colors , gene_no , intensity , n_neighbours_neg and n_neighbours_pos .","title":"Saving OMP results"},{"location":"pipeline/omp/#diagnostics","text":"As well as view_omp_score , there are a few other functions using matplotlib which may help to debug this section of the pipeline. The view_codes , view_spot and view_intensity functions can also be used for OMP spots if the argument method has the value 'omp' when calling them.","title":"Diagnostics"},{"location":"pipeline/omp/#histogram_score","text":"When histogram_score is run with method = 'omp' , the histogram of the OMP score , \\(\\gamma_s\\) , will be plotted: OMP Score All Plots The Score Multiplier textbox can then be used to see how the value of \\(\\rho\\) affects the distribution. There is also the option to show the 5 histograms available when histogram_score is run with method = 'anchor' . These show the distribution of the dot product score, \\(\\Delta_{s0g_s}\\) , of each spot, \\(s\\) , with gene \\(g_s=\\) nb.omp.gene_no[s] , if gene \\(g_s\\) was added on the first iteration. I.e. this is the score the spot would have had if it was a reference spot instead of an OMP spot. Clearly, we see that \\(\\Delta_{s0g_s}\\) tends to be larger than \\(\\gamma_s\\) .","title":"histogram_score"},{"location":"pipeline/omp/#histogram_2d_score","text":"The histogram_2d_score function shows the bivariate histogram to see the correlation between the omp spot score, \\(\\gamma_s\\) and the dot product score, \\(\\Delta_{s0g_s}\\) (between the orange \\(\\gamma_s\\) and cyan Dot Product Score line in the above plot): \\(\\rho=0.95\\) \\(\\rho=10\\) Plp1 Like with the histogram_score plot, you can use the textboxes to change the bin spacing, see how \\(\\rho\\) affects the distribution (compare first and second plots to see how a larger \\(\\rho\\) creates two clusters of spots, one at \\(\\Delta=0.4, \\gamma=0.1\\) and one at \\(\\Delta=0.9, \\gamma=0.8\\) ) and see the distribution of a single gene (see Plp1 image above).","title":"histogram_2d_score"},{"location":"pipeline/omp/#gene_counts","text":"If the gene_counts function is run and the Notebook contains the OMP page, then the number of OMP spots with \\(\\gamma_s >\\) omp_score_thresh and nb.omp.intensity > intensity_thresh assigned to each gene will also be shown: Default Score Thresholds Score Thresholds = 0 The default omp_score_thresh , omp_score_multiplier and intensity_thresh are config['thresholds']['score_omp'] , config['thresholds']['score_omp_multiplier'] and config['thresholds']['intensity'] respectively. Clearly, when all the score thresholds are set to 0, there are many more OMP spots because we can detect multiple spots at the same location if they belong to different genes. The Fake Genes plot has nothing to do with the OMP spots.","title":"gene_counts"},{"location":"pipeline/omp/#view_omp","text":"This function is useful for seeing all gene coefficients fit in the neighbourhood of a spot.","title":"view_omp"},{"location":"pipeline/omp/#view_omp_fit","text":"This function is useful for seeing how the OMP algorithm proceeded on the single pixel at the location of the spot. If you right-click on a column, it will run the view_score function to indicate how the dot product was calculated for that gene at that iteration. For example, right cicking the third column in the view_omp_fit image below, produces the view_omp_score image indicating how the dot product score for Trp53i11 was calculated on iteration 1. view_omp_fit view_score view_weight view_background","title":"view_omp_fit"},{"location":"pipeline/omp/#view_score","text":"The view_score is the same as described for reference spots method . However, for the OMP method, \\(\\Delta_s\\) is computed for each gene on each iteration. The calculation of all these different \\(\\Delta_s\\) values can be viewed by changing the gene/iteration in the textboxes. For example, the view_score image above shows the calculation for Trp53i11 on iteration 1, but if you wanted to know why Pde1a was not fit on iteration 1, you can type in Pde1a or 42 in the Gene textbox: To get back to the best gene on any iteration, just type in any invalid number/letter into the Gene textbox. After iteration 0, once actual genes have been fit, the role of the weighting becomes much more important . If the Weight Squared plot is clicked on, it will run the view_weight function for the current iteration, as shown in the view_weight image above .","title":"view_score"},{"location":"pipeline/omp/#view_weight","text":"The view_weight function produces a plot to indicate how the weight squared value for spot \\(s\\) , \\(\\pmb{\\omega}^2_{si}\\) , is calculated from the genes fit prior to iteration \\(i\\) . The view_weight image above shows how the background and Snca gene combine to produce \\(\\pmb{\\omega}^2_{si}\\) on iteration 1. Clearly, the rounds/channels where Snca is strong have an extremely low weight, as does channel 0 where the strongest background has been fit. The \\(\\alpha\\) and \\(\\beta\\) textboxes can be used to see how these parameters affect the final weighting. The iteration textbox can be used to see how the weighting and gene coefficients ( \\(\\mu\\) values in titles of bottom row plots) change once more or less genes have been added. If the Background plot is clicked on, it will run the view_background function, as shown in the view_background image above .","title":"view_weight"},{"location":"pipeline/omp/#view_background","text":"The background calculation and view_background function are exactly the same as described for reference spots method .","title":"view_background"},{"location":"pipeline/omp/#psuedocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . There is more detailed pseudocode about how OMP was run on each pixel and how the spot_shape was found . color_norm_factor: nb.call_spots.color_norm_factor [n_rounds x n_channels] t_most_spots: tile with most spots found on it in the call_reference_spots function. tile_sz: nb.basic_info.tile_sz nz: nb.basic_info.nz Alter use_tiles so t_most_spots will be the first tile. for t in use_tiles: for z in use_z: Load in pixel_colors of all pixels on tile t, z-plane z. [n_pixels_z x n_rounds x n_channels] Normalise pixel_colors pixel_colors = pixel_colors / color_norm_factor Compute pixel_intensity from abs(pixel_colors) [n_pixels_z] Only keep pixels with intensity above threshold pixel_colors = pixel_colors[pixel_intensity > initial_intensity_thresh [n_keep x n_rounds x n_channels] for s in range(n_keep): Remove background from pixel_colors[s] Run OMP algorithm to obtain coefficient of each gene. Save coefficient for spot s, gene g as pixel_coefs_z[s, g] [n_keep x n_genes] Concatenate all pixel_coefs_z arrays into one big pixel_coefs array containing all pixels across all z-planes. [n_pixels x n_genes] If t == use_tiles[0]: Compute spot_shape from pixel_coefs on first tile if not provided. Save spot_shape to output directory. for g in range(n_genes): Convert pixel_coefs[:, g] into coef_image [tile_sz x tile_sz x nz] Find yxz coordinates of local maxima in coef_image [n_spots_g x 3] Using spot_shape, find n_neighbours_pos [n_spots_g] Using spot_shape, find n_neighbours_neg [n_spots_g] Combine spots on all genes to create spot_info_t [n_spots_t x 7] Where: spot_info_t[s, :3] are the yxz coordinates of spot s. spot_info_t[s, 3] is the gene assigned to spot s. spot_info_t[s, 4] is n_neighbours_pos for spot s. spot_info_t[s, 5] is n_neighbours_neg for spot s. spot_info_t[:, 6] = t Threshold based on n_neighbours_pos so only keep spots for which n_neighbours_pos > n_pos_thresh: spot_info_t = spot_info_t[spot_info_t[s, 4]>n_pos_thresh, :] [n_save_t x 7] Save spot_info_t to output directory Load in spot_info_t from all tiles and combine into large spot_info array [n_spots x 7] Remove duplicate spots from spot_info [n_non_duplicate x 7] Load in spot_colors corresponding to the spots with yxz coordinates given by spot_info[:, :3] [n_non_duplicate x n_rounds x n_channels] Save spot_info to Notebook: nb.omp.local_yxz = spot_info[:, :3] nb.omp.gene_no = spot_info[:, 3] nb.omp.n_neighbours_pos = spot_info[:, 4] nb.omp.n_neighbours_neg = spot_info[:, 5] nb.omp.tile = spot_info[:, 6] nb.omp.colors = spot_colors","title":"Psuedocode"},{"location":"pipeline/overview/","text":"Pipeline Here we outline in detail, how each part of the pipeline works including what each parameter in the configuration file is used for. Some useful plots that can be run for debugging purposes or to help understand what is going on are indicated. Potential errors that can be hit during each stage of the pipeline are also mentioned. Instructions on how to re-run a section of the pipeline are given here . A brief description on what each step of the pipeline does is given below: Times The times taken for a 3D experiment with 4 tiles, 7 rounds, 7 channels and tiles of dimension \\(2048\\times 2048\\times 50\\) are given for each section. There are three sets of profiling results: One for the whole pipeline, but with code that is a bit outdated. One with up to date code, excluding the extract and filter step. One with up to date code, excluding the extract and filter step and not using the optimised code. These times were obtained on an M1 2021 Macbook Pro . Because the nd2 library does not work on this computer, the nd2reader library was used instead, hence the time taken for the extract and filter stage may not be accurate. Note that nd2reader does not work for QuadCam data, hence we use the nd2 library. Extract and Filter : This loads in an image from the input directory for each tile, round and channel. It then filters each one and saves them as npy files in the tile directory . Time: 57 minutes Find Spots : For each tile, round and channel, this loads in the filtered image from the tile directory . It then detects spots on each one and saves the \\(yxz\\) coordinates of each spot to the Notebook . This now gives us a point cloud for each tile, round and channel. Time: 7 minutes, 10 seconds Time (not optimised): 61 minutes, 50 seconds Stitching : This takes the point clouds on neighbouring tiles of the reference round / reference channel and uses them to find the overlap between the tiles. After doing this for all sets of neighbouring tiles, we obtain a \\(yxz\\) origin coordinate (bottom left corner) for each tile such that a global coordinate system is created (i.e. given a coordinate on a tile, we can obtain the global coordinate by adding the origin coordinate of that tile). Time: 43 seconds (14 seconds per shift) Register Initial : For each tile, this finds the shift between the reference round / reference channel and each sequencing round through an exhaustive search using the point clouds. In total, \\(n_{tiles} \\times n_{rounds}\\) shifts are found. Time: 5 minutes, 48 seconds (12.4 seconds per shift) Register : This takes the shifts found in the Register Initial step and uses them as a starting point to determine the affine transform between the reference round / reference channel and each sequencing round and channel for each tile. This is done through an iterative closest point algorithm ( ICP ) and in total, \\(n_{tiles} \\times n_{rounds} \\times n_{channels}\\) transforms are found. Time: 20 seconds Get Reference Spots : For each spot detected on the reference round / reference channel , this uses the transforms obtained in the Register step to determine the corresponding coordinate in each sequencing round and channel. For each sequencing round and chanel, it then loads in the filtered image from the tile directory and reads off the intensity at the computed coordinate. This gives a \\(n_{rounds}\\times n_{channels}\\) spot color for each reference spot. Time: 3 minutes, 18 seconds Time (not optimised): 3 minutes, 18 seconds Call Reference Spots : For each gene, a \\(n_{rounds}\\times n_{channels}\\) bled code is obtained which indicates what the spot color of spots assigned to that gene should look like. For each spot, we determine which gene it corresponds to by computing a dot product between its spot color and each gene bled code . The spot is assigned to the gene for which this dot product is the largest. Time: 16 seconds Time (not optimised): 30 seconds OMP : For each pixel in the global coordinates, the spot color is obtained. Then multiple gene bled codes are fit until the residual spot color cannot be explained by any further genes. For each gene that is fit, we obtain a coefficient indicating how much of that gene's bled code is required to explain the spot color . Once this is done for all pixels, we have a coefficient for each gene at each pixel. This allows us to build a coefficient image for each gene. Spots are then detected on these images. This gives us a second distribution of genes which allows for overlapping spots and spots at locations not detected in the reference round / reference channel . Time: 1 hour, 16 minutes Time (not optimised): 20 hours, 53 minutes Total Time: 2 hours, 33 minutes Total Time (not optimised): 23 hours, 6 minutes Order of Steps The results of the Stitching part of the pipeline are first used in the Get Reference Spots step. Thus, the Stitching part can actually be run anywhere between the Find Spots and Get Reference Spots steps. All other steps must be run in the order indicated.","title":"Overview"},{"location":"pipeline/overview/#pipeline","text":"Here we outline in detail, how each part of the pipeline works including what each parameter in the configuration file is used for. Some useful plots that can be run for debugging purposes or to help understand what is going on are indicated. Potential errors that can be hit during each stage of the pipeline are also mentioned. Instructions on how to re-run a section of the pipeline are given here . A brief description on what each step of the pipeline does is given below: Times The times taken for a 3D experiment with 4 tiles, 7 rounds, 7 channels and tiles of dimension \\(2048\\times 2048\\times 50\\) are given for each section. There are three sets of profiling results: One for the whole pipeline, but with code that is a bit outdated. One with up to date code, excluding the extract and filter step. One with up to date code, excluding the extract and filter step and not using the optimised code. These times were obtained on an M1 2021 Macbook Pro . Because the nd2 library does not work on this computer, the nd2reader library was used instead, hence the time taken for the extract and filter stage may not be accurate. Note that nd2reader does not work for QuadCam data, hence we use the nd2 library. Extract and Filter : This loads in an image from the input directory for each tile, round and channel. It then filters each one and saves them as npy files in the tile directory . Time: 57 minutes Find Spots : For each tile, round and channel, this loads in the filtered image from the tile directory . It then detects spots on each one and saves the \\(yxz\\) coordinates of each spot to the Notebook . This now gives us a point cloud for each tile, round and channel. Time: 7 minutes, 10 seconds Time (not optimised): 61 minutes, 50 seconds Stitching : This takes the point clouds on neighbouring tiles of the reference round / reference channel and uses them to find the overlap between the tiles. After doing this for all sets of neighbouring tiles, we obtain a \\(yxz\\) origin coordinate (bottom left corner) for each tile such that a global coordinate system is created (i.e. given a coordinate on a tile, we can obtain the global coordinate by adding the origin coordinate of that tile). Time: 43 seconds (14 seconds per shift) Register Initial : For each tile, this finds the shift between the reference round / reference channel and each sequencing round through an exhaustive search using the point clouds. In total, \\(n_{tiles} \\times n_{rounds}\\) shifts are found. Time: 5 minutes, 48 seconds (12.4 seconds per shift) Register : This takes the shifts found in the Register Initial step and uses them as a starting point to determine the affine transform between the reference round / reference channel and each sequencing round and channel for each tile. This is done through an iterative closest point algorithm ( ICP ) and in total, \\(n_{tiles} \\times n_{rounds} \\times n_{channels}\\) transforms are found. Time: 20 seconds Get Reference Spots : For each spot detected on the reference round / reference channel , this uses the transforms obtained in the Register step to determine the corresponding coordinate in each sequencing round and channel. For each sequencing round and chanel, it then loads in the filtered image from the tile directory and reads off the intensity at the computed coordinate. This gives a \\(n_{rounds}\\times n_{channels}\\) spot color for each reference spot. Time: 3 minutes, 18 seconds Time (not optimised): 3 minutes, 18 seconds Call Reference Spots : For each gene, a \\(n_{rounds}\\times n_{channels}\\) bled code is obtained which indicates what the spot color of spots assigned to that gene should look like. For each spot, we determine which gene it corresponds to by computing a dot product between its spot color and each gene bled code . The spot is assigned to the gene for which this dot product is the largest. Time: 16 seconds Time (not optimised): 30 seconds OMP : For each pixel in the global coordinates, the spot color is obtained. Then multiple gene bled codes are fit until the residual spot color cannot be explained by any further genes. For each gene that is fit, we obtain a coefficient indicating how much of that gene's bled code is required to explain the spot color . Once this is done for all pixels, we have a coefficient for each gene at each pixel. This allows us to build a coefficient image for each gene. Spots are then detected on these images. This gives us a second distribution of genes which allows for overlapping spots and spots at locations not detected in the reference round / reference channel . Time: 1 hour, 16 minutes Time (not optimised): 20 hours, 53 minutes Total Time: 2 hours, 33 minutes Total Time (not optimised): 23 hours, 6 minutes Order of Steps The results of the Stitching part of the pipeline are first used in the Get Reference Spots step. Thus, the Stitching part can actually be run anywhere between the Find Spots and Get Reference Spots steps. All other steps must be run in the order indicated.","title":"Pipeline"},{"location":"pipeline/register/","text":"Register The register step of the pipeline finds an affine transform between the reference round/reference channel ( \\(r_{ref}\\) / \\(c_{ref}\\) ) and each imaging round and channel for every tile. The affine transform to tile \\(t\\) , round \\(r\\) , channel \\(c\\) is found through iterative closest point using the shift, nb.register_initial.shift[t, r] , found during the register_initial step of the pipeline as the starting point. It is saved to the Notebook as nb.register.transform[t, r, c] . These transforms are used later in the pipeline to determine the intensity of a pixel in each round and channel. The register and register_debug NotebookPages are added to the Notebook after this stage is finished. Affine Transform We want to find the transform from tile \\(t\\) , round \\(r_{ref}\\) , channel \\(c_{ref}\\) to round \\(r\\) , channel \\(c\\) for all tiles, rounds and channels such that there is pixel-level alignment between the images. The pixel-level alignment is important because most spots are only a few pixels in size, so even a one-pixel registration error can compromise the spot_colors found in the next stage of the pipeline and thus the gene assignment . The shifts found in the register_initial step of the pipeline are not sufficient for this, because of chromatic aberration which will cause a scaling between color channels. There may also be small rotational or non-rigid shifts; thus we find affine transformations which can include shifts, scalings, rotations and shears. Starting Transform The affine transforms are found using the iterative-closest point ( ICP ) algorithm. This is highly sensitive to local maxima, so it is initialized with the shifts found in the register_initial step, nb.register_initial.shift . The starting transform to a particular round and tile is the same for all channels. The shifts are put into the form of an affine transform ( \\(4 \\times 3\\) array) using the transform_from_scale_shift function. Starting transform from shifts The code below indicates how the initial shifts ( \\(n_{tiles} \\times n_{rounds} \\times 3\\) array) are converted into initial transforms ( \\(n_{tiles} \\times n_{rounds} \\times n_{channels} \\times 4 \\times 3\\) array). Code Output import numpy as np from coppafish.register.base import transform_from_scale_shift t_print = 1 r_print = 1 initial_shifts = nb . register_initial . shift # z shift is in z-pixel units print ( f \"Initial shift for tile { t_print } , round { r_print } : \\n { initial_shifts [ t_print , r_print ] } \" ) # Convert z shift into same units as yx pixels initial_shifts = initial_shifts . astype ( float ) z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] for t in nb . basic_info . use_tiles : for r in nb . basic_info . use_rounds : initial_shifts [ t , r ] = initial_shifts [ t , r ] * z_scale print ( f \"Initial shift for tile { t_print } , round { r_print } (z shift in YX pixels): \\n \" f \" { np . around ( initial_shifts [ t_print , r_print ], 2 ) } \" ) # Convert shifts to affine transform 4 x 3 form initial_scale = np . ones (( nb . basic_info . n_channels , 3 )) # set all scalings to 1 initially # as just want shift. initial_transforms = transform_from_scale_shift ( initial_scale , initial_shifts ) # Show transform different for rounds but same for channel within round for r in range ( 2 ): for c in range ( 2 ): print ( f \"Initial transform for tile { t_print } , round { r } , channel { c } : \\n \" f \" { np . around ( initial_transforms [ t_print , r , c ], 2 ) } \" ) Initial Shifts for tile 1, round 1: [25 14 1] Initial shift for tile 1, round 1 (z shift in YX pixels): [25. 14. 5.99] Initial transform for tile 1, round 0, channel 0: [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.] [48. 30. 0.]] Initial transform for tile 1, round 0, channel 1: [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.] [48. 30. 0.]] Initial transform for tile 1, round 1, channel 0: [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ] [25. 14. 5.99]] Initial transform for tile 1, round 1, channel 1: [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ] [25. 14. 5.99]] ICP The pseudocode for the ICP algorithm to find the affine transform for tile \\(t\\) round \\(r\\) , channel \\(c\\) is indicated below. The shape of the various arrays are indicated in the comments (#). Preparing point clouds Prior to the ICP algorithm, the \\(yxz\\) coordinates of spots on a tile are centered. This is because, it makes more sense to me, if any rotation is applied around the centre of the tile. Also, like for the initial shifts, the z-coordinates must be converted into units of yx-pixels, so overall: spot_yxz = spot_yxz - nb . basic_info . tile_centre # center coordinates # Convert units of z-coordinates z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] spot_yxz = spot_yxz * z_scale For the non-reference point clouds we only keep spots which are isolated. This is because the ICP algorithm is less likely to fall into local maxima if the spots are quite well separated. We deam a spot isolated if the nearest spot to it is further away than 2 * neighb_dist . Where neighb_dist = config['register']['neighb_dist_thresh_2d'] if 2D and config['register']['neighb_dist_thresh_3d'] if 3D . n_iter is the maximum number of iterations and is set by config['register']['n_iter'] . neighb_dist_thresh is the distance in \\(yx\\) pixels below which neighbours are a good match. It is given by config['register']['neighb_dist_thresh_2d'] if 2D and config['register']['neighb_dist_thresh_3d'] if 3D . Only neighbours which are closer than this are used when computing the transform. Padding ref_spot_yxz ref_spot_yxz has shape [n_ref x 4] not [n_ref x 3] because to be able to be multiplied by the affine transform ( [4 x 3] ), it must be padded with ones i.e. ref_spot_yxz[:, 3] = 1 . ref_spot_yxz @ transform is then the same as ref_spot_yxz[:, :3] @ transform[:3, :] + transform[3] i.e. rotation/scaling matrix applied and then the shift is added. spot_yxz = yxz coordinates for spots detected on tile t, round r, channel c. # [n_base x 3] ref_spot_yxz = padded yxz coordinates for spots detected on tile t, reference round, reference channel. # [n_ref x 4] transform_initial = starting transform for tile t, round r, channel c. transform = transform_initial # [4 x 3] neighb_ind = zeros(n_ref) # [n_ref] neighb_ind_last = neighb_ind # [n_ref] i = 0 while i < n_iter: 1. Transform ref_spot_yxz according to transform to give ref_spot_yxz_transform. # [n_ref x 3] This is just matrix multiplication. 2. neighb_ind[s] is index of spot in spot_yxz closest to ref_spot_yxz_transform[s]. 3. dist[s] is the distance between spot_yxz[neighb_ind[s]] and ref_spot_yxz_transform[s] # [n_ref] 4. Choose which spots to use to update the transform: use = dist < neighb_dist_thresh spot_yxz_use = spot_yxz[neighb_ind[use]] # [n_use x 3] ref_spot_yxz_use = ref_spot_yxz[use] # [n_use x 4] 5. Update transform to be the 4x3 matrix which multiplies ref_spot_yxz_use such that the distances between the transformed spots and spot_yxz_use are minimised. This is the least squares solution. 6. If neighb_ind and neighb_ind_last are identical, then stop iteration i.e. i = n_iter. 7. neighb_ind_last = neighb_ind 8. i = i + 1 Checking ICP results Once the ICP algorithm has obtained a transform, transform[t, r, c] for each tile, round and channel, we want to determine if they are acceptable. Three criteria must be satisfied for transform[t, r, c] to be considered acceptable : Number of matches exceeds threshold Diagonal elements of transform[t, r, c] are near to what we broadly expect them to be . Shift part of transform[t, r, c] i.e. transform[t, r, c][3] is near to what we broadly expect it to be . Number of matches The number of matches, n_matches[t, r, c] , are the number of pairs of spots used to compute transform[t, r, c] , i.e. n_use in the above pseudocode, and thus it depends on the neighb_dist_thresh parameter. The number of matches are saved to the Notebook as nb.register_debug.n_matches[t, r, c] . transform[t, r, c] is deemed to have failed if n_matches[t, r, c] < n_matches_thresh[t, r, c] . n_matches_thresh[t, r, c] is computed as follows: # n_ref[t] - number of spots detected on tile t, # reference round, reference channel # n_base[t, r, c] - number of spots detected on tile t, # round r, channel c # Set threshold to be fraction of max possible n_matches n_matches_thresh [ t , r , c ] = thresh_fract * min ( n_ref [ t ], n_base [ t , r , c ]) # Clip threshold between min and max if n_matches_thresh [ t , r , c ] < thresh_min : n_matches_thresh [ t , r , c ] = thresh_min if n_matches_thresh [ t , r , c ] > thresh_max : n_matches_thresh [ t , r , c ] = thresh_max Where: thresh_fract = config['register']['matches_thresh_fract] . Default: 0.25 thresh_min = config['register']['matches_thresh_min] . Default: 25 thresh_max = config['register']['matches_thresh_max] . Default: 300 The default are parameters are such that in most cases, n_matches_thresh[t, r, c] = thresh_max so we require more than 300 matches for a transform to be acceptable. The thresholds are saved to the Notebook as nb.register_debug.n_matches_thresh[t, r, c] . Chromatic Aberration Our expectation of the transforms found is that there should be a scaling factor to correct for chromatic aberration between color channels. We do not expect significant scaling between rounds or tiles though. Thus, we can compute the expected scaling for each transform to channel \\(c\\) by averaging over all tiles and rounds. We also avoid transforms which have failed based on their matches . Code Output # transforms: [n_tiles x n_rounds x n_channels x 4 x 3] import numpy as np failed = n_matches < n_matches_thresh # [n_tiles x n_rounds x n_channels] scaling = transforms [:, :, :, np . arange ( 3 ), np . arange ( 3 )] # [n_tiles x n_rounds x n_channels x 3] scaling = np . moveaxis ( scaling , - 1 , 0 ) # [3 x n_tiles x n_rounds x n_channels] failed = np . expand_dims ( failed , 0 ) . repeat ( 3 , 0 ) # [3 x n_tiles x n_riounds x n_channels] scaling [ failed ] = np . nan # don't use failed t/r/c in average # Use median to take average as to avoid outlier values av_scaling = np . nanmedian ( scaling , axis = [ 1 , 2 ]) # average over tiles and rounds av_scaling = np . moveaxis ( av_scaling , 0 , - 1 ) # [n_channels x 3] print ( av_scaling ) [[0.99863349 0.99863439 1. ] [1.001767 1.00164772 1. ] [1.00026354 1.00017105 1. ] [1.00129138 1.00123909 1. ] [1.00002731 0.99993372 1. ] [0.99826052 0.99822415 1. ] [0.99554262 0.99556903 1. ]] This gives us av_scaling[c, i] which is the scale factor for channel \\(c\\) in dimension \\(i\\) (order is \\(yxz\\) ). This is saved to the Notebook as nb.register_debug.av_scaling . From the Output above, there is a clear difference between channels, as we expect. Also, the \\(y\\) and \\(x\\) scaling for a particular channel tend to be very similar but the \\(z\\) scaling is always 1. This is because the z-pixels are larger than the yx-pixels so there are no two pixels very close in \\(z\\) . transform[t, r, c] is deemed to have failed if the scaling is significantly different from av_scaling[c] as quantified by: np.abs(transform[t, r, c][i, i] - av_scaling[c, i]) > scale_thresh[i] for any dimension \\(i\\) . Here, scale_thresh is config['register']['scale_dev_thresh'] . The default value of [0.01, 0.01, 0.1] is intended to be quite hard to exceed i.e. only really awful scaling will fail in this way. The \\(z\\) threshold is larger because if there is scaling in \\(z\\) , you would expect larger variance because the \\(z\\) -pixels are larger than the \\(yx\\) -pixels (this may need looking at though, I am not too sure about it, but I think I made it larger because quite a lot of transforms were failing based on the z-scaling). Shift Our expectation of the transforms found is that there should be a shift between rounds for a particular tile (in register_initial we expected the shift to a particular round should be quite similar across tiles - we relax that here). We do not expect significant shifts between channels for the same tile and round though. Thus, we can compute the expected shift for each transform of tile \\(t\\) to round \\(r\\) by averaging over all channels. We also avoid transforms which have failed based on their matches or scaling . Code Output # transforms: [n_tiles x n_rounds x n_channels x 4 x 3] import numpy as np failed = np . logical_or ( failed_matches , failed_scale ) # [n_tiles x n_rounds x n_channels] shifts = np . moveaxis ( transforms [:, :, :, 3 ], - 1 , 0 ) # [3 x n_tiles x n_rounds x n_channels] failed = np . expand_dims ( failed , 0 ) . repeat ( 3 , 0 ) # [3 x n_tiles x n_rounds x n_channels] shifts [ failed ] = np . nan # don't use failed t/r/c in average # Use median to take average as to avoid outlier values av_shifts = np . nanmedian ( shifts , axis = 3 ) # average over channels av_shifts = np . moveaxis ( av_shifts , 0 , - 1 ) # [n_tiles x n_rounds x 3] print ( np . around ( av_shifts [: 4 , : 3 ,:], 2 )) [[[35.79 24.04 0. ] [14.49 9.23 -0. ] [-2.25 -2.43 5.99]] [[47.74 30.01 0. ] [25.26 13.78 5.99] [10.35 4.43 -0. ]] [[52.54 36.04 0. ] [32.24 21.99 5.99] [16.64 11.6 5.99]] [[57.75 38.31 -5.99] [38.57 27.41 -0. ] [26.15 16.23 0. ]]] This gives us av_shifts[t, r, i] which is the average shift for tile \\(t\\) , round \\(r\\) in dimension \\(i\\) (order is \\(yxz\\) ). This is saved to the Notebook as nb.register_debug.av_shifts . From the first 4 tiles and 3 rounds printed in Output above, we see that for a given round, there is significant variance across tiles (note that the z-shift is in units of \\(yx\\) -pixels, 5.99 in these units is 1 in z-pixels). Systematic shift between tiles In the above Output , there seems to be a systematic shift between tiles: The \\(y\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 11. The \\(x\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 6. The \\(y\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 6. The \\(x\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 7. The \\(y\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5. The \\(x\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5. The \\(z\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts - 5.99. I am not sure if this is seen in all experiments but if it is, it may be useful to incorporate it into the code, both here and in the register initial section. transform[t, r, c] is deemed to have failed if the shift is significantly different from av_shifts[t, r] as quantified by: np.abs(transform[t, r, c][3, i] - av_shifts[t, r, i]) > shift_thresh[i] for any dimension \\(i\\) . Here, shift_thresh is config['register']['shift_dev_thresh'] . The default value of [15, 15, 5] is intended to be quite hard to exceed i.e. only really awful shifts will fail in this way. config['register']['shift_dev_thresh'][2] is in units of z-pixels and is converted to \\(yx\\) -pixels before applying the thresholding (the default value of 5 will become 29.95 for our examples). Error - average transform When computing av_shifts[t, r] , we require that for tile \\(t\\) , round \\(r\\) , at least one channel has not failed based on matches or scaling . If they have all failed, it cannot be computed and an error will be raised indicating the problematic tile/round pairs. If this error occurs, it is probably worth using the diagnostics to see why registration produces few matches or bad scaling for these tile/round pairs. If it seems to be a single tile or round that is the problem, it may be worth removing it from use_tiles / use_rounds and re-running . When computing av_scaling[c] , we require that for channel \\(c\\) , at least one round and tile has not failed based on matches . If they have all failed, it cannot be computed and an error will be raised indicating the problematic channels. As \\(n_{tiles} \\times n_{rounds} > n_{channels}\\) , this error is less likely to occur, but if it does, it is probably worth removing the indicated channels from use_channels and re-running . Regularized ICP For the transforms which failed (those for which nb.register_debug.failed[t, r, c] == True ), we save the failed transform as nb.register_debug.transform_outlier[t, r, c] . We then re-run ICP to compute a new transform. This time, though we want to ensure that transform[t, r, c] is pretty close to the av_transform[t, r, c] . av_transform av_transform[t, r, c] is the expected transform for tile \\(t\\) , round \\(r\\) , channel \\(c\\) . It is assumed to have no rotation, a scaling consistent with acceptable transforms to channel \\(c\\) and a shift consistent with acceptable transforms for tile \\(t\\) to round \\(r\\) . The Output below shows the average transform for tile 1, round 0, channel 0 using the av_scaling and av_shifts produced in the earlier sections. Code Output av_transform = np . zeros (( n_tiles , n_rounds , n_channels , 4 , 3 )) for t in use_tiles : for r in use_rounds : for c in use_channels : av_transform [ t , r , c , 3 ] = av_shifts [ t , r ] for i in range ( 3 ): av_transform [ t , r , c , i , i ] = av_scaling [ c , i ] print ( np . around ( av_transform [ 1 , 0 , 0 ], 5 )) [[ 0.99863 0. 0. ] [ 0. 0.99863 0. ] [ 0. 0. 1. ] [47.73948 30.00533 0. ]] To do this, we set the initial transform for transform[t, r, c] to be av_transform[t, r, c] . The ICP algorithm is then the same as in the pseudocode except for section 5. For normal least squares, we are finding the \\(4\\times3\\) transform \\(M\\) , such that the following loss function is minimised: \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 = \\sum_{s=0}^{n_{neighb}-1} (y_s - x_sM)^2 \\] Where: \\(n_{neighb}\\) is n_use introduced in step 4 of the pseudocode . \\(y_s\\) is spot_yxz_use[s] introduced in step 4 of the pseudocode ( \\(n_{neighb} \\times 3\\) ) \\(x_s\\) is ref_spot_yxz_use[s] introduced in step 4 of the pseudocode ( \\(n_{neighb} \\times 4\\) ) To keep \\(M\\) close to the average transform, \\(M_a\\) , we find \\(M\\) through regularized least squares which minimises the following loss function: \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2) \\] Where: \\(D_{scale}^2 = \\sum_{i=0}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2\\) is the squared distance between transform[:3, :] and av_transform[:3, :] i.e. between the rotation/scaling \\(3\\times3\\) matrix part of the transforms. \\(D_{shift}^2 = \\sum_{j=0}^2(M_{3j} - M_{a_{3j}})^2\\) is the squared distance between transform[3] and av_transform[3] i.e. between the shift part of the transforms. \\(\\lambda\\) is config['register']['regularize_constant'] . Default: 500 \\(\\mu\\) is config['register']['regularize_factor'] . Default: \\(5\\times10^4\\) Implementing regularized least squares in python Lets suppose we are doing normal least squares with \\(n_{neighb} = 4\\) , \\[ x = \\begin{pmatrix} \\lambda_0 & 0 & 0 & 0\\\\ 0 & \\lambda_0 & 0 & 0\\\\ 0 & 0 & \\lambda_0 & 0\\\\ 0 & 0 & 0 & \\lambda_1 \\end{pmatrix}, \\] \\[ y = xM_a = \\begin{pmatrix} \\lambda_0M_{a_{00}} & 0 & 0\\\\ 0 & \\lambda_0M_{a_{11}} & 0\\\\ 0 & 0 & \\lambda_0M_{a_{22}}\\\\ \\lambda_1M_{a_{30}} & \\lambda_1M_{a_{31}} & \\lambda_1M_{a_{32}} \\end{pmatrix} \\] Then the least squares solution for \\(M\\) will be the one which minimises the loss function: \\[ L = \\sum(y - xM)^2 = \\sum x^2(M_a - M)^2 = \\sum_{i=0}^3x_{ii}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2 = \\lambda_0^2 D_{scale}^2 + \\lambda_1^2 D_{shift}^2 \\] This matches the additional term in the regularised least squares loss function if: \\[ \\lambda_0 = \\sqrt{0.5\\lambda\\mu} \\] \\[ \\lambda_1 = \\sqrt{0.5\\lambda} \\] Hence in python , we append to the \\(y\\) array containing spot_yxz_use the form of y here so it now has shape \\((n_{neighb}+4) \\times 3\\) . We also append to the \\(x\\) array containing ref_spot_yxz_use the form of x here so it now has shape \\((n_{neighb}+4) \\times 4\\) . If we then just do normal least squares ( np.linalg.lstsq(x, y) ) on these modified \\(y\\) and \\(x\\) arrays, we get the regularized least squares solution for \\(M\\) . The intuition for suitable values of \\(\\lambda\\) and \\(\\mu\\) comes from the following: The desired distance between the shift found and the target shift, \\(D_{shift}\\) , is the equal to the average distance between neighbouring spots (the average of \\(D_s\\) ). If \\(\\lambda = n_{neighb}\\) , the contribution of the two terms in the loss function will then be equal if \\(\\mu = D_{shift}^2/D_{scale}^2\\) (in this case, regularised term is \\(n_{neighb}D_{shift}^2\\) but \\(D_{shift}^2\\) is average of \\(D_s^2\\) so this is equivalent to \\(\\sum_{s=0}^{n_{neighb}-1} D_{s}^2\\) ). A typical value of \\(D_s\\) is 2 (must be below neighb_dist_thresh ) and a typical target value of \\(D_{scale}\\) is 0.009 hence \\(\\mu = 2^2/0.009^2 \\approx 5\\times10^4\\) . The larger \\(\\mu\\) , the more the regularization will affect the scaling/rotation at the expense of the shift. Regularised term dominates the loss function if \\(\\lambda > n_{neighb}\\) so that as \\(\\lambda \\rightarrow \\infty\\) , \\(M \\rightarrow M_a\\) . Regularised term has little effect if \\(\\lambda < n_{neighb}\\) such that \\(M\\) tends to the normal least squares solution as \\(\\lambda \\rightarrow 0\\) . Hence, the value of \\(\\lambda\\) should be thought of as an \\(n_{neighb}\\) threshold. If there are more neighbours than \\(\\lambda\\) used to determine \\(M\\) then we trust that this is enough to get the correct transform. If there are less than \\(\\lambda\\) neighbours used, we don't think this is enough to trust the transform it would produce freely, so we restrict the value of \\(M\\) produced to be near the expected \\(M_a\\) . Typically, 500 neighbours is quite a good value, config['register']['regularize_constant'] should always be larger than config['register']['matches_thresh_max'] . view_icp_reg The function view_icp_reg is also useful for investigating the effect of \\(\\lambda\\) and \\(\\mu\\) . It is very similar to view_icp but view_icp_reg(t, r, c, reg_constant=[lambda1, lambda2], reg_factor=[mu1, mu2]) will have blue point clouds corresponding to: Reference: Reference ( \\(r_{ref}\\) / \\(c_{ref}\\) ) point cloud for tile \\(t\\) with no transform applied. \\(\\lambda=0\\) : Reference point cloud transformed according to transform found with no regularization. \\(\\lambda=\\infty\\) : Reference point cloud transformed according to target transform for regularization, \\(M_a\\) . This is found from av_scaling[c] and av_shifts[t, r] unless it is provided. \\(\\lambda=\\) lambda1 , \\(\\mu=\\) mu1 : Reference point cloud transformed according to transform found with regularized ICP using lambda1 and mu1 as the regularization parameters. \\(\\lambda=\\) lambda2 , \\(\\mu=\\) mu2 : Reference point cloud transformed according to transform found with regularized ICP using lambda2 and mu2 as the regularization parameters. If reg_constant and reg_factor are not provided, config['register']['regularize_constant'] and config['register']['regularize_factor'] will be used. The example below shows the point clouds produced after running view_icp_reg(0, 1, 0, reg_constant=[3e4], reg_factor=[1e6]) : No transform \\(\\lambda=0\\) \\(\\lambda=\\infty\\) \\(\\lambda=30000, \\mu=1\\times10^6\\) Here we see that all the transforms are visibly different but the \\(\\lambda=30000, \\mu=1\\times10^6\\) case is closer to \\(\\lambda=\\infty\\) than \\(\\lambda=0\\) . This makes sense because as shown in the right sidebar of the plots, the number of matches is about 5600 so \\(\\lambda = 30000 > n_{neighb}\\) so the final transform should be close to the target transform we are regularising towards. If view_icp_reg is run with plot_residual=True , it produces another plot which indicates how \\(D_{shift}\\) and \\(D_{scale}\\) vary with different values of reg_constant and reg_factor . view_icp_reg(nb, 0, 1, 0, reg_constant=np.logspace(0, 7, 8), reg_factor=[5e4]*8, plot_residual=True) produces the following additional plot: Here, each colored marker refers to a different \\(\\lambda/\\mu\\) combination. The \\(\\lambda=0\\) horizontal line indicates the values with no regularisation and the \\(n_{matches}\\) line indicates the number of matches found with \\(\\lambda=\\infty\\) . This plot nicely shows that for \\(\\lambda < n_{matches}\\) , the transform found is pretty close to the normal least squares solution but for \\(\\lambda > n_{matches}\\) , it is closer to the target transform we are regularising towards. view_icp_reg(nb, 0, 1, 0, reg_constant=[500] * 8, reg_factor=np.logspace(2, 9, 8), plot_residual=True) produces the following additional plot: This shows that varying \\(\\mu\\) while keeping \\(\\lambda\\) constant does not affect \\(D_{shift}\\) much but \\(D_{scale}\\) does decrease as \\(\\mu\\) increases. Error - too few matches After the call reference spots step, check_transforms will be run. This will produce a warning for any tile, round, channel for which nb.register_debug.n_matches < nb.register_debug.n_matches_thresh An error will be raised if any of the following is satisfied: For any given channel, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty channels should then be removed from use_channels . For any given tile, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty tiles should then be removed from use_tiles . For any given round, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty rounds should then be removed from use_rounds . error_fraction is given by config['register']['n_transforms_error_fraction ]. Debugging There are a few functions using matplotlib which may help to debug this section of the pipeline. view_affine_shift_info The view_affine_shift_info function plots the shift part of the affine transform ( nb.register.transform[t, r, c, 3] ) to all tiles of a given round and channel on the same plot (there are 3 plots for each round and channel). It also includes a plot of nb.register_debug.n_matches vs nb.register_debug.error for each round and channel. The error is the root-mean-square distance between neighbours used to compute the transform. Thus, the lower this value, and the higher \\(n_{matches}\\) , the better the transform. Channel 0 Channel 2 The channel shown is changed by scrolling with the mouse and as a sanity check, we do not expect the top two plots to vary much when the channel changes (the round 0 shifts above is quite a good example). The numbers refer to the tile, and they are blue if nb.register_debug.n_matches[t, r, c] > nb.register_debug.n_matches_thresh[t, r, c] . Otherwise, they are red, as with tile 2 in round 1 and 4 of the channel 2 plots. As with view_register_shift_info , this function can also be used to show nb.register_debug.transform_outlier[t, r, c, 3] by setting outlier=True , although the bottom plot will always show nb.register_debug.n_matches vs nb.register_debug.error . scale_box_plots This function produces two or three plots, one for each dimension. In plot \\(i\\) , there is a boxplot for each round and channel of nb.register.transform[:, r, c, i, i] : In this plot, we expect for a given channel, the scaling should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any outlier tiles (white crosses, +) not far from the boxplot). Also, we expect the Scaling - Y and Scaling - X to be very similar. Scaling - Z will likely be different, but it won't be shown if the range of Scaling - Z is less than 0.00001, which usually the case. view_icp This is similar to view_stitch_overlap . view_icp(nb, t, r, c) will always show the local coordinates of the point cloud for tile \\(t\\) , round \\(r\\) , channel \\(c\\) in red. This is spot_yxz in the psuedocode . There are then buttons to select which reference point cloud for tile \\(t\\) , round \\(r_{ref}\\) , channel \\(c_{ref}\\) is plotted in blue: No transform : This is the reference point cloud with no transform applied ( ref_spot_yxz in the psuedocode ). Shift : This is the reference point cloud, shifted according to nb.register_initial.shifts[t, r] ( ref_spot_yxz_transform computed in the first iteration in the psuedocode ) Affine : This is the reference point cloud, transformed according to nb.register.transform[t, r, c] ( ref_spot_yxz_transform computed in the last iteration in the psuedocode ) An example is shown below: No transform Shift Affine If regularized ICP was required for the chosen tile/round/channel, an additional button will be present titled Regularized . The Affine button will then indicate the reference point cloud, transformed according to nb.register_debug.transform_outlier (i.e. with no regularization). The Regularized button will indicate the reference point cloud, transformed according to nb.register.transform[t, r, c] (i.e. final transform found with regularization). Can I use view_icp before running the register stage of the pipeline? The view_icp function can be used after the find_spots page has been added to the Notebook . In the case where the Notebook does not have the register_initial page, the shift will be computed. In the case where the Notebook does not have the register page, the affine transform will be computed with no regularization. Psuedocode This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the transform is found, see the ICP section. r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for t in use_tiles: Center reference point cloud: spot_yxz[t, r_ref, c_ref] = spot_yxz[t, r_ref, c_ref] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[t, r_ref, c_ref][:, 2] = spot_yxz[t, r_ref, c_ref][:, 2] * z_scale for r in use_rounds: for c in use_channels: Center point cloud: spot_yxz[t, r, c] = spot_yxz[t, r, c] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[t, r, c][:, 2] = spot_yxz[t, r, c][:, 2] * z_scale Only keep spots whose nearest neighbour is far spot_yxz[t, r, c] = spot_yxz[t, r, c][isolated] Find transform between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c] using ICP. Compute av_scaling and av_shift. transforms[t, r, c] is failed if one of the following is satisfied: - n_matches < n_matches_thresh. - transforms[t, r, c][i, i] significantly different to av_scaling[c, i] for any i. - transforms[t, r, c][3, i] significantly different to av_shift[t, r, i] for any i. For failed transforms, recompute transform between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c] using regularized ICP. Add transform to register NotebookPage. Add debugging information to register_debug NotebookPage. Add register and register_debug NotebookPages to Notebook.","title":"Register"},{"location":"pipeline/register/#register","text":"The register step of the pipeline finds an affine transform between the reference round/reference channel ( \\(r_{ref}\\) / \\(c_{ref}\\) ) and each imaging round and channel for every tile. The affine transform to tile \\(t\\) , round \\(r\\) , channel \\(c\\) is found through iterative closest point using the shift, nb.register_initial.shift[t, r] , found during the register_initial step of the pipeline as the starting point. It is saved to the Notebook as nb.register.transform[t, r, c] . These transforms are used later in the pipeline to determine the intensity of a pixel in each round and channel. The register and register_debug NotebookPages are added to the Notebook after this stage is finished.","title":"Register"},{"location":"pipeline/register/#affine-transform","text":"We want to find the transform from tile \\(t\\) , round \\(r_{ref}\\) , channel \\(c_{ref}\\) to round \\(r\\) , channel \\(c\\) for all tiles, rounds and channels such that there is pixel-level alignment between the images. The pixel-level alignment is important because most spots are only a few pixels in size, so even a one-pixel registration error can compromise the spot_colors found in the next stage of the pipeline and thus the gene assignment . The shifts found in the register_initial step of the pipeline are not sufficient for this, because of chromatic aberration which will cause a scaling between color channels. There may also be small rotational or non-rigid shifts; thus we find affine transformations which can include shifts, scalings, rotations and shears.","title":"Affine Transform"},{"location":"pipeline/register/#starting-transform","text":"The affine transforms are found using the iterative-closest point ( ICP ) algorithm. This is highly sensitive to local maxima, so it is initialized with the shifts found in the register_initial step, nb.register_initial.shift . The starting transform to a particular round and tile is the same for all channels. The shifts are put into the form of an affine transform ( \\(4 \\times 3\\) array) using the transform_from_scale_shift function. Starting transform from shifts The code below indicates how the initial shifts ( \\(n_{tiles} \\times n_{rounds} \\times 3\\) array) are converted into initial transforms ( \\(n_{tiles} \\times n_{rounds} \\times n_{channels} \\times 4 \\times 3\\) array). Code Output import numpy as np from coppafish.register.base import transform_from_scale_shift t_print = 1 r_print = 1 initial_shifts = nb . register_initial . shift # z shift is in z-pixel units print ( f \"Initial shift for tile { t_print } , round { r_print } : \\n { initial_shifts [ t_print , r_print ] } \" ) # Convert z shift into same units as yx pixels initial_shifts = initial_shifts . astype ( float ) z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] for t in nb . basic_info . use_tiles : for r in nb . basic_info . use_rounds : initial_shifts [ t , r ] = initial_shifts [ t , r ] * z_scale print ( f \"Initial shift for tile { t_print } , round { r_print } (z shift in YX pixels): \\n \" f \" { np . around ( initial_shifts [ t_print , r_print ], 2 ) } \" ) # Convert shifts to affine transform 4 x 3 form initial_scale = np . ones (( nb . basic_info . n_channels , 3 )) # set all scalings to 1 initially # as just want shift. initial_transforms = transform_from_scale_shift ( initial_scale , initial_shifts ) # Show transform different for rounds but same for channel within round for r in range ( 2 ): for c in range ( 2 ): print ( f \"Initial transform for tile { t_print } , round { r } , channel { c } : \\n \" f \" { np . around ( initial_transforms [ t_print , r , c ], 2 ) } \" ) Initial Shifts for tile 1, round 1: [25 14 1] Initial shift for tile 1, round 1 (z shift in YX pixels): [25. 14. 5.99] Initial transform for tile 1, round 0, channel 0: [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.] [48. 30. 0.]] Initial transform for tile 1, round 0, channel 1: [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.] [48. 30. 0.]] Initial transform for tile 1, round 1, channel 0: [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ] [25. 14. 5.99]] Initial transform for tile 1, round 1, channel 1: [[ 1. 0. 0. ] [ 0. 1. 0. ] [ 0. 0. 1. ] [25. 14. 5.99]]","title":"Starting Transform"},{"location":"pipeline/register/#icp","text":"The pseudocode for the ICP algorithm to find the affine transform for tile \\(t\\) round \\(r\\) , channel \\(c\\) is indicated below. The shape of the various arrays are indicated in the comments (#). Preparing point clouds Prior to the ICP algorithm, the \\(yxz\\) coordinates of spots on a tile are centered. This is because, it makes more sense to me, if any rotation is applied around the centre of the tile. Also, like for the initial shifts, the z-coordinates must be converted into units of yx-pixels, so overall: spot_yxz = spot_yxz - nb . basic_info . tile_centre # center coordinates # Convert units of z-coordinates z_scale = [ 1 , 1 , nb . basic_info . pixel_size_z / nb . basic_info . pixel_size_xy ] spot_yxz = spot_yxz * z_scale For the non-reference point clouds we only keep spots which are isolated. This is because the ICP algorithm is less likely to fall into local maxima if the spots are quite well separated. We deam a spot isolated if the nearest spot to it is further away than 2 * neighb_dist . Where neighb_dist = config['register']['neighb_dist_thresh_2d'] if 2D and config['register']['neighb_dist_thresh_3d'] if 3D . n_iter is the maximum number of iterations and is set by config['register']['n_iter'] . neighb_dist_thresh is the distance in \\(yx\\) pixels below which neighbours are a good match. It is given by config['register']['neighb_dist_thresh_2d'] if 2D and config['register']['neighb_dist_thresh_3d'] if 3D . Only neighbours which are closer than this are used when computing the transform. Padding ref_spot_yxz ref_spot_yxz has shape [n_ref x 4] not [n_ref x 3] because to be able to be multiplied by the affine transform ( [4 x 3] ), it must be padded with ones i.e. ref_spot_yxz[:, 3] = 1 . ref_spot_yxz @ transform is then the same as ref_spot_yxz[:, :3] @ transform[:3, :] + transform[3] i.e. rotation/scaling matrix applied and then the shift is added. spot_yxz = yxz coordinates for spots detected on tile t, round r, channel c. # [n_base x 3] ref_spot_yxz = padded yxz coordinates for spots detected on tile t, reference round, reference channel. # [n_ref x 4] transform_initial = starting transform for tile t, round r, channel c. transform = transform_initial # [4 x 3] neighb_ind = zeros(n_ref) # [n_ref] neighb_ind_last = neighb_ind # [n_ref] i = 0 while i < n_iter: 1. Transform ref_spot_yxz according to transform to give ref_spot_yxz_transform. # [n_ref x 3] This is just matrix multiplication. 2. neighb_ind[s] is index of spot in spot_yxz closest to ref_spot_yxz_transform[s]. 3. dist[s] is the distance between spot_yxz[neighb_ind[s]] and ref_spot_yxz_transform[s] # [n_ref] 4. Choose which spots to use to update the transform: use = dist < neighb_dist_thresh spot_yxz_use = spot_yxz[neighb_ind[use]] # [n_use x 3] ref_spot_yxz_use = ref_spot_yxz[use] # [n_use x 4] 5. Update transform to be the 4x3 matrix which multiplies ref_spot_yxz_use such that the distances between the transformed spots and spot_yxz_use are minimised. This is the least squares solution. 6. If neighb_ind and neighb_ind_last are identical, then stop iteration i.e. i = n_iter. 7. neighb_ind_last = neighb_ind 8. i = i + 1","title":"ICP"},{"location":"pipeline/register/#checking-icp-results","text":"Once the ICP algorithm has obtained a transform, transform[t, r, c] for each tile, round and channel, we want to determine if they are acceptable. Three criteria must be satisfied for transform[t, r, c] to be considered acceptable : Number of matches exceeds threshold Diagonal elements of transform[t, r, c] are near to what we broadly expect them to be . Shift part of transform[t, r, c] i.e. transform[t, r, c][3] is near to what we broadly expect it to be .","title":"Checking ICP results"},{"location":"pipeline/register/#number-of-matches","text":"The number of matches, n_matches[t, r, c] , are the number of pairs of spots used to compute transform[t, r, c] , i.e. n_use in the above pseudocode, and thus it depends on the neighb_dist_thresh parameter. The number of matches are saved to the Notebook as nb.register_debug.n_matches[t, r, c] . transform[t, r, c] is deemed to have failed if n_matches[t, r, c] < n_matches_thresh[t, r, c] . n_matches_thresh[t, r, c] is computed as follows: # n_ref[t] - number of spots detected on tile t, # reference round, reference channel # n_base[t, r, c] - number of spots detected on tile t, # round r, channel c # Set threshold to be fraction of max possible n_matches n_matches_thresh [ t , r , c ] = thresh_fract * min ( n_ref [ t ], n_base [ t , r , c ]) # Clip threshold between min and max if n_matches_thresh [ t , r , c ] < thresh_min : n_matches_thresh [ t , r , c ] = thresh_min if n_matches_thresh [ t , r , c ] > thresh_max : n_matches_thresh [ t , r , c ] = thresh_max Where: thresh_fract = config['register']['matches_thresh_fract] . Default: 0.25 thresh_min = config['register']['matches_thresh_min] . Default: 25 thresh_max = config['register']['matches_thresh_max] . Default: 300 The default are parameters are such that in most cases, n_matches_thresh[t, r, c] = thresh_max so we require more than 300 matches for a transform to be acceptable. The thresholds are saved to the Notebook as nb.register_debug.n_matches_thresh[t, r, c] .","title":"Number of matches"},{"location":"pipeline/register/#chromatic-aberration","text":"Our expectation of the transforms found is that there should be a scaling factor to correct for chromatic aberration between color channels. We do not expect significant scaling between rounds or tiles though. Thus, we can compute the expected scaling for each transform to channel \\(c\\) by averaging over all tiles and rounds. We also avoid transforms which have failed based on their matches . Code Output # transforms: [n_tiles x n_rounds x n_channels x 4 x 3] import numpy as np failed = n_matches < n_matches_thresh # [n_tiles x n_rounds x n_channels] scaling = transforms [:, :, :, np . arange ( 3 ), np . arange ( 3 )] # [n_tiles x n_rounds x n_channels x 3] scaling = np . moveaxis ( scaling , - 1 , 0 ) # [3 x n_tiles x n_rounds x n_channels] failed = np . expand_dims ( failed , 0 ) . repeat ( 3 , 0 ) # [3 x n_tiles x n_riounds x n_channels] scaling [ failed ] = np . nan # don't use failed t/r/c in average # Use median to take average as to avoid outlier values av_scaling = np . nanmedian ( scaling , axis = [ 1 , 2 ]) # average over tiles and rounds av_scaling = np . moveaxis ( av_scaling , 0 , - 1 ) # [n_channels x 3] print ( av_scaling ) [[0.99863349 0.99863439 1. ] [1.001767 1.00164772 1. ] [1.00026354 1.00017105 1. ] [1.00129138 1.00123909 1. ] [1.00002731 0.99993372 1. ] [0.99826052 0.99822415 1. ] [0.99554262 0.99556903 1. ]] This gives us av_scaling[c, i] which is the scale factor for channel \\(c\\) in dimension \\(i\\) (order is \\(yxz\\) ). This is saved to the Notebook as nb.register_debug.av_scaling . From the Output above, there is a clear difference between channels, as we expect. Also, the \\(y\\) and \\(x\\) scaling for a particular channel tend to be very similar but the \\(z\\) scaling is always 1. This is because the z-pixels are larger than the yx-pixels so there are no two pixels very close in \\(z\\) . transform[t, r, c] is deemed to have failed if the scaling is significantly different from av_scaling[c] as quantified by: np.abs(transform[t, r, c][i, i] - av_scaling[c, i]) > scale_thresh[i] for any dimension \\(i\\) . Here, scale_thresh is config['register']['scale_dev_thresh'] . The default value of [0.01, 0.01, 0.1] is intended to be quite hard to exceed i.e. only really awful scaling will fail in this way. The \\(z\\) threshold is larger because if there is scaling in \\(z\\) , you would expect larger variance because the \\(z\\) -pixels are larger than the \\(yx\\) -pixels (this may need looking at though, I am not too sure about it, but I think I made it larger because quite a lot of transforms were failing based on the z-scaling).","title":"Chromatic Aberration"},{"location":"pipeline/register/#shift","text":"Our expectation of the transforms found is that there should be a shift between rounds for a particular tile (in register_initial we expected the shift to a particular round should be quite similar across tiles - we relax that here). We do not expect significant shifts between channels for the same tile and round though. Thus, we can compute the expected shift for each transform of tile \\(t\\) to round \\(r\\) by averaging over all channels. We also avoid transforms which have failed based on their matches or scaling . Code Output # transforms: [n_tiles x n_rounds x n_channels x 4 x 3] import numpy as np failed = np . logical_or ( failed_matches , failed_scale ) # [n_tiles x n_rounds x n_channels] shifts = np . moveaxis ( transforms [:, :, :, 3 ], - 1 , 0 ) # [3 x n_tiles x n_rounds x n_channels] failed = np . expand_dims ( failed , 0 ) . repeat ( 3 , 0 ) # [3 x n_tiles x n_rounds x n_channels] shifts [ failed ] = np . nan # don't use failed t/r/c in average # Use median to take average as to avoid outlier values av_shifts = np . nanmedian ( shifts , axis = 3 ) # average over channels av_shifts = np . moveaxis ( av_shifts , 0 , - 1 ) # [n_tiles x n_rounds x 3] print ( np . around ( av_shifts [: 4 , : 3 ,:], 2 )) [[[35.79 24.04 0. ] [14.49 9.23 -0. ] [-2.25 -2.43 5.99]] [[47.74 30.01 0. ] [25.26 13.78 5.99] [10.35 4.43 -0. ]] [[52.54 36.04 0. ] [32.24 21.99 5.99] [16.64 11.6 5.99]] [[57.75 38.31 -5.99] [38.57 27.41 -0. ] [26.15 16.23 0. ]]] This gives us av_shifts[t, r, i] which is the average shift for tile \\(t\\) , round \\(r\\) in dimension \\(i\\) (order is \\(yxz\\) ). This is saved to the Notebook as nb.register_debug.av_shifts . From the first 4 tiles and 3 rounds printed in Output above, we see that for a given round, there is significant variance across tiles (note that the z-shift is in units of \\(yx\\) -pixels, 5.99 in these units is 1 in z-pixels). Systematic shift between tiles In the above Output , there seems to be a systematic shift between tiles: The \\(y\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 11. The \\(x\\) shifts for tile 1 for all rounds seem to be approximately equal to the tile 0 shifts + 6. The \\(y\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 6. The \\(x\\) shifts for tile 2 for all rounds seem to be approximately equal to the tile 1 shifts + 7. The \\(y\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5. The \\(x\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts + 5. The \\(z\\) shifts for tile 3 for all rounds seem to be approximately equal to the tile 2 shifts - 5.99. I am not sure if this is seen in all experiments but if it is, it may be useful to incorporate it into the code, both here and in the register initial section. transform[t, r, c] is deemed to have failed if the shift is significantly different from av_shifts[t, r] as quantified by: np.abs(transform[t, r, c][3, i] - av_shifts[t, r, i]) > shift_thresh[i] for any dimension \\(i\\) . Here, shift_thresh is config['register']['shift_dev_thresh'] . The default value of [15, 15, 5] is intended to be quite hard to exceed i.e. only really awful shifts will fail in this way. config['register']['shift_dev_thresh'][2] is in units of z-pixels and is converted to \\(yx\\) -pixels before applying the thresholding (the default value of 5 will become 29.95 for our examples).","title":"Shift"},{"location":"pipeline/register/#error-average-transform","text":"When computing av_shifts[t, r] , we require that for tile \\(t\\) , round \\(r\\) , at least one channel has not failed based on matches or scaling . If they have all failed, it cannot be computed and an error will be raised indicating the problematic tile/round pairs. If this error occurs, it is probably worth using the diagnostics to see why registration produces few matches or bad scaling for these tile/round pairs. If it seems to be a single tile or round that is the problem, it may be worth removing it from use_tiles / use_rounds and re-running . When computing av_scaling[c] , we require that for channel \\(c\\) , at least one round and tile has not failed based on matches . If they have all failed, it cannot be computed and an error will be raised indicating the problematic channels. As \\(n_{tiles} \\times n_{rounds} > n_{channels}\\) , this error is less likely to occur, but if it does, it is probably worth removing the indicated channels from use_channels and re-running .","title":"Error - average transform"},{"location":"pipeline/register/#regularized-icp","text":"For the transforms which failed (those for which nb.register_debug.failed[t, r, c] == True ), we save the failed transform as nb.register_debug.transform_outlier[t, r, c] . We then re-run ICP to compute a new transform. This time, though we want to ensure that transform[t, r, c] is pretty close to the av_transform[t, r, c] . av_transform av_transform[t, r, c] is the expected transform for tile \\(t\\) , round \\(r\\) , channel \\(c\\) . It is assumed to have no rotation, a scaling consistent with acceptable transforms to channel \\(c\\) and a shift consistent with acceptable transforms for tile \\(t\\) to round \\(r\\) . The Output below shows the average transform for tile 1, round 0, channel 0 using the av_scaling and av_shifts produced in the earlier sections. Code Output av_transform = np . zeros (( n_tiles , n_rounds , n_channels , 4 , 3 )) for t in use_tiles : for r in use_rounds : for c in use_channels : av_transform [ t , r , c , 3 ] = av_shifts [ t , r ] for i in range ( 3 ): av_transform [ t , r , c , i , i ] = av_scaling [ c , i ] print ( np . around ( av_transform [ 1 , 0 , 0 ], 5 )) [[ 0.99863 0. 0. ] [ 0. 0.99863 0. ] [ 0. 0. 1. ] [47.73948 30.00533 0. ]] To do this, we set the initial transform for transform[t, r, c] to be av_transform[t, r, c] . The ICP algorithm is then the same as in the pseudocode except for section 5. For normal least squares, we are finding the \\(4\\times3\\) transform \\(M\\) , such that the following loss function is minimised: \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 = \\sum_{s=0}^{n_{neighb}-1} (y_s - x_sM)^2 \\] Where: \\(n_{neighb}\\) is n_use introduced in step 4 of the pseudocode . \\(y_s\\) is spot_yxz_use[s] introduced in step 4 of the pseudocode ( \\(n_{neighb} \\times 3\\) ) \\(x_s\\) is ref_spot_yxz_use[s] introduced in step 4 of the pseudocode ( \\(n_{neighb} \\times 4\\) ) To keep \\(M\\) close to the average transform, \\(M_a\\) , we find \\(M\\) through regularized least squares which minimises the following loss function: \\[ L = \\sum_{s=0}^{n_{neighb}-1} D_{s}^2 + 0.5\\lambda (\\mu D_{scale}^2 + D_{shift}^2) \\] Where: \\(D_{scale}^2 = \\sum_{i=0}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2\\) is the squared distance between transform[:3, :] and av_transform[:3, :] i.e. between the rotation/scaling \\(3\\times3\\) matrix part of the transforms. \\(D_{shift}^2 = \\sum_{j=0}^2(M_{3j} - M_{a_{3j}})^2\\) is the squared distance between transform[3] and av_transform[3] i.e. between the shift part of the transforms. \\(\\lambda\\) is config['register']['regularize_constant'] . Default: 500 \\(\\mu\\) is config['register']['regularize_factor'] . Default: \\(5\\times10^4\\) Implementing regularized least squares in python Lets suppose we are doing normal least squares with \\(n_{neighb} = 4\\) , \\[ x = \\begin{pmatrix} \\lambda_0 & 0 & 0 & 0\\\\ 0 & \\lambda_0 & 0 & 0\\\\ 0 & 0 & \\lambda_0 & 0\\\\ 0 & 0 & 0 & \\lambda_1 \\end{pmatrix}, \\] \\[ y = xM_a = \\begin{pmatrix} \\lambda_0M_{a_{00}} & 0 & 0\\\\ 0 & \\lambda_0M_{a_{11}} & 0\\\\ 0 & 0 & \\lambda_0M_{a_{22}}\\\\ \\lambda_1M_{a_{30}} & \\lambda_1M_{a_{31}} & \\lambda_1M_{a_{32}} \\end{pmatrix} \\] Then the least squares solution for \\(M\\) will be the one which minimises the loss function: \\[ L = \\sum(y - xM)^2 = \\sum x^2(M_a - M)^2 = \\sum_{i=0}^3x_{ii}^2\\sum_{j=0}^2(M_{ij} - M_{a_{ij}})^2 = \\lambda_0^2 D_{scale}^2 + \\lambda_1^2 D_{shift}^2 \\] This matches the additional term in the regularised least squares loss function if: \\[ \\lambda_0 = \\sqrt{0.5\\lambda\\mu} \\] \\[ \\lambda_1 = \\sqrt{0.5\\lambda} \\] Hence in python , we append to the \\(y\\) array containing spot_yxz_use the form of y here so it now has shape \\((n_{neighb}+4) \\times 3\\) . We also append to the \\(x\\) array containing ref_spot_yxz_use the form of x here so it now has shape \\((n_{neighb}+4) \\times 4\\) . If we then just do normal least squares ( np.linalg.lstsq(x, y) ) on these modified \\(y\\) and \\(x\\) arrays, we get the regularized least squares solution for \\(M\\) . The intuition for suitable values of \\(\\lambda\\) and \\(\\mu\\) comes from the following: The desired distance between the shift found and the target shift, \\(D_{shift}\\) , is the equal to the average distance between neighbouring spots (the average of \\(D_s\\) ). If \\(\\lambda = n_{neighb}\\) , the contribution of the two terms in the loss function will then be equal if \\(\\mu = D_{shift}^2/D_{scale}^2\\) (in this case, regularised term is \\(n_{neighb}D_{shift}^2\\) but \\(D_{shift}^2\\) is average of \\(D_s^2\\) so this is equivalent to \\(\\sum_{s=0}^{n_{neighb}-1} D_{s}^2\\) ). A typical value of \\(D_s\\) is 2 (must be below neighb_dist_thresh ) and a typical target value of \\(D_{scale}\\) is 0.009 hence \\(\\mu = 2^2/0.009^2 \\approx 5\\times10^4\\) . The larger \\(\\mu\\) , the more the regularization will affect the scaling/rotation at the expense of the shift. Regularised term dominates the loss function if \\(\\lambda > n_{neighb}\\) so that as \\(\\lambda \\rightarrow \\infty\\) , \\(M \\rightarrow M_a\\) . Regularised term has little effect if \\(\\lambda < n_{neighb}\\) such that \\(M\\) tends to the normal least squares solution as \\(\\lambda \\rightarrow 0\\) . Hence, the value of \\(\\lambda\\) should be thought of as an \\(n_{neighb}\\) threshold. If there are more neighbours than \\(\\lambda\\) used to determine \\(M\\) then we trust that this is enough to get the correct transform. If there are less than \\(\\lambda\\) neighbours used, we don't think this is enough to trust the transform it would produce freely, so we restrict the value of \\(M\\) produced to be near the expected \\(M_a\\) . Typically, 500 neighbours is quite a good value, config['register']['regularize_constant'] should always be larger than config['register']['matches_thresh_max'] .","title":"Regularized ICP"},{"location":"pipeline/register/#view_icp_reg","text":"The function view_icp_reg is also useful for investigating the effect of \\(\\lambda\\) and \\(\\mu\\) . It is very similar to view_icp but view_icp_reg(t, r, c, reg_constant=[lambda1, lambda2], reg_factor=[mu1, mu2]) will have blue point clouds corresponding to: Reference: Reference ( \\(r_{ref}\\) / \\(c_{ref}\\) ) point cloud for tile \\(t\\) with no transform applied. \\(\\lambda=0\\) : Reference point cloud transformed according to transform found with no regularization. \\(\\lambda=\\infty\\) : Reference point cloud transformed according to target transform for regularization, \\(M_a\\) . This is found from av_scaling[c] and av_shifts[t, r] unless it is provided. \\(\\lambda=\\) lambda1 , \\(\\mu=\\) mu1 : Reference point cloud transformed according to transform found with regularized ICP using lambda1 and mu1 as the regularization parameters. \\(\\lambda=\\) lambda2 , \\(\\mu=\\) mu2 : Reference point cloud transformed according to transform found with regularized ICP using lambda2 and mu2 as the regularization parameters. If reg_constant and reg_factor are not provided, config['register']['regularize_constant'] and config['register']['regularize_factor'] will be used. The example below shows the point clouds produced after running view_icp_reg(0, 1, 0, reg_constant=[3e4], reg_factor=[1e6]) : No transform \\(\\lambda=0\\) \\(\\lambda=\\infty\\) \\(\\lambda=30000, \\mu=1\\times10^6\\) Here we see that all the transforms are visibly different but the \\(\\lambda=30000, \\mu=1\\times10^6\\) case is closer to \\(\\lambda=\\infty\\) than \\(\\lambda=0\\) . This makes sense because as shown in the right sidebar of the plots, the number of matches is about 5600 so \\(\\lambda = 30000 > n_{neighb}\\) so the final transform should be close to the target transform we are regularising towards. If view_icp_reg is run with plot_residual=True , it produces another plot which indicates how \\(D_{shift}\\) and \\(D_{scale}\\) vary with different values of reg_constant and reg_factor . view_icp_reg(nb, 0, 1, 0, reg_constant=np.logspace(0, 7, 8), reg_factor=[5e4]*8, plot_residual=True) produces the following additional plot: Here, each colored marker refers to a different \\(\\lambda/\\mu\\) combination. The \\(\\lambda=0\\) horizontal line indicates the values with no regularisation and the \\(n_{matches}\\) line indicates the number of matches found with \\(\\lambda=\\infty\\) . This plot nicely shows that for \\(\\lambda < n_{matches}\\) , the transform found is pretty close to the normal least squares solution but for \\(\\lambda > n_{matches}\\) , it is closer to the target transform we are regularising towards. view_icp_reg(nb, 0, 1, 0, reg_constant=[500] * 8, reg_factor=np.logspace(2, 9, 8), plot_residual=True) produces the following additional plot: This shows that varying \\(\\mu\\) while keeping \\(\\lambda\\) constant does not affect \\(D_{shift}\\) much but \\(D_{scale}\\) does decrease as \\(\\mu\\) increases.","title":"view_icp_reg"},{"location":"pipeline/register/#error-too-few-matches","text":"After the call reference spots step, check_transforms will be run. This will produce a warning for any tile, round, channel for which nb.register_debug.n_matches < nb.register_debug.n_matches_thresh An error will be raised if any of the following is satisfied: For any given channel, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty channels should then be removed from use_channels . For any given tile, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty tiles should then be removed from use_tiles . For any given round, the number of transforms with n_matches < n_matches_thresh exceeds error_fraction . The faulty rounds should then be removed from use_rounds . error_fraction is given by config['register']['n_transforms_error_fraction ].","title":"Error - too few matches"},{"location":"pipeline/register/#debugging","text":"There are a few functions using matplotlib which may help to debug this section of the pipeline.","title":"Debugging"},{"location":"pipeline/register/#view_affine_shift_info","text":"The view_affine_shift_info function plots the shift part of the affine transform ( nb.register.transform[t, r, c, 3] ) to all tiles of a given round and channel on the same plot (there are 3 plots for each round and channel). It also includes a plot of nb.register_debug.n_matches vs nb.register_debug.error for each round and channel. The error is the root-mean-square distance between neighbours used to compute the transform. Thus, the lower this value, and the higher \\(n_{matches}\\) , the better the transform. Channel 0 Channel 2 The channel shown is changed by scrolling with the mouse and as a sanity check, we do not expect the top two plots to vary much when the channel changes (the round 0 shifts above is quite a good example). The numbers refer to the tile, and they are blue if nb.register_debug.n_matches[t, r, c] > nb.register_debug.n_matches_thresh[t, r, c] . Otherwise, they are red, as with tile 2 in round 1 and 4 of the channel 2 plots. As with view_register_shift_info , this function can also be used to show nb.register_debug.transform_outlier[t, r, c, 3] by setting outlier=True , although the bottom plot will always show nb.register_debug.n_matches vs nb.register_debug.error .","title":"view_affine_shift_info"},{"location":"pipeline/register/#scale_box_plots","text":"This function produces two or three plots, one for each dimension. In plot \\(i\\) , there is a boxplot for each round and channel of nb.register.transform[:, r, c, i, i] : In this plot, we expect for a given channel, the scaling should be similar across all rounds and tiles (i.e. boxplots of the same color should be at the same height, and they should have quite small ranges with any outlier tiles (white crosses, +) not far from the boxplot). Also, we expect the Scaling - Y and Scaling - X to be very similar. Scaling - Z will likely be different, but it won't be shown if the range of Scaling - Z is less than 0.00001, which usually the case.","title":"scale_box_plots"},{"location":"pipeline/register/#view_icp","text":"This is similar to view_stitch_overlap . view_icp(nb, t, r, c) will always show the local coordinates of the point cloud for tile \\(t\\) , round \\(r\\) , channel \\(c\\) in red. This is spot_yxz in the psuedocode . There are then buttons to select which reference point cloud for tile \\(t\\) , round \\(r_{ref}\\) , channel \\(c_{ref}\\) is plotted in blue: No transform : This is the reference point cloud with no transform applied ( ref_spot_yxz in the psuedocode ). Shift : This is the reference point cloud, shifted according to nb.register_initial.shifts[t, r] ( ref_spot_yxz_transform computed in the first iteration in the psuedocode ) Affine : This is the reference point cloud, transformed according to nb.register.transform[t, r, c] ( ref_spot_yxz_transform computed in the last iteration in the psuedocode ) An example is shown below: No transform Shift Affine If regularized ICP was required for the chosen tile/round/channel, an additional button will be present titled Regularized . The Affine button will then indicate the reference point cloud, transformed according to nb.register_debug.transform_outlier (i.e. with no regularization). The Regularized button will indicate the reference point cloud, transformed according to nb.register.transform[t, r, c] (i.e. final transform found with regularization). Can I use view_icp before running the register stage of the pipeline? The view_icp function can be used after the find_spots page has been added to the Notebook . In the case where the Notebook does not have the register_initial page, the shift will be computed. In the case where the Notebook does not have the register page, the affine transform will be computed with no regularization.","title":"view_icp"},{"location":"pipeline/register/#psuedocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the transform is found, see the ICP section. r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for t in use_tiles: Center reference point cloud: spot_yxz[t, r_ref, c_ref] = spot_yxz[t, r_ref, c_ref] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[t, r_ref, c_ref][:, 2] = spot_yxz[t, r_ref, c_ref][:, 2] * z_scale for r in use_rounds: for c in use_channels: Center point cloud: spot_yxz[t, r, c] = spot_yxz[t, r, c] - tile_centre Convert z coordinate into yx-pixels: spot_yxz[t, r, c][:, 2] = spot_yxz[t, r, c][:, 2] * z_scale Only keep spots whose nearest neighbour is far spot_yxz[t, r, c] = spot_yxz[t, r, c][isolated] Find transform between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c] using ICP. Compute av_scaling and av_shift. transforms[t, r, c] is failed if one of the following is satisfied: - n_matches < n_matches_thresh. - transforms[t, r, c][i, i] significantly different to av_scaling[c, i] for any i. - transforms[t, r, c][3, i] significantly different to av_shift[t, r, i] for any i. For failed transforms, recompute transform between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c] using regularized ICP. Add transform to register NotebookPage. Add debugging information to register_debug NotebookPage. Add register and register_debug NotebookPages to Notebook.","title":"Psuedocode"},{"location":"pipeline/register_initial/","text":"Register Initial The register initial step of the pipeline uses the point clouds added to the Notebook during the find_spots step to find the shift between the reference round and each imaging round for every tile. The \\(yxz\\) shift to tile \\(t\\) , round \\(r\\) is saved as nb.register_initial.shift[t, r] . This shift is then used as a starting point for finding the affine transforms in the register step of the pipeline to all channels of tile \\(t\\) , round \\(r\\) . The register_initial NotebookPage is added to the Notebook after this stage is finished. Shift The channel used for finding the shifts is specified by \\(c_{shift}\\) = config['register_initial']['shift_channel'] . If it is left blank, it will be set to \\(c_{ref}\\) ( nb.basic_info.ref_channel ). This channel should be one with lots of spots and if an error is hit, it may be worth re-running with a different value of this parameter. So, for tile \\(t\\) , round \\(r\\) , we find the shift between \\(r_{ref}\\) / \\(c_{ref}\\) and \\(r\\) / \\(c_{shift}\\) . The function to compute these shifts is exactly the same as the one used in the stitch section of the pipeline and the parameters in the register initial section of the config file do the same thing as the corresponding parameters in the stitch section. A few details are different though, as explained below. Initial range The difference to the stitch case is that config['register_initial']['shift_min'] and config['register_initial']['shift_max'] are always used. We expect the shift between rounds to be quite small hence the default values , which perform an exhaustive search centered on 0 in each direction with a range of 200 in \\(y\\) and \\(x\\) and a range of 6 in \\(z\\) . Updating initial range We assume that the shifts to a given round will be approximately the same for all tiles. So, after we have found at least 3 shifts to a round which have score > score_thresh , we update our initial exhaustive search range to save time for future tiles. See the example in the stitch section for how the update is performed. Amend low score shifts This is very similar to the stitch case, but the names of the variables saved to the Notebook are slightly different: After the shifts to all rounds for all tiles have been found, the ones with score < score_thresh are amended. If for round \\(r\\) , tile \\(t\\) , the best shift found had a score < score_thresh , the shift and score are saved in the notebook as nb.register_initial.shift_outlier[t, r] and nb.register_initial.shift_score_outlier[t, r] respectively. The shift is then re-computed using a new initial exhaustive search range (saved as nb.register_initial.final_shift_search ). This range is computed using the update_shifts function to centre it on all the shifts found to round \\(r\\) for which score > score_thresh . For this re-computation, no widening is allowed either. The idea behind this, is that it will force the shift to be within the range we expect based on the successful shifts. I.e. a shift with a slightly lower score but with a shift more similar to the successful shifts is probably more reliable than a shift with a slightly higher score but with a shift significantly different from the successful ones. The new shift and score will be saved in nb.register_initial.shift[t, r] and nb.register_initial.shift_score[t, r] respectively. Error - too many bad shifts After the call reference spots step, check_shifts_register will be run. This will produce a warning for any shift found with score < score_thresh . An error will be raised if the fraction of shifts with score < score_thresh exceeds config['register_initial']['n_shifts_error_fraction'] . If this error does occur, it is probably worth looking at the Viewer and debugging plots to see if the shifts found looks good enough to use as a starting point for the iterative closest point algorithm or if it should be re-run with different configuration file parameters (e.g. different config['register_initial']['shift_channel'] corresponding to a channel with more spots, smaller config['register_initial']['shift_step'] or larger config['register_initial']['shift_max_range'] ). Debugging There are a few functions using matplotlib which may help to debug this section of the pipeline. To view how the shift matches up the point clouds, an analogous function to view_stitch_overlap is view_icp , which is explained in the next step of the pipeline. The register stage of the pipeline does not need to have been run to use this function though. view_register_shift_info The view_register_shift_info function plots the shifts to all tiles of a given round on the same plot (there are 3 plots for each round). This allows you to see if they are similar, as we expect or if there are some outliers. It also includes a plot of score vs score_thresh for each round: In this case, all the shifts seem reasonable as the top two plots show quite a small range and the bottom plot shows score > score_thresh for every shift (blue numbers are all above the green line). If a shift had score < score_thresh , it would be shown in red in each of the three plots for that direction. The numbers refer to the tile. Viewing outlier shifts The shifts saved as nb.register_initial.shift_outlier can be viewed by calling view_register_shift_info(nb, True) . The example below shows the difference between the outlier shifts and the final shifts ( nb.register_initial.shift ) saved for a data set which did not work well. shift_outlier shift Clearly from the top plot, the range of shift is much smaller than the range of shift_outlier , as we expect from the reduced range of the exhaustive search to find these. view_register_search The view_register_search function is exactly the same as view_stitch_search . Pseudocode This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the best shift is found, see the shift section. r_ref = reference round c_ref = reference round c_shift = config['register_initial']['shift_channel'] spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for r in use_rounds: for t in use_tiles: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c_shift]. If 3 or more shifts to round r with score > score_thresh: Update search range around these shifts. Amend shifts with score < score_thresh using new search range for each round. Add shifts and debugging info to register_initial NotebookPage. Add register_initial NotebookPage to Notebook.","title":"Register Initial"},{"location":"pipeline/register_initial/#register-initial","text":"The register initial step of the pipeline uses the point clouds added to the Notebook during the find_spots step to find the shift between the reference round and each imaging round for every tile. The \\(yxz\\) shift to tile \\(t\\) , round \\(r\\) is saved as nb.register_initial.shift[t, r] . This shift is then used as a starting point for finding the affine transforms in the register step of the pipeline to all channels of tile \\(t\\) , round \\(r\\) . The register_initial NotebookPage is added to the Notebook after this stage is finished.","title":"Register Initial"},{"location":"pipeline/register_initial/#shift","text":"The channel used for finding the shifts is specified by \\(c_{shift}\\) = config['register_initial']['shift_channel'] . If it is left blank, it will be set to \\(c_{ref}\\) ( nb.basic_info.ref_channel ). This channel should be one with lots of spots and if an error is hit, it may be worth re-running with a different value of this parameter. So, for tile \\(t\\) , round \\(r\\) , we find the shift between \\(r_{ref}\\) / \\(c_{ref}\\) and \\(r\\) / \\(c_{shift}\\) . The function to compute these shifts is exactly the same as the one used in the stitch section of the pipeline and the parameters in the register initial section of the config file do the same thing as the corresponding parameters in the stitch section. A few details are different though, as explained below.","title":"Shift"},{"location":"pipeline/register_initial/#initial-range","text":"The difference to the stitch case is that config['register_initial']['shift_min'] and config['register_initial']['shift_max'] are always used. We expect the shift between rounds to be quite small hence the default values , which perform an exhaustive search centered on 0 in each direction with a range of 200 in \\(y\\) and \\(x\\) and a range of 6 in \\(z\\) .","title":"Initial range"},{"location":"pipeline/register_initial/#updating-initial-range","text":"We assume that the shifts to a given round will be approximately the same for all tiles. So, after we have found at least 3 shifts to a round which have score > score_thresh , we update our initial exhaustive search range to save time for future tiles. See the example in the stitch section for how the update is performed.","title":"Updating initial range"},{"location":"pipeline/register_initial/#amend-low-score-shifts","text":"This is very similar to the stitch case, but the names of the variables saved to the Notebook are slightly different: After the shifts to all rounds for all tiles have been found, the ones with score < score_thresh are amended. If for round \\(r\\) , tile \\(t\\) , the best shift found had a score < score_thresh , the shift and score are saved in the notebook as nb.register_initial.shift_outlier[t, r] and nb.register_initial.shift_score_outlier[t, r] respectively. The shift is then re-computed using a new initial exhaustive search range (saved as nb.register_initial.final_shift_search ). This range is computed using the update_shifts function to centre it on all the shifts found to round \\(r\\) for which score > score_thresh . For this re-computation, no widening is allowed either. The idea behind this, is that it will force the shift to be within the range we expect based on the successful shifts. I.e. a shift with a slightly lower score but with a shift more similar to the successful shifts is probably more reliable than a shift with a slightly higher score but with a shift significantly different from the successful ones. The new shift and score will be saved in nb.register_initial.shift[t, r] and nb.register_initial.shift_score[t, r] respectively.","title":"Amend low score shifts"},{"location":"pipeline/register_initial/#error-too-many-bad-shifts","text":"After the call reference spots step, check_shifts_register will be run. This will produce a warning for any shift found with score < score_thresh . An error will be raised if the fraction of shifts with score < score_thresh exceeds config['register_initial']['n_shifts_error_fraction'] . If this error does occur, it is probably worth looking at the Viewer and debugging plots to see if the shifts found looks good enough to use as a starting point for the iterative closest point algorithm or if it should be re-run with different configuration file parameters (e.g. different config['register_initial']['shift_channel'] corresponding to a channel with more spots, smaller config['register_initial']['shift_step'] or larger config['register_initial']['shift_max_range'] ).","title":"Error - too many bad shifts"},{"location":"pipeline/register_initial/#debugging","text":"There are a few functions using matplotlib which may help to debug this section of the pipeline. To view how the shift matches up the point clouds, an analogous function to view_stitch_overlap is view_icp , which is explained in the next step of the pipeline. The register stage of the pipeline does not need to have been run to use this function though.","title":"Debugging"},{"location":"pipeline/register_initial/#view_register_shift_info","text":"The view_register_shift_info function plots the shifts to all tiles of a given round on the same plot (there are 3 plots for each round). This allows you to see if they are similar, as we expect or if there are some outliers. It also includes a plot of score vs score_thresh for each round: In this case, all the shifts seem reasonable as the top two plots show quite a small range and the bottom plot shows score > score_thresh for every shift (blue numbers are all above the green line). If a shift had score < score_thresh , it would be shown in red in each of the three plots for that direction. The numbers refer to the tile. Viewing outlier shifts The shifts saved as nb.register_initial.shift_outlier can be viewed by calling view_register_shift_info(nb, True) . The example below shows the difference between the outlier shifts and the final shifts ( nb.register_initial.shift ) saved for a data set which did not work well. shift_outlier shift Clearly from the top plot, the range of shift is much smaller than the range of shift_outlier , as we expect from the reduced range of the exhaustive search to find these.","title":"view_register_shift_info"},{"location":"pipeline/register_initial/#view_register_search","text":"The view_register_search function is exactly the same as view_stitch_search .","title":"view_register_search"},{"location":"pipeline/register_initial/#pseudocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the best shift is found, see the shift section. r_ref = reference round c_ref = reference round c_shift = config['register_initial']['shift_channel'] spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for r in use_rounds: for t in use_tiles: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t, r, c_shift]. If 3 or more shifts to round r with score > score_thresh: Update search range around these shifts. Amend shifts with score < score_thresh using new search range for each round. Add shifts and debugging info to register_initial NotebookPage. Add register_initial NotebookPage to Notebook.","title":"Pseudocode"},{"location":"pipeline/stitch/","text":"Stitch The stitch step of the pipeline uses the reference point clouds (all tiles of ref_round / ref_channel ) added to the Notebook during the find_spots step to find the overlap between neighbouring tiles in the form of shifts. It then uses these shifts to get the origin of each tile in a global coordinate system . The tile origins are saved to the Notebook as nb.stitch.tile_origins and this is the only variable computed in this section which is used later in the pipeline. The stitch NotebookPage is added to the Notebook after this stage is finished. Shift We need to find the overlap between each pair of neighbouring tiles. To do this, for each tile, we ask whether there is a tile to the north of it, and if there is we compute the shift between the two. We then ask if there is a tile to the east of it, and if there is we compute the shift between the two. Example For a \\(2\\times3\\) ( \\(n_y \\times n_x\\) ) grid of tiles, the indices are: | 2 | 1 | 0 | | 5 | 4 | 3 | We consider each tile in turn: Tile 0 has no tiles to the north or east so we go to tile 1. Tile 1 has a tile to the east (0) so we find the shift between tile 1 and tile 0. Tile 2 has a tile to the east (1) so we find the shift between tile 2 and tile 1. Tile 3 has a tile to the north (0) so we find the shift between tile 3 and tile 0. Tile 4 has a tile to the north (1) so we find the shift between tile 4 and tile 1. Tile 4 also has a tile to the east (3) so we find the shift between tile 4 and tile 3. Tile 5 has a tile to the north (2) so we find the shift between tile 5 and tile 2. Tile 5 also has a tile to the east (4) so we find the shift between tile 5 and tile 4. We will always be finding the offset of a tile relative to a tile with a smaller index . The tile indices for neighbours for which we find the overlap in the north/south direction are saved as nb.stitch.south_pairs . The shift between tile nb.stitch.south_pairs[i, 0] and tile nb.stitch.south_pairs[i, 1] is saved as nb.stitch.south_shifts[i] . Initial range We compute the shift through an exhaustive search in a given range. The initial range used for a tile to the north can be specified through config['stitch']['shift_south_min'] and config['stitch']['shift_south_max'] . The range used for a tile to the east can be specified through config['stitch']['shift_west_min'] and config['stitch']['shift_west_max'] . Confusion between north/south and east/west For finding the shift to a tile in the north, the parameters used in the config file and saved to the NotebookPage have the south prefix. This is because if tile B is to the north of tile A, the shift applied to tile A to get the correct overlap is to the south (i.e. negative in the y direction). Equally, for finding the shift to a tile in the east, the parameters used in the config file and saved to the NotebookPage have the west prefix. This is because if tile B is to the east of tile A, the shift applied to tile A to get the correct overlap is to the west (i.e. negative in the x direction). The range in the \\(i\\) direction will then be between shift_min[i] and shift_max[i] with a spacing of config['stitch']['shift_step'][i] . If these are left blank, the range will be computed automatically using config['stitch']['expected_overlap'] and config['stitch']['auto_n_shifts'] . Example automatic range calculation For an experiment with nb.basic_info.tile_sz = 2048 config['stitch']['expected_overlap'] = 0.1 config['stitch']['auto_n_shifts'] = 20, 20, 1 config['stitch']['shift_step'] = 5, 5, 3 the range used for a tile to the north is computed as follows: Code Output import numpy as np expected_overlap = config [ 'stitch' ][ 'expected_overlap' ] auto_n_shifts = config [ 'stitch' ][ 'auto_n_shifts' ] shift_step = config [ 'stitch' ][ 'shift_step' ] expected_shift = np . array ([ - ( 1 - expected_overlap ]) * [ tile_sz , 0 , 0 ]) . astype ( int ) print ( f \"Expected shift = { expected_shift } \" ) range_extent = auto_n_shifts * shift_step print ( f \"YXZ range extent: { range_extent } \" ) range_min = expected_shift - range_extent range_max = expected_shift + range_extent print ( f \"YXZ range min: { range_min } \" ) print ( f \"YXZ range max: { range_max } \" ) shifts_y = np . arange ( range_min [ 0 ], range_max [ 0 ] + shift_step [ 0 ] / 2 , shift_step [ 0 ]) print ( f \"Y exhaustive search shifts: \\n { shifts_y } \" ) shifts_x = np . arange ( range_min [ 1 ], range_max [ 1 ] + shift_step [ 1 ] / 2 , shift_step [ 1 ]) print ( f \"X exhaustive search shifts: \\n { shifts_x } \" ) shifts_z = np . arange ( range_min [ 2 ], range_max [ 2 ] + shift_step [ 2 ] / 2 , shift_step [ 2 ]) print ( f \"Z exhaustive search shifts: \\n { shifts_z } \" ) Expected shift = [-1843 0 0] YXZ range extent: [100 100 3] YXZ range min: [-1943 -100 -3] YXZ range max: [-1743 100 3] Y exhaustive search shifts: [-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888 -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828 -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768 -1763 -1758 -1753 -1748 -1743] X exhaustive search shifts: [-100 -95 -90 -85 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100] Z exhaustive search shifts: [-3 0 3] For a tile to the east, the calculation is exactly the same except expected shift = [0 -1843 0] . The range used for north/south overlap is saved as nb.stitch.south_start_shift_search . Obtaining best shift Here is some pseudocode for how we obtain the best shift between tile 5 and tile 4 from an exhaustive search. The comments (#) give the shape of the indicated array. function find_neighbour_distances(yxz_0, yxz_1): # yxz_0: [n_spots_0 x 3] # yxz_1: [n_spots_1 x 3] For i in range(n_spots_0): find nearest spot in yxz_1 to yxz_0[i] to be the one at index j. distances[i] = distance between yxz_0[i] and yxz_1[j]. return distances # [n_spots_0] function get_score(distances, dist_thresh): # distances: [n_spots] This is a function that basically counts the number of values in distances which are below dist_thresh. I.e. the more close neighbours, the better the shift and thus the score should be larger. The function used in the pipeline returns the float given by: score = sum(exp(-distances ** 2 / (2 * dist_thresh ** 2))) If all values in distances where 0 (perfect), score = n_spots. If all values in distances where infinity or much larger than dist_thresh (bad), score = 0. # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search: [n_shifts x 3] for shift in exhaustive_search: tile_5_yxz_shifted = tile_5_yxz + shift # [n_spots_t5 x 3] distances = find_neighbour_distances(tile_5_yxz_shifted, tile_4_yxz) # [n_spots_t5] score = get_score(distances, dist_thresh) # float best_shift is the shift with the best score score In the score function , the dist_thresh parameter thus specifies the distance below which neighbours are a good match. It is specified through config['stitch']['neighb_dist_thresh'] . The score computed with this function is approximately the number of neighbouring points between the two point clouds with a distance between them less than config['stitch']['neighb_dist_thresh'] . 3D For speed, rather than considering an exhaustive search in three dimensions, we first ignore any shift in \\(z\\) and just find the best \\(yx\\) shift . To do this, we split each 3D point cloud into a number of 2D point clouds. The number is determined by config['stitch']['nz_collapse'] to be: ceil ( nb . basic_info . nz / config [ 'stitch' ][ 'nz_collapse' ]) We then consider the corresponding point clouds independently. Pseudocode - \\(yx\\) shift Lets consider finding the best yx shift between tile 5 and tile 4 with: nb.basic_info.nz = 50 config['stitch']['nz_collapse'] = 30 config['stitch']['neighb_dist_thresh'] = 2 The pseudocode is: # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search_yx: [n_shifts_yx x 2] n_2d_point_clouds = ceil(50 / 30) = 2 so we need 2 2D point clouds split tile_5_yxz into tile_5A_yx and tile_5B_yx: - tile_5A_yx are the yx coordinates of every spot in tile_5_yxz with z coordinate between 0 and 24 inclusive. # [n_spots_t5A x 2] - tile_5B_yx are the yx coordinates of every spot in tile_5_yxz with z coordinate between 25 and 49 inclusive. # [n_spots_t5B x 2] split tile_4_yxz into tile_4A_yx and tile_4B_yx: - tile_4A_yx are the yx coordinates of every spot in tile_4_yxz with z coordinate between 0 and 24 inclusive. # [n_spots_t4A x 2] - tile_4B_yx are the yx coordinates of every spot in tile_4_yxz with z coordinate between 25 and 49 inclusive. # [n_spots_t4B x 2] for shift_yx in exhaustive_search_yx: tile_5A_yx_shifted = tile_5A_yx + shift_yx # [n_spots_t5A x 2] distancesA = find_neighbour_distances(tile_5A_yx_shifted, tile_4A_yx) # [n_spots_t5A] scoreA = get_score(distancesA, 2) # float tile_5B_yx_shifted = tile_5B_yx + shift_yx # [n_spots_t5B x 2] distances = find_neighbour_distances(tile_5B_yx_shifted, tile_4B_yx) # [n_spots_t5B] scoreB = get_score(distancesB, 2) # float score = scoreA + scoreB best_shift_yx is the shift_yx with the best score Once the best \\(yx\\) shift is found (and after any necessary widening of the range), the shift in \\(z\\) is found by using the full 3D point clouds again but doing just an exhaustive search in \\(z\\) . Before this is done, it is important that the \\(z\\) coordinate of the point clouds is in the same unit as the \\(yx\\) coordinate so distances are computed correctly. The conversion from \\(z\\) pixel units to \\(yx\\) pixel units is achieved by multiplying the \\(z\\) coordinate by nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy . The \\(z\\) shifts in the exhaustive search must also be put into \\(yx\\) pixel units. Pseudocode - \\(z\\) shift If the previous example found the best \\(yx\\) shift to be best_shift_yx , the pseudocode below is what follows this to find the best \\(yxz\\) shift. # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search_z: [n_shifts_z] # best_shift_yx: [2] shift_yxz = [0, 0, 0] The yx shift is constant in this search, always set to the best yx shift we found in the 2D search. shift_yxz[0] = best_shift_yx[0] shift_yxz[1] = best_shift_yx[1] for shift_z in exhaustive_search_z: shift_yxz[2] = shift_z tile_5_yxz_shifted = tile_5_yxz + shift_yxz # [n_spots_t5 x 3] distances = find_neighbour_distances(tile_5_yxz_shifted, tile_4_yxz) # [n_spots_t5] score = get_score(distances, 2) # float best_shift is the shift with the best score Score Threshold Once we have found the best shift in the exhaustive search and its score, we need to determine if the score is large enough for us to accept the shift or if we should widen the range. We accept the shift if the score is above a score_thresh . This can either be specified through config['stitch']['shift_score_thresh'] or if this is left empty, it is computed for each shift. This computation is done after we have found the best \\(yx\\) shift to be best_shift_yx , as explained by the following pseudocode (also shown with view_stitch_search ): # exhaustive_search_yx: [n_shifts_yx x 2] # best_shift_yx: [2] shifts_yx_use = all shifts between min_dist and max_dist from best_shift_yx in exhaustive_search_yx # [n_shifts_use x 2] shift_yx_thresh = shift in shifts_yx_use with the max score. score_thresh = the score corresponding to shift_yx_thresh multiplied by thresh_multiplier. Where various parameters are specified through the configuration file: min_dist : config['stitch']['shift_score_thresh_min_dist'] max_dist : config['stitch']['shift_score_thresh_max_dist'] thresh_multiplier : config['stitch']['shift_score_thresh_multiplier'] view_stitch_search A good debugging tool to visualise how the best shift was computed is view_stitch_search . In 2D , it shows the score for all \\(yx\\) shifts in the exhaustive search: White in the colorbar refers to the value of score_thresh . The green x indicates the shift that was found after the initial exhaustive search. The black x indicates the final shift found after the refined search . This plot is also useful for understanding the score_thresh computation . The green + indicates the shift_yx_thresh , the shift with the largest score between the two green circles. The inner circle has a radius of config['stitch']['shift_score_thresh_min_dist'] and is centered on the green x . The outer circle has a radius of config['stitch']['shift_score_thresh_max_dist'] and is centered on the green x . score_thresh is then set to config['stitch']['shift_score_thresh_multiplier'] multiplied by the score at the green + . In this case, this multiplier is 2 and thus the score at the green + appears blue ( score at the green + is approximately 150 and so score_thresh and the white in the image is about 300). We use this method of determining score_thresh because the most striking feature of the plot is the sharp gradient near the global maxima in score . Requiring the score for an acceptable shift to be much larger than the max nearby score is just saying we require a large gradient. score_thresh in 3D For the 3D pipeline, the score_thresh is computed in exactly the same way to determine if the initial \\(yx\\) shift found is acceptable or the \\(yx\\) range needs widening : Once an acceptable \\(yx\\) shift has been found, we set score_thresh for the 3D search to the max score at the \\(yx\\) shift used to find score_thresh in 2D across all z planes searched, multiplied by config['stitch']['shift_score_thresh_multiplier'] . I.e. the green + in the Z=0 plot is at the same \\(yx\\) coordinate as the green + in the 2D plot. The maximum score at this \\(yx\\) coordinate across z-shifts between -6 and 6 occurs at Z=0 . Thus, we set the score_thresh to be equal to the score at this \\(yxz\\) shift multiplied by config['stitch']['shift_score_thresh_multiplier'] . The score_thresh is smaller when looking for the \\(yxz\\) shift because we expect in 3D , if neighbouring points are on slightly different z-planes but very close in \\(yx\\) they would have a small contribution to the score . This is because the \\(z\\) pixels are larger than the \\(yx\\) pixels. In 2D , a pair such as this would give a large contribution to the score because only the \\(yx\\) distance would matter. See the example in the refined search section to see how a shift of 1 in \\(z\\) can almost double the score. Widening range If the best shift found has score < score_thresh , then exhaustive search will continue until either a shift with score > score_thresh is found or the search range exceeds a max_range : while best_score < score_thresh: extend exhaustive search if range of exhaustive search > max_range in all dimensions: Return current best_shift and best_score. else: Find best_shift and corresponding best_score in new larger exhaustive_search. The max_range in the \\(y\\) , \\(x\\) , \\(z\\) direction is set to be config['stitch']['shift_max_range'] . The exhaustive search is extended using config['stitch']['shift_widen'] . The possible shifts in dimension \\(i\\) are extended by config['stitch']['shift_widen'][i] values either side of the current min/max values of the shifts while maintaining the same spacing. Example exhaustive search extension Lets consider a single dimension with initial an initial exhaustive search containing the following shifts: array ([ - 10 , - 5 , 0 , 5 , 10 ]) With shift_widen = 10 in this dimension, the updated exhaustive search would contain the following shifts: array ([ - 60 , - 55 , - 50 , - 45 , - 40 , - 35 , - 30 , - 25 , - 20 , - 15 , - 10 , - 5 , 0 , 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 ]) This new search has a range of 120 so if max_range in this dimension was 100, the updated exhaustive search would not take place. An example in 2D where the widening worked successfully is shown below using view_stitch_search : Pre-widen Post-widen Here we see that before widening the search range, the maximum score is barely distinguishable from the rest and thus falls below score_thresh . After widening though, we uncover a shift with a score far exceeding score_thresh . 3D In 3D , we do the \\(yx\\) search first and then widen according to config['stitch']['shift_widen'][:2] and config['stitch']['shift_max_range'][:2] to find the best \\(yx\\) shift. Then we do the \\(z\\) search keeping the \\(yx\\) shift equal to the best \\(yx\\) shift and widen according to config['stitch']['shift_widen'][2] and config['stitch']['shift_max_range'][2] . This gives us the best \\(yxz\\) shift. The example below shows a case where widening was required in \\(z\\) but not in \\(yx\\) : Pre-widen Post-widen Refined search After we have found the best \\(yxz\\) shift from the exhaustive search, we find the final shift by looking in the neighbourhood of this shift with a smaller spacing . Initially, we halve the spacing, then we reduce the spacing to 1. Example Lets consider a 3D example where an exhaustive search with \\(yxz\\) spacing [5, 5, 3] found the best \\(yxz\\) shift to be [-1848, 20, 0] with a score of 247.7. The refined search range in this neighbourhood with half the step size is set to: Y shifts : [ - 1858 - 1855 - 1852 - 1849 - 1846 - 1843 - 1840 - 1837 ] X shifts : [ 10 13 16 19 22 25 28 31 ] Z shifts : [ - 6 - 4 - 2 0 2 4 6 ] This search finds the best \\(yxz\\) shift to be [-1846, 19, 0] with a score of 298.3. The final search with a spacing of 1 is set to: Y shifts : [ - 1849 - 1848 - 1847 - 1846 - 1845 - 1844 - 1843 ] X shifts : [ 16 17 18 19 20 21 22 ] Z shifts : [ - 2 - 1 0 1 2 ] This search finds the best \\(yxz\\) shift to be [-1846, 20, 1] with a score of 423.8. This is why the view_stitch_search plot shows more detail near the black x and why the black x is in a different location from the green x even when no widening is required. Updating initial range We assume that all tiles overlapping in the same direction should have approximately the same shift between them. So, after we have found at least 3 shifts in a given direction which have score > score_thresh , we update our initial exhaustive search range to save time for future tiles. Example The code below shows how update_shifts works to refine the initial search range, given that the following \\(yxz\\) shifts have been found for north/south overlapping tiles (all with score > score_thresh ): [-1846, 20, 1] [-1853, 22, 0] [-1854, 20, 0] Code Output import numpy as np from coppafish.stitch.shift import update_shifts y_shifts_found = [ - 1846 , - 1853 , - 1854 ] x_shifts_found = [ 20 , 22 , 0 ] z_shifts_found = [ 1 , 0 , 0 ] step = [ 5 , 5 , 3 ] y_search = np . arange ( - 1943 , - 1743 + step [ 0 ], step [ 0 ]) x_search = np . arange ( - 100 , 100 + step [ 1 ], step [ 1 ]) z_search = np . arange ( - 3 , 3 + step [ 2 ], step [ 2 ]) # y print ( f \"Initial y search: \\n { y_search } \" ) y_search_new = update_shifts ( y_search , y_shifts_found ) print ( f \"Updated y search: \\n { y_search_new } \" ) # x print ( f \"Initial x search: \\n { x_search } \" ) x_search_new = update_shifts ( x_search , x_shifts_found ) print ( f \"Updated x search: \\n { x_search_new } \" ) # z print ( f \"Initial z search: \\n { z_search } \" ) z_search_new = update_shifts ( z_search , z_shifts_found ) print ( f \"Updated z search: \\n { z_search_new } \" ) # Number of shifts to search print ( f \"Initial number of shifts in search: \" f \" { y_search . size * x_search . size * z_search . size } \" ) print ( f \"Updated number of shifts in search: \" f \" { y_search_new . size * x_search_new . size * z_search_new . size } \" ) Initial y search: [-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888 -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828 -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768 -1763 -1758 -1753 -1748 -1743] Updated y search: [-1861 -1856 -1851 -1846 -1841] Initial x search: [-100 -95 -90 -85 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100] Updated x search: [-1 4 9 14 19 24 29] Initial z search: [-3 0 3] Updated z search: [-3 0 3] Initial number of shifts in search: 5043 Updated number of shifts in search: 105 Amend low score shifts After all the shifts between neighbouring tiles have been found, the ones with score < score_thresh are amended. If for a particular pair of overlapping tiles in the north/south direction, the best shift found had a score < score_thresh , the shift and score are saved in the notebook in nb.stitch.south_outlier_shifts and nb.stitch.south_outlier_score respectively. The shift is then re-computed using a new initial exhaustive search range (saved as nb.stitch.south_final_shift_search ). This range is computed using the update_shifts function to centre it on all the shifts found in the south direction for which score > score_thresh . For this re-computation, no widening is allowed either. The idea behind this is that it will force the shift to be within the range we expect based on the successful shifts. I.e. a shift with a slightly lower score but with a shift more similar to the successful shifts is probably more reliable than a shift with a slightly higher score but with a shift significantly different from the successful ones. The new shift and score will be saved in nb.stitch.south_shifts and nb.stitch.south_score respectively. Global coordinates After finding the overlap for the set of neighbouring tile pairs ( \\(\\mathcal{R}\\) ), we are left with a shift vector \\(\\pmb{\\Delta}_{T_1, T_2}\\) for every pair of neighbouring tiles \\(T_1\\) and \\(T_2\\) , that specifies the \\(yxz\\) offsets of tile \\(T_2\\) relative to tile \\(T_1\\) . We define a single global coordinate system by finding the coordinate origin \\(\\pmb{\\mathrm{X}}_T\\) (bottom left corner) for each tile \\(T\\) . Note however that this problem is overdetermined as there are more neighbor pairs than there are tiles. We therefore compute the offsets by minimizing the loss function: \\[ L = \\sum_{(T_1, T_2) \\in \\mathcal{R}} \\pmb{|} \\pmb{\\mathrm{X}}_{T_1} - \\pmb{\\mathrm{X}}_{T_2} - \\pmb{\\Delta}_{T_1, T_2} \\pmb{|}^2 \\] Differentiating this loss function with respect to \\(\\pmb{\\mathrm{X}}_T\\) yields a set of simultaneous linear equations, whose solution yields the origins of each tile on the reference round/channel. This procedure is done with the get_tile_origin function, with the tile origins saved to the Notebook as nb.stitch.tile_origin . Error - too many bad shifts After the call reference spots step, check_shifts_stitch will be run. This will produce a warning for any shift found with score < score_thresh . An error will be raised if the fraction of shifts with score < score_thresh exceeds config['stitch']['n_shifts_error_fraction'] . If this error does occur, it is probably worth looking at the Viewer and the debugging plots to see if the stitching looks good enough to continue with the rest of the pipeline or if it should be re-run with different configuration file parameters (e.g. smaller config['stitch']['shift_step'] or larger config['stitch']['shift_max_range'] ). Saving stitched images After the tile_origin has been computed and the stitch NotebookPage has been added to the Notebook , a stitched image of the ref_round / ref_channel will be saved to the output_dir as a npz file with the file name nb.file_names.big_anchor_image . To save memory, the stitched reference image will be saved as int16 after rescaling to fill the range. We do this because the image is useful for plotting, but we do not care much about the actual pixel values. DAPI If dapi_channel is specified, a stitched image of the anchor_round / dapi_channel will be saved to the output_dir as a npz file with the file name nb.file_names.big_dapi_image . If DAPI tophat filtering was specified , the filtered images save to tile_dir will be loaded in and stitched together. Otherwise, the raw data will be loading in from the input_dir and stitched together with no filtering. I.e. from_raw == True in save_stitched . The stitched DAPI image will be saved as uint16 with no rescaling. Also, to save memory, all pixels with absolute value less than config['stitch']['save_image_zero_thresh'] will have their pixel value set to \\(0\\) before saving. Debugging There are a few functions using matplotlib which may help to debug this section of the pipeline. view_stitch_shift_info The view_stitch_shift_info function plots the shifts found for all neighbouring tiles in a given direction on the same plot (there are 3 plots for each direction). This allows you to see if they are similar, as we expect or if there are some outliers. It also includes a plot of score vs score_thresh for each pair of neighbouring tiles: In this case, all the shifts seem reasonable as the top two plots show quite a small range, and the bottom plot shows score > score_thresh for every shift (blue numbers are all above the green line). If a shift had score < score_thresh , it would be shown in red in each of the three plots for that direction. The numbers refer to the tile with a neighbouring tile to either the north or east of it. This example is for a \\(4\\) ( \\(n_y\\) ) \\(\\times\\) \\(3\\) ( \\(n_x\\) ) grid of tiles, so the number \\(1\\) in the West column of plots refers to the shift found between tile \\(1\\) and tile \\(0\\) . The number \\(3\\) in the South column of plots refers to the shift found between tile \\(3\\) and tile \\(0\\) . This function can also be used to view nb.stitch.outlier_shifts by running view_stitch_shift_info(nb, True) . view_stitch_overlap For an experiment with tile \\(0\\) to the north of tile \\(1\\) , view_stitch_overlap(nb, 1, 'north') will always show the global coordinates of the point cloud for tile \\(0\\) in red ( global_yxz = local_yxz + nb.stitch.tile_origin[0] ). There are then buttons to select which point cloud for tile \\(1\\) is plotted in blue: No overlap: This is assuming there is \\(0\\%\\) overlap between the two tiles. \\(x\\%\\) overlap: \\(x\\) here will be config['stitch']['expected_overlap'] . This is our starting guess, i.e. the expected overlap in \\(y\\) and a shift of 0 in \\(x\\) and \\(z\\) . Shift: This is the best shift found, saved in nb.stitch.south_shifts . Final: This is the coordinates of tile \\(1\\) spots in the global coordinate system ( local_yxz + nb.stitch.tile_origin[1] ). An example is shown below: No overlap 10% overlap Shift Final The z-plane is changed by scrolling with the mouse. You can change the value of z-thick in the bottom right. Spots detected on the current z-plane and this many z-planes either side of it will be shown. The white lines (only really visible in the Final plot) indicate neighbouring points with a distance between them of less than or equal to config['stitch']['neighb_dist_thresh'] . The number of matches listed on the right is then the number of these white lines (across all z-planes), this will be similar to the score . view_stitch Another useful function is view_stitch . This plots all the spots found in the ref_round / ref_channel in the global coordinate system specified by nb.stitch.tile_origin . The example below is for a \\(4\\times3\\) grid of tiles in 2D . Full Zoom The blue spots are duplicate spots (detected on a tile which is not the tile whose centre they are closest to). For each duplicate spot, we expect there is a non-duplicate spot in red, detected on a different tile but with the same global coordinate. We can see this in the Zoom plot, showing the intersection between tile 1 and tile 2 (indicated by a green box in the Full image). These duplicate spots will be removed in the get reference spots step of the pipeline, so we don't double count the same spot. The white lines and number of matches are the same as for view_stitch_overlap . Also in 3D , you can scroll between z-planes with the mouse and specify z-thick in the same way. Pseudocode This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the best shift is found, see the shift section. r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for t in use_tiles: if tile to north of t: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t_north, r_ref, c_ref]. If 3 or more north/south shifts with score > score_thresh: Update search range around these shifts. if tile to east of t: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t_east, r_ref, c_ref]. If 3 or more east/west shifts with score > score_thresh: Update search range around these shifts. Amend shifts with score < score_thresh using new search range for each direction. Find tile_origin specifying global coordinate system. Add tile_origin and debugging info to stitch NotebookPage. Add stitch NotebookPage to Notebook. Use tile_origin to save stitched ref_channel (and DAPI) image to output directory.","title":"Stitch"},{"location":"pipeline/stitch/#stitch","text":"The stitch step of the pipeline uses the reference point clouds (all tiles of ref_round / ref_channel ) added to the Notebook during the find_spots step to find the overlap between neighbouring tiles in the form of shifts. It then uses these shifts to get the origin of each tile in a global coordinate system . The tile origins are saved to the Notebook as nb.stitch.tile_origins and this is the only variable computed in this section which is used later in the pipeline. The stitch NotebookPage is added to the Notebook after this stage is finished.","title":"Stitch"},{"location":"pipeline/stitch/#shift","text":"We need to find the overlap between each pair of neighbouring tiles. To do this, for each tile, we ask whether there is a tile to the north of it, and if there is we compute the shift between the two. We then ask if there is a tile to the east of it, and if there is we compute the shift between the two. Example For a \\(2\\times3\\) ( \\(n_y \\times n_x\\) ) grid of tiles, the indices are: | 2 | 1 | 0 | | 5 | 4 | 3 | We consider each tile in turn: Tile 0 has no tiles to the north or east so we go to tile 1. Tile 1 has a tile to the east (0) so we find the shift between tile 1 and tile 0. Tile 2 has a tile to the east (1) so we find the shift between tile 2 and tile 1. Tile 3 has a tile to the north (0) so we find the shift between tile 3 and tile 0. Tile 4 has a tile to the north (1) so we find the shift between tile 4 and tile 1. Tile 4 also has a tile to the east (3) so we find the shift between tile 4 and tile 3. Tile 5 has a tile to the north (2) so we find the shift between tile 5 and tile 2. Tile 5 also has a tile to the east (4) so we find the shift between tile 5 and tile 4. We will always be finding the offset of a tile relative to a tile with a smaller index . The tile indices for neighbours for which we find the overlap in the north/south direction are saved as nb.stitch.south_pairs . The shift between tile nb.stitch.south_pairs[i, 0] and tile nb.stitch.south_pairs[i, 1] is saved as nb.stitch.south_shifts[i] .","title":"Shift"},{"location":"pipeline/stitch/#initial-range","text":"We compute the shift through an exhaustive search in a given range. The initial range used for a tile to the north can be specified through config['stitch']['shift_south_min'] and config['stitch']['shift_south_max'] . The range used for a tile to the east can be specified through config['stitch']['shift_west_min'] and config['stitch']['shift_west_max'] . Confusion between north/south and east/west For finding the shift to a tile in the north, the parameters used in the config file and saved to the NotebookPage have the south prefix. This is because if tile B is to the north of tile A, the shift applied to tile A to get the correct overlap is to the south (i.e. negative in the y direction). Equally, for finding the shift to a tile in the east, the parameters used in the config file and saved to the NotebookPage have the west prefix. This is because if tile B is to the east of tile A, the shift applied to tile A to get the correct overlap is to the west (i.e. negative in the x direction). The range in the \\(i\\) direction will then be between shift_min[i] and shift_max[i] with a spacing of config['stitch']['shift_step'][i] . If these are left blank, the range will be computed automatically using config['stitch']['expected_overlap'] and config['stitch']['auto_n_shifts'] . Example automatic range calculation For an experiment with nb.basic_info.tile_sz = 2048 config['stitch']['expected_overlap'] = 0.1 config['stitch']['auto_n_shifts'] = 20, 20, 1 config['stitch']['shift_step'] = 5, 5, 3 the range used for a tile to the north is computed as follows: Code Output import numpy as np expected_overlap = config [ 'stitch' ][ 'expected_overlap' ] auto_n_shifts = config [ 'stitch' ][ 'auto_n_shifts' ] shift_step = config [ 'stitch' ][ 'shift_step' ] expected_shift = np . array ([ - ( 1 - expected_overlap ]) * [ tile_sz , 0 , 0 ]) . astype ( int ) print ( f \"Expected shift = { expected_shift } \" ) range_extent = auto_n_shifts * shift_step print ( f \"YXZ range extent: { range_extent } \" ) range_min = expected_shift - range_extent range_max = expected_shift + range_extent print ( f \"YXZ range min: { range_min } \" ) print ( f \"YXZ range max: { range_max } \" ) shifts_y = np . arange ( range_min [ 0 ], range_max [ 0 ] + shift_step [ 0 ] / 2 , shift_step [ 0 ]) print ( f \"Y exhaustive search shifts: \\n { shifts_y } \" ) shifts_x = np . arange ( range_min [ 1 ], range_max [ 1 ] + shift_step [ 1 ] / 2 , shift_step [ 1 ]) print ( f \"X exhaustive search shifts: \\n { shifts_x } \" ) shifts_z = np . arange ( range_min [ 2 ], range_max [ 2 ] + shift_step [ 2 ] / 2 , shift_step [ 2 ]) print ( f \"Z exhaustive search shifts: \\n { shifts_z } \" ) Expected shift = [-1843 0 0] YXZ range extent: [100 100 3] YXZ range min: [-1943 -100 -3] YXZ range max: [-1743 100 3] Y exhaustive search shifts: [-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888 -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828 -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768 -1763 -1758 -1753 -1748 -1743] X exhaustive search shifts: [-100 -95 -90 -85 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100] Z exhaustive search shifts: [-3 0 3] For a tile to the east, the calculation is exactly the same except expected shift = [0 -1843 0] . The range used for north/south overlap is saved as nb.stitch.south_start_shift_search .","title":"Initial range"},{"location":"pipeline/stitch/#obtaining-best-shift","text":"Here is some pseudocode for how we obtain the best shift between tile 5 and tile 4 from an exhaustive search. The comments (#) give the shape of the indicated array. function find_neighbour_distances(yxz_0, yxz_1): # yxz_0: [n_spots_0 x 3] # yxz_1: [n_spots_1 x 3] For i in range(n_spots_0): find nearest spot in yxz_1 to yxz_0[i] to be the one at index j. distances[i] = distance between yxz_0[i] and yxz_1[j]. return distances # [n_spots_0] function get_score(distances, dist_thresh): # distances: [n_spots] This is a function that basically counts the number of values in distances which are below dist_thresh. I.e. the more close neighbours, the better the shift and thus the score should be larger. The function used in the pipeline returns the float given by: score = sum(exp(-distances ** 2 / (2 * dist_thresh ** 2))) If all values in distances where 0 (perfect), score = n_spots. If all values in distances where infinity or much larger than dist_thresh (bad), score = 0. # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search: [n_shifts x 3] for shift in exhaustive_search: tile_5_yxz_shifted = tile_5_yxz + shift # [n_spots_t5 x 3] distances = find_neighbour_distances(tile_5_yxz_shifted, tile_4_yxz) # [n_spots_t5] score = get_score(distances, dist_thresh) # float best_shift is the shift with the best score","title":"Obtaining best shift"},{"location":"pipeline/stitch/#score","text":"In the score function , the dist_thresh parameter thus specifies the distance below which neighbours are a good match. It is specified through config['stitch']['neighb_dist_thresh'] . The score computed with this function is approximately the number of neighbouring points between the two point clouds with a distance between them less than config['stitch']['neighb_dist_thresh'] .","title":"score"},{"location":"pipeline/stitch/#3d","text":"For speed, rather than considering an exhaustive search in three dimensions, we first ignore any shift in \\(z\\) and just find the best \\(yx\\) shift . To do this, we split each 3D point cloud into a number of 2D point clouds. The number is determined by config['stitch']['nz_collapse'] to be: ceil ( nb . basic_info . nz / config [ 'stitch' ][ 'nz_collapse' ]) We then consider the corresponding point clouds independently. Pseudocode - \\(yx\\) shift Lets consider finding the best yx shift between tile 5 and tile 4 with: nb.basic_info.nz = 50 config['stitch']['nz_collapse'] = 30 config['stitch']['neighb_dist_thresh'] = 2 The pseudocode is: # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search_yx: [n_shifts_yx x 2] n_2d_point_clouds = ceil(50 / 30) = 2 so we need 2 2D point clouds split tile_5_yxz into tile_5A_yx and tile_5B_yx: - tile_5A_yx are the yx coordinates of every spot in tile_5_yxz with z coordinate between 0 and 24 inclusive. # [n_spots_t5A x 2] - tile_5B_yx are the yx coordinates of every spot in tile_5_yxz with z coordinate between 25 and 49 inclusive. # [n_spots_t5B x 2] split tile_4_yxz into tile_4A_yx and tile_4B_yx: - tile_4A_yx are the yx coordinates of every spot in tile_4_yxz with z coordinate between 0 and 24 inclusive. # [n_spots_t4A x 2] - tile_4B_yx are the yx coordinates of every spot in tile_4_yxz with z coordinate between 25 and 49 inclusive. # [n_spots_t4B x 2] for shift_yx in exhaustive_search_yx: tile_5A_yx_shifted = tile_5A_yx + shift_yx # [n_spots_t5A x 2] distancesA = find_neighbour_distances(tile_5A_yx_shifted, tile_4A_yx) # [n_spots_t5A] scoreA = get_score(distancesA, 2) # float tile_5B_yx_shifted = tile_5B_yx + shift_yx # [n_spots_t5B x 2] distances = find_neighbour_distances(tile_5B_yx_shifted, tile_4B_yx) # [n_spots_t5B] scoreB = get_score(distancesB, 2) # float score = scoreA + scoreB best_shift_yx is the shift_yx with the best score Once the best \\(yx\\) shift is found (and after any necessary widening of the range), the shift in \\(z\\) is found by using the full 3D point clouds again but doing just an exhaustive search in \\(z\\) . Before this is done, it is important that the \\(z\\) coordinate of the point clouds is in the same unit as the \\(yx\\) coordinate so distances are computed correctly. The conversion from \\(z\\) pixel units to \\(yx\\) pixel units is achieved by multiplying the \\(z\\) coordinate by nbp_basic.pixel_size_z / nbp_basic.pixel_size_xy . The \\(z\\) shifts in the exhaustive search must also be put into \\(yx\\) pixel units. Pseudocode - \\(z\\) shift If the previous example found the best \\(yx\\) shift to be best_shift_yx , the pseudocode below is what follows this to find the best \\(yxz\\) shift. # tile_5_yxz: [n_spots_t5 x 3] # tile_4_yxz: [n_spots_t4 x 3] # exhaustive_search_z: [n_shifts_z] # best_shift_yx: [2] shift_yxz = [0, 0, 0] The yx shift is constant in this search, always set to the best yx shift we found in the 2D search. shift_yxz[0] = best_shift_yx[0] shift_yxz[1] = best_shift_yx[1] for shift_z in exhaustive_search_z: shift_yxz[2] = shift_z tile_5_yxz_shifted = tile_5_yxz + shift_yxz # [n_spots_t5 x 3] distances = find_neighbour_distances(tile_5_yxz_shifted, tile_4_yxz) # [n_spots_t5] score = get_score(distances, 2) # float best_shift is the shift with the best score","title":"3D"},{"location":"pipeline/stitch/#score-threshold","text":"Once we have found the best shift in the exhaustive search and its score, we need to determine if the score is large enough for us to accept the shift or if we should widen the range. We accept the shift if the score is above a score_thresh . This can either be specified through config['stitch']['shift_score_thresh'] or if this is left empty, it is computed for each shift. This computation is done after we have found the best \\(yx\\) shift to be best_shift_yx , as explained by the following pseudocode (also shown with view_stitch_search ): # exhaustive_search_yx: [n_shifts_yx x 2] # best_shift_yx: [2] shifts_yx_use = all shifts between min_dist and max_dist from best_shift_yx in exhaustive_search_yx # [n_shifts_use x 2] shift_yx_thresh = shift in shifts_yx_use with the max score. score_thresh = the score corresponding to shift_yx_thresh multiplied by thresh_multiplier. Where various parameters are specified through the configuration file: min_dist : config['stitch']['shift_score_thresh_min_dist'] max_dist : config['stitch']['shift_score_thresh_max_dist'] thresh_multiplier : config['stitch']['shift_score_thresh_multiplier']","title":"Score Threshold"},{"location":"pipeline/stitch/#view_stitch_search","text":"A good debugging tool to visualise how the best shift was computed is view_stitch_search . In 2D , it shows the score for all \\(yx\\) shifts in the exhaustive search: White in the colorbar refers to the value of score_thresh . The green x indicates the shift that was found after the initial exhaustive search. The black x indicates the final shift found after the refined search . This plot is also useful for understanding the score_thresh computation . The green + indicates the shift_yx_thresh , the shift with the largest score between the two green circles. The inner circle has a radius of config['stitch']['shift_score_thresh_min_dist'] and is centered on the green x . The outer circle has a radius of config['stitch']['shift_score_thresh_max_dist'] and is centered on the green x . score_thresh is then set to config['stitch']['shift_score_thresh_multiplier'] multiplied by the score at the green + . In this case, this multiplier is 2 and thus the score at the green + appears blue ( score at the green + is approximately 150 and so score_thresh and the white in the image is about 300). We use this method of determining score_thresh because the most striking feature of the plot is the sharp gradient near the global maxima in score . Requiring the score for an acceptable shift to be much larger than the max nearby score is just saying we require a large gradient. score_thresh in 3D For the 3D pipeline, the score_thresh is computed in exactly the same way to determine if the initial \\(yx\\) shift found is acceptable or the \\(yx\\) range needs widening : Once an acceptable \\(yx\\) shift has been found, we set score_thresh for the 3D search to the max score at the \\(yx\\) shift used to find score_thresh in 2D across all z planes searched, multiplied by config['stitch']['shift_score_thresh_multiplier'] . I.e. the green + in the Z=0 plot is at the same \\(yx\\) coordinate as the green + in the 2D plot. The maximum score at this \\(yx\\) coordinate across z-shifts between -6 and 6 occurs at Z=0 . Thus, we set the score_thresh to be equal to the score at this \\(yxz\\) shift multiplied by config['stitch']['shift_score_thresh_multiplier'] . The score_thresh is smaller when looking for the \\(yxz\\) shift because we expect in 3D , if neighbouring points are on slightly different z-planes but very close in \\(yx\\) they would have a small contribution to the score . This is because the \\(z\\) pixels are larger than the \\(yx\\) pixels. In 2D , a pair such as this would give a large contribution to the score because only the \\(yx\\) distance would matter. See the example in the refined search section to see how a shift of 1 in \\(z\\) can almost double the score.","title":"view_stitch_search"},{"location":"pipeline/stitch/#widening-range","text":"If the best shift found has score < score_thresh , then exhaustive search will continue until either a shift with score > score_thresh is found or the search range exceeds a max_range : while best_score < score_thresh: extend exhaustive search if range of exhaustive search > max_range in all dimensions: Return current best_shift and best_score. else: Find best_shift and corresponding best_score in new larger exhaustive_search. The max_range in the \\(y\\) , \\(x\\) , \\(z\\) direction is set to be config['stitch']['shift_max_range'] . The exhaustive search is extended using config['stitch']['shift_widen'] . The possible shifts in dimension \\(i\\) are extended by config['stitch']['shift_widen'][i] values either side of the current min/max values of the shifts while maintaining the same spacing. Example exhaustive search extension Lets consider a single dimension with initial an initial exhaustive search containing the following shifts: array ([ - 10 , - 5 , 0 , 5 , 10 ]) With shift_widen = 10 in this dimension, the updated exhaustive search would contain the following shifts: array ([ - 60 , - 55 , - 50 , - 45 , - 40 , - 35 , - 30 , - 25 , - 20 , - 15 , - 10 , - 5 , 0 , 5 , 10 , 15 , 20 , 25 , 30 , 35 , 40 , 45 , 50 , 55 , 60 ]) This new search has a range of 120 so if max_range in this dimension was 100, the updated exhaustive search would not take place. An example in 2D where the widening worked successfully is shown below using view_stitch_search : Pre-widen Post-widen Here we see that before widening the search range, the maximum score is barely distinguishable from the rest and thus falls below score_thresh . After widening though, we uncover a shift with a score far exceeding score_thresh . 3D In 3D , we do the \\(yx\\) search first and then widen according to config['stitch']['shift_widen'][:2] and config['stitch']['shift_max_range'][:2] to find the best \\(yx\\) shift. Then we do the \\(z\\) search keeping the \\(yx\\) shift equal to the best \\(yx\\) shift and widen according to config['stitch']['shift_widen'][2] and config['stitch']['shift_max_range'][2] . This gives us the best \\(yxz\\) shift. The example below shows a case where widening was required in \\(z\\) but not in \\(yx\\) : Pre-widen Post-widen","title":"Widening range"},{"location":"pipeline/stitch/#refined-search","text":"After we have found the best \\(yxz\\) shift from the exhaustive search, we find the final shift by looking in the neighbourhood of this shift with a smaller spacing . Initially, we halve the spacing, then we reduce the spacing to 1. Example Lets consider a 3D example where an exhaustive search with \\(yxz\\) spacing [5, 5, 3] found the best \\(yxz\\) shift to be [-1848, 20, 0] with a score of 247.7. The refined search range in this neighbourhood with half the step size is set to: Y shifts : [ - 1858 - 1855 - 1852 - 1849 - 1846 - 1843 - 1840 - 1837 ] X shifts : [ 10 13 16 19 22 25 28 31 ] Z shifts : [ - 6 - 4 - 2 0 2 4 6 ] This search finds the best \\(yxz\\) shift to be [-1846, 19, 0] with a score of 298.3. The final search with a spacing of 1 is set to: Y shifts : [ - 1849 - 1848 - 1847 - 1846 - 1845 - 1844 - 1843 ] X shifts : [ 16 17 18 19 20 21 22 ] Z shifts : [ - 2 - 1 0 1 2 ] This search finds the best \\(yxz\\) shift to be [-1846, 20, 1] with a score of 423.8. This is why the view_stitch_search plot shows more detail near the black x and why the black x is in a different location from the green x even when no widening is required.","title":"Refined search"},{"location":"pipeline/stitch/#updating-initial-range","text":"We assume that all tiles overlapping in the same direction should have approximately the same shift between them. So, after we have found at least 3 shifts in a given direction which have score > score_thresh , we update our initial exhaustive search range to save time for future tiles. Example The code below shows how update_shifts works to refine the initial search range, given that the following \\(yxz\\) shifts have been found for north/south overlapping tiles (all with score > score_thresh ): [-1846, 20, 1] [-1853, 22, 0] [-1854, 20, 0] Code Output import numpy as np from coppafish.stitch.shift import update_shifts y_shifts_found = [ - 1846 , - 1853 , - 1854 ] x_shifts_found = [ 20 , 22 , 0 ] z_shifts_found = [ 1 , 0 , 0 ] step = [ 5 , 5 , 3 ] y_search = np . arange ( - 1943 , - 1743 + step [ 0 ], step [ 0 ]) x_search = np . arange ( - 100 , 100 + step [ 1 ], step [ 1 ]) z_search = np . arange ( - 3 , 3 + step [ 2 ], step [ 2 ]) # y print ( f \"Initial y search: \\n { y_search } \" ) y_search_new = update_shifts ( y_search , y_shifts_found ) print ( f \"Updated y search: \\n { y_search_new } \" ) # x print ( f \"Initial x search: \\n { x_search } \" ) x_search_new = update_shifts ( x_search , x_shifts_found ) print ( f \"Updated x search: \\n { x_search_new } \" ) # z print ( f \"Initial z search: \\n { z_search } \" ) z_search_new = update_shifts ( z_search , z_shifts_found ) print ( f \"Updated z search: \\n { z_search_new } \" ) # Number of shifts to search print ( f \"Initial number of shifts in search: \" f \" { y_search . size * x_search . size * z_search . size } \" ) print ( f \"Updated number of shifts in search: \" f \" { y_search_new . size * x_search_new . size * z_search_new . size } \" ) Initial y search: [-1943 -1938 -1933 -1928 -1923 -1918 -1913 -1908 -1903 -1898 -1893 -1888 -1883 -1878 -1873 -1868 -1863 -1858 -1853 -1848 -1843 -1838 -1833 -1828 -1823 -1818 -1813 -1808 -1803 -1798 -1793 -1788 -1783 -1778 -1773 -1768 -1763 -1758 -1753 -1748 -1743] Updated y search: [-1861 -1856 -1851 -1846 -1841] Initial x search: [-100 -95 -90 -85 -80 -75 -70 -65 -60 -55 -50 -45 -40 -35 -30 -25 -20 -15 -10 -5 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100] Updated x search: [-1 4 9 14 19 24 29] Initial z search: [-3 0 3] Updated z search: [-3 0 3] Initial number of shifts in search: 5043 Updated number of shifts in search: 105","title":"Updating initial range"},{"location":"pipeline/stitch/#amend-low-score-shifts","text":"After all the shifts between neighbouring tiles have been found, the ones with score < score_thresh are amended. If for a particular pair of overlapping tiles in the north/south direction, the best shift found had a score < score_thresh , the shift and score are saved in the notebook in nb.stitch.south_outlier_shifts and nb.stitch.south_outlier_score respectively. The shift is then re-computed using a new initial exhaustive search range (saved as nb.stitch.south_final_shift_search ). This range is computed using the update_shifts function to centre it on all the shifts found in the south direction for which score > score_thresh . For this re-computation, no widening is allowed either. The idea behind this is that it will force the shift to be within the range we expect based on the successful shifts. I.e. a shift with a slightly lower score but with a shift more similar to the successful shifts is probably more reliable than a shift with a slightly higher score but with a shift significantly different from the successful ones. The new shift and score will be saved in nb.stitch.south_shifts and nb.stitch.south_score respectively.","title":"Amend low score shifts"},{"location":"pipeline/stitch/#global-coordinates","text":"After finding the overlap for the set of neighbouring tile pairs ( \\(\\mathcal{R}\\) ), we are left with a shift vector \\(\\pmb{\\Delta}_{T_1, T_2}\\) for every pair of neighbouring tiles \\(T_1\\) and \\(T_2\\) , that specifies the \\(yxz\\) offsets of tile \\(T_2\\) relative to tile \\(T_1\\) . We define a single global coordinate system by finding the coordinate origin \\(\\pmb{\\mathrm{X}}_T\\) (bottom left corner) for each tile \\(T\\) . Note however that this problem is overdetermined as there are more neighbor pairs than there are tiles. We therefore compute the offsets by minimizing the loss function: \\[ L = \\sum_{(T_1, T_2) \\in \\mathcal{R}} \\pmb{|} \\pmb{\\mathrm{X}}_{T_1} - \\pmb{\\mathrm{X}}_{T_2} - \\pmb{\\Delta}_{T_1, T_2} \\pmb{|}^2 \\] Differentiating this loss function with respect to \\(\\pmb{\\mathrm{X}}_T\\) yields a set of simultaneous linear equations, whose solution yields the origins of each tile on the reference round/channel. This procedure is done with the get_tile_origin function, with the tile origins saved to the Notebook as nb.stitch.tile_origin .","title":"Global coordinates"},{"location":"pipeline/stitch/#error-too-many-bad-shifts","text":"After the call reference spots step, check_shifts_stitch will be run. This will produce a warning for any shift found with score < score_thresh . An error will be raised if the fraction of shifts with score < score_thresh exceeds config['stitch']['n_shifts_error_fraction'] . If this error does occur, it is probably worth looking at the Viewer and the debugging plots to see if the stitching looks good enough to continue with the rest of the pipeline or if it should be re-run with different configuration file parameters (e.g. smaller config['stitch']['shift_step'] or larger config['stitch']['shift_max_range'] ).","title":"Error - too many bad shifts"},{"location":"pipeline/stitch/#saving-stitched-images","text":"After the tile_origin has been computed and the stitch NotebookPage has been added to the Notebook , a stitched image of the ref_round / ref_channel will be saved to the output_dir as a npz file with the file name nb.file_names.big_anchor_image . To save memory, the stitched reference image will be saved as int16 after rescaling to fill the range. We do this because the image is useful for plotting, but we do not care much about the actual pixel values. DAPI If dapi_channel is specified, a stitched image of the anchor_round / dapi_channel will be saved to the output_dir as a npz file with the file name nb.file_names.big_dapi_image . If DAPI tophat filtering was specified , the filtered images save to tile_dir will be loaded in and stitched together. Otherwise, the raw data will be loading in from the input_dir and stitched together with no filtering. I.e. from_raw == True in save_stitched . The stitched DAPI image will be saved as uint16 with no rescaling. Also, to save memory, all pixels with absolute value less than config['stitch']['save_image_zero_thresh'] will have their pixel value set to \\(0\\) before saving.","title":"Saving stitched images"},{"location":"pipeline/stitch/#debugging","text":"There are a few functions using matplotlib which may help to debug this section of the pipeline.","title":"Debugging"},{"location":"pipeline/stitch/#view_stitch_shift_info","text":"The view_stitch_shift_info function plots the shifts found for all neighbouring tiles in a given direction on the same plot (there are 3 plots for each direction). This allows you to see if they are similar, as we expect or if there are some outliers. It also includes a plot of score vs score_thresh for each pair of neighbouring tiles: In this case, all the shifts seem reasonable as the top two plots show quite a small range, and the bottom plot shows score > score_thresh for every shift (blue numbers are all above the green line). If a shift had score < score_thresh , it would be shown in red in each of the three plots for that direction. The numbers refer to the tile with a neighbouring tile to either the north or east of it. This example is for a \\(4\\) ( \\(n_y\\) ) \\(\\times\\) \\(3\\) ( \\(n_x\\) ) grid of tiles, so the number \\(1\\) in the West column of plots refers to the shift found between tile \\(1\\) and tile \\(0\\) . The number \\(3\\) in the South column of plots refers to the shift found between tile \\(3\\) and tile \\(0\\) . This function can also be used to view nb.stitch.outlier_shifts by running view_stitch_shift_info(nb, True) .","title":"view_stitch_shift_info"},{"location":"pipeline/stitch/#view_stitch_overlap","text":"For an experiment with tile \\(0\\) to the north of tile \\(1\\) , view_stitch_overlap(nb, 1, 'north') will always show the global coordinates of the point cloud for tile \\(0\\) in red ( global_yxz = local_yxz + nb.stitch.tile_origin[0] ). There are then buttons to select which point cloud for tile \\(1\\) is plotted in blue: No overlap: This is assuming there is \\(0\\%\\) overlap between the two tiles. \\(x\\%\\) overlap: \\(x\\) here will be config['stitch']['expected_overlap'] . This is our starting guess, i.e. the expected overlap in \\(y\\) and a shift of 0 in \\(x\\) and \\(z\\) . Shift: This is the best shift found, saved in nb.stitch.south_shifts . Final: This is the coordinates of tile \\(1\\) spots in the global coordinate system ( local_yxz + nb.stitch.tile_origin[1] ). An example is shown below: No overlap 10% overlap Shift Final The z-plane is changed by scrolling with the mouse. You can change the value of z-thick in the bottom right. Spots detected on the current z-plane and this many z-planes either side of it will be shown. The white lines (only really visible in the Final plot) indicate neighbouring points with a distance between them of less than or equal to config['stitch']['neighb_dist_thresh'] . The number of matches listed on the right is then the number of these white lines (across all z-planes), this will be similar to the score .","title":"view_stitch_overlap"},{"location":"pipeline/stitch/#view_stitch","text":"Another useful function is view_stitch . This plots all the spots found in the ref_round / ref_channel in the global coordinate system specified by nb.stitch.tile_origin . The example below is for a \\(4\\times3\\) grid of tiles in 2D . Full Zoom The blue spots are duplicate spots (detected on a tile which is not the tile whose centre they are closest to). For each duplicate spot, we expect there is a non-duplicate spot in red, detected on a different tile but with the same global coordinate. We can see this in the Zoom plot, showing the intersection between tile 1 and tile 2 (indicated by a green box in the Full image). These duplicate spots will be removed in the get reference spots step of the pipeline, so we don't double count the same spot. The white lines and number of matches are the same as for view_stitch_overlap . Also in 3D , you can scroll between z-planes with the mouse and specify z-thick in the same way.","title":"view_stitch"},{"location":"pipeline/stitch/#pseudocode","text":"This is the pseudocode outlining the basics of this step of the pipeline . For more detailed pseudocode about how the best shift is found, see the shift section. r_ref = reference round c_ref = reference round spot_yxz[t, r, c] = yxz coordinates for spots detected on tile t, round r, channel c. for t in use_tiles: if tile to north of t: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t_north, r_ref, c_ref]. If 3 or more north/south shifts with score > score_thresh: Update search range around these shifts. if tile to east of t: Find best_shift between spot_yxz[t, r_ref, c_ref] and spot_yxz[t_east, r_ref, c_ref]. If 3 or more east/west shifts with score > score_thresh: Update search range around these shifts. Amend shifts with score < score_thresh using new search range for each direction. Find tile_origin specifying global coordinate system. Add tile_origin and debugging info to stitch NotebookPage. Add stitch NotebookPage to Notebook. Use tile_origin to save stitched ref_channel (and DAPI) image to output directory.","title":"Pseudocode"}]}